{
  "file_path": "..\\test_pdfs\\FULLTEXT01.pdf",
  "file_hash": "04bfd80dc3ea071cb8a0fa5b37a241a8",
  "timestamp": "2025-06-17T20:49:57.258476",
  "page_count": 59,
  "total_words": 11182,
  "total_chars": 79084,
  "fonts_used": [
    "Arial-BoldMT (12.0pt)",
    "Arial-BoldMT (14.0pt)",
    "Arial-BoldMT (18.0pt)",
    "Arial-BoldMT (22.0pt)",
    "ArialMT (10.0pt)",
    "ArialMT (12.0pt)",
    "ArialMT (13.0pt)",
    "ArialMT (14.0pt)",
    "CambriaMath (12.0pt)",
    "CambriaMath (7.0pt)",
    "CambriaMath (8.5pt)",
    "MS-Gothic (11.0pt)",
    "PalatinoLinotype-Bold (12.0pt)",
    "PalatinoLinotype-Italic (12.0pt)",
    "PalatinoLinotype-Roman (11.0pt)",
    "PalatinoLinotype-Roman (12.0pt)",
    "SymbolMT (12.0pt)"
  ],
  "images_count": 17,
  "metadata": {
    "format": "PDF 1.7",
    "title": "Automating Data Extraction from Documents Using Large Language Models",
    "author": "Lucas Persson",
    "creator": "Microsoft® Word för Microsoft 365",
    "producer": "Microsoft® Word för Microsoft 365",
    "creationDate": "D:20240917110746+02'00'",
    "modDate": "D:20240917110746+02'00'"
  },
  "pages": [
    {
      "page_number": 1,
      "text": "Automating Data Extraction from Documents \nUsing Large Language Models \nA Study Exploring How AI Can Be Used to Transform Unstructured \nData into Structured Formats \nLucas Persson \nDocument type – Bachelor Thesis \nMain field of study: Computer engineering \nCredits: 15 \nSemester/year: Spring/2024 \nSupervisor: Martin Kjellqvist \nExaminer: Stefan Forsstöm \nCourse code: DT099G \n",
      "word_count": 50,
      "char_count": 377,
      "fonts": [
        "ArialMT (13.0pt)",
        "Arial-BoldMT (18.0pt)",
        "ArialMT (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 2,
      "text": "i \n \n \nAt Mid Sweden University, it is possible to publish the thesis in full text in DiVA \n(see appendix for publishing conditions). The publication is open access, which \nmeans that the work will be freely available to read and download online. This \nincreases the dissemination and visibility of the degree project. \nOpen access is becoming the norm for disseminating scientific information \nonline. Mid Sweden University recommends both researchers and students to \npublish their work open access. \nI/we allow publishing in full text (free available online, open access): \n☒  \nYes, I/we agree to the terms of publication. \n☐ \nNo, I/we do not accept that my independent work is published in \nthe public interface in DiVA (only archiving in DiVA). \n \nLocation \nand \ndate \nSundsvall 2024-06-16 \n \nProgramme/Course \nBachelor of Science in Engineering, Computer Engineering \n \nName \n(all \nauthors \nnames) \nLucas Persson \n \nYear \nof \nbirth \n(all \nauthors \nyear \nof \nbirth) \n2001-07-27 \n",
      "word_count": 146,
      "char_count": 984,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "MS-Gothic (11.0pt)",
        "PalatinoLinotype-Roman (11.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 3,
      "text": "ii \n \n \nAbstract \nThe objective of this thesis is to accurately extract at least 70% of external \nreferences from one of three test documents and to compare the \nperformance of two Large Language Models (LLMs) using quantitative \nmethods. This is measured by evaluating both the number of identified \nreferences and the amount of these references that are similar to the \nexpected output. The process begins with extracting text from a PDF \ndocument, followed by dividing the text into sentences. Embeddings are \nthen generated for each sentence. Cosine similarity is performed on these \nembeddings to filter out sentences that do not contain the requested data. \nThe remaining sentences are processed using two OpenAI models, gpt-\n3.5-turbo-0125 and gpt-4-turbo-2024-04-09, accessed via their API. Each \nmodel is instructed to extract external references from the filtered \nsentences. The extracted references are then compared against the \nexpected outputs in two ways: by the number of correctly identified \nreferences and by the level of detail in the extracted references. The gpt-\n4-turbo-2024-04-09 model successfully extracted 42 out of 43 references, \nwith 41 being optimal and the remaining missing some information. The \ngpt-3.5-turbo-0125 model extracted 41 out of 43 references, with 31 \nmatching the expected output perfectly. These results demonstrate the \npotential of Large Language Models in accurately extracting data from \nunstructured sources. \nKeywords: Large Language Models, data extraction, unstructured \nsources, embeddings, cosine similarity \n \n \n",
      "word_count": 226,
      "char_count": 1574,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (22.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 4,
      "text": "iii \n \n \nSammanfattning \nMålet med detta examensarbete är att extrahera minst 70 % av externa \nreferenser från ett av tre testdokument och att jämföra resultaten mellan \ntvå stora språkmodeller (LLM) med kvantitativa metoder. Detta mättes \ngenom att utvärdera både antalet korrekt identifierade referenser och \nlikheten hos dessa referenser jämfört med det förväntade resultatet. \nProcessen börjar med att extrahera text från ett PDF-dokument, följt av \natt dela upp texten i meningar. “Embeddings” genereras sedan för varje \nmening. “Cosine similarity” utförs på dessa “embeddings” för att filtrera \nbort meningar som inte innehåller den begärda informationen. De \nåterstående meningarna bearbetas med två OpenAI-modeller, gpt-3.5-\nturbo-0125 och gpt-4-turbo-2024-04-09, som nås via deras API. Varje \nmodell får sedan en detaljerad instruktion att extrahera externa \nreferenser från de filtrerade meningarna. De extraherade referenserna \njämförs sedan mot de förväntade ut datat på två sätt: med antalet korrekt \nidentifierade referenser och med detaljnivån i de extraherade \nreferenserna. \nModellen \ngpt-4-turbo-2024-04-09 \nextraherade \nframgångsrikt 42 av 43 referenser, där 41 var optimala och den \nåterstående saknade viss information. Modellen gpt-3.5-turbo-0125 \nextraherade 41 av 43 referenser, där 31 matchade det förväntade ut datat \nperfekt. Dessa resultat visar potentialen hos stora språkmodeller för att \nextrahera data från ostrukturerade källor med en högre träffsäkerhet. \n \nNyckelord: Stora språkmodeller, dataextrahering, ostrukturerad data, \nembeddings, cosine similarity \n \n \n",
      "word_count": 208,
      "char_count": 1597,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (22.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 5,
      "text": "iv \n \n \nAcknowledgements \nI would like to give a huge thank you to my supervisor Martin Kjellqvist \nat Mid Sweden University and to both my supervisors at Standard \nSolution Group (SSG) for the help and guidance through the whole \nproject. \n",
      "word_count": 39,
      "char_count": 241,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (22.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 6,
      "text": "v \n \n \nTable of Contents \nAbstract ............................................................................................................ ii \nSammanfattning ............................................................................................ iii \nAcknowledgements .......................................................................................iv \nTerminology / Notation .............................................................................. vii \n1 \nIntroduction ............................................................................................ 1 \n1.1 \nBackground and motivation................................................................. 1 \n1.2 \nOverall aim and problem statement ................................................... 2 \n1.3 \nResearch questions/Scientific goals/Verifiable goals ........................ 2 \n1.4 \nScope ........................................................................................................ 2 \n1.5 \nOutline ..................................................................................................... 3 \n2 \nTheory ...................................................................................................... 4 \n2.1 \nt-SNE ........................................................................................................ 4 \n2.2 \nText representation ................................................................................ 4 \n2.2.1 Tokenization and tokens ....................................................................... 4 \n2.2.2 Embeddings............................................................................................. 4 \n2.3 \nLarge Language Models........................................................................ 5 \n2.3.1 Context size ............................................................................................. 6 \n2.3.2 Prompting ................................................................................................ 6 \n2.3.3 Transformer ............................................................................................. 7 \n2.3.4 Scaling laws ............................................................................................. 9 \n2.4 \nCosine similarity .................................................................................... 9 \n2.5 \nAccuracy and F1-Score ........................................................................ 10 \n2.6 \nRelated work ......................................................................................... 11 \n2.6.1 Extracting Financial Data from Unstructured Sources: Leveraging \nLarge Language Models................................................................................ 11 \n3 \nMethodology ........................................................................................ 13 \n3.1 \nScientific method description ............................................................. 13 \n3.2 \nProject method description................................................................. 14 \n3.2.1 Phase 1: Pre-study ................................................................................ 14 \n3.2.2 Phase 2: Extract text ............................................................................. 14 \n3.2.3 Phase 3: Divide text .............................................................................. 14 \n3.2.4 Phase 4: Generate embeddings ........................................................... 15 \n3.2.5 Phase 5: Similarity search .................................................................... 15 \n3.2.6 Phase 6: LLM ......................................................................................... 15 \n3.2.7 Phase 7: Gather results......................................................................... 16 \n3.2.8 Phase 8: Evaluation and discussions ................................................. 16 \n4 \nImplementation ................................................................................... 17 \n",
      "word_count": 195,
      "char_count": 4002,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (22.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 7,
      "text": "vi \n \n \n4.1 \nExtract text ............................................................................................ 18 \n4.2 \nDivide text ............................................................................................. 18 \n4.3 \nGenerate embeddings.......................................................................... 19 \n4.4 \nCosine similarity .................................................................................. 19 \n4.5 \nLarge Language Model ....................................................................... 19 \n4.6 \nEvaluation setup .................................................................................. 20 \n5 \nResults ................................................................................................... 22 \n5.1 \nGenerated embeddings ....................................................................... 22 \n5.2 \nCosine similarity .................................................................................. 24 \n5.3 \nLarge Language Model ....................................................................... 28 \n5.3.1 First document ...................................................................................... 29 \n5.3.2 Second document ................................................................................. 34 \n5.3.3 Third document .................................................................................... 37 \n6 \nDiscussion ............................................................................................. 40 \n6.1 \nAnalysis and discussion of results .................................................... 40 \n6.2 \nWork method discussion .................................................................... 41 \n6.3 \nScientific discussion ............................................................................. 43 \n6.4 \nConsequence analysis.......................................................................... 44 \n6.5 \nEthical and societal discussion ........................................................... 44 \n7 \nConclusions .......................................................................................... 46 \n7.1 \nOptimize this project ........................................................................... 47 \n7.2 \nExplore open-source models .............................................................. 47 \n7.3 \nNatural language preprocess ............................................................. 47 \nReferences ...................................................................................................... 48 \nAppendix A: Source Code ........................................................................... 51 \n \n \n",
      "word_count": 131,
      "char_count": 2686,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 8,
      "text": "vii \n \n \nTerminology / Notation \n \nAcronyms/Abbreviations \nAI \n \nArtificial Intelligence \nLLM \n \nLarge language model \nt-SNE \n \nt-distributed stochastic neighbor embedding\n",
      "word_count": 17,
      "char_count": 172,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (22.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 9,
      "text": "1 \n1 \nIntroduction \nThis thesis investigates the use of Artificial Intelligence (AI) for extracting \ninformation from unstructured data, a significant challenge given the \nexponential growth of data across various formats. The aim is to evaluate \nand compare different Large Language Models (LLM) to identify \nefficient, accurate, and scalable solutions for data extraction. This work \nis proposed and supported by the company Standard Solution Group \n(SSG). \nThe aim of this chapter is to provide an overview of the background, \nproblem motivation, and the overall purpose and context of this study. \n1.1 \nBackground and motivation \nIn the rapidly evolving digital landscape, the need for sophisticated \nartificial intelligence (AI) systems has never been greater. This big shift \nisn't just because AI can take over everyday tasks. It's more about how \nAI can greatly improve things and speed up processes. As companies \nand entire industries rush to make the most of AI, there's a growing need \nfor AI-powered tools that can analyze and extract valuable information \nfrom large piles of unorganized data. \nAI-powered tools are essential because of the challenges associated with \nhandling unstructured data, which is characterized by its complexity and \nvariability in structure. Since traditional data processing systems are \ntypically designed to handle structured data that follow a specific format \nand are easy to categorize, it causes them to struggle with processing the \ncomplexity and diversity of unstructured data.  \nIn this thesis will the AI-powered tool Large Language Models (LLM) be \nutilized. A Large Language Model (LLM) is an advanced AI system \ndesigned to understand and generate human-like text by learning from \na large database of existing content. These models, which include well-\nknown examples like OpenAIs GPT series and Googles Gemini, use \ncomplex algorithms to process and produce language in a way that \nmimics human conversations. Capable of tasks such as translation, \nsummarization, and even creative writing, LLMs are increasingly \nintegral to various applications, from virtual assistants to content \ngeneration. [1] \n",
      "word_count": 320,
      "char_count": 2159,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "Arial-BoldMT (22.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 10,
      "text": "2 \n1.2 \nOverall aim and problem statement \nThe specific problem investigated in this thesis is: How can LLMs be \nutilized to accurately convert unstructured datasets into structured \ndatasets? This involves detailed exploration into methods of text \nhandling and embeddings. The solution to this problem will be \nevaluated by how accurate LLMs can transform unstructured data into \nstructured datasets including both quantity and quality. The higher \nreason for this work is the following: \n• Unstructured data poses challenges in data analysis, limiting \norganizations' ability to derive actionable insights. \n• Current data extraction methods may not efficiently convert \nunstructured data into usable formats. \n• There is a need to explore AI-driven solutions for transforming \nunstructured data into structured datasets effectively and \naccurately. \n1.3 \nResearch questions/Scientific goals/Verifiable goals \nThe objectives of this thesis are: \n1. Successfully extract at least 70% correct external reference from \none of the documents. \n2. Compare the performance of two Large Language Models (LLMs). \nThe research questions to be answered are the following: \n1. How does the accuracy differentiate between the models when \ncomparing the amount of identified external references? \n2. How does the quality differentiate between the models when \ncomparing the identified external references? \n1.4 \nScope \nThis thesis has been delimited in some areas, the chosen document to test \non are standards produced by Standard Solution Group (SSG), the task \nhas been to extract external references, and the documents are only in \nswedish. This has been tested on two different LLMs with the same \ninstruction to extract external references. \n",
      "word_count": 251,
      "char_count": 1737,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "ArialMT (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "SymbolMT (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 11,
      "text": "3 \n1.5 \nOutline \nThis report begins with Chapter 2, which sets the theoretical groundwork \nnecessary for the thesis. Chapter 3 then details the scientific \nmethodologies and evaluation techniques used. Chapter 4 which covers \nthe actual implementation itself. Chapter 5 presents the results of the \nstudy, while Chapter 6 engages in a discussion about these findings. The \nreport concludes with Chapter 7, which synthesizes the conclusions \ndrawn from the research. \n",
      "word_count": 69,
      "char_count": 467,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 12,
      "text": "4 \n2 \nTheory  \nThis chapter presents relevant background theory for a better \nunderstanding of this thesis.  \n2.1 \nt-SNE \nt-SNE (t-distributed Stochastic Neighbor Embedding) is a technique \ndesigned to visualize high-dimensional data by mapping each data point \nto a two or three-dimensional space. This method is an enhanced version \nof the original Stochastic Neighbor Embedding, offering improved \nvisualization capabilities. [2] \n2.2 \nText representation \nIn this thesis, text representation is addressed through two fundamental \ntechniques: Tokenization and Word Embeddings. These methods are \nessential for processing and understanding text within Large Language \nModels (LLMs). Tokenization serves as the initial step in deconstructing \ntext into manageable pieces, while word embeddings provide a way to \ntranslate these pieces into numerical forms. \n2.2.1 \nTokenization and tokens \nTokenization is the process of turning text into a list of \"tokens,\" which \ncan be characters, parts of words or whole words. The purpose of \ntokenization in natural language processing is to break down text into \nthese manageable units, enabling machine learning models to process \nand analyze the text numerically. This crucial step transforms raw text \ninto a structured form that algorithms can understand and manipulate, \nenabling effective data analysis and interpretation. [3] \n2.2.2 \nEmbeddings \nEmbeddings are a technique to convert text into numerical values to \nrepresent words, phrases, or entire documents as vectors in a vector \nspace. This transformation captures not just the identity of textual \nelements but also their semantic and syntactic relationships, allowing \nsimilar words to have similar numerical representations. The purpose of \nembeddings is to reduce the high dimensionality of text data into a more \nmanageable form, facilitating more effective computation and enabling \nmachine learning models to perform complex tasks. [4] \n",
      "word_count": 275,
      "char_count": 1949,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "Arial-BoldMT (22.0pt)",
        "Arial-BoldMT (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 13,
      "text": "5 \nFigure 2-1: Example showing how words relate to others in the \nnumerical space [4]. \nAs shown in figure 2-1, the embedding dimensions have been reduced \nto two dimensions using a t-SNE model for clearer visualization. In this \nvisual representation, words are colored as blue, red, and green, with \neach color forming clusters. This clustering indicates that the \nembeddings capture and group words with similar meanings or \nsentiments closely together. For instance, positive words like 'good', \n'amazing', and 'wonderful' are clustered in one area, while negative \nwords such as 'bad', 'worse', and 'worst' are grouped in another area. This \nseparation and clustering highlight how embeddings, through t-SNE, \nenable the visualization of complex semantic relationships within the \ndata. \nTwo types of embedding methods are static embeddings and contextual \nembeddings. In static embedding, is each word assigned a fixed vector, \nmeaning that no matter the context in which the word appears, it will \nalways have the same embedding. This contrasts with contextual \nembeddings, where the embedding for a word can vary depending on its \ncontext within a sentence. Therefore, the same word may have different \nembeddings based on the surrounding words and the overall semantic \nmeaning of the sentence. [4] \n2.3 \nLarge Language Models \nLarge Language Models (LLMs) such as Chat-GPT, PaLM and LLaMA \nare advanced transformer-based models characterized by their wide \nscale, typically containing hundreds of billions of parameters. These \nmodels are trained on a huge amount of text datasets, enabling them to \ndevelop a profound understanding of natural language and the ability to \nperform complex text generation tasks. [5] \n",
      "word_count": 259,
      "char_count": 1727,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 869,
          "height": 311,
          "ext": "png",
          "size_bytes": 49430
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 14,
      "text": "6 \n2.3.1 \nContext size \nContext size, also known as context window or length when referring to \nLLMs, is the maximum number of tokens the model can handle at one \ntime. This limitation includes both the input tokens it receives and the \noutput tokens it generates. The context size is crucial because it \ndetermines how much information the model can process when making \npredictions or generating text. \nA larger context size can lead to a better understanding of the input and \nenable the generation of more descriptive and contextual outputs. This is \nparticularly important for tasks that require a deep understanding of \nlong texts, such as summarizing lengthy documents or answering \ncomplex questions. [6] \n2.3.2 \nPrompting \nPrompting is a crucial technique used for interacting with Large \nLanguage Models (LLMs), where users provide a \"prompt\", an input \nstring that contains instructions or tasks for the LLM to process. Common \nstrategies include: \nZero-Shot Prompting: LLMs utilize their training data to respond to new \nqueries without needing specific examples within the prompt. \nIn-Context Learning (Few-Shot Learning): This approach involves \npresenting the LLM with multiple input-output pairs, which helps guide \nthe model to generate responses by mimicking these examples. \nChain-of-Thought (CoT): Prompts under this strategy contain detailed \nreasoning steps, assisting the LLM in articulating a step-by-step \nreasoning process for more complex problem-solving. \nMulti-Turn Instructions: This technique engages LLMs in an ongoing \ndialogue, allowing each response to influence subsequent prompts. It is \nparticularly effective in interactive applications such as chatbots or \ncomplex task navigation. \nThese prompting strategies enhance the LLMs ability to generate \naccurate and contextually relevant responses across a variety of tasks. [6] \n \n \n",
      "word_count": 268,
      "char_count": 1869,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 15,
      "text": "7 \n2.3.3 \nTransformer \nTransformers are typically the architecture referring to when discussing \nLLMs. Transformers use positional encodings to maintain sequence \norder and stacked self-attention layers to understand context and play a \ncrucial role for LLMs. The following sections will describe the structure \nof the transformer shown in figure 2-2. [7] \nFirst is the input query and input embedding. The input query is the \ninstructions the user has written for the LLM but as tokens, and the input \nembedding is the embedded versions of these tokens, transforming them \ninto numerical vectors that the model can process. \nNext is the positional encoding. This step creates an understanding of the \norder in the input, which is crucial because the model processes tokens \nin parallel, meaning that the order of tokens is not naturally preserved in \nthe process. These positional encodings are typically calculated using \nsine and cosine mathematical functions. Each position in the sentence \nreceives unique sine and cosine values based on its sequence by the \nfollowing method.  \n𝑃𝐸 (𝑝𝑜𝑠,  2𝑖)  =   sin(𝑝𝑜𝑠  ÷  10 0002𝑖÷𝑑) (2.1) \n𝑃𝐸 (𝑝𝑜𝑠,  2𝑖  +  1)  =   cos(𝑝𝑜𝑠  ÷  10 0002𝑖÷𝑑) (2.2) \nWhere pos is the position in the sentence, i is the dimension and d are the \nnumber of dimensions, which are the same amount as the input \nembeddings. For more information about this, see source. These values \nare then combined with the word embeddings, providing context about \nword positions that the transformer uses to understand the text better. \nThis method ensures that despite processing words in parallel, the model \nretains information about the order of words. [7] \nNext step is the encoder, following the input stage, consisting of N \nidentical layers (number of iterations). Each layer is equipped with two \nsub-layers, a multi-head self-attention mechanism and a position-wise \nfully connected feed-forward network. The multi-head self-attention \nmechanism's purpose is to receive information about every token in the \ngiven input, based on the positions and the relations to the other tokens \nin the input. Meanwhile, the feed-forward network enhances the token \nrepresentations refined by the attention mechanism, applying \n",
      "word_count": 341,
      "char_count": 2230,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (12.0pt)",
        "CambriaMath (8.5pt)",
        "PalatinoLinotype-Italic (12.0pt)",
        "CambriaMath (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 16,
      "text": "8 \ntransformations that integrate learned contextual information into each \ntoken's embedding. [7] \n \nFigure 2-2: Image containing the structure of a transformer. This \nimage has been slightly modified to clarify the components by \nadding “Encoder” and “Decoder”. [7] \n \nSimilar to the encoder, the decoder contains N layers (number of \niterations), each mirroring the encoder's structure with an additional \nsub-layer. The initial sub-layer in each decoder layer features a masked \nself-attention mechanism, which ensures that the generation of each \n",
      "word_count": 77,
      "char_count": 552,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 499,
          "height": 571,
          "ext": "jpeg",
          "size_bytes": 36405
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 17,
      "text": "9 \ntoken is influenced only by preceding tokens, thereby preserving the \nautoregressive property of the decoder. Following this, the encoder-\ndecoder attention layer integrates the context from the entire input \nsequence by focusing on the most relevant parts of the encoded \nembeddings. This layer crucially aligns the decoder's output with the \nencoder's input, facilitating accurate and context-aware token generation. \nFinally, the feed-forward network in the decoder operates similarly to \nthat in the encoder, further processing each token's data to produce the \nfinal output sequence. [7] \nThe final step of the transformer model's process is the output generation, \nwhich begins with a linear layer. This layer takes the output embeddings \nfrom the decoder, which represent the current token's features informed \nby both the encoding and decoding phases. The linear layer transforms \nthese embeddings into a vector whose dimension matches the size of the \nmodel’s vocabulary, producing a set of scores for each possible token. \nThese scores are then passed to a SoftMax layer, which converts them \ninto probabilities, reflecting the likelihood of each token being the next \nin the sequence. Based on these probabilities, the model selects the most \nappropriate token using its selection strategy. This process repeats for \neach token until the model generates an end-of-sequence token, signaling \nthe completion of the output. [7] \n2.3.4 \nScaling laws \nScaling laws are an important component of the optimization process for \nLLMs. They enable researchers and developers to predict the potential \nperformance of these models based on various input parameters. By \nsystematically analyzing how changes in model size, data volume, and \ncomputational resources affect model outcomes. Scaling laws provide a \npredictive framework that can guide strategic decision-making during \nthe development and training phases. [5] \n2.4 \nCosine similarity \nCosine similarity is used to measure the similarity by calculating the \ncosine angle between two vectors, the vectors need to have the same \nnumber of dimensions. This is done by calculating the dot product of the \nvectors. \n𝑎  ⋅  𝑏  =  |𝑎| |𝑏| 𝑐𝑜𝑠𝜃 (2.3) \n",
      "word_count": 329,
      "char_count": 2206,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "CambriaMath (12.0pt)",
        "Arial-BoldMT (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 18,
      "text": "10 \n𝑐𝑜𝑠𝜃  =  \n𝑎  ⋅  𝑏\n||𝑎|| ||𝑏|| (2.4) \nThe cosine of the angle θ between the vectors can then be calculated by \ndividing the dot product by the product of the magnitudes. This result, \nwhich ranges from -1 to 1, indicates how similar the two vectors are. A \nvalue of 1 means the vectors are perfectly aligned (the angle is 0 degrees), \nindicating maximum similarity. A value of 0 indicates that the angle is 90 \ndegrees, and a value of -1 indicates that the vectors are diametrically \nopposed (the angle is 180 degrees). [8] \n2.5 \nAccuracy and F1-Score \nF1-Score is a calculation that utilizes precision and recall. Precision \nmeasures the accuracy for the positive predictions. This is done by \ncounting how many times the positive prediction was correct and \nincorrect and then dividing the correct amount by the sum of the correct \nand incorrect. [9] \nPrecision  =  \n𝑇𝑃\n𝑇𝑃 + 𝐹𝑃  (2.5) \nAnd recall is used to measure how well the model can identify the \npositive cases. This is calculated by dividing the true positive (correct \npositive) by the sum of true positive and false negative (unidentified \ncorrect). [9] \nRecall  =  \n𝑇𝑃\n𝑇𝑃 + 𝐹𝑁  (2.6) \nThe F1 Score combines the precision and the recall to a single value by \neither dividing 2 by the sum of 1 divided by the precision and 1 divided \nby \nthe \nrecall. \nCan also be calculated by dividing 2 multiplied by the precision and \nrecall with the sum of precision and recall [10]. \nF1 Score  =  \n2\n1\n𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + \n1\n𝑟𝑒𝑐𝑎𝑙𝑙\n (2.7) \nF1 Score  =  \n2 ⋅ 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ⋅ 𝑟𝑒𝑐𝑎𝑙𝑙\n𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑟𝑒𝑐𝑎𝑙𝑙  (2.8) \nThis will produce a 2x2 metric that represents the true positive (TP), false \npositive (FP), false negative (FN) and true negative (TN). Where true \nmeans that the model identified is correct and false that it is incorrect. \n",
      "word_count": 309,
      "char_count": 1774,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "CambriaMath (7.0pt)",
        "Arial-BoldMT (14.0pt)",
        "CambriaMath (8.5pt)",
        "PalatinoLinotype-Bold (12.0pt)",
        "CambriaMath (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 19,
      "text": "11 \nThe first row in the metric will represent the predicted positive where \nthe first column is the amount of correctly and the second column is the \namount of incorrect. Same with the second row that includes the negative, \nthe first column is the correct negative and the second column is the \nincorrect negative. [9] \n \n \nFigure 2-3: A 2x2 matrix showing how the F1 score will be presented. \n \n2.6 \nRelated work \nThe following subsections present related work and their similarities \nwith this thesis. \n2.6.1 \nExtracting Financial Data from Unstructured Sources: Leveraging \nLarge Language Models \nThe study conducted by Huaxia Li, Haoyun Gao, Chengzhang Wu, and \nMiklos A. Vasarhelyi addresses the significant challenge of extracting \nfinancial data from unstructured sources. Utilizing LLMs, their research \nintroduces a framework designed specifically for automated data \nextraction from PDF-formatted documents. This framework employs a \ncombination of text mining and prompt engineering techniques. \nThe developed framework was tested on governmental annual reports \nand corporate ESG reports, showcasing its effectiveness in parsing \ncomplex financial documents. The framework demonstrated a high \n",
      "word_count": 173,
      "char_count": 1208,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "PalatinoLinotype-Bold (12.0pt)",
        "Arial-BoldMT (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 444,
          "height": 427,
          "ext": "png",
          "size_bytes": 12841
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 20,
      "text": "12 \naccuracy rate, achieving an average of 99.5% in initial tests and \nmaintaining an overall accuracy of around 96% in subsequent larger-\nscale tests. [11] \nBoth this study and the current thesis work with PDF files and aim to \nharness the capabilities of large language models to extract data from \nunstructured sources, highlighting a shared objective in advancing data \nextraction methodologies using AI technologies. \n \n \n",
      "word_count": 64,
      "char_count": 427,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 21,
      "text": "13 \n3 \nMethodology \nThis chapter presents the methodology used in this thesis including the \nscientific / project method descriptions and evaluation method. \n3.1 \nScientific method description \nIn this thesis, quantitative methods will be employed to systematically \ninvestigate and analyze the performance. Quantitative research is ideal \nfor this study as it allows for precise measurement for the number of \nidentified references and to compare the output to the expected output. \nTo further address the research problem, a mix of comparative and \nexperimental research designs will be utilized alongside quantitative \nmethods. \nThis thesis builds upon the research “Efficient Automated Processing of \nthe Unstructured Documents Using Artificial Intelligence: A Systematic \nLiterature Review and Future Directions” (2021), which highlights the \nsignificant potential of AI-based approaches for extracting useful \ninformation from unstructured documents. Their systematic literature \nreview identifies key challenges and gaps in the existing techniques, \nparticularly in handling complex document layouts and the need for \nhigh-quality datasets. [12] \nThe first research question is how the accuracy differs between the \nmodels when comparing the number of identified references. To answer \nthis needs all the implementation steps to have been implemented: \nStep 1 Extracting and dividing text: This includes processes like \nextracting the text from the PDF and dividing the text into sections. \nStep 2 Generate embeddings: This step generates embeddings for each \nsection. \nStep 3 Cosine similarity: This step performs cosine similarity on the \nembedded sections to identify and extract sections that could contain the \nexternal references. \nStep 4 Large Language Model: The extracted sections from the cosine \nsimilarity are then sent into a LLM to extract the external references as a \nstructured dataset. \n",
      "word_count": 269,
      "char_count": 1912,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "Arial-BoldMT (22.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 22,
      "text": "14 \nWhen all the implementation steps are complete, they will be tested with \nthree different documents. These extracted datasets are then compared \nbetween the models to evaluate the differences. \nThe second research question, similar to the first one, needs the \nimplementation steps to be complete. This question compares the quality \nof the identified references between the models by setting either “partial \nmatch” or “exact match” on a correctly identified reference. This \nmeasures the number of identified references that contain the expected \ninformation. \n3.2 \nProject method description \nThis thesis is divided into eight phases, the initial phase is a pre-study \nand consists of method discovery and planning. This is followed by \nphases 2 through 6 and are the implementation phases. And last phase 7 \nand 8 which gather the data for the results and discuss the result. \n3.2.1 \nPhase 1: Pre-study \nThe initial phase of this thesis, the pre-study, includes method discovery \nand planning. This step is fundamental to the entire research project as it \nestablishes the theoretical and practical work that will be applied. This \nwas done by a comprehensive review of existing methods in the field of \nartificial intelligence, specifically focusing on Large Language Models \n(LLMs) and their applications in data extraction. \n3.2.2 \nPhase 2: Extract text \nThe second phase signifies the onset of the practical implementation, \nfocusing on utilizing libraries to extract text from PDF documents. This \nphase involves selecting the appropriate tools that reliably convert PDF \ncontent into manageable text while preserving the original formatting to \nthe greatest extent possible. \n3.2.3 \nPhase 3: Divide text \nAfter the text is extracted, it must be divided into segments and every \nsegment will later be embedded. The division strategy could be based on \nnatural language cues such as chapters, headings, paragraphs, sentences \nor just a fixed size of characters. This segmentation is critical as it forms \nthe basic units of text that will be compared in the similarity search. \n \n",
      "word_count": 319,
      "char_count": 2092,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "Arial-BoldMT (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 23,
      "text": "15 \n3.2.4 \nPhase 4: Generate embeddings \nThis phase involves the embedding of text segments obtained from the \nprevious step. During generation of embeddings, is each text segment \ntransformed into embeddings, also known as vectors. These embeddings \nare essentially lists of numbers, ranging between -1 and 1, where each \nnumber represents a dimension of the text's features captured by the \nmodel. \nThe dimensionality of these embeddings, essentially the number of \nnumbers in each embedding, is determined by the selected model. \nHigher-dimensional embeddings can capture more detailed information \nabout the text but at the cost of increased computational complexity. \nEach embedding essentially serves as a numerical fingerprint of a text \nsegment, taking semantics and possible context if the model is trained for \nit in account. \nProperly executed embeddings is critical as it converts qualitative text \ndata into a quantitative form that can be systematically compared and \nanalyzed in the next phase of similarity search. This transformation is \nwhat allows the algorithms of the next steps to perform comparisons \nbetween different text segments. \n3.2.5 \nPhase 5: Similarity search \nPhase 5 focuses on the similarity search, a critical step in uncovering \nrelationships and patterns in the dataset. In this phase, the embeddings \ngenerated are compared using similarity metrics to determine how \nclosely the segments of text are related in terms of their semantic or \ncontextual content. \nThis process involves algorithms that can efficiently handle and process \nlarge matrices of embeddings to compute the similarity scores. \nThe results from this search can be used to cluster segments that share \nsimilarities. \n3.2.6 \nPhase 6: LLM \nPhase 6 represents the end of the implementation process, where the \nsegments identified as similar or relevant by the similarity search are \nfurther analyzed using an LLM. \n",
      "word_count": 286,
      "char_count": 1920,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 24,
      "text": "16 \nThe process begins by inputting the related text segments into the LLM. \nDepending on the requirements, the LLM can be utilized for various \ntasks such as extracting data and relationships, summarizing \ninformation, answering questions based on the text, or even generating \nnew text based on the learned context. But as mentioned before, this \nthesis will focus on extracting data. \n3.2.7 \nPhase 7: Gather results \nUpon completion of the implementation steps, the results from the \nembeddings, similarity search and LLMs are collected. During the \nembedding and similarity search phases, the vectors are split into \ncategories to visually be able to see how the vectors are placed. In the \nLLM phase, the outputs generated by the LLMs are listed. To visualize \nthe data, all vectors are reduced to two dimensions using a t-SNE model \nand plotted within the context of their respective documents. The \noutputs from the LLMs are compared against the expected outputs, and \nthe precision, recall, and F1 scores are calculated to measure the \nperformance and accuracy of the LLMs in identifying references \ncorrectly. \n3.2.8 \nPhase 8: Evaluation and discussions \nAfter the results have been gathered from the documents and the LLMs, \nthey are compared between the different documents and a discussion \nabout the results and the whole process is made. \n",
      "word_count": 212,
      "char_count": 1353,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 25,
      "text": "17 \n4 \nImplementation \nFigure 4-1 provides an overview of the entire process, beginning with \ntext extraction from a PDF. The extracted text is divided into sections, \nwhich are then converted into embeddings. These embeddings represent \nboth the text itself and the relationships within the text as numerical \nvalues. A similarity search is then conducted on these embeddings, using \na transformed input. Texts that align with the search input are extracted \nand fed into a LLM, which is tasked with specific functions such as \nextracting external references or charts. The final output is a list of the \ndata identified by the LLM. \n \nFigure 4-1: An overview of the entire process. \nFigure 4-2 Also shows an overview of the entire process, but this figure \nshows how the data flows in the process. It starts with the entire text from \nthe PDF, the text is then divided into sections. Embeddings are then \ngenerated for each section. A cosine similarity search is then performed \non each section with a set input transformed into embeddings to identify \nwhich of the sections that could contain the requested information. The \ntexts of the embeddings that matched in the cosine similarity search are \nthen sent to a LLM which is instructed to extract the data. For the \ncomplete source code, see Appendix A. \n",
      "word_count": 215,
      "char_count": 1310,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (22.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1169,
          "height": 172,
          "ext": "jpeg",
          "size_bytes": 25751
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 26,
      "text": "18 \nFigure 4-2: An overview of the entire process, showing how the data \nis processed in every step. \n4.1 \nExtract text \nThe initial step involves extracting text from a PDF file using the PyPDF2 \nlibrary [13]. This will take all the text from the PDF into one string. \n4.2 \nDivide text \nOnce the text is extracted, it is divided into sections based on sentences, \nutilizing the NLTK library [14]. \n \n \n",
      "word_count": 69,
      "char_count": 403,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "ArialMT (14.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 718,
          "height": 781,
          "ext": "png",
          "size_bytes": 48165
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 27,
      "text": "19 \n4.3 \nGenerate embeddings \nThe next step involves generating embeddings for each section using \nOpenAI's \"text-embedding-3-small\" model which produces embeddings \nwith 1536 dimensions. To use OpenAI's models, users must have credits \nin their account and initialize an API key within the code. [15] [16] \n4.4 \nCosine similarity \nThis stage involves performing cosine similarity analysis between a \nsearch input (once transformed into embeddings) and the embeddings \ngenerated in the previous step. Embeddings that result in a similarity \nscore above the chosen threshold (45%) are selected for further analysis. \nThis was done with sklearn's cosine similarity function. [17] \n4.5 \nLarge Language Model \nIn the final step, the selected sentences are inputted into an LLM. The \nLLM is tasked with extracting specific data from these sentences based \non given instructions. The prompt given to the LLMs to extract external \nreferences from standard was the following, it is in Swedish since the \ntargeted documents are in Swedish: \n“Du är en hjälpsam assistent som har i uppdrag att hitta externa \nreferenser från meningar. Dessa externa referenser är formaterade som \nsifferkod eller standarder som vanligtvis erkänns i tekniska och \nvetenskapliga dokument, till exempel 'SS', 'EN' eller 'ISO' eller en \nkombination av dem, men det finns även fler. Så din uppgift är följande: \nhitta alla externa referenser. Det kan vara så att det också finns vilken del \nav standarden som den refererar till och kan göras på två sätt, ena är \ngenom ett '-x' där 'x' är en siffra, och det andra är genom att skriva 'del x' \ndär 'x' är en siffra. Om du hittar en referens som skrivs med 'del' sättet så \nomvandla det till '-x' och inkludera det i referensen. Det kan också vara \nså att referensen innehåller årtal på följande sätt ':xxxx' där 'xxxx' är ett \nårtal. Inkludera detta i referensen om det finns. Sen ta bort referenser till \nallmänna publikationer, företagsinterna dokument från 'SSG' eller några \nhyperlänkar. Nästa steg är att ta bort referenser som du har hittat flera \ngånger, detta inkluderar då hittade referenser som har samma kod men \ndel nummer och årtal får vara olika, ta bort duplikat genom att exkludera \nde referenser som innehåller minst information. Svara enbart med en \nlista på de hittade externa referenserna, om det skulle vara så att du inte \nhittade någon svara med ‘ ’ ” \n",
      "word_count": 382,
      "char_count": 2392,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "ArialMT (14.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 28,
      "text": "20 \nBut here is the English version: “You are a helpful assistant tasked with \nfinding external references from sentences. These external references are \nformatted as numeric codes or standards commonly recognized in \ntechnical and scientific documents, such as 'SS', 'EN' or 'ISO' or a \ncombination of them, but there are more. So your task is the following: \nfind all external references. It may also include what part of the standard \nit refers to and can be done in two ways, one is through a '-x' where 'x' is \na number, and the other is by writing 'part x' where 'x' is a number. If you \nfind a reference written in the 'part' way, convert it to '-x' and include it \nin the reference. It may also be that the reference contains a year in the \nfollowing way ':xxxx' where 'xxxx' is a year. Include this in the reference \nif available. Then remove references to public publications, internal \ncompany documents from 'SSG' or any hyperlinks. The next step is to \nremove references that you have found multiple times, this includes \nreferences found that have the same code but the part number and year \nmay be different, remove duplicates by excluding the references that \ncontain the least information. Reply only with a list of the external \nreferences found, if you did not find any reply with ’ ’ ”. \nThe chosen LLMs were gpt-4-turbo-2024-04-09, this model has a context \nwindow of 128,000 tokens and is trained on data up to December 2023. \nAnd gpt-3.5-turbo-0125 which has a context size of 16,385 tokens. Both \nalso used the configuration temperature 0.2 and top_p 0.1. A lower \ntemperature configuration reduces the randomness when given an \noutput and top_p reduces the number of tokens taken into consideration \nwhen producing the output [18]. That is important since the task is to \nextract references and only focus on that. [19] \n4.6 \nEvaluation setup \nTo evaluate the generated embeddings a t-SNE model will be used to \nreduce the number of dimensions. This enables the embeddings to be \nviewed in a two-dimensional diagram to see how the sentences that \ncontain the external references are placed compared to the sentences \nwithout references. The cosine similarity will also use this kind of \nvisualization of the embeddings, but this diagram will also show what \nsentences that have been matched with the search input and where the \ninput embeddings have been placed on the diagram. And the LLMs will \nthen be evaluated by comparing the output to the correct answers, this is \ndone by measuring the F1 score by counting the correct (TP), incorrect \n",
      "word_count": 430,
      "char_count": 2569,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "ArialMT (14.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 29,
      "text": "21 \n(FP), missed (FN) and number of tokens (TN). And to evaluate the \nquality of the output, each identified reference will get a tag either \n“correct” or “partially correct”. This will be used to measure what \npercentage of the identified references that is correct or partially correct. \n \n \n",
      "word_count": 47,
      "char_count": 294,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 30,
      "text": "22 \n5 \nResults \nThis chapter outlines the results obtained in this thesis, illustrated \nthrough various diagrams that represent the embeddings generated for \neach document, the results of the cosine similarity search, and the final \noutputs from the LLMs. \n5.1 \nGenerated embeddings \nThe following diagrams represent the embeddings generated on each \nsentence in its document, to reduce the dimensions a t-SNE model was \nused. The blue dots are sentences consisting of at least one external \nreference while the red text does not contain any external reference.  \nFigure 5-1 shows the generated embeddings for the first document. This \ndiagram displays a total of 458 data points, of which 70 contain external \nreferences and 388 do not. \n \nFigure 5-1: A visualization of the embeddings for the first document. \n \nThe next figure, figure 5-2 represents the generated embeddings on the \nsecond document. Presented in this figure are the embeddings for 313 \nsentences, with 29 featuring external references and 284 lacking them. \n",
      "word_count": 157,
      "char_count": 1028,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "Arial-BoldMT (22.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 802,
          "height": 490,
          "ext": "png",
          "size_bytes": 40558
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 31,
      "text": "23 \n \nFigure 5-2: A visualization of the embeddings for the second \ndocument. \n \nThe last diagram of this subchapter, figure 5-3. This diagram contains 369 \ndata points. Where, 30 sentences include external references and 339 \nsentences do not. \n \n",
      "word_count": 37,
      "char_count": 248,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 674,
          "height": 440,
          "ext": "png",
          "size_bytes": 29514
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 32,
      "text": "24 \nFigure 5-3: A visualization of the embeddings for the third document. \n \n5.2 \nCosine similarity  \nThe diagrams shown in this chapter are based on the embeddings shown \nin the previous chapter but include one additional data point. These \ndiagrams have also been made with the help of a t-SNE model. The \nsimilarity search consists of an input “Enligt standard SS EN ISO 5555” \nin English “according to standard SS EN ISO 5555” and a threshold of \n45%. The blue dots are sentences that contain at least one external \nreference and matched with the input embeddings, the turquoise dots \nare also sentences that contain at least one external reference but were \nnot matched with the input embedding. While the red dots are sentences \nthat do not consist of any external reference but were matched with the \ninput embedding, and the pink dots are sentences that do not contain \nany external references and did not match with the input. Meaning, it's \nthe blue and red dots that will be further analyzed in the next step.  \nFigure 5-4 shows the embeddings for the first document after a cosine \nsimilarity has been performed. The search ended up removing 243 \nsentences that do not contain any external reference and 4 that do contain \nexternal reference. Which means 211 sentences were matched with the \nsearch, see table 1. \n",
      "word_count": 222,
      "char_count": 1326,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 675,
          "height": 439,
          "ext": "png",
          "size_bytes": 32154
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 33,
      "text": "25 \nTable 1: Overview of the results for cosine similarity on the first \ndocument \n \nInitial amount \nof sentences \nAmount of \nsentences after \ncosine \nsimilarity \nAmount of \nremoved \nsentences \nSentences \ncontaining \nexternal \nreferences \n70 \n66 \n-4 \nSentences not \ncontaining \nexternal \nreferences \n388 \n145 \n-243 \nTotal \n458 \n211 \n-247 \n \nFigure 5-4: A visualization on the embeddings after cosine similarity \nhas been performed on the first document. \n \nFigure 5-5 presents the embeddings for the second document after the \ncosine similarity was performed. In this document did the search remove \n",
      "word_count": 86,
      "char_count": 600,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 795,
          "height": 487,
          "ext": "png",
          "size_bytes": 44649
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 34,
      "text": "26 \na total of 202 sentences where 195 did not contain any external reference \nand 7 did, see table 2.  \nTabel 2: Overview of the results for cosine similarity on the second \ndocument \n \nInitial \namount of \nsentences \nAmount of \nsentences \nafter cosine \nsimilarity \nAmount of \nremoved \nsentences \nSentences \ncontaining \nexternal \nreferences \n29 \n22 \n-7 \nSentences not \ncontaining \nexternal \nreferences \n284 \n89 \n-195 \nTotal \n313 \n111 \n-202 \n \n",
      "word_count": 66,
      "char_count": 443,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 35,
      "text": "27 \nFigure 5-5: A visualization on the embeddings after cosine similarity \nhas been performed on the second document. \n \nFigure 5-6 show the results after the cosine similarity was performed on \nthe third document. That ended up removing a total of 309 sentences \nwhere 302 do not contain external references and 7 did, see table 3. \nTabel 3: Overview of the results for cosine similarity on the third \ndocument \n \nInitial \namount of \nsentences \nAmount of \nsentences \nafter cosine \nsimilarity \nAmount of \nremoved \nsentences \nSentences \ncontaining \nexternal \nreferences \n30 \n23 \n-7 \nSentences not \ncontaining \n339 \n37 \n-302 \n",
      "word_count": 95,
      "char_count": 624,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 711,
          "height": 433,
          "ext": "png",
          "size_bytes": 57441
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 36,
      "text": "28 \nexternal \nreferences \nTotal \n369 \n60 \n-309 \n \nFigure 5-6: A visualization on the embeddings after cosine similarity \nhas been performed on the third document. \n \n5.3 \nLarge Language Model \nThe following three sections present the results from the models on each \ndocument. Each section contains a table with five columns, the first one \nis the correct answer, and this is followed by two pairs where the first \none in the pair is the model output and the second one is the quality of \nthe predicted output. Meaning it will get tagged either, match, partial \nmatch or incorrect. These tags means that the predicted contains as much \nor more information as the expected output “exact match”, the predicted \noutput contains less information that the expected output “partial \nmatch”, “incorrect” if the prediction was incorrect, “Not found” if the \nreference was not identified or “Removed by cosine similarity” if \nremoved by the previous step. The tags that are associated with an \n",
      "word_count": 158,
      "char_count": 985,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 712,
          "height": 435,
          "ext": "png",
          "size_bytes": 57049
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 37,
      "text": "29 \nidentified reference also have a color associated with it, “exact match” \ngreen, “partial match” orange and “incorrect” red. \nEach row in the tested model's column represents an identified reference \nand the one in the first column is the expected answer. An empty row on \nany of the model’s columns but not on the first means that the reference \nwas not identified. On the bottom of the table can there be empty rows \non the first column but not on the tested model's columns. This means \nthat the model has identified an incorrect reference.  \nEach of the following three sections will also have two 2x2 matrixes that \nwere used to measure the F1 score for each model. The true negative (NT) \nvalue in these matrixes is set to the number of tokens in the text extracted \nwith the cosine similarity measured by OpenAI's tokenizer [20]. \n5.3.1 \nFirst document \nTable 4 presents the output from the LLMs for the first document. It \nshows that gpt-4-turbo managed to identify 30 out of 31 (≈97%) correctly \nbut also 4 incorrect references. This gives the gpt-4-turbo a precision of \n≈0.88, a recall of ≈0.97 and a F1 score of ≈0.92. And 29 out of the 30 (≈97%) \nidentified references were an exact match with the expected output. \nWhile the gpt-3.5-turbo managed to identify 29 out of 31 (≈87%) correctly \nwith 1 incorrect prediction. This gives the gpt-3.5-turbo a precision of \n≈0.97, a recall of ≈0.92 and a F1 score of 0.95. And 21 out of the 29 (≈72%) \nidentified were an exact match with the expected output. \nTable 4: A comparison between the correct and model’s outputs on \nthe first document. \nExpected \noutput \ngpt-4-\nturbo-\n2024-04-\n09 \ngpt-4-\nturbo-\n2024-04-\n09 quality \ngpt-3.5-\nturbo-\n0125 \ngpt-3.5-\nturbo-\n0125 \nquality \nISO 23309 ISO 23309 \nExact \nmatch \nISO 23309 \nExact \nmatch \nSS-ISO \n3320 \nSS-ISO \n3320 \nExact \nmatch \nSS-ISO \n3320 \nExact \nmatch \n",
      "word_count": 315,
      "char_count": 1868,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)",
        "Arial-BoldMT (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 38,
      "text": "30 \nSS-ISO \n4393 \nSS-ISO \n4393 \nExact \nmatch \nSS-ISO \n4393 \nExact \nmatch \nSS-ISO \n4406:2021 \nSS-ISO \n4406:2021 \nExact \nmatch\n \n \nSS-ISO \n4406:2021 \nExact \nmatch\n \n \nSS-ISO \n4413: 2010 \nSS-EN \nISO \n4413:2010 \nExact \nmatch \nSS-EN \nISO \n4413:2010 \nExact \nmatch \nSS-ISO \n6022: 2006 \nSS-ISO \n6022:2006 \nExact \nmatch \nSS-ISO \n6022 \nPartial \nmatch\n \n \nSS-ISO \n6162-1 \nSS-ISO \n6162-1 \nExact \nmatch \nSS-ISO \n6162-1 \nExact \nmatch \nSS-EN \nISO 8434-\n1:2007 \nSS-EN \nISO 8434-\n1:2007 \nExact  \nmatch\n \n \nSS-EN \nISO 8434-\n1 \nPartial \nmatch\n \n \nSS-EN \n837-1 \nSS-EN \n837-1 \nExact \nmatch \nSS-EN \n837-1 \nExact \nmatch \nSS-EN \n837-3 \nSS-EN \n837-3 \nExact \nmatch \nSS-EN \n837-3 \nExact \nmatch \nSS-EN \n853 \nSS-EN 853 \nExact \nmatch \nSS-EN 853 \nExact \nmatch \nSS-EN \n856 \nSS-EN 856 \nExact \nmatch \nSS-EN 856 \nExact \nmatch \nSS-EN \n857 \nSS-EN 857 \nExact \nmatch \nSS-EN 857 \nExact \nmatch \nSS-EN \n10088-\n2:2014 \nSS-EN \n10088-\n2:2014 \nExact \nmatch\n \n \nSS-EN \n10088-2 \nPartial \nmatch\n \n \n",
      "word_count": 133,
      "char_count": 950,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 39,
      "text": "31 \nSS-EN \n10088-\n3:2014 \nSS-EN \n10088-\n3:2014 \nExact \nmatch \nSS-EN \n10088-3 \nPartial \nmatch\n \n \nSS-EN \n10204: \n2005 \nSS-EN \n10204:200\n5 \nExact \nmatch\n \n \nSS-EN \n10204 \nPartial \nmatch\n \n \nSS-EN \n10216-5: \n2013 \nSS-EN \n10216-\n5:2013 \nExact \nmatch\n \n \nSS-EN \n10216-5 \nPartial \nmatch\n \n \nSS-EN \n60204-1 \nSS-EN \n60204-1 \nExact \nmatch \n- \nNot found \nSS-EN \nISO \n12100:201\n0 \nSS-EN \nISO \n12100:201\n0 \nExact \nmatch \nSS-EN \nISO \n12100:201\n0 \nExact \nmatch \nSS-ISO \n8132: 2016 \nSS-ISO \n8132:2016 \nExact \nmatch \nSS-ISO \n8132:2016 \nExact \nmatch \nSS-ISO \n16889: \n2011 \nSS-ISO \n16889:201\n1 \nExact \nmatch\n \n \nSS-ISO \n16889 \nPartial \nmatch\n \n \nSS-EN \n12760: \n2016 \nSS-EN \n12760 \nPartial \nmatch\n \n \nSS-EN \n12760 \nPartial \nmatch\n \n \nSS-EN \n12627 \nSS-EN \n12627 \nExact \nmatch \nSS-EN \n12627 \nExact \nmatch \nSS-EN \n13480-5 \nSS-EN \n13480-5 \nExact \nmatch \nSS-EN \n13480-5 \nExact \nmatch \nSS-EN \n10305-4 \nSS-EN \n10305-4 \nExact \nmatch \nSS-EN \n10305-4 \nExact \nmatch \nAFS \n2005:16  \nAFS \n2005:16 \nExact \nmatch \nAFS \n2005:16 \nExact \nmatch \n",
      "word_count": 136,
      "char_count": 1008,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 40,
      "text": "32 \nAFS \n2011:19  \nAFS \n2011:19 \nExact \nmatch \nAFS \n2011:19 \nExact \nmatch \nAFS \n2017:3 \nAFS \n2017:3 \nExact \nmatch \nAFS \n2017:3 \nExact \nmatch \nDIN 2353 \nDIN 2353 \nExact \nmatch \nDIN 2353 \nExact \nmatch \nDIN 3015-\n2 \n- \nNot found \n- \nNot found \nSAE J518 \nSAE \nJ518/1 \nExact \nmatch \nSAE \nJ518/1 \nExact \nmatch \n- \nSSG 2113 \nIncorrect \nSSG 3800 \nIncorrect \n- \nSSG 4700 \nIncorrect \n- \n- \n- \nSSG 7571 \nIncorrect \n- \n- \n- \nEN 1.4401 \nIncorrect \n- \n- \n \nFigure 5-7 and figure 5-8 presents the result from the models as a \nsummarized version for the first test document and does not take detail \ninto account. The true positive value represents the correctly identified \nreferences, the false positive is the incorrectly identified references and \nthe false negative is the references that were not included in the output \nbut should have been. The true negative value is set to the number of \ntokens in the text that was processed by the LLM. Figure 5-7 summarizes \nthe result for gpt-4-turbo-2024-04-09 gaining 30 true positive, 4 false \npositive, 1 false negative and 14238 true negative. \n \n",
      "word_count": 178,
      "char_count": 1083,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 41,
      "text": "33 \n \nFigure 5-7: A 2x2 matrix showing how the model gpt-4-turbo \npredicted for the first document. \n \nFigure 5-8 shows the result from the gpt-3.5-turbo-0125 model. Gaining \n29 true positive, 1 false positive, 2 false negative and 14238 true negative \nsince the same text has been processed. \n \n",
      "word_count": 46,
      "char_count": 296,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 500,
          "height": 500,
          "ext": "png",
          "size_bytes": 17336
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 42,
      "text": "34 \n \nFigure 5-8: A 2x2 matrix showing how the model gpt-3.5-turbo \npredicted for the first document. \n \n5.3.2 \nSecond document \nTable 5 presents the output from the LLMs for the second document. The \nthree references that were not identified because they were removed by \nthe cosine similarity which means that gpt-4-turbo managed to identify \n10 out of 10 (100%) correctly with 0 incorrect references, see figure 5-9. \nThis gives the gpt-4-turbo a precision, recall and F1 score of 1. And 10 \nout of the 10 (100%) identified references are an exact match with the \nexpected output. While the gpt-3.5-turbo managed to identify 10 out of \n10 (100%) correctly with 1 incorrect prediction, see figure 5-10. This gives \nthe gpt-3.5-turbo a precision of ≈0.91, a recall of 1 and a F1 score of ≈0.95. \nAnd 10 out of the 10 (100%) identified were an exact match with the \nexpected output. \n \n \n \n \n \n",
      "word_count": 148,
      "char_count": 894,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)",
        "Arial-BoldMT (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 500,
          "height": 500,
          "ext": "png",
          "size_bytes": 17274
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 43,
      "text": "35 \nTable 5: A comparison between the correct and model’s outputs on the \nsecond document. \nExpected \noutput \ngpt-4-\nturbo-\n2024-04-\n09 \ngpt-4-\nturbo-\n2024-04-\n09 quality \ngpt-3.5-\nturbo-\n0125 \ngpt-3.5-\nturbo-\n0125 \nquality \nSS-ISO \n10816-3  \nSS-ISO \n10816-3  \nExact \nmatch \nSS-ISO \n10816-3 \nExact \nmatch \nSS-ISO \n10816-6  \nSS-ISO \n10816-6  \nExact \nmatch \nSS-ISO \n10816-6 \nExact \nmatch \nSS-ISO \n10816-7  \nSS-ISO \n10816-7 \nExact \nmatch \nSS-ISO \n10816-7 \nExact \nmatch \nSS-ISO \n20816-1 \nSS-ISO \n20816-1 \nExact \nmatch \nSS-ISO \n20816-1 \nExact \nmatch \nSS-ISO \n20816-2  \n- \nRemoved \nby cosine \nsimilarity \n- \nRemoved \nby cosine \nsimilarity \nSS-ISO \n20816-4  \n- \nRemoved \nby cosine \nsimilarity \n- \nRemoved \nby cosine \nsimilarity \nSS-ISO \n20816-5  \n- \nRemoved \nby cosine \nsimilarity \n- \nRemoved \nby cosine \nsimilarity \nISO \n21940-11 \nSS-ISO \n21940-11 \nExact \nmatch \nISO \n21940-11 \nExact \nmatch \nISO \n21940-31 \nISO \n21940-31 \nExact \nmatch \nISO \n21940-31 \nExact \nmatch \nISO \n21940-32 \nISO \n21940-32 \nExact \nmatch \nISO \n21940-32 \nExact \nmatch \n",
      "word_count": 139,
      "char_count": 1032,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 44,
      "text": "36 \nSS-ISO \n11342 \nSS-ISO \n11342 \nExact \nmatch \nSS-ISO \n11342 \nExact \nmatch \nISO 14694 ISO 14694 \nExact \nmatch \nISO 14694 \nExact \nmatch \nISO 8528-\n9 \nISO 8528-\n9 \nExact \nmatch \nISO 8528-\n9 \nExact \nmatch \n- \n- \n- \nSSG 7640 \nIncorrect \n \nFigure 5-9 and figure 5-10 summarizes the outputs shown in table 5 but \ndoes not take detail into account. Figure 5-9 shows a summarized result \nfor gpt-4-turbo-2024-04-09, gaining 10 true positive, 0 false positive, 0 \nfalse negative and 9238 true negative. \n \n \nFigure 5-9: A 2x2 matrix showing how the model gpt-4-turbo \npredicted for the second document. \n \n \n",
      "word_count": 96,
      "char_count": 600,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 500,
          "height": 500,
          "ext": "png",
          "size_bytes": 17500
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 45,
      "text": "37 \nAnd figure 5-10 present a summarized result for the gpt-3.5-turbo-0125 \nmodel, achieving 10 true positive, 1 false positive, 0 false negative and \n9238 true negative. \n \n \n \nFigure 5-10: A 2x2 matrix showing how the model gpt-3.5-turbo \npredicted for the second document. \n \n5.3.3 \nThird document \nTable 6 presents the output from the LLMs for the third document. It \nshows that gpt-4-turbo managed to identify 2 out of 2 (100%) correctly \nwith 0 incorrect references, see figure 5-11. This gives the gpt-4-turbo a \nprecision, recall and F1 score of 1. And 2 out of the 2 (100%) identified \nreferences are an exact match with the expected output. While the gpt-\n3.5-turbo managed to identify 2 out of 2 correctly with 1 incorrect \nprediction, see figure 5-12. This gives the gpt-3.5-turbo a precision of \n≈0.67, a recall of 1 and a F1 score of 0.80. And 0 out of the 2 (0%) identified \nwere an exact match with the expected output. \n \n \n",
      "word_count": 158,
      "char_count": 941,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)",
        "Arial-BoldMT (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 500,
          "height": 500,
          "ext": "png",
          "size_bytes": 16975
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 46,
      "text": "38 \nTable 6: A comparison between the correct and model’s outputs on the \nthird document. \nExpected \noutput \ngpt-4-\nturbo-\n2024-04-\n09 \ngpt-4-\nturbo-\n2024-04-\n09 quality \ngpt-3.5-\nturbo-\n0125 \ngpt-3.5-\nturbo-\n0125 \nquality \nSS-EN \n13306:201\n7 \nSS-EN \n13306:201\n7 \nExact \nmatch \nSS-EN \n13306 \nPartial \nmatch\n \n \nSS-EN \n15341:201\n9 \nSS-EN \n15341:201\n9 \nExact \nmatch \nSS-EN \n15341 \nPartial \nmatch\n \n \n- \n- \n- \nSSG 2001 \nIncorrect \n \nFigure 5-11 and figure 5-12 serve as summarized versions of table 6. \nFigure 5-11 showing the result for the gpt-4-turbo-2024-04-09 model, \ngaining 2 true positive, 0 false positive, 0 false negative and 6752 true \nnegative. \n \n \n",
      "word_count": 98,
      "char_count": 660,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 500,
          "height": 500,
          "ext": "png",
          "size_bytes": 17107
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 47,
      "text": "39 \nFigure 5-11: A 2x2 matrix showing how the model gpt-4-turbo \npredicted for the third document. \n \nAnd figure 5-12 summarize the result for the gpt-3.5-turbo-0125 model, \nachieving 2 true positive, 1 false positive, 0 false negative and 6752 true \nnegative. \n \n \nFigure 5-12: A 2x2 matrix showing how the model gpt-3.5-turbo \npredicted for the third document. \n \n \n",
      "word_count": 55,
      "char_count": 368,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "PalatinoLinotype-Bold (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 500,
          "height": 500,
          "ext": "png",
          "size_bytes": 16575
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 48,
      "text": "40 \n6 \nDiscussion \nThis chapter includes discussions and analyses starting with an analysis \nand discussion of the result, then a discussion about the work methods, \nafter that a scientific discussion, then a consequence analysis and last an \nethical and societal discussion. \n6.1 \nAnalysis and discussion of results \nThe result of the embeddings for the first document shows that most of \nthe vectors have been placed in one area while there are a few that stand \nout compared to the others. It also shows that the references for the most \npart are not mixed with the unwanted text. In the second document the \nresult was a little different. It shows that the references have been placed \nin two different areas, one upper area where most of the references are \nclustered together and one area further down where some are clustered \ntogether and some mixed with the unwanted text. And the embeddings \non the third document are all mixed with the unwanted text, even though \nmost of the references are close to each other.  \nThis is a consequence caused by the chosen method to divide the text by \nsentences. The chosen embeddings model uses context when \ntransforming the text into embeddings. This allows sentences where the \nexternal reference has been used in the same way to match better with \nsentences containing big differences in phrasing. But on the other hand, \ncan the contextual essence also make the embeddings focus on the wrong \nthing in the sentence, and working with larger sentences just amplifies \nthis behavior. Which is the case in the third document, that document \ncontains a lot of tables meaning that much unwanted text gets mixed \nwith the text containing references. This explains why the sentences \ncontaining references are more mixed with the unwanted text.  \nIn the analysis of the first document using cosine similarity, ≈6% of the \nsentences containing references (4 out of 70) and about ≈63% of the \nsentences without references (243 out of 388) were filtered out. Initially, \nthe document contained 458 sentences, but post-filtering, only 211 \nsentences remained, 66 containing references and 145 without. \nFor the second document, the cosine similarity removed about 23% of the \nsentences with references (7 out of 29) and 69% without any references \n(195 out of 284), and unfortunately 3 references were removed from the \n",
      "word_count": 383,
      "char_count": 2360,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)",
        "Arial-BoldMT (22.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 49,
      "text": "41 \ntext in this process. Starting with 313 sentences, the process resulted in a \nfinal count of 111 sentences, with 22 containing references and 89 not \ncontaining. \nRegarding the third document, the cosine similarity process eliminated \nroughly 23% of the sentences with references (7 out of 30) and 89% of \nthose without references (302 out of 339). After the similarity search, only \n23 sentences were left, of which 23 contained references and 37 did not. \nAnd regarding the input and threshold for the cosine similarity, the \nchosen input “Enligt standard SS EN ISO 555” and threshold 45%. \nWorked well to remove unwanted sentences, having high percentages in \nremoval for the unwanted texts. But also having a bit too high percentage \nwhen removing sentences containing references in the second and third \ndocument which can end up losing data. This could also be a \nconsequence of the sentences being too large, since there are shorter and \nlonger sentences is it hard to find a general input to include both the short \nand long ones. And also, that the long sentences can vary a lot, and the \nshorter ones are more limited. \nWhen analyzing the removed sentences for the second document, it is \nshown that the reason behind three references not getting identified are \nbecause their sentences were not matched with the cosine similarity, and \nare only presented in one sentence each. \nAnd lastly the results from the LLMs. Starting with the model gpt-4-\nturbo-2024-04-09, this model extracted a total of 42 out of 43 out of all the \ndocuments where 41 of the 42 were good quality meaning one missed \nsome information. While the gpt-3.5-turbo-0125 model managed to \nextract 41 out of the 43 where 31 was good quality. These results show \nthat gpt-4-turbo performed best, both at identifying external references \nand at including more details and also gaining higher F1 scores on each \ndocument. \n6.2 \nWork method discussion \nThe initial phase, the pre-study, was crucial in laying the groundwork \nfor the entire project. During this phase, I conducted extensive literature \nreviews and explored various methods for text extraction, embedding \ngeneration, and similarity searches. This helped me identify the most \nsuitable tools and techniques for implementing the project. \n",
      "word_count": 366,
      "char_count": 2282,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 50,
      "text": "42 \nThe second phase, extracting text was done and implemented \nsuccessfully. The third phase, used the extracted text to be divided into \nsections based on sentences. This was also implemented successfully \nalthough the sizes of the sentences can vary a lot. For instance, if the \nsentences instead would have been divided as a fixed size, meaning that \nevery section contains the same number of characters. This method \nwould have allowed each section to be more focused on the external \nreference but could remove some contextual capabilities since important \nwords around the reference could get cut from that section. But using \nthis method does also mean that the number of sections could become a \nlot so preparing structured tests would require more time.  \nThe next phase, generating embeddings was also implemented \nsuccessfully. The chosen model “text-embedding-3-small” by OpenAI \nshowed good performance by for the most part separating the sentences \ncontaining references from the sentences without. The performance on \nthis phase is also dependent on what method that was chosen in the \nprevious phase. Since the chosen embeddings model takes context into \naccount is it more reasonable to use sentences as each section to include \nthe relations between the words. If the text instead was divided by a fixed \nsmaller size a context-applying model could generate more inaccurate \nembeddings since important words in sentences could have been \nremoved from that section.  \nNext phase, the cosine similarity, was also implemented and successful. \nBy removing at least 85% unwanted sentences on each document with \nthe input “Enligt standard SS EN ISO 5555” and a 45% threshold. Better \nperformance was of course possible, but since the length and content of \nsentences can vary does it make it difficult to find a general input that \ncan match with all possible sentences. \nAn alternative to this phase could be a machine learning model for text \nclassification. But this approach would require more time since more \ndata to train the model would be needed. \nThe last phase was also successful. Both models showed great \nperformance while gpt-4-turbo performed better with more detailed \noutput. It successfully extracted 42 out of 43 references since 3 got lost in \nthe previous phase, with 41 being optimal and the remaining missing \n",
      "word_count": 369,
      "char_count": 2348,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 51,
      "text": "43 \nsome information. While the gpt-3.5-turbo-0125 model extracted 41 out \nof 43 references, with 31 matching the expected output perfectly.  \nA significant reason for the results is the detailed prompt provided to the \nLLMs. When constructing a prompt, it is crucial to make it as clear and \nsimple as possible while also providing all the necessary information for \nwhat needs to be extracted. Typically, a prompt starts small to see the \nbasic information that can be extracted. And then gradually add more \ndetails. However, it is important not to add too many details at once, as \nthe model can be sensitive to overly complex prompts or just incorrectly \nphrased sentences.  Which is the case in my prompt, even though \nmentioning that no ‘SSG’ documents should be included. Were they \nincluded a total of 6 times. Meaning, that sentence should have been \nreformatted to make it more clear for the LLMs. Therefore, continuous \ntesting is essential to ensure optimal performance. \n6.3 \nScientific discussion \nAnswering and Discussing Research Questions: \n1. How does the accuracy differentiate between the models when \ncomparing the amount of identified external references? \nThe first model, gpt-4-turbo-2024-04-09 managed to extract a total of 42 \nout of 43 getting an accuracy of roughly 97.7%. While the gpt-3.5-turbo-\n0125 model successfully extracted 41 out of 43 references giving it an \naccuracy of about 95.3%. Meaning the gpt-4-turbo-2024-04-09 model \ngained a slightly higher accuracy with about 2 percentage points. \n2. How does the quality differentiate between the models when \ncomparing the identified external references? \nFrom the gpt-4-turbo-2024-04-09 model 41 out of the extracted 42 was \noptimal outputs resulting in 97.6% containing the expected information. \nWhile 31 out of 41 were optimal outputs from the gpt-3.5-turbo-0125 \nmodel resulting in roughly 75.6%. This indicates that gpt-4-turbo-2024-\n04-09 provided a more detailed output with an improvement of about 22 \npercentage points over gpt-3.5-turbo-0125. \n \n \n",
      "word_count": 308,
      "char_count": 2046,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "ArialMT (12.0pt)",
        "Arial-BoldMT (14.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 52,
      "text": "44 \nThe scientific knowledge gained from this thesis are the following. \n1. The preprocess needs to be implemented properly to not lose any \ndata. \n2. GPT 4 turbo showed the best performance, both in quantity and \nquality when given the instruction to extract data. \nThe first point is more general, since the preprocess phase is crucial for \nhow accurate the text gets filtered before sent to a LLM, in my case this \nwas the divide text, generate embeddings and cosine similarity phases. \nWhile the second point is more project specific, for it to be more general \nmore and diverse tests would have to be made. \n6.4 \nConsequence analysis \nThe consequence of this work is that, although not optimal \nconfigurations, show that it is possible to extract detailed data from \nunstructured sources. Since this work was done in cooperation with the \ncompany Standard Solution Group (SSG) I recommend them to continue \npursuing research on LLMs and how it could be used in other cases. \n6.5 \nEthical and societal discussion \nA significant ethical issue in the development of LLMs is the way training \ndata is collected. There have been instances where large corporations \nhave been accused of data theft during this process. For example, the \nnews organization New York Times has sued both OpenAI and \nMicrosoft over the unauthorized use of their content for training LLMs, \nand there are several more. \nAnother critical area is the possibility of harmful content generation. \nSince LLMs are powerful and capable of generating coherent and \ncontextually appropriate text, they can also produce biased, offensive, or \nharmful content. This raises questions about the responsibility of \ndevelopers to implement robust filtering and monitoring mechanisms to \nprevent these behaviors. Additionally, the training data itself may \ncontain inherent biases, which can be reflected in the model’s outputs. \nThe integration of LLMs into various sectors have the potential to \nautomate complex tasks, improving efficiency, and providing advanced \nanalytical capabilities. For example, in document processing, LLMs can \n",
      "word_count": 325,
      "char_count": 2101,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "ArialMT (12.0pt)",
        "Arial-BoldMT (14.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 53,
      "text": "45 \nsignificantly speed up the extraction and analysis of information, leading \nto faster decision-making and productivity gains. \n",
      "word_count": 17,
      "char_count": 131,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 54,
      "text": "46 \n7 \nConclusions  \nThis chapter presents an answer to the problem statement, objective of \nthe thesis and research questions to conclude this whole work. \nSo how can LLMs be utilized to accurately convert unstructured datasets \ninto structured datasets? To handle larger documents is preprocessing \nnecessary due to the context size limitation associated with Large \nLanguage Models (LLMs), meaning they can only process a fixed \nnumber of tokens at a time. The process begins with extracting text from \nthe document, which is then divided into sections, with sentences being \nthe chosen method for division. Embeddings are generated for each \nsentence, converting them into lists of numerical values between -1 and \n1 that capture words and context. Next, cosine similarity is performed \nwith an input text and a threshold to filter out sentences that do not \nmatch the input. The matched text is then sent to a Large Language \nModel (LLM), which has been given detailed instructions to extract \nexternal references. \nThe objective of this thesis was to successfully identify 70% of the \nreferences from one of the three test documents and compare the \nperformance of two Large Language Models (LLMs). The results for the \nthree test documents demonstrate that the gpt-4-turbo-2024-04-09 model \ngenerally outperformed the gpt-3.5-turbo-0125 model in terms of \naccuracy and detail. For the first document, gpt-4-turbo identified 30 out \nof 31 references correctly (≈97% accuracy) with an F1 score of ≈0.92. In \ncomparison, gpt-3.5-turbo identified 29 out of 31 references correctly (≈87% \naccuracy) with an F1 score of 0.95. For the second document, gpt-4-turbo \nidentified 10 out of 10 possible references correctly (100% accuracy) with \nan F1 score of 1 because the cosine similarity removed three references \nfrom the text. gpt-3.5-turbo also identified 10 out of 10 references \ncorrectly (100% accuracy) with an F1 score of ≈0.95. For the third \ndocument, gpt-4-turbo identified all 2 references correctly (100% \naccuracy) with an F1 score of 1. gpt-3.5-turbo identified 2 out of 2 \nreferences correctly but with 1 incorrect prediction, resulting in an F1 \nscore of 0.80. Meaning that the objective of this thesis was achieved. \nThe first research question was about “How do the accuracy differentiate \nbetween the models when comparing the amount of identified external \nreferences”, this study shows the following. The first model, gpt-4-turbo-\n",
      "word_count": 376,
      "char_count": 2453,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (22.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 55,
      "text": "47 \n2024-04-09, managed to extract a total of 42 out of 43 references, achieving \nan accuracy of roughly 97.7%. In comparison, the gpt-3.5-turbo-0125 \nmodel successfully extracted 41 out of 43 references, resulting in an \naccuracy of approximately 95.3%. This indicates that the gpt-4-turbo-\n2024-04-09 model achieved a slightly higher accuracy, with an \nimprovement of about 2 percentage points. \nAnd regarding the second research question “How do the quality \ndifferentiate between the models when comparing the identified external \nreferences”. Was 41 out of the extracted 42 outputs were optimal from \nthe gpt-4-turbo-2024-04-09 model, resulting in 97.6% containing the \nexpected information. In comparison, 31 out of 41 outputs from the gpt-\n3.5-turbo-0125 model were optimal, resulting in approximately 75.6%. \nThis indicates that gpt-4-turbo-2024-04-09 provided a more detailed \noutput, with an improvement of about 20 percentage points over gpt-3.5-\nturbo-0125. \n7.1 \nOptimize this project \nI believe a better result for sure is possible by optimizing the method for \ndividing the text to something fixed and relatively small, the input text \nat the cosine similarity step and the instruction given to the LLMs. And \nthe result should give a more accurate view of the maximum possible \nperformance of LLMs. \n7.2 \nExplore open-source models \nExplore possible open-source embeddings models and LLMs that can \nreplace OpenAI's models and be run locally to be more in control of how \ndata is being handled and stored. \n7.3 \nNatural language preprocess \nAnother possible method instead of LLMs could be natural language \npreprocessing (NLP) with Named Entity Recognition (NER). This is an \nalternative for extracting external references but would require a own \ntrained model since this is a specific problem. NER makes it easy to \nextract data by just giving the model an input text and the output will be \na list text with its associated tag for example external reference. This is \nnot as complex but also not as flexible when compared to the possibilities \nof an LLM. \n",
      "word_count": 319,
      "char_count": 2076,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (14.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 56,
      "text": "48 \nReferences \n[1]  IBM, “What are large language models (LLMs)?” [Online], \nAvailable: https://www.ibm.com/topics/large-language-models \n[Accessed May 2, 2024]. \n \n[2]  Journal of Machine Learning Research, “Visualizing Data using t-\nSNE,” \n[Online], \nAvailable: \nhttps://jmlr.org/papers/volume9/vandermaaten08a/vandermaate\nn08a.pdf \n[Accessed May 2, 2024]. \n \n[3]  L. Tunstall, L. von Werra, and T. Wolf, \"From Text to Tokens\" in \nNatural Language Processing with Transformers. Sebastopol, CA, \nUSA: O'Reilly Media, Inc., 2022, pp. 30-34. \n \n[4]  Stanford University, “Speech and Language Processing (3rd ed. \ndraft)”, \n[Online], \nAvailable: \nhttps://web.stanford.edu/~jurafsky/slp3/ed3book.pdf   \n[Accessed May 3, 2024]. \n \n[5]  Wayne Xin Zhao, Kun Zhou*, Junyi Li*, Tianyi Tang, Xiaolei \nWang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, \nZican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, \nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, \nPeiyu Liu, Jian-Yun Nie and Ji-Rong Wen, “A Survey of Large \nLanguage \nModels”, \n[Online], \nAvailable: \nhttps://arxiv.org/pdf/2303.18223   \n[Accessed May 3, 2024]. \n \n[6]  Humza Naveeda, Asad Ullah Khana, Shi Qiub, Muhammad \nSaqibc, Saeed Anware, Muhammad Usmane, Naveed Akhtarg, \nNick Barnesh, Ajmal Miani, “A Comprehensive Overview of \nLarge \nLanguage \nModels”, \n[Online], \nAvailable: \nhttps://arxiv.org/pdf/2307.06435 \n[Accessed May 4, 2024]. \n \n[7]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, \nLlion Jones, Aidan N. Gomez, Łukasz Kaiser and Illia Polosukhin, \n",
      "word_count": 194,
      "char_count": 1559,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "ArialMT (12.0pt)",
        "Arial-BoldMT (22.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 57,
      "text": "49 \n“Attention \nIs \nAll \nYou \nNeed”, \n[Online], \nAvailable: \nhttps://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee2\n43547dee91fbd053c1c4a845aa-Paper.pdf \n[Accessed May 4, 2024]. \n \n \n[8]  Daniel Jurafsky and James H. Martin, “Speech and Language \nProcessing \n(3rd \ned. \ndraft)”, \n[Online], \nAvailable: \nhttps://web.stanford.edu/~jurafsky/slp3/ed3book.pdf   \n[Accessed May 4, 2024]. \n \n[9]  David M.W. Powers, “What the F-measure doesn’t measure…”, \n[Online], Available: https://arxiv.org/pdf/1503.06410 \n[Accessed May 7, 2024]. \n \n[10]  \nGeeksforGeeks, “F1 Score in Machine Learning”, [Online], \nAvailable: \nhttps://www.geeksforgeeks.org/f1-score-in-machine-\nlearning/   \n[Accessed May 7, 2024]. \n  \n[11]  \nHuaxia Li, Haoyun Gao, Chengzhang Wu and Miklos A. \nVasarheli, “Extracting Financial Data from Unstructured Sources: \nLeveraging Large Language Models”, [Online], Available: \nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4567607   \n[Accessed May 5, 2024]. \n  \n[12]  \nDipali Baviskar, Swati Ahirrao, Vidyasagar Potdar and \nKetan Kotecha, “Efficient Automated Processing of the \nUnstructured Documents Using Artificial Intelligence: A \nSystematic Literature Review and Future Directions”, [Online], \nAvailable: https://ieeexplore.ieee.org/abstract/document/9402739 \n[Accessed May 6, 2024]. \n  \n[13]  \nPyPI, \n“PyPDF2”, \n[Online], \nAvailable: \nhttps://pypi.org/project/PyPDF2/ \n[Accessed May 6, 2024]. \n \n[14]  \nNLTK, \n“nltk.tokenize”, \n[Online], \nAvailable: \nhttps://www.nltk.org/api/nltk.tokenize.html \n",
      "word_count": 144,
      "char_count": 1525,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "ArialMT (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 58,
      "text": "50 \n[Accessed May 6, 2024]. \n  \n[15]  \nOpenAI, \n“Embeddings”, \n[Online], \nAvailable: \nhttps://platform.openai.com/docs/guides/embeddings \n[Accessed May 5, 2024]. \n  \n[16]  \nOpenAI, \n“Python \nLibrary”, \n[Online], \nAvailable: \nhttps://platform.openai.com/docs/libraries/python-library \n[Accessed May 6, 2024]. \n  \n[17]  \nscikit-learn, \n“sklearn.metrics.pairwise.cosine_similarity”, \n[Online], \nAvailable: \nhttps://scikit-\nlearn.org/stable/modules/generated/sklearn.metrics.pairwise.cosi\nne_similarity.html \n[Accessed May 6, 2024]. \n  \n[18]  \nOpenAI, “API Reference - Chat Completion”, [Online], \nAvailable: \nhttps://platform.openai.com/docs/api-\nreference/chat/create#chat-create-temperature \n[Accessed May 21, 2024]. \n  \n[19]  \nOpenAI, “GPT-4 Turbo and GPT-4”, [Online], Available: \nhttps://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4 \n[Accessed May 16, 2024]. \n \n[20]  \nOpenAI, \n“Tokenizer”, \n[Online], \nAvailable: \nhttps://platform.openai.com/tokenizer \n[Accessed May 13, 2024].\n",
      "word_count": 76,
      "char_count": 990,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "ArialMT (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    },
    {
      "page_number": 59,
      "text": "51 \nAppendix A: Source Code \nhttps://github.com/Lurre13/Examensarbete-SSG  \n",
      "word_count": 6,
      "char_count": 76,
      "fonts": [
        "PalatinoLinotype-Roman (12.0pt)",
        "Arial-BoldMT (22.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.5,
        842.0
      ]
    }
  ],
  "extraction_time": 0.972686767578125,
  "file_size_mb": 1.269632339477539,
  "basic_extraction": {
    "page_count": 59,
    "text_stats": {
      "total_words": 11182,
      "total_chars": 79084
    },
    "fonts_used": [
      "Arial-BoldMT (12.0pt)",
      "Arial-BoldMT (14.0pt)",
      "Arial-BoldMT (18.0pt)",
      "Arial-BoldMT (22.0pt)",
      "ArialMT (10.0pt)",
      "ArialMT (12.0pt)",
      "ArialMT (13.0pt)",
      "ArialMT (14.0pt)",
      "CambriaMath (12.0pt)",
      "CambriaMath (7.0pt)",
      "CambriaMath (8.5pt)",
      "MS-Gothic (11.0pt)",
      "PalatinoLinotype-Bold (12.0pt)",
      "PalatinoLinotype-Italic (12.0pt)",
      "PalatinoLinotype-Roman (11.0pt)",
      "PalatinoLinotype-Roman (12.0pt)",
      "SymbolMT (12.0pt)"
    ],
    "images_count": 17,
    "metadata": {
      "format": "PDF 1.7",
      "title": "Automating Data Extraction from Documents Using Large Language Models",
      "author": "Lucas Persson",
      "creator": "Microsoft® Word för Microsoft 365",
      "producer": "Microsoft® Word för Microsoft 365",
      "creationDate": "D:20240917110746+02'00'",
      "modDate": "D:20240917110746+02'00'"
    },
    "pages": [
      {
        "page_number": 1,
        "text": "Automating Data Extraction from Documents \nUsing Large Language Models \nA Study Exploring How AI Can Be Used to Transform Unstructured \nData into Structured Formats \nLucas Persson \nDocument type – Bachelor Thesis \nMain field of study: Computer engineering \nCredits: 15 \nSemester/year: Spring/2024 \nSupervisor: Martin Kjellqvist \nExaminer: Stefan Forsstöm \nCourse code: DT099G \n",
        "word_count": 50,
        "char_count": 377,
        "fonts": [
          "ArialMT (13.0pt)",
          "Arial-BoldMT (18.0pt)",
          "ArialMT (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 2,
        "text": "i \n \n \nAt Mid Sweden University, it is possible to publish the thesis in full text in DiVA \n(see appendix for publishing conditions). The publication is open access, which \nmeans that the work will be freely available to read and download online. This \nincreases the dissemination and visibility of the degree project. \nOpen access is becoming the norm for disseminating scientific information \nonline. Mid Sweden University recommends both researchers and students to \npublish their work open access. \nI/we allow publishing in full text (free available online, open access): \n☒  \nYes, I/we agree to the terms of publication. \n☐ \nNo, I/we do not accept that my independent work is published in \nthe public interface in DiVA (only archiving in DiVA). \n \nLocation \nand \ndate \nSundsvall 2024-06-16 \n \nProgramme/Course \nBachelor of Science in Engineering, Computer Engineering \n \nName \n(all \nauthors \nnames) \nLucas Persson \n \nYear \nof \nbirth \n(all \nauthors \nyear \nof \nbirth) \n2001-07-27 \n",
        "word_count": 146,
        "char_count": 984,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "MS-Gothic (11.0pt)",
          "PalatinoLinotype-Roman (11.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 3,
        "text": "ii \n \n \nAbstract \nThe objective of this thesis is to accurately extract at least 70% of external \nreferences from one of three test documents and to compare the \nperformance of two Large Language Models (LLMs) using quantitative \nmethods. This is measured by evaluating both the number of identified \nreferences and the amount of these references that are similar to the \nexpected output. The process begins with extracting text from a PDF \ndocument, followed by dividing the text into sentences. Embeddings are \nthen generated for each sentence. Cosine similarity is performed on these \nembeddings to filter out sentences that do not contain the requested data. \nThe remaining sentences are processed using two OpenAI models, gpt-\n3.5-turbo-0125 and gpt-4-turbo-2024-04-09, accessed via their API. Each \nmodel is instructed to extract external references from the filtered \nsentences. The extracted references are then compared against the \nexpected outputs in two ways: by the number of correctly identified \nreferences and by the level of detail in the extracted references. The gpt-\n4-turbo-2024-04-09 model successfully extracted 42 out of 43 references, \nwith 41 being optimal and the remaining missing some information. The \ngpt-3.5-turbo-0125 model extracted 41 out of 43 references, with 31 \nmatching the expected output perfectly. These results demonstrate the \npotential of Large Language Models in accurately extracting data from \nunstructured sources. \nKeywords: Large Language Models, data extraction, unstructured \nsources, embeddings, cosine similarity \n \n \n",
        "word_count": 226,
        "char_count": 1574,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (22.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 4,
        "text": "iii \n \n \nSammanfattning \nMålet med detta examensarbete är att extrahera minst 70 % av externa \nreferenser från ett av tre testdokument och att jämföra resultaten mellan \ntvå stora språkmodeller (LLM) med kvantitativa metoder. Detta mättes \ngenom att utvärdera både antalet korrekt identifierade referenser och \nlikheten hos dessa referenser jämfört med det förväntade resultatet. \nProcessen börjar med att extrahera text från ett PDF-dokument, följt av \natt dela upp texten i meningar. “Embeddings” genereras sedan för varje \nmening. “Cosine similarity” utförs på dessa “embeddings” för att filtrera \nbort meningar som inte innehåller den begärda informationen. De \nåterstående meningarna bearbetas med två OpenAI-modeller, gpt-3.5-\nturbo-0125 och gpt-4-turbo-2024-04-09, som nås via deras API. Varje \nmodell får sedan en detaljerad instruktion att extrahera externa \nreferenser från de filtrerade meningarna. De extraherade referenserna \njämförs sedan mot de förväntade ut datat på två sätt: med antalet korrekt \nidentifierade referenser och med detaljnivån i de extraherade \nreferenserna. \nModellen \ngpt-4-turbo-2024-04-09 \nextraherade \nframgångsrikt 42 av 43 referenser, där 41 var optimala och den \nåterstående saknade viss information. Modellen gpt-3.5-turbo-0125 \nextraherade 41 av 43 referenser, där 31 matchade det förväntade ut datat \nperfekt. Dessa resultat visar potentialen hos stora språkmodeller för att \nextrahera data från ostrukturerade källor med en högre träffsäkerhet. \n \nNyckelord: Stora språkmodeller, dataextrahering, ostrukturerad data, \nembeddings, cosine similarity \n \n \n",
        "word_count": 208,
        "char_count": 1597,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (22.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 5,
        "text": "iv \n \n \nAcknowledgements \nI would like to give a huge thank you to my supervisor Martin Kjellqvist \nat Mid Sweden University and to both my supervisors at Standard \nSolution Group (SSG) for the help and guidance through the whole \nproject. \n",
        "word_count": 39,
        "char_count": 241,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (22.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 6,
        "text": "v \n \n \nTable of Contents \nAbstract ............................................................................................................ ii \nSammanfattning ............................................................................................ iii \nAcknowledgements .......................................................................................iv \nTerminology / Notation .............................................................................. vii \n1 \nIntroduction ............................................................................................ 1 \n1.1 \nBackground and motivation................................................................. 1 \n1.2 \nOverall aim and problem statement ................................................... 2 \n1.3 \nResearch questions/Scientific goals/Verifiable goals ........................ 2 \n1.4 \nScope ........................................................................................................ 2 \n1.5 \nOutline ..................................................................................................... 3 \n2 \nTheory ...................................................................................................... 4 \n2.1 \nt-SNE ........................................................................................................ 4 \n2.2 \nText representation ................................................................................ 4 \n2.2.1 Tokenization and tokens ....................................................................... 4 \n2.2.2 Embeddings............................................................................................. 4 \n2.3 \nLarge Language Models........................................................................ 5 \n2.3.1 Context size ............................................................................................. 6 \n2.3.2 Prompting ................................................................................................ 6 \n2.3.3 Transformer ............................................................................................. 7 \n2.3.4 Scaling laws ............................................................................................. 9 \n2.4 \nCosine similarity .................................................................................... 9 \n2.5 \nAccuracy and F1-Score ........................................................................ 10 \n2.6 \nRelated work ......................................................................................... 11 \n2.6.1 Extracting Financial Data from Unstructured Sources: Leveraging \nLarge Language Models................................................................................ 11 \n3 \nMethodology ........................................................................................ 13 \n3.1 \nScientific method description ............................................................. 13 \n3.2 \nProject method description................................................................. 14 \n3.2.1 Phase 1: Pre-study ................................................................................ 14 \n3.2.2 Phase 2: Extract text ............................................................................. 14 \n3.2.3 Phase 3: Divide text .............................................................................. 14 \n3.2.4 Phase 4: Generate embeddings ........................................................... 15 \n3.2.5 Phase 5: Similarity search .................................................................... 15 \n3.2.6 Phase 6: LLM ......................................................................................... 15 \n3.2.7 Phase 7: Gather results......................................................................... 16 \n3.2.8 Phase 8: Evaluation and discussions ................................................. 16 \n4 \nImplementation ................................................................................... 17 \n",
        "word_count": 195,
        "char_count": 4002,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (22.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 7,
        "text": "vi \n \n \n4.1 \nExtract text ............................................................................................ 18 \n4.2 \nDivide text ............................................................................................. 18 \n4.3 \nGenerate embeddings.......................................................................... 19 \n4.4 \nCosine similarity .................................................................................. 19 \n4.5 \nLarge Language Model ....................................................................... 19 \n4.6 \nEvaluation setup .................................................................................. 20 \n5 \nResults ................................................................................................... 22 \n5.1 \nGenerated embeddings ....................................................................... 22 \n5.2 \nCosine similarity .................................................................................. 24 \n5.3 \nLarge Language Model ....................................................................... 28 \n5.3.1 First document ...................................................................................... 29 \n5.3.2 Second document ................................................................................. 34 \n5.3.3 Third document .................................................................................... 37 \n6 \nDiscussion ............................................................................................. 40 \n6.1 \nAnalysis and discussion of results .................................................... 40 \n6.2 \nWork method discussion .................................................................... 41 \n6.3 \nScientific discussion ............................................................................. 43 \n6.4 \nConsequence analysis.......................................................................... 44 \n6.5 \nEthical and societal discussion ........................................................... 44 \n7 \nConclusions .......................................................................................... 46 \n7.1 \nOptimize this project ........................................................................... 47 \n7.2 \nExplore open-source models .............................................................. 47 \n7.3 \nNatural language preprocess ............................................................. 47 \nReferences ...................................................................................................... 48 \nAppendix A: Source Code ........................................................................... 51 \n \n \n",
        "word_count": 131,
        "char_count": 2686,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 8,
        "text": "vii \n \n \nTerminology / Notation \n \nAcronyms/Abbreviations \nAI \n \nArtificial Intelligence \nLLM \n \nLarge language model \nt-SNE \n \nt-distributed stochastic neighbor embedding\n",
        "word_count": 17,
        "char_count": 172,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (22.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 9,
        "text": "1 \n1 \nIntroduction \nThis thesis investigates the use of Artificial Intelligence (AI) for extracting \ninformation from unstructured data, a significant challenge given the \nexponential growth of data across various formats. The aim is to evaluate \nand compare different Large Language Models (LLM) to identify \nefficient, accurate, and scalable solutions for data extraction. This work \nis proposed and supported by the company Standard Solution Group \n(SSG). \nThe aim of this chapter is to provide an overview of the background, \nproblem motivation, and the overall purpose and context of this study. \n1.1 \nBackground and motivation \nIn the rapidly evolving digital landscape, the need for sophisticated \nartificial intelligence (AI) systems has never been greater. This big shift \nisn't just because AI can take over everyday tasks. It's more about how \nAI can greatly improve things and speed up processes. As companies \nand entire industries rush to make the most of AI, there's a growing need \nfor AI-powered tools that can analyze and extract valuable information \nfrom large piles of unorganized data. \nAI-powered tools are essential because of the challenges associated with \nhandling unstructured data, which is characterized by its complexity and \nvariability in structure. Since traditional data processing systems are \ntypically designed to handle structured data that follow a specific format \nand are easy to categorize, it causes them to struggle with processing the \ncomplexity and diversity of unstructured data.  \nIn this thesis will the AI-powered tool Large Language Models (LLM) be \nutilized. A Large Language Model (LLM) is an advanced AI system \ndesigned to understand and generate human-like text by learning from \na large database of existing content. These models, which include well-\nknown examples like OpenAIs GPT series and Googles Gemini, use \ncomplex algorithms to process and produce language in a way that \nmimics human conversations. Capable of tasks such as translation, \nsummarization, and even creative writing, LLMs are increasingly \nintegral to various applications, from virtual assistants to content \ngeneration. [1] \n",
        "word_count": 320,
        "char_count": 2159,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "Arial-BoldMT (22.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 10,
        "text": "2 \n1.2 \nOverall aim and problem statement \nThe specific problem investigated in this thesis is: How can LLMs be \nutilized to accurately convert unstructured datasets into structured \ndatasets? This involves detailed exploration into methods of text \nhandling and embeddings. The solution to this problem will be \nevaluated by how accurate LLMs can transform unstructured data into \nstructured datasets including both quantity and quality. The higher \nreason for this work is the following: \n• Unstructured data poses challenges in data analysis, limiting \norganizations' ability to derive actionable insights. \n• Current data extraction methods may not efficiently convert \nunstructured data into usable formats. \n• There is a need to explore AI-driven solutions for transforming \nunstructured data into structured datasets effectively and \naccurately. \n1.3 \nResearch questions/Scientific goals/Verifiable goals \nThe objectives of this thesis are: \n1. Successfully extract at least 70% correct external reference from \none of the documents. \n2. Compare the performance of two Large Language Models (LLMs). \nThe research questions to be answered are the following: \n1. How does the accuracy differentiate between the models when \ncomparing the amount of identified external references? \n2. How does the quality differentiate between the models when \ncomparing the identified external references? \n1.4 \nScope \nThis thesis has been delimited in some areas, the chosen document to test \non are standards produced by Standard Solution Group (SSG), the task \nhas been to extract external references, and the documents are only in \nswedish. This has been tested on two different LLMs with the same \ninstruction to extract external references. \n",
        "word_count": 251,
        "char_count": 1737,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "ArialMT (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "SymbolMT (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 11,
        "text": "3 \n1.5 \nOutline \nThis report begins with Chapter 2, which sets the theoretical groundwork \nnecessary for the thesis. Chapter 3 then details the scientific \nmethodologies and evaluation techniques used. Chapter 4 which covers \nthe actual implementation itself. Chapter 5 presents the results of the \nstudy, while Chapter 6 engages in a discussion about these findings. The \nreport concludes with Chapter 7, which synthesizes the conclusions \ndrawn from the research. \n",
        "word_count": 69,
        "char_count": 467,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 12,
        "text": "4 \n2 \nTheory  \nThis chapter presents relevant background theory for a better \nunderstanding of this thesis.  \n2.1 \nt-SNE \nt-SNE (t-distributed Stochastic Neighbor Embedding) is a technique \ndesigned to visualize high-dimensional data by mapping each data point \nto a two or three-dimensional space. This method is an enhanced version \nof the original Stochastic Neighbor Embedding, offering improved \nvisualization capabilities. [2] \n2.2 \nText representation \nIn this thesis, text representation is addressed through two fundamental \ntechniques: Tokenization and Word Embeddings. These methods are \nessential for processing and understanding text within Large Language \nModels (LLMs). Tokenization serves as the initial step in deconstructing \ntext into manageable pieces, while word embeddings provide a way to \ntranslate these pieces into numerical forms. \n2.2.1 \nTokenization and tokens \nTokenization is the process of turning text into a list of \"tokens,\" which \ncan be characters, parts of words or whole words. The purpose of \ntokenization in natural language processing is to break down text into \nthese manageable units, enabling machine learning models to process \nand analyze the text numerically. This crucial step transforms raw text \ninto a structured form that algorithms can understand and manipulate, \nenabling effective data analysis and interpretation. [3] \n2.2.2 \nEmbeddings \nEmbeddings are a technique to convert text into numerical values to \nrepresent words, phrases, or entire documents as vectors in a vector \nspace. This transformation captures not just the identity of textual \nelements but also their semantic and syntactic relationships, allowing \nsimilar words to have similar numerical representations. The purpose of \nembeddings is to reduce the high dimensionality of text data into a more \nmanageable form, facilitating more effective computation and enabling \nmachine learning models to perform complex tasks. [4] \n",
        "word_count": 275,
        "char_count": 1949,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "Arial-BoldMT (22.0pt)",
          "Arial-BoldMT (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 13,
        "text": "5 \nFigure 2-1: Example showing how words relate to others in the \nnumerical space [4]. \nAs shown in figure 2-1, the embedding dimensions have been reduced \nto two dimensions using a t-SNE model for clearer visualization. In this \nvisual representation, words are colored as blue, red, and green, with \neach color forming clusters. This clustering indicates that the \nembeddings capture and group words with similar meanings or \nsentiments closely together. For instance, positive words like 'good', \n'amazing', and 'wonderful' are clustered in one area, while negative \nwords such as 'bad', 'worse', and 'worst' are grouped in another area. This \nseparation and clustering highlight how embeddings, through t-SNE, \nenable the visualization of complex semantic relationships within the \ndata. \nTwo types of embedding methods are static embeddings and contextual \nembeddings. In static embedding, is each word assigned a fixed vector, \nmeaning that no matter the context in which the word appears, it will \nalways have the same embedding. This contrasts with contextual \nembeddings, where the embedding for a word can vary depending on its \ncontext within a sentence. Therefore, the same word may have different \nembeddings based on the surrounding words and the overall semantic \nmeaning of the sentence. [4] \n2.3 \nLarge Language Models \nLarge Language Models (LLMs) such as Chat-GPT, PaLM and LLaMA \nare advanced transformer-based models characterized by their wide \nscale, typically containing hundreds of billions of parameters. These \nmodels are trained on a huge amount of text datasets, enabling them to \ndevelop a profound understanding of natural language and the ability to \nperform complex text generation tasks. [5] \n",
        "word_count": 259,
        "char_count": 1727,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 869,
            "height": 311,
            "ext": "png",
            "size_bytes": 49430
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 14,
        "text": "6 \n2.3.1 \nContext size \nContext size, also known as context window or length when referring to \nLLMs, is the maximum number of tokens the model can handle at one \ntime. This limitation includes both the input tokens it receives and the \noutput tokens it generates. The context size is crucial because it \ndetermines how much information the model can process when making \npredictions or generating text. \nA larger context size can lead to a better understanding of the input and \nenable the generation of more descriptive and contextual outputs. This is \nparticularly important for tasks that require a deep understanding of \nlong texts, such as summarizing lengthy documents or answering \ncomplex questions. [6] \n2.3.2 \nPrompting \nPrompting is a crucial technique used for interacting with Large \nLanguage Models (LLMs), where users provide a \"prompt\", an input \nstring that contains instructions or tasks for the LLM to process. Common \nstrategies include: \nZero-Shot Prompting: LLMs utilize their training data to respond to new \nqueries without needing specific examples within the prompt. \nIn-Context Learning (Few-Shot Learning): This approach involves \npresenting the LLM with multiple input-output pairs, which helps guide \nthe model to generate responses by mimicking these examples. \nChain-of-Thought (CoT): Prompts under this strategy contain detailed \nreasoning steps, assisting the LLM in articulating a step-by-step \nreasoning process for more complex problem-solving. \nMulti-Turn Instructions: This technique engages LLMs in an ongoing \ndialogue, allowing each response to influence subsequent prompts. It is \nparticularly effective in interactive applications such as chatbots or \ncomplex task navigation. \nThese prompting strategies enhance the LLMs ability to generate \naccurate and contextually relevant responses across a variety of tasks. [6] \n \n \n",
        "word_count": 268,
        "char_count": 1869,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 15,
        "text": "7 \n2.3.3 \nTransformer \nTransformers are typically the architecture referring to when discussing \nLLMs. Transformers use positional encodings to maintain sequence \norder and stacked self-attention layers to understand context and play a \ncrucial role for LLMs. The following sections will describe the structure \nof the transformer shown in figure 2-2. [7] \nFirst is the input query and input embedding. The input query is the \ninstructions the user has written for the LLM but as tokens, and the input \nembedding is the embedded versions of these tokens, transforming them \ninto numerical vectors that the model can process. \nNext is the positional encoding. This step creates an understanding of the \norder in the input, which is crucial because the model processes tokens \nin parallel, meaning that the order of tokens is not naturally preserved in \nthe process. These positional encodings are typically calculated using \nsine and cosine mathematical functions. Each position in the sentence \nreceives unique sine and cosine values based on its sequence by the \nfollowing method.  \n𝑃𝐸 (𝑝𝑜𝑠,  2𝑖)  =   sin(𝑝𝑜𝑠  ÷  10 0002𝑖÷𝑑) (2.1) \n𝑃𝐸 (𝑝𝑜𝑠,  2𝑖  +  1)  =   cos(𝑝𝑜𝑠  ÷  10 0002𝑖÷𝑑) (2.2) \nWhere pos is the position in the sentence, i is the dimension and d are the \nnumber of dimensions, which are the same amount as the input \nembeddings. For more information about this, see source. These values \nare then combined with the word embeddings, providing context about \nword positions that the transformer uses to understand the text better. \nThis method ensures that despite processing words in parallel, the model \nretains information about the order of words. [7] \nNext step is the encoder, following the input stage, consisting of N \nidentical layers (number of iterations). Each layer is equipped with two \nsub-layers, a multi-head self-attention mechanism and a position-wise \nfully connected feed-forward network. The multi-head self-attention \nmechanism's purpose is to receive information about every token in the \ngiven input, based on the positions and the relations to the other tokens \nin the input. Meanwhile, the feed-forward network enhances the token \nrepresentations refined by the attention mechanism, applying \n",
        "word_count": 341,
        "char_count": 2230,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (12.0pt)",
          "CambriaMath (8.5pt)",
          "PalatinoLinotype-Italic (12.0pt)",
          "CambriaMath (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 16,
        "text": "8 \ntransformations that integrate learned contextual information into each \ntoken's embedding. [7] \n \nFigure 2-2: Image containing the structure of a transformer. This \nimage has been slightly modified to clarify the components by \nadding “Encoder” and “Decoder”. [7] \n \nSimilar to the encoder, the decoder contains N layers (number of \niterations), each mirroring the encoder's structure with an additional \nsub-layer. The initial sub-layer in each decoder layer features a masked \nself-attention mechanism, which ensures that the generation of each \n",
        "word_count": 77,
        "char_count": 552,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 499,
            "height": 571,
            "ext": "jpeg",
            "size_bytes": 36405
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 17,
        "text": "9 \ntoken is influenced only by preceding tokens, thereby preserving the \nautoregressive property of the decoder. Following this, the encoder-\ndecoder attention layer integrates the context from the entire input \nsequence by focusing on the most relevant parts of the encoded \nembeddings. This layer crucially aligns the decoder's output with the \nencoder's input, facilitating accurate and context-aware token generation. \nFinally, the feed-forward network in the decoder operates similarly to \nthat in the encoder, further processing each token's data to produce the \nfinal output sequence. [7] \nThe final step of the transformer model's process is the output generation, \nwhich begins with a linear layer. This layer takes the output embeddings \nfrom the decoder, which represent the current token's features informed \nby both the encoding and decoding phases. The linear layer transforms \nthese embeddings into a vector whose dimension matches the size of the \nmodel’s vocabulary, producing a set of scores for each possible token. \nThese scores are then passed to a SoftMax layer, which converts them \ninto probabilities, reflecting the likelihood of each token being the next \nin the sequence. Based on these probabilities, the model selects the most \nappropriate token using its selection strategy. This process repeats for \neach token until the model generates an end-of-sequence token, signaling \nthe completion of the output. [7] \n2.3.4 \nScaling laws \nScaling laws are an important component of the optimization process for \nLLMs. They enable researchers and developers to predict the potential \nperformance of these models based on various input parameters. By \nsystematically analyzing how changes in model size, data volume, and \ncomputational resources affect model outcomes. Scaling laws provide a \npredictive framework that can guide strategic decision-making during \nthe development and training phases. [5] \n2.4 \nCosine similarity \nCosine similarity is used to measure the similarity by calculating the \ncosine angle between two vectors, the vectors need to have the same \nnumber of dimensions. This is done by calculating the dot product of the \nvectors. \n𝑎  ⋅  𝑏  =  |𝑎| |𝑏| 𝑐𝑜𝑠𝜃 (2.3) \n",
        "word_count": 329,
        "char_count": 2206,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "CambriaMath (12.0pt)",
          "Arial-BoldMT (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 18,
        "text": "10 \n𝑐𝑜𝑠𝜃  =  \n𝑎  ⋅  𝑏\n||𝑎|| ||𝑏|| (2.4) \nThe cosine of the angle θ between the vectors can then be calculated by \ndividing the dot product by the product of the magnitudes. This result, \nwhich ranges from -1 to 1, indicates how similar the two vectors are. A \nvalue of 1 means the vectors are perfectly aligned (the angle is 0 degrees), \nindicating maximum similarity. A value of 0 indicates that the angle is 90 \ndegrees, and a value of -1 indicates that the vectors are diametrically \nopposed (the angle is 180 degrees). [8] \n2.5 \nAccuracy and F1-Score \nF1-Score is a calculation that utilizes precision and recall. Precision \nmeasures the accuracy for the positive predictions. This is done by \ncounting how many times the positive prediction was correct and \nincorrect and then dividing the correct amount by the sum of the correct \nand incorrect. [9] \nPrecision  =  \n𝑇𝑃\n𝑇𝑃 + 𝐹𝑃  (2.5) \nAnd recall is used to measure how well the model can identify the \npositive cases. This is calculated by dividing the true positive (correct \npositive) by the sum of true positive and false negative (unidentified \ncorrect). [9] \nRecall  =  \n𝑇𝑃\n𝑇𝑃 + 𝐹𝑁  (2.6) \nThe F1 Score combines the precision and the recall to a single value by \neither dividing 2 by the sum of 1 divided by the precision and 1 divided \nby \nthe \nrecall. \nCan also be calculated by dividing 2 multiplied by the precision and \nrecall with the sum of precision and recall [10]. \nF1 Score  =  \n2\n1\n𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + \n1\n𝑟𝑒𝑐𝑎𝑙𝑙\n (2.7) \nF1 Score  =  \n2 ⋅ 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ⋅ 𝑟𝑒𝑐𝑎𝑙𝑙\n𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑟𝑒𝑐𝑎𝑙𝑙  (2.8) \nThis will produce a 2x2 metric that represents the true positive (TP), false \npositive (FP), false negative (FN) and true negative (TN). Where true \nmeans that the model identified is correct and false that it is incorrect. \n",
        "word_count": 309,
        "char_count": 1774,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "CambriaMath (7.0pt)",
          "Arial-BoldMT (14.0pt)",
          "CambriaMath (8.5pt)",
          "PalatinoLinotype-Bold (12.0pt)",
          "CambriaMath (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 19,
        "text": "11 \nThe first row in the metric will represent the predicted positive where \nthe first column is the amount of correctly and the second column is the \namount of incorrect. Same with the second row that includes the negative, \nthe first column is the correct negative and the second column is the \nincorrect negative. [9] \n \n \nFigure 2-3: A 2x2 matrix showing how the F1 score will be presented. \n \n2.6 \nRelated work \nThe following subsections present related work and their similarities \nwith this thesis. \n2.6.1 \nExtracting Financial Data from Unstructured Sources: Leveraging \nLarge Language Models \nThe study conducted by Huaxia Li, Haoyun Gao, Chengzhang Wu, and \nMiklos A. Vasarhelyi addresses the significant challenge of extracting \nfinancial data from unstructured sources. Utilizing LLMs, their research \nintroduces a framework designed specifically for automated data \nextraction from PDF-formatted documents. This framework employs a \ncombination of text mining and prompt engineering techniques. \nThe developed framework was tested on governmental annual reports \nand corporate ESG reports, showcasing its effectiveness in parsing \ncomplex financial documents. The framework demonstrated a high \n",
        "word_count": 173,
        "char_count": 1208,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "PalatinoLinotype-Bold (12.0pt)",
          "Arial-BoldMT (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 444,
            "height": 427,
            "ext": "png",
            "size_bytes": 12841
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 20,
        "text": "12 \naccuracy rate, achieving an average of 99.5% in initial tests and \nmaintaining an overall accuracy of around 96% in subsequent larger-\nscale tests. [11] \nBoth this study and the current thesis work with PDF files and aim to \nharness the capabilities of large language models to extract data from \nunstructured sources, highlighting a shared objective in advancing data \nextraction methodologies using AI technologies. \n \n \n",
        "word_count": 64,
        "char_count": 427,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 21,
        "text": "13 \n3 \nMethodology \nThis chapter presents the methodology used in this thesis including the \nscientific / project method descriptions and evaluation method. \n3.1 \nScientific method description \nIn this thesis, quantitative methods will be employed to systematically \ninvestigate and analyze the performance. Quantitative research is ideal \nfor this study as it allows for precise measurement for the number of \nidentified references and to compare the output to the expected output. \nTo further address the research problem, a mix of comparative and \nexperimental research designs will be utilized alongside quantitative \nmethods. \nThis thesis builds upon the research “Efficient Automated Processing of \nthe Unstructured Documents Using Artificial Intelligence: A Systematic \nLiterature Review and Future Directions” (2021), which highlights the \nsignificant potential of AI-based approaches for extracting useful \ninformation from unstructured documents. Their systematic literature \nreview identifies key challenges and gaps in the existing techniques, \nparticularly in handling complex document layouts and the need for \nhigh-quality datasets. [12] \nThe first research question is how the accuracy differs between the \nmodels when comparing the number of identified references. To answer \nthis needs all the implementation steps to have been implemented: \nStep 1 Extracting and dividing text: This includes processes like \nextracting the text from the PDF and dividing the text into sections. \nStep 2 Generate embeddings: This step generates embeddings for each \nsection. \nStep 3 Cosine similarity: This step performs cosine similarity on the \nembedded sections to identify and extract sections that could contain the \nexternal references. \nStep 4 Large Language Model: The extracted sections from the cosine \nsimilarity are then sent into a LLM to extract the external references as a \nstructured dataset. \n",
        "word_count": 269,
        "char_count": 1912,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "Arial-BoldMT (22.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 22,
        "text": "14 \nWhen all the implementation steps are complete, they will be tested with \nthree different documents. These extracted datasets are then compared \nbetween the models to evaluate the differences. \nThe second research question, similar to the first one, needs the \nimplementation steps to be complete. This question compares the quality \nof the identified references between the models by setting either “partial \nmatch” or “exact match” on a correctly identified reference. This \nmeasures the number of identified references that contain the expected \ninformation. \n3.2 \nProject method description \nThis thesis is divided into eight phases, the initial phase is a pre-study \nand consists of method discovery and planning. This is followed by \nphases 2 through 6 and are the implementation phases. And last phase 7 \nand 8 which gather the data for the results and discuss the result. \n3.2.1 \nPhase 1: Pre-study \nThe initial phase of this thesis, the pre-study, includes method discovery \nand planning. This step is fundamental to the entire research project as it \nestablishes the theoretical and practical work that will be applied. This \nwas done by a comprehensive review of existing methods in the field of \nartificial intelligence, specifically focusing on Large Language Models \n(LLMs) and their applications in data extraction. \n3.2.2 \nPhase 2: Extract text \nThe second phase signifies the onset of the practical implementation, \nfocusing on utilizing libraries to extract text from PDF documents. This \nphase involves selecting the appropriate tools that reliably convert PDF \ncontent into manageable text while preserving the original formatting to \nthe greatest extent possible. \n3.2.3 \nPhase 3: Divide text \nAfter the text is extracted, it must be divided into segments and every \nsegment will later be embedded. The division strategy could be based on \nnatural language cues such as chapters, headings, paragraphs, sentences \nor just a fixed size of characters. This segmentation is critical as it forms \nthe basic units of text that will be compared in the similarity search. \n \n",
        "word_count": 319,
        "char_count": 2092,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "Arial-BoldMT (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 23,
        "text": "15 \n3.2.4 \nPhase 4: Generate embeddings \nThis phase involves the embedding of text segments obtained from the \nprevious step. During generation of embeddings, is each text segment \ntransformed into embeddings, also known as vectors. These embeddings \nare essentially lists of numbers, ranging between -1 and 1, where each \nnumber represents a dimension of the text's features captured by the \nmodel. \nThe dimensionality of these embeddings, essentially the number of \nnumbers in each embedding, is determined by the selected model. \nHigher-dimensional embeddings can capture more detailed information \nabout the text but at the cost of increased computational complexity. \nEach embedding essentially serves as a numerical fingerprint of a text \nsegment, taking semantics and possible context if the model is trained for \nit in account. \nProperly executed embeddings is critical as it converts qualitative text \ndata into a quantitative form that can be systematically compared and \nanalyzed in the next phase of similarity search. This transformation is \nwhat allows the algorithms of the next steps to perform comparisons \nbetween different text segments. \n3.2.5 \nPhase 5: Similarity search \nPhase 5 focuses on the similarity search, a critical step in uncovering \nrelationships and patterns in the dataset. In this phase, the embeddings \ngenerated are compared using similarity metrics to determine how \nclosely the segments of text are related in terms of their semantic or \ncontextual content. \nThis process involves algorithms that can efficiently handle and process \nlarge matrices of embeddings to compute the similarity scores. \nThe results from this search can be used to cluster segments that share \nsimilarities. \n3.2.6 \nPhase 6: LLM \nPhase 6 represents the end of the implementation process, where the \nsegments identified as similar or relevant by the similarity search are \nfurther analyzed using an LLM. \n",
        "word_count": 286,
        "char_count": 1920,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 24,
        "text": "16 \nThe process begins by inputting the related text segments into the LLM. \nDepending on the requirements, the LLM can be utilized for various \ntasks such as extracting data and relationships, summarizing \ninformation, answering questions based on the text, or even generating \nnew text based on the learned context. But as mentioned before, this \nthesis will focus on extracting data. \n3.2.7 \nPhase 7: Gather results \nUpon completion of the implementation steps, the results from the \nembeddings, similarity search and LLMs are collected. During the \nembedding and similarity search phases, the vectors are split into \ncategories to visually be able to see how the vectors are placed. In the \nLLM phase, the outputs generated by the LLMs are listed. To visualize \nthe data, all vectors are reduced to two dimensions using a t-SNE model \nand plotted within the context of their respective documents. The \noutputs from the LLMs are compared against the expected outputs, and \nthe precision, recall, and F1 scores are calculated to measure the \nperformance and accuracy of the LLMs in identifying references \ncorrectly. \n3.2.8 \nPhase 8: Evaluation and discussions \nAfter the results have been gathered from the documents and the LLMs, \nthey are compared between the different documents and a discussion \nabout the results and the whole process is made. \n",
        "word_count": 212,
        "char_count": 1353,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 25,
        "text": "17 \n4 \nImplementation \nFigure 4-1 provides an overview of the entire process, beginning with \ntext extraction from a PDF. The extracted text is divided into sections, \nwhich are then converted into embeddings. These embeddings represent \nboth the text itself and the relationships within the text as numerical \nvalues. A similarity search is then conducted on these embeddings, using \na transformed input. Texts that align with the search input are extracted \nand fed into a LLM, which is tasked with specific functions such as \nextracting external references or charts. The final output is a list of the \ndata identified by the LLM. \n \nFigure 4-1: An overview of the entire process. \nFigure 4-2 Also shows an overview of the entire process, but this figure \nshows how the data flows in the process. It starts with the entire text from \nthe PDF, the text is then divided into sections. Embeddings are then \ngenerated for each section. A cosine similarity search is then performed \non each section with a set input transformed into embeddings to identify \nwhich of the sections that could contain the requested information. The \ntexts of the embeddings that matched in the cosine similarity search are \nthen sent to a LLM which is instructed to extract the data. For the \ncomplete source code, see Appendix A. \n",
        "word_count": 215,
        "char_count": 1310,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (22.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1169,
            "height": 172,
            "ext": "jpeg",
            "size_bytes": 25751
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 26,
        "text": "18 \nFigure 4-2: An overview of the entire process, showing how the data \nis processed in every step. \n4.1 \nExtract text \nThe initial step involves extracting text from a PDF file using the PyPDF2 \nlibrary [13]. This will take all the text from the PDF into one string. \n4.2 \nDivide text \nOnce the text is extracted, it is divided into sections based on sentences, \nutilizing the NLTK library [14]. \n \n \n",
        "word_count": 69,
        "char_count": 403,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "ArialMT (14.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 718,
            "height": 781,
            "ext": "png",
            "size_bytes": 48165
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 27,
        "text": "19 \n4.3 \nGenerate embeddings \nThe next step involves generating embeddings for each section using \nOpenAI's \"text-embedding-3-small\" model which produces embeddings \nwith 1536 dimensions. To use OpenAI's models, users must have credits \nin their account and initialize an API key within the code. [15] [16] \n4.4 \nCosine similarity \nThis stage involves performing cosine similarity analysis between a \nsearch input (once transformed into embeddings) and the embeddings \ngenerated in the previous step. Embeddings that result in a similarity \nscore above the chosen threshold (45%) are selected for further analysis. \nThis was done with sklearn's cosine similarity function. [17] \n4.5 \nLarge Language Model \nIn the final step, the selected sentences are inputted into an LLM. The \nLLM is tasked with extracting specific data from these sentences based \non given instructions. The prompt given to the LLMs to extract external \nreferences from standard was the following, it is in Swedish since the \ntargeted documents are in Swedish: \n“Du är en hjälpsam assistent som har i uppdrag att hitta externa \nreferenser från meningar. Dessa externa referenser är formaterade som \nsifferkod eller standarder som vanligtvis erkänns i tekniska och \nvetenskapliga dokument, till exempel 'SS', 'EN' eller 'ISO' eller en \nkombination av dem, men det finns även fler. Så din uppgift är följande: \nhitta alla externa referenser. Det kan vara så att det också finns vilken del \nav standarden som den refererar till och kan göras på två sätt, ena är \ngenom ett '-x' där 'x' är en siffra, och det andra är genom att skriva 'del x' \ndär 'x' är en siffra. Om du hittar en referens som skrivs med 'del' sättet så \nomvandla det till '-x' och inkludera det i referensen. Det kan också vara \nså att referensen innehåller årtal på följande sätt ':xxxx' där 'xxxx' är ett \nårtal. Inkludera detta i referensen om det finns. Sen ta bort referenser till \nallmänna publikationer, företagsinterna dokument från 'SSG' eller några \nhyperlänkar. Nästa steg är att ta bort referenser som du har hittat flera \ngånger, detta inkluderar då hittade referenser som har samma kod men \ndel nummer och årtal får vara olika, ta bort duplikat genom att exkludera \nde referenser som innehåller minst information. Svara enbart med en \nlista på de hittade externa referenserna, om det skulle vara så att du inte \nhittade någon svara med ‘ ’ ” \n",
        "word_count": 382,
        "char_count": 2392,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "ArialMT (14.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 28,
        "text": "20 \nBut here is the English version: “You are a helpful assistant tasked with \nfinding external references from sentences. These external references are \nformatted as numeric codes or standards commonly recognized in \ntechnical and scientific documents, such as 'SS', 'EN' or 'ISO' or a \ncombination of them, but there are more. So your task is the following: \nfind all external references. It may also include what part of the standard \nit refers to and can be done in two ways, one is through a '-x' where 'x' is \na number, and the other is by writing 'part x' where 'x' is a number. If you \nfind a reference written in the 'part' way, convert it to '-x' and include it \nin the reference. It may also be that the reference contains a year in the \nfollowing way ':xxxx' where 'xxxx' is a year. Include this in the reference \nif available. Then remove references to public publications, internal \ncompany documents from 'SSG' or any hyperlinks. The next step is to \nremove references that you have found multiple times, this includes \nreferences found that have the same code but the part number and year \nmay be different, remove duplicates by excluding the references that \ncontain the least information. Reply only with a list of the external \nreferences found, if you did not find any reply with ’ ’ ”. \nThe chosen LLMs were gpt-4-turbo-2024-04-09, this model has a context \nwindow of 128,000 tokens and is trained on data up to December 2023. \nAnd gpt-3.5-turbo-0125 which has a context size of 16,385 tokens. Both \nalso used the configuration temperature 0.2 and top_p 0.1. A lower \ntemperature configuration reduces the randomness when given an \noutput and top_p reduces the number of tokens taken into consideration \nwhen producing the output [18]. That is important since the task is to \nextract references and only focus on that. [19] \n4.6 \nEvaluation setup \nTo evaluate the generated embeddings a t-SNE model will be used to \nreduce the number of dimensions. This enables the embeddings to be \nviewed in a two-dimensional diagram to see how the sentences that \ncontain the external references are placed compared to the sentences \nwithout references. The cosine similarity will also use this kind of \nvisualization of the embeddings, but this diagram will also show what \nsentences that have been matched with the search input and where the \ninput embeddings have been placed on the diagram. And the LLMs will \nthen be evaluated by comparing the output to the correct answers, this is \ndone by measuring the F1 score by counting the correct (TP), incorrect \n",
        "word_count": 430,
        "char_count": 2569,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "ArialMT (14.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 29,
        "text": "21 \n(FP), missed (FN) and number of tokens (TN). And to evaluate the \nquality of the output, each identified reference will get a tag either \n“correct” or “partially correct”. This will be used to measure what \npercentage of the identified references that is correct or partially correct. \n \n \n",
        "word_count": 47,
        "char_count": 294,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 30,
        "text": "22 \n5 \nResults \nThis chapter outlines the results obtained in this thesis, illustrated \nthrough various diagrams that represent the embeddings generated for \neach document, the results of the cosine similarity search, and the final \noutputs from the LLMs. \n5.1 \nGenerated embeddings \nThe following diagrams represent the embeddings generated on each \nsentence in its document, to reduce the dimensions a t-SNE model was \nused. The blue dots are sentences consisting of at least one external \nreference while the red text does not contain any external reference.  \nFigure 5-1 shows the generated embeddings for the first document. This \ndiagram displays a total of 458 data points, of which 70 contain external \nreferences and 388 do not. \n \nFigure 5-1: A visualization of the embeddings for the first document. \n \nThe next figure, figure 5-2 represents the generated embeddings on the \nsecond document. Presented in this figure are the embeddings for 313 \nsentences, with 29 featuring external references and 284 lacking them. \n",
        "word_count": 157,
        "char_count": 1028,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "Arial-BoldMT (22.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 802,
            "height": 490,
            "ext": "png",
            "size_bytes": 40558
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 31,
        "text": "23 \n \nFigure 5-2: A visualization of the embeddings for the second \ndocument. \n \nThe last diagram of this subchapter, figure 5-3. This diagram contains 369 \ndata points. Where, 30 sentences include external references and 339 \nsentences do not. \n \n",
        "word_count": 37,
        "char_count": 248,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 674,
            "height": 440,
            "ext": "png",
            "size_bytes": 29514
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 32,
        "text": "24 \nFigure 5-3: A visualization of the embeddings for the third document. \n \n5.2 \nCosine similarity  \nThe diagrams shown in this chapter are based on the embeddings shown \nin the previous chapter but include one additional data point. These \ndiagrams have also been made with the help of a t-SNE model. The \nsimilarity search consists of an input “Enligt standard SS EN ISO 5555” \nin English “according to standard SS EN ISO 5555” and a threshold of \n45%. The blue dots are sentences that contain at least one external \nreference and matched with the input embeddings, the turquoise dots \nare also sentences that contain at least one external reference but were \nnot matched with the input embedding. While the red dots are sentences \nthat do not consist of any external reference but were matched with the \ninput embedding, and the pink dots are sentences that do not contain \nany external references and did not match with the input. Meaning, it's \nthe blue and red dots that will be further analyzed in the next step.  \nFigure 5-4 shows the embeddings for the first document after a cosine \nsimilarity has been performed. The search ended up removing 243 \nsentences that do not contain any external reference and 4 that do contain \nexternal reference. Which means 211 sentences were matched with the \nsearch, see table 1. \n",
        "word_count": 222,
        "char_count": 1326,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 675,
            "height": 439,
            "ext": "png",
            "size_bytes": 32154
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 33,
        "text": "25 \nTable 1: Overview of the results for cosine similarity on the first \ndocument \n \nInitial amount \nof sentences \nAmount of \nsentences after \ncosine \nsimilarity \nAmount of \nremoved \nsentences \nSentences \ncontaining \nexternal \nreferences \n70 \n66 \n-4 \nSentences not \ncontaining \nexternal \nreferences \n388 \n145 \n-243 \nTotal \n458 \n211 \n-247 \n \nFigure 5-4: A visualization on the embeddings after cosine similarity \nhas been performed on the first document. \n \nFigure 5-5 presents the embeddings for the second document after the \ncosine similarity was performed. In this document did the search remove \n",
        "word_count": 86,
        "char_count": 600,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 795,
            "height": 487,
            "ext": "png",
            "size_bytes": 44649
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 34,
        "text": "26 \na total of 202 sentences where 195 did not contain any external reference \nand 7 did, see table 2.  \nTabel 2: Overview of the results for cosine similarity on the second \ndocument \n \nInitial \namount of \nsentences \nAmount of \nsentences \nafter cosine \nsimilarity \nAmount of \nremoved \nsentences \nSentences \ncontaining \nexternal \nreferences \n29 \n22 \n-7 \nSentences not \ncontaining \nexternal \nreferences \n284 \n89 \n-195 \nTotal \n313 \n111 \n-202 \n \n",
        "word_count": 66,
        "char_count": 443,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 35,
        "text": "27 \nFigure 5-5: A visualization on the embeddings after cosine similarity \nhas been performed on the second document. \n \nFigure 5-6 show the results after the cosine similarity was performed on \nthe third document. That ended up removing a total of 309 sentences \nwhere 302 do not contain external references and 7 did, see table 3. \nTabel 3: Overview of the results for cosine similarity on the third \ndocument \n \nInitial \namount of \nsentences \nAmount of \nsentences \nafter cosine \nsimilarity \nAmount of \nremoved \nsentences \nSentences \ncontaining \nexternal \nreferences \n30 \n23 \n-7 \nSentences not \ncontaining \n339 \n37 \n-302 \n",
        "word_count": 95,
        "char_count": 624,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 711,
            "height": 433,
            "ext": "png",
            "size_bytes": 57441
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 36,
        "text": "28 \nexternal \nreferences \nTotal \n369 \n60 \n-309 \n \nFigure 5-6: A visualization on the embeddings after cosine similarity \nhas been performed on the third document. \n \n5.3 \nLarge Language Model \nThe following three sections present the results from the models on each \ndocument. Each section contains a table with five columns, the first one \nis the correct answer, and this is followed by two pairs where the first \none in the pair is the model output and the second one is the quality of \nthe predicted output. Meaning it will get tagged either, match, partial \nmatch or incorrect. These tags means that the predicted contains as much \nor more information as the expected output “exact match”, the predicted \noutput contains less information that the expected output “partial \nmatch”, “incorrect” if the prediction was incorrect, “Not found” if the \nreference was not identified or “Removed by cosine similarity” if \nremoved by the previous step. The tags that are associated with an \n",
        "word_count": 158,
        "char_count": 985,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 712,
            "height": 435,
            "ext": "png",
            "size_bytes": 57049
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 37,
        "text": "29 \nidentified reference also have a color associated with it, “exact match” \ngreen, “partial match” orange and “incorrect” red. \nEach row in the tested model's column represents an identified reference \nand the one in the first column is the expected answer. An empty row on \nany of the model’s columns but not on the first means that the reference \nwas not identified. On the bottom of the table can there be empty rows \non the first column but not on the tested model's columns. This means \nthat the model has identified an incorrect reference.  \nEach of the following three sections will also have two 2x2 matrixes that \nwere used to measure the F1 score for each model. The true negative (NT) \nvalue in these matrixes is set to the number of tokens in the text extracted \nwith the cosine similarity measured by OpenAI's tokenizer [20]. \n5.3.1 \nFirst document \nTable 4 presents the output from the LLMs for the first document. It \nshows that gpt-4-turbo managed to identify 30 out of 31 (≈97%) correctly \nbut also 4 incorrect references. This gives the gpt-4-turbo a precision of \n≈0.88, a recall of ≈0.97 and a F1 score of ≈0.92. And 29 out of the 30 (≈97%) \nidentified references were an exact match with the expected output. \nWhile the gpt-3.5-turbo managed to identify 29 out of 31 (≈87%) correctly \nwith 1 incorrect prediction. This gives the gpt-3.5-turbo a precision of \n≈0.97, a recall of ≈0.92 and a F1 score of 0.95. And 21 out of the 29 (≈72%) \nidentified were an exact match with the expected output. \nTable 4: A comparison between the correct and model’s outputs on \nthe first document. \nExpected \noutput \ngpt-4-\nturbo-\n2024-04-\n09 \ngpt-4-\nturbo-\n2024-04-\n09 quality \ngpt-3.5-\nturbo-\n0125 \ngpt-3.5-\nturbo-\n0125 \nquality \nISO 23309 ISO 23309 \nExact \nmatch \nISO 23309 \nExact \nmatch \nSS-ISO \n3320 \nSS-ISO \n3320 \nExact \nmatch \nSS-ISO \n3320 \nExact \nmatch \n",
        "word_count": 315,
        "char_count": 1868,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)",
          "Arial-BoldMT (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 38,
        "text": "30 \nSS-ISO \n4393 \nSS-ISO \n4393 \nExact \nmatch \nSS-ISO \n4393 \nExact \nmatch \nSS-ISO \n4406:2021 \nSS-ISO \n4406:2021 \nExact \nmatch\n \n \nSS-ISO \n4406:2021 \nExact \nmatch\n \n \nSS-ISO \n4413: 2010 \nSS-EN \nISO \n4413:2010 \nExact \nmatch \nSS-EN \nISO \n4413:2010 \nExact \nmatch \nSS-ISO \n6022: 2006 \nSS-ISO \n6022:2006 \nExact \nmatch \nSS-ISO \n6022 \nPartial \nmatch\n \n \nSS-ISO \n6162-1 \nSS-ISO \n6162-1 \nExact \nmatch \nSS-ISO \n6162-1 \nExact \nmatch \nSS-EN \nISO 8434-\n1:2007 \nSS-EN \nISO 8434-\n1:2007 \nExact  \nmatch\n \n \nSS-EN \nISO 8434-\n1 \nPartial \nmatch\n \n \nSS-EN \n837-1 \nSS-EN \n837-1 \nExact \nmatch \nSS-EN \n837-1 \nExact \nmatch \nSS-EN \n837-3 \nSS-EN \n837-3 \nExact \nmatch \nSS-EN \n837-3 \nExact \nmatch \nSS-EN \n853 \nSS-EN 853 \nExact \nmatch \nSS-EN 853 \nExact \nmatch \nSS-EN \n856 \nSS-EN 856 \nExact \nmatch \nSS-EN 856 \nExact \nmatch \nSS-EN \n857 \nSS-EN 857 \nExact \nmatch \nSS-EN 857 \nExact \nmatch \nSS-EN \n10088-\n2:2014 \nSS-EN \n10088-\n2:2014 \nExact \nmatch\n \n \nSS-EN \n10088-2 \nPartial \nmatch\n \n \n",
        "word_count": 133,
        "char_count": 950,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 39,
        "text": "31 \nSS-EN \n10088-\n3:2014 \nSS-EN \n10088-\n3:2014 \nExact \nmatch \nSS-EN \n10088-3 \nPartial \nmatch\n \n \nSS-EN \n10204: \n2005 \nSS-EN \n10204:200\n5 \nExact \nmatch\n \n \nSS-EN \n10204 \nPartial \nmatch\n \n \nSS-EN \n10216-5: \n2013 \nSS-EN \n10216-\n5:2013 \nExact \nmatch\n \n \nSS-EN \n10216-5 \nPartial \nmatch\n \n \nSS-EN \n60204-1 \nSS-EN \n60204-1 \nExact \nmatch \n- \nNot found \nSS-EN \nISO \n12100:201\n0 \nSS-EN \nISO \n12100:201\n0 \nExact \nmatch \nSS-EN \nISO \n12100:201\n0 \nExact \nmatch \nSS-ISO \n8132: 2016 \nSS-ISO \n8132:2016 \nExact \nmatch \nSS-ISO \n8132:2016 \nExact \nmatch \nSS-ISO \n16889: \n2011 \nSS-ISO \n16889:201\n1 \nExact \nmatch\n \n \nSS-ISO \n16889 \nPartial \nmatch\n \n \nSS-EN \n12760: \n2016 \nSS-EN \n12760 \nPartial \nmatch\n \n \nSS-EN \n12760 \nPartial \nmatch\n \n \nSS-EN \n12627 \nSS-EN \n12627 \nExact \nmatch \nSS-EN \n12627 \nExact \nmatch \nSS-EN \n13480-5 \nSS-EN \n13480-5 \nExact \nmatch \nSS-EN \n13480-5 \nExact \nmatch \nSS-EN \n10305-4 \nSS-EN \n10305-4 \nExact \nmatch \nSS-EN \n10305-4 \nExact \nmatch \nAFS \n2005:16  \nAFS \n2005:16 \nExact \nmatch \nAFS \n2005:16 \nExact \nmatch \n",
        "word_count": 136,
        "char_count": 1008,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 40,
        "text": "32 \nAFS \n2011:19  \nAFS \n2011:19 \nExact \nmatch \nAFS \n2011:19 \nExact \nmatch \nAFS \n2017:3 \nAFS \n2017:3 \nExact \nmatch \nAFS \n2017:3 \nExact \nmatch \nDIN 2353 \nDIN 2353 \nExact \nmatch \nDIN 2353 \nExact \nmatch \nDIN 3015-\n2 \n- \nNot found \n- \nNot found \nSAE J518 \nSAE \nJ518/1 \nExact \nmatch \nSAE \nJ518/1 \nExact \nmatch \n- \nSSG 2113 \nIncorrect \nSSG 3800 \nIncorrect \n- \nSSG 4700 \nIncorrect \n- \n- \n- \nSSG 7571 \nIncorrect \n- \n- \n- \nEN 1.4401 \nIncorrect \n- \n- \n \nFigure 5-7 and figure 5-8 presents the result from the models as a \nsummarized version for the first test document and does not take detail \ninto account. The true positive value represents the correctly identified \nreferences, the false positive is the incorrectly identified references and \nthe false negative is the references that were not included in the output \nbut should have been. The true negative value is set to the number of \ntokens in the text that was processed by the LLM. Figure 5-7 summarizes \nthe result for gpt-4-turbo-2024-04-09 gaining 30 true positive, 4 false \npositive, 1 false negative and 14238 true negative. \n \n",
        "word_count": 178,
        "char_count": 1083,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 41,
        "text": "33 \n \nFigure 5-7: A 2x2 matrix showing how the model gpt-4-turbo \npredicted for the first document. \n \nFigure 5-8 shows the result from the gpt-3.5-turbo-0125 model. Gaining \n29 true positive, 1 false positive, 2 false negative and 14238 true negative \nsince the same text has been processed. \n \n",
        "word_count": 46,
        "char_count": 296,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 500,
            "height": 500,
            "ext": "png",
            "size_bytes": 17336
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 42,
        "text": "34 \n \nFigure 5-8: A 2x2 matrix showing how the model gpt-3.5-turbo \npredicted for the first document. \n \n5.3.2 \nSecond document \nTable 5 presents the output from the LLMs for the second document. The \nthree references that were not identified because they were removed by \nthe cosine similarity which means that gpt-4-turbo managed to identify \n10 out of 10 (100%) correctly with 0 incorrect references, see figure 5-9. \nThis gives the gpt-4-turbo a precision, recall and F1 score of 1. And 10 \nout of the 10 (100%) identified references are an exact match with the \nexpected output. While the gpt-3.5-turbo managed to identify 10 out of \n10 (100%) correctly with 1 incorrect prediction, see figure 5-10. This gives \nthe gpt-3.5-turbo a precision of ≈0.91, a recall of 1 and a F1 score of ≈0.95. \nAnd 10 out of the 10 (100%) identified were an exact match with the \nexpected output. \n \n \n \n \n \n",
        "word_count": 148,
        "char_count": 894,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)",
          "Arial-BoldMT (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 500,
            "height": 500,
            "ext": "png",
            "size_bytes": 17274
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 43,
        "text": "35 \nTable 5: A comparison between the correct and model’s outputs on the \nsecond document. \nExpected \noutput \ngpt-4-\nturbo-\n2024-04-\n09 \ngpt-4-\nturbo-\n2024-04-\n09 quality \ngpt-3.5-\nturbo-\n0125 \ngpt-3.5-\nturbo-\n0125 \nquality \nSS-ISO \n10816-3  \nSS-ISO \n10816-3  \nExact \nmatch \nSS-ISO \n10816-3 \nExact \nmatch \nSS-ISO \n10816-6  \nSS-ISO \n10816-6  \nExact \nmatch \nSS-ISO \n10816-6 \nExact \nmatch \nSS-ISO \n10816-7  \nSS-ISO \n10816-7 \nExact \nmatch \nSS-ISO \n10816-7 \nExact \nmatch \nSS-ISO \n20816-1 \nSS-ISO \n20816-1 \nExact \nmatch \nSS-ISO \n20816-1 \nExact \nmatch \nSS-ISO \n20816-2  \n- \nRemoved \nby cosine \nsimilarity \n- \nRemoved \nby cosine \nsimilarity \nSS-ISO \n20816-4  \n- \nRemoved \nby cosine \nsimilarity \n- \nRemoved \nby cosine \nsimilarity \nSS-ISO \n20816-5  \n- \nRemoved \nby cosine \nsimilarity \n- \nRemoved \nby cosine \nsimilarity \nISO \n21940-11 \nSS-ISO \n21940-11 \nExact \nmatch \nISO \n21940-11 \nExact \nmatch \nISO \n21940-31 \nISO \n21940-31 \nExact \nmatch \nISO \n21940-31 \nExact \nmatch \nISO \n21940-32 \nISO \n21940-32 \nExact \nmatch \nISO \n21940-32 \nExact \nmatch \n",
        "word_count": 139,
        "char_count": 1032,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 44,
        "text": "36 \nSS-ISO \n11342 \nSS-ISO \n11342 \nExact \nmatch \nSS-ISO \n11342 \nExact \nmatch \nISO 14694 ISO 14694 \nExact \nmatch \nISO 14694 \nExact \nmatch \nISO 8528-\n9 \nISO 8528-\n9 \nExact \nmatch \nISO 8528-\n9 \nExact \nmatch \n- \n- \n- \nSSG 7640 \nIncorrect \n \nFigure 5-9 and figure 5-10 summarizes the outputs shown in table 5 but \ndoes not take detail into account. Figure 5-9 shows a summarized result \nfor gpt-4-turbo-2024-04-09, gaining 10 true positive, 0 false positive, 0 \nfalse negative and 9238 true negative. \n \n \nFigure 5-9: A 2x2 matrix showing how the model gpt-4-turbo \npredicted for the second document. \n \n \n",
        "word_count": 96,
        "char_count": 600,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 500,
            "height": 500,
            "ext": "png",
            "size_bytes": 17500
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 45,
        "text": "37 \nAnd figure 5-10 present a summarized result for the gpt-3.5-turbo-0125 \nmodel, achieving 10 true positive, 1 false positive, 0 false negative and \n9238 true negative. \n \n \n \nFigure 5-10: A 2x2 matrix showing how the model gpt-3.5-turbo \npredicted for the second document. \n \n5.3.3 \nThird document \nTable 6 presents the output from the LLMs for the third document. It \nshows that gpt-4-turbo managed to identify 2 out of 2 (100%) correctly \nwith 0 incorrect references, see figure 5-11. This gives the gpt-4-turbo a \nprecision, recall and F1 score of 1. And 2 out of the 2 (100%) identified \nreferences are an exact match with the expected output. While the gpt-\n3.5-turbo managed to identify 2 out of 2 correctly with 1 incorrect \nprediction, see figure 5-12. This gives the gpt-3.5-turbo a precision of \n≈0.67, a recall of 1 and a F1 score of 0.80. And 0 out of the 2 (0%) identified \nwere an exact match with the expected output. \n \n \n",
        "word_count": 158,
        "char_count": 941,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)",
          "Arial-BoldMT (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 500,
            "height": 500,
            "ext": "png",
            "size_bytes": 16975
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 46,
        "text": "38 \nTable 6: A comparison between the correct and model’s outputs on the \nthird document. \nExpected \noutput \ngpt-4-\nturbo-\n2024-04-\n09 \ngpt-4-\nturbo-\n2024-04-\n09 quality \ngpt-3.5-\nturbo-\n0125 \ngpt-3.5-\nturbo-\n0125 \nquality \nSS-EN \n13306:201\n7 \nSS-EN \n13306:201\n7 \nExact \nmatch \nSS-EN \n13306 \nPartial \nmatch\n \n \nSS-EN \n15341:201\n9 \nSS-EN \n15341:201\n9 \nExact \nmatch \nSS-EN \n15341 \nPartial \nmatch\n \n \n- \n- \n- \nSSG 2001 \nIncorrect \n \nFigure 5-11 and figure 5-12 serve as summarized versions of table 6. \nFigure 5-11 showing the result for the gpt-4-turbo-2024-04-09 model, \ngaining 2 true positive, 0 false positive, 0 false negative and 6752 true \nnegative. \n \n \n",
        "word_count": 98,
        "char_count": 660,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 500,
            "height": 500,
            "ext": "png",
            "size_bytes": 17107
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 47,
        "text": "39 \nFigure 5-11: A 2x2 matrix showing how the model gpt-4-turbo \npredicted for the third document. \n \nAnd figure 5-12 summarize the result for the gpt-3.5-turbo-0125 model, \nachieving 2 true positive, 1 false positive, 0 false negative and 6752 true \nnegative. \n \n \nFigure 5-12: A 2x2 matrix showing how the model gpt-3.5-turbo \npredicted for the third document. \n \n \n",
        "word_count": 55,
        "char_count": 368,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "PalatinoLinotype-Bold (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 500,
            "height": 500,
            "ext": "png",
            "size_bytes": 16575
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 48,
        "text": "40 \n6 \nDiscussion \nThis chapter includes discussions and analyses starting with an analysis \nand discussion of the result, then a discussion about the work methods, \nafter that a scientific discussion, then a consequence analysis and last an \nethical and societal discussion. \n6.1 \nAnalysis and discussion of results \nThe result of the embeddings for the first document shows that most of \nthe vectors have been placed in one area while there are a few that stand \nout compared to the others. It also shows that the references for the most \npart are not mixed with the unwanted text. In the second document the \nresult was a little different. It shows that the references have been placed \nin two different areas, one upper area where most of the references are \nclustered together and one area further down where some are clustered \ntogether and some mixed with the unwanted text. And the embeddings \non the third document are all mixed with the unwanted text, even though \nmost of the references are close to each other.  \nThis is a consequence caused by the chosen method to divide the text by \nsentences. The chosen embeddings model uses context when \ntransforming the text into embeddings. This allows sentences where the \nexternal reference has been used in the same way to match better with \nsentences containing big differences in phrasing. But on the other hand, \ncan the contextual essence also make the embeddings focus on the wrong \nthing in the sentence, and working with larger sentences just amplifies \nthis behavior. Which is the case in the third document, that document \ncontains a lot of tables meaning that much unwanted text gets mixed \nwith the text containing references. This explains why the sentences \ncontaining references are more mixed with the unwanted text.  \nIn the analysis of the first document using cosine similarity, ≈6% of the \nsentences containing references (4 out of 70) and about ≈63% of the \nsentences without references (243 out of 388) were filtered out. Initially, \nthe document contained 458 sentences, but post-filtering, only 211 \nsentences remained, 66 containing references and 145 without. \nFor the second document, the cosine similarity removed about 23% of the \nsentences with references (7 out of 29) and 69% without any references \n(195 out of 284), and unfortunately 3 references were removed from the \n",
        "word_count": 383,
        "char_count": 2360,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)",
          "Arial-BoldMT (22.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 49,
        "text": "41 \ntext in this process. Starting with 313 sentences, the process resulted in a \nfinal count of 111 sentences, with 22 containing references and 89 not \ncontaining. \nRegarding the third document, the cosine similarity process eliminated \nroughly 23% of the sentences with references (7 out of 30) and 89% of \nthose without references (302 out of 339). After the similarity search, only \n23 sentences were left, of which 23 contained references and 37 did not. \nAnd regarding the input and threshold for the cosine similarity, the \nchosen input “Enligt standard SS EN ISO 555” and threshold 45%. \nWorked well to remove unwanted sentences, having high percentages in \nremoval for the unwanted texts. But also having a bit too high percentage \nwhen removing sentences containing references in the second and third \ndocument which can end up losing data. This could also be a \nconsequence of the sentences being too large, since there are shorter and \nlonger sentences is it hard to find a general input to include both the short \nand long ones. And also, that the long sentences can vary a lot, and the \nshorter ones are more limited. \nWhen analyzing the removed sentences for the second document, it is \nshown that the reason behind three references not getting identified are \nbecause their sentences were not matched with the cosine similarity, and \nare only presented in one sentence each. \nAnd lastly the results from the LLMs. Starting with the model gpt-4-\nturbo-2024-04-09, this model extracted a total of 42 out of 43 out of all the \ndocuments where 41 of the 42 were good quality meaning one missed \nsome information. While the gpt-3.5-turbo-0125 model managed to \nextract 41 out of the 43 where 31 was good quality. These results show \nthat gpt-4-turbo performed best, both at identifying external references \nand at including more details and also gaining higher F1 scores on each \ndocument. \n6.2 \nWork method discussion \nThe initial phase, the pre-study, was crucial in laying the groundwork \nfor the entire project. During this phase, I conducted extensive literature \nreviews and explored various methods for text extraction, embedding \ngeneration, and similarity searches. This helped me identify the most \nsuitable tools and techniques for implementing the project. \n",
        "word_count": 366,
        "char_count": 2282,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 50,
        "text": "42 \nThe second phase, extracting text was done and implemented \nsuccessfully. The third phase, used the extracted text to be divided into \nsections based on sentences. This was also implemented successfully \nalthough the sizes of the sentences can vary a lot. For instance, if the \nsentences instead would have been divided as a fixed size, meaning that \nevery section contains the same number of characters. This method \nwould have allowed each section to be more focused on the external \nreference but could remove some contextual capabilities since important \nwords around the reference could get cut from that section. But using \nthis method does also mean that the number of sections could become a \nlot so preparing structured tests would require more time.  \nThe next phase, generating embeddings was also implemented \nsuccessfully. The chosen model “text-embedding-3-small” by OpenAI \nshowed good performance by for the most part separating the sentences \ncontaining references from the sentences without. The performance on \nthis phase is also dependent on what method that was chosen in the \nprevious phase. Since the chosen embeddings model takes context into \naccount is it more reasonable to use sentences as each section to include \nthe relations between the words. If the text instead was divided by a fixed \nsmaller size a context-applying model could generate more inaccurate \nembeddings since important words in sentences could have been \nremoved from that section.  \nNext phase, the cosine similarity, was also implemented and successful. \nBy removing at least 85% unwanted sentences on each document with \nthe input “Enligt standard SS EN ISO 5555” and a 45% threshold. Better \nperformance was of course possible, but since the length and content of \nsentences can vary does it make it difficult to find a general input that \ncan match with all possible sentences. \nAn alternative to this phase could be a machine learning model for text \nclassification. But this approach would require more time since more \ndata to train the model would be needed. \nThe last phase was also successful. Both models showed great \nperformance while gpt-4-turbo performed better with more detailed \noutput. It successfully extracted 42 out of 43 references since 3 got lost in \nthe previous phase, with 41 being optimal and the remaining missing \n",
        "word_count": 369,
        "char_count": 2348,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 51,
        "text": "43 \nsome information. While the gpt-3.5-turbo-0125 model extracted 41 out \nof 43 references, with 31 matching the expected output perfectly.  \nA significant reason for the results is the detailed prompt provided to the \nLLMs. When constructing a prompt, it is crucial to make it as clear and \nsimple as possible while also providing all the necessary information for \nwhat needs to be extracted. Typically, a prompt starts small to see the \nbasic information that can be extracted. And then gradually add more \ndetails. However, it is important not to add too many details at once, as \nthe model can be sensitive to overly complex prompts or just incorrectly \nphrased sentences.  Which is the case in my prompt, even though \nmentioning that no ‘SSG’ documents should be included. Were they \nincluded a total of 6 times. Meaning, that sentence should have been \nreformatted to make it more clear for the LLMs. Therefore, continuous \ntesting is essential to ensure optimal performance. \n6.3 \nScientific discussion \nAnswering and Discussing Research Questions: \n1. How does the accuracy differentiate between the models when \ncomparing the amount of identified external references? \nThe first model, gpt-4-turbo-2024-04-09 managed to extract a total of 42 \nout of 43 getting an accuracy of roughly 97.7%. While the gpt-3.5-turbo-\n0125 model successfully extracted 41 out of 43 references giving it an \naccuracy of about 95.3%. Meaning the gpt-4-turbo-2024-04-09 model \ngained a slightly higher accuracy with about 2 percentage points. \n2. How does the quality differentiate between the models when \ncomparing the identified external references? \nFrom the gpt-4-turbo-2024-04-09 model 41 out of the extracted 42 was \noptimal outputs resulting in 97.6% containing the expected information. \nWhile 31 out of 41 were optimal outputs from the gpt-3.5-turbo-0125 \nmodel resulting in roughly 75.6%. This indicates that gpt-4-turbo-2024-\n04-09 provided a more detailed output with an improvement of about 22 \npercentage points over gpt-3.5-turbo-0125. \n \n \n",
        "word_count": 308,
        "char_count": 2046,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "ArialMT (12.0pt)",
          "Arial-BoldMT (14.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 52,
        "text": "44 \nThe scientific knowledge gained from this thesis are the following. \n1. The preprocess needs to be implemented properly to not lose any \ndata. \n2. GPT 4 turbo showed the best performance, both in quantity and \nquality when given the instruction to extract data. \nThe first point is more general, since the preprocess phase is crucial for \nhow accurate the text gets filtered before sent to a LLM, in my case this \nwas the divide text, generate embeddings and cosine similarity phases. \nWhile the second point is more project specific, for it to be more general \nmore and diverse tests would have to be made. \n6.4 \nConsequence analysis \nThe consequence of this work is that, although not optimal \nconfigurations, show that it is possible to extract detailed data from \nunstructured sources. Since this work was done in cooperation with the \ncompany Standard Solution Group (SSG) I recommend them to continue \npursuing research on LLMs and how it could be used in other cases. \n6.5 \nEthical and societal discussion \nA significant ethical issue in the development of LLMs is the way training \ndata is collected. There have been instances where large corporations \nhave been accused of data theft during this process. For example, the \nnews organization New York Times has sued both OpenAI and \nMicrosoft over the unauthorized use of their content for training LLMs, \nand there are several more. \nAnother critical area is the possibility of harmful content generation. \nSince LLMs are powerful and capable of generating coherent and \ncontextually appropriate text, they can also produce biased, offensive, or \nharmful content. This raises questions about the responsibility of \ndevelopers to implement robust filtering and monitoring mechanisms to \nprevent these behaviors. Additionally, the training data itself may \ncontain inherent biases, which can be reflected in the model’s outputs. \nThe integration of LLMs into various sectors have the potential to \nautomate complex tasks, improving efficiency, and providing advanced \nanalytical capabilities. For example, in document processing, LLMs can \n",
        "word_count": 325,
        "char_count": 2101,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "ArialMT (12.0pt)",
          "Arial-BoldMT (14.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 53,
        "text": "45 \nsignificantly speed up the extraction and analysis of information, leading \nto faster decision-making and productivity gains. \n",
        "word_count": 17,
        "char_count": 131,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 54,
        "text": "46 \n7 \nConclusions  \nThis chapter presents an answer to the problem statement, objective of \nthe thesis and research questions to conclude this whole work. \nSo how can LLMs be utilized to accurately convert unstructured datasets \ninto structured datasets? To handle larger documents is preprocessing \nnecessary due to the context size limitation associated with Large \nLanguage Models (LLMs), meaning they can only process a fixed \nnumber of tokens at a time. The process begins with extracting text from \nthe document, which is then divided into sections, with sentences being \nthe chosen method for division. Embeddings are generated for each \nsentence, converting them into lists of numerical values between -1 and \n1 that capture words and context. Next, cosine similarity is performed \nwith an input text and a threshold to filter out sentences that do not \nmatch the input. The matched text is then sent to a Large Language \nModel (LLM), which has been given detailed instructions to extract \nexternal references. \nThe objective of this thesis was to successfully identify 70% of the \nreferences from one of the three test documents and compare the \nperformance of two Large Language Models (LLMs). The results for the \nthree test documents demonstrate that the gpt-4-turbo-2024-04-09 model \ngenerally outperformed the gpt-3.5-turbo-0125 model in terms of \naccuracy and detail. For the first document, gpt-4-turbo identified 30 out \nof 31 references correctly (≈97% accuracy) with an F1 score of ≈0.92. In \ncomparison, gpt-3.5-turbo identified 29 out of 31 references correctly (≈87% \naccuracy) with an F1 score of 0.95. For the second document, gpt-4-turbo \nidentified 10 out of 10 possible references correctly (100% accuracy) with \nan F1 score of 1 because the cosine similarity removed three references \nfrom the text. gpt-3.5-turbo also identified 10 out of 10 references \ncorrectly (100% accuracy) with an F1 score of ≈0.95. For the third \ndocument, gpt-4-turbo identified all 2 references correctly (100% \naccuracy) with an F1 score of 1. gpt-3.5-turbo identified 2 out of 2 \nreferences correctly but with 1 incorrect prediction, resulting in an F1 \nscore of 0.80. Meaning that the objective of this thesis was achieved. \nThe first research question was about “How do the accuracy differentiate \nbetween the models when comparing the amount of identified external \nreferences”, this study shows the following. The first model, gpt-4-turbo-\n",
        "word_count": 376,
        "char_count": 2453,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (22.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 55,
        "text": "47 \n2024-04-09, managed to extract a total of 42 out of 43 references, achieving \nan accuracy of roughly 97.7%. In comparison, the gpt-3.5-turbo-0125 \nmodel successfully extracted 41 out of 43 references, resulting in an \naccuracy of approximately 95.3%. This indicates that the gpt-4-turbo-\n2024-04-09 model achieved a slightly higher accuracy, with an \nimprovement of about 2 percentage points. \nAnd regarding the second research question “How do the quality \ndifferentiate between the models when comparing the identified external \nreferences”. Was 41 out of the extracted 42 outputs were optimal from \nthe gpt-4-turbo-2024-04-09 model, resulting in 97.6% containing the \nexpected information. In comparison, 31 out of 41 outputs from the gpt-\n3.5-turbo-0125 model were optimal, resulting in approximately 75.6%. \nThis indicates that gpt-4-turbo-2024-04-09 provided a more detailed \noutput, with an improvement of about 20 percentage points over gpt-3.5-\nturbo-0125. \n7.1 \nOptimize this project \nI believe a better result for sure is possible by optimizing the method for \ndividing the text to something fixed and relatively small, the input text \nat the cosine similarity step and the instruction given to the LLMs. And \nthe result should give a more accurate view of the maximum possible \nperformance of LLMs. \n7.2 \nExplore open-source models \nExplore possible open-source embeddings models and LLMs that can \nreplace OpenAI's models and be run locally to be more in control of how \ndata is being handled and stored. \n7.3 \nNatural language preprocess \nAnother possible method instead of LLMs could be natural language \npreprocessing (NLP) with Named Entity Recognition (NER). This is an \nalternative for extracting external references but would require a own \ntrained model since this is a specific problem. NER makes it easy to \nextract data by just giving the model an input text and the output will be \na list text with its associated tag for example external reference. This is \nnot as complex but also not as flexible when compared to the possibilities \nof an LLM. \n",
        "word_count": 319,
        "char_count": 2076,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (14.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 56,
        "text": "48 \nReferences \n[1]  IBM, “What are large language models (LLMs)?” [Online], \nAvailable: https://www.ibm.com/topics/large-language-models \n[Accessed May 2, 2024]. \n \n[2]  Journal of Machine Learning Research, “Visualizing Data using t-\nSNE,” \n[Online], \nAvailable: \nhttps://jmlr.org/papers/volume9/vandermaaten08a/vandermaate\nn08a.pdf \n[Accessed May 2, 2024]. \n \n[3]  L. Tunstall, L. von Werra, and T. Wolf, \"From Text to Tokens\" in \nNatural Language Processing with Transformers. Sebastopol, CA, \nUSA: O'Reilly Media, Inc., 2022, pp. 30-34. \n \n[4]  Stanford University, “Speech and Language Processing (3rd ed. \ndraft)”, \n[Online], \nAvailable: \nhttps://web.stanford.edu/~jurafsky/slp3/ed3book.pdf   \n[Accessed May 3, 2024]. \n \n[5]  Wayne Xin Zhao, Kun Zhou*, Junyi Li*, Tianyi Tang, Xiaolei \nWang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, \nZican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, \nJinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, \nPeiyu Liu, Jian-Yun Nie and Ji-Rong Wen, “A Survey of Large \nLanguage \nModels”, \n[Online], \nAvailable: \nhttps://arxiv.org/pdf/2303.18223   \n[Accessed May 3, 2024]. \n \n[6]  Humza Naveeda, Asad Ullah Khana, Shi Qiub, Muhammad \nSaqibc, Saeed Anware, Muhammad Usmane, Naveed Akhtarg, \nNick Barnesh, Ajmal Miani, “A Comprehensive Overview of \nLarge \nLanguage \nModels”, \n[Online], \nAvailable: \nhttps://arxiv.org/pdf/2307.06435 \n[Accessed May 4, 2024]. \n \n[7]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, \nLlion Jones, Aidan N. Gomez, Łukasz Kaiser and Illia Polosukhin, \n",
        "word_count": 194,
        "char_count": 1559,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "ArialMT (12.0pt)",
          "Arial-BoldMT (22.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 57,
        "text": "49 \n“Attention \nIs \nAll \nYou \nNeed”, \n[Online], \nAvailable: \nhttps://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee2\n43547dee91fbd053c1c4a845aa-Paper.pdf \n[Accessed May 4, 2024]. \n \n \n[8]  Daniel Jurafsky and James H. Martin, “Speech and Language \nProcessing \n(3rd \ned. \ndraft)”, \n[Online], \nAvailable: \nhttps://web.stanford.edu/~jurafsky/slp3/ed3book.pdf   \n[Accessed May 4, 2024]. \n \n[9]  David M.W. Powers, “What the F-measure doesn’t measure…”, \n[Online], Available: https://arxiv.org/pdf/1503.06410 \n[Accessed May 7, 2024]. \n \n[10]  \nGeeksforGeeks, “F1 Score in Machine Learning”, [Online], \nAvailable: \nhttps://www.geeksforgeeks.org/f1-score-in-machine-\nlearning/   \n[Accessed May 7, 2024]. \n  \n[11]  \nHuaxia Li, Haoyun Gao, Chengzhang Wu and Miklos A. \nVasarheli, “Extracting Financial Data from Unstructured Sources: \nLeveraging Large Language Models”, [Online], Available: \nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4567607   \n[Accessed May 5, 2024]. \n  \n[12]  \nDipali Baviskar, Swati Ahirrao, Vidyasagar Potdar and \nKetan Kotecha, “Efficient Automated Processing of the \nUnstructured Documents Using Artificial Intelligence: A \nSystematic Literature Review and Future Directions”, [Online], \nAvailable: https://ieeexplore.ieee.org/abstract/document/9402739 \n[Accessed May 6, 2024]. \n  \n[13]  \nPyPI, \n“PyPDF2”, \n[Online], \nAvailable: \nhttps://pypi.org/project/PyPDF2/ \n[Accessed May 6, 2024]. \n \n[14]  \nNLTK, \n“nltk.tokenize”, \n[Online], \nAvailable: \nhttps://www.nltk.org/api/nltk.tokenize.html \n",
        "word_count": 144,
        "char_count": 1525,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "ArialMT (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 58,
        "text": "50 \n[Accessed May 6, 2024]. \n  \n[15]  \nOpenAI, \n“Embeddings”, \n[Online], \nAvailable: \nhttps://platform.openai.com/docs/guides/embeddings \n[Accessed May 5, 2024]. \n  \n[16]  \nOpenAI, \n“Python \nLibrary”, \n[Online], \nAvailable: \nhttps://platform.openai.com/docs/libraries/python-library \n[Accessed May 6, 2024]. \n  \n[17]  \nscikit-learn, \n“sklearn.metrics.pairwise.cosine_similarity”, \n[Online], \nAvailable: \nhttps://scikit-\nlearn.org/stable/modules/generated/sklearn.metrics.pairwise.cosi\nne_similarity.html \n[Accessed May 6, 2024]. \n  \n[18]  \nOpenAI, “API Reference - Chat Completion”, [Online], \nAvailable: \nhttps://platform.openai.com/docs/api-\nreference/chat/create#chat-create-temperature \n[Accessed May 21, 2024]. \n  \n[19]  \nOpenAI, “GPT-4 Turbo and GPT-4”, [Online], Available: \nhttps://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4 \n[Accessed May 16, 2024]. \n \n[20]  \nOpenAI, \n“Tokenizer”, \n[Online], \nAvailable: \nhttps://platform.openai.com/tokenizer \n[Accessed May 13, 2024].\n",
        "word_count": 76,
        "char_count": 990,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "ArialMT (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      },
      {
        "page_number": 59,
        "text": "51 \nAppendix A: Source Code \nhttps://github.com/Lurre13/Examensarbete-SSG  \n",
        "word_count": 6,
        "char_count": 76,
        "fonts": [
          "PalatinoLinotype-Roman (12.0pt)",
          "Arial-BoldMT (22.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.5,
          842.0
        ]
      }
    ]
  }
}