{
  "file_path": "..\\test_pdfs\\2402.06196v3.pdf",
  "file_hash": "26fae06605dc7ee23b9c6eabeadcd9cf",
  "timestamp": "2025-06-17T20:49:56.242465",
  "page_count": 44,
  "total_words": 32034,
  "total_chars": 209028,
  "fonts_used": [
    "Arial-BoldItalicMT (6.9pt)",
    "Arial-BoldMT (2.9pt)",
    "Arial-BoldMT (3.3pt)",
    "Arial-BoldMT (3.7pt)",
    "Arial-BoldMT (4.1pt)",
    "Arial-BoldMT (4.5pt)",
    "Arial-BoldMT (4.8pt)",
    "Arial-BoldMT (4.9pt)",
    "Arial-BoldMT (5.2pt)",
    "Arial-BoldMT (5.7pt)",
    "Arial-BoldMT (6.0pt)",
    "Arial-BoldMT (6.3pt)",
    "Arial-BoldMT (6.9pt)",
    "Arial-BoldMT (8.2pt)",
    "Arial-BoldMT (9.8pt)",
    "ArialMT (3.2pt)",
    "ArialMT (3.8pt)",
    "ArialMT (4.1pt)",
    "ArialMT (4.8pt)",
    "ArialMT (5.2pt)",
    "ArialMT (5.4pt)",
    "ArialMT (6.5pt)",
    "ArialMT (6.9pt)",
    "ArialMT (7.5pt)",
    "ArialMT (8.2pt)",
    "CMEX10 (10.0pt)",
    "CMMI10 (10.0pt)",
    "CMMI5 (5.0pt)",
    "CMMI7 (7.0pt)",
    "CMMI8 (8.0pt)",
    "CMR10 (10.0pt)",
    "CMR6 (6.0pt)",
    "CMR7 (7.0pt)",
    "CMR8 (8.0pt)",
    "CMSY10 (10.0pt)",
    "CMSY6 (6.0pt)",
    "CMSY7 (7.0pt)",
    "ComicSansMS (6.5pt)",
    "MSAM10 (10.0pt)",
    "MSAM7 (7.0pt)",
    "MSBM10 (10.0pt)",
    "NimbusRomNo9L-Medi (10.0pt)",
    "NimbusRomNo9L-Medi (7.0pt)",
    "NimbusRomNo9L-Medi (9.0pt)",
    "NimbusRomNo9L-MediItal (10.0pt)",
    "NimbusRomNo9L-MediItal (9.0pt)",
    "NimbusRomNo9L-Regu (10.0pt)",
    "NimbusRomNo9L-Regu (11.0pt)",
    "NimbusRomNo9L-Regu (23.9pt)",
    "NimbusRomNo9L-Regu (6.0pt)",
    "NimbusRomNo9L-Regu (7.0pt)",
    "NimbusRomNo9L-Regu (8.0pt)",
    "NimbusRomNo9L-Regu (9.0pt)",
    "NimbusRomNo9L-ReguItal (10.0pt)",
    "NimbusRomNo9L-ReguItal (8.0pt)",
    "Times-Roman (20.0pt)",
    "rsfs10 (10.0pt)"
  ],
  "images_count": 50,
  "metadata": {
    "format": "PDF 1.5",
    "creator": "LaTeX with hyperref",
    "producer": "pdfTeX-1.40.25",
    "creationDate": "D:20250325010006Z",
    "modDate": "D:20250325010006Z"
  },
  "pages": [
    {
      "page_number": 1,
      "text": "Large Language Models: A Survey\nShervin Minaee1, Tomas Mikolov2, Narjes Nikzad3, Meysam Chenaghlu4\nRichard Socher5, Xavier Amatriain6, Jianfeng Gao7\n1 Applied Scientist, Amazon Inc\n2 Senior Researcher, CIIRC CTU\n3 Cologne University of Applied Sciences\n4 Staff Machine Learning Scientist, Ultimate.ai\n5 CEO, You.com\n6 VP of Product, AI and Compute Enablement, Google Inc\n7 VP of Deep Learning Group, Microsoft Research\nAbstract—Large Language Models (LLMs) have drawn a\nlot of attention due to their strong performance on a wide\nrange of natural language tasks, since the release of ChatGPT\nin November 2022. LLMs’ ability of general-purpose language\nunderstanding and generation is acquired by training billions of\nmodel’s parameters on massive amounts of text data, as predicted\nby scaling laws [1], [2]. The research area of LLMs, while very\nrecent, is evolving rapidly in many different ways. In this paper,\nwe review some of the most prominent LLMs, including three\npopular LLM families (GPT, LLaMA, PaLM), and discuss their\ncharacteristics, contributions and limitations. We also give an\noverview of techniques developed to build, and augment LLMs.\nWe then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation\nmetrics, and compare the performance of several popular LLMs\non a set of representative benchmarks. Finally, we conclude\nthe paper by discussing open challenges and future research\ndirections.\nI.\nINTRODUCTION\nLanguage modeling is a long-standing research topic, dat-\ning back to the 1950s with Shannon’s application of informa-\ntion theory to human language, where he measured how well\nsimple n-gram language models predict or compress natural\nlanguage text [3]. Since then, statistical language modeling\nbecame fundamental to many natural language understanding\nand generation tasks, ranging from speech recognition, ma-\nchine translation, to information retrieval [4], [5], [6].\nThe recent advances on transformer-based large language\nmodels (LMs), pretrained on Web-scale text corpora, signifi-\ncantly extended the capabilities of language models (LLMs).\nFor example, OpenAI’s ChatGPT and GPT-4 can be used not\nonly for natural language processing, but also as general task\nsolvers to power Microsoft’s Co-Pilot systems, for instance,\ncan follow human instructions of complex new tasks per-\nforming multi-step reasoning when needed. LLMs are thus\nbecoming the basic building block for the development of\ngeneral-purpose AI agents or artificial general intelligence\n(AGI).\nAs the field of LLMs is moving fast, with new findings,\nmodels and techniques being published in a matter of months\nor weeks [7], [8], [9], [10], [11], AI researchers and practi-\ntioners often find it challenging to figure out the best recipes\nto build LLM-powered AI systems for their tasks. This paper\ngives a timely survey of the recent advances on LLMs. We\nhope this survey will prove a valuable and accessible resource\nfor students, researchers and developers.\nLLMs are large-scale, pre-trained, statistical language mod-\nels based on neural networks. The recent success of LLMs is\nan accumulation of decades of research and development of\nlanguage models, which can be categorized into four waves\nthat have different starting points and velocity: statistical lan-\nguage models, neural language models, pre-trained language\nmodels and LLMs.\nStatistical language models (SLMs) view text as a sequence\nof words, and estimate the probability of text as the product\nof their word probabilities. The dominating form of SLMs\nare Markov chain models known as the n-gram models,\nwhich compute the probability of a word conditioned on its\nimmediate proceeding n − 1 words. Since word probabilities\nare estimated using word and n-gram counts collected from\ntext corpora, the model needs to deal with data sparsity (i.e.,\nassigning zero probabilities to unseen words or n-grams) by\nusing smoothing, where some probability mass of the model\nis reserved for unseen n-grams [12]. N-gram models are\nwidely used in many NLP systems. However, these models\nare incomplete in that they cannot fully capture the diversity\nand variability of natural language due to data sparsity.\nEarly neural language models (NLMs) [13], [14], [15], [16]\ndeal with data sparsity by mapping words to low-dimensional\ncontinuous vectors (embedding vectors) and predict the next\nword based on the aggregation of the embedding vectors of\nits proceeding words using neural networks. The embedding\nvectors learned by NLMs define a hidden space where the\nsemantic similarity between vectors can be readily computed\nas their distance. This opens the door to computing semantic\nsimilarity of any two inputs regardless their forms (e.g., queries\nvs. documents in Web search [17], [18], sentences in different\nlanguages in machine translation [19], [20]) or modalities (e.g.,\nimage and text in image captioning [21], [22]). Early NLMs are\ntask-specific models, in that they are trained on task-specific\ndata and their learned hidden space is task-specific.\nPre-trained language models (PLMs), unlike early NLMs,\nare task-agnostic. This generality also extends to the learned\narXiv:2402.06196v3  [cs.CL]  23 Mar 2025\n",
      "word_count": 791,
      "char_count": 5225,
      "fonts": [
        "NimbusRomNo9L-MediItal (9.0pt)",
        "NimbusRomNo9L-Regu (11.0pt)",
        "NimbusRomNo9L-Medi (9.0pt)",
        "NimbusRomNo9L-Regu (23.9pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMMI10 (10.0pt)",
        "Times-Roman (20.0pt)",
        "CMSY10 (10.0pt)",
        "CMR8 (8.0pt)",
        "CMR10 (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 2,
      "text": "hidden embedding space. The training and inference of PLMs\nfollows the pre-training and fine-tuning paradigm, where lan-\nguage models with recurrent neural networks [23] or trans-\nformers [24], [25], [26] are pre-trained on Web-scale unlabeled\ntext corpora for general tasks such as word prediction, and then\nfinetuned to specific tasks using small amounts of (labeled)\ntask-specific data. Recent surveys on PLMs include [8], [27],\n[28].\nLarge language models mainly refer to transformer-based\nneural language models 1 that contain tens to hundreds of\nbillions of parameters, which are pre-trained on massive text\ndata, such as PaLM [31], LLaMA [32], and GPT-4 [33], as\nsummarized in Table III. Compared to PLMs, LLMs are not\nonly much larger in model size, but also exhibit stronger\nlanguage understanding and generation abilities, and more\nimportantly, emergent abilities that are not present in smaller-\nscale language models. As illustrated in Fig. 1, these emergent\nabilities include (1) in-context learning, where LLMs learn\na new task from a small set of examples presented in the\nprompt at inference time, (2) instruction following, where\nLLMs, after instruction tuning, can follow the instructions\nfor new types of tasks without using explicit examples, and\n(3) multi-step reasoning, where LLMs can solve a complex\ntask by breaking down that task into intermediate reasoning\nsteps as demonstrated in the chain-of-thought prompt [34].\nLLMs can also be augmented by using external knowledge\nand tools [35], [36] so that they can effectively interact with\nusers and environment [37], and continually improve itself\nusing feedback data collected through interactions (e.g. via\nreinforcement learning with human feedback (RLHF)).\nThrough advanced usage and augmentation techniques,\nLLMs can be deployed as so-called AI agents: artificial entities\nthat sense their environment, make decisions, and take actions.\nPrevious research has focused on developing agents for specific\ntasks and domains. The emergent abilities demonstrated by\nLLMs make it possible to build general-purpose AI agents\nbased on LLMs. While LLMs are trained to produce responses\nin static settings, AI agents need to take actions to interact with\ndynamic environment. Therefore, LLM-based agents often\nneed to augment LLMs to e.g., obtain updated information\nfrom external knowledge bases, verify whether a system action\nproduces the expected result, and cope with when things do\nnot go as expected, etc. We will discuss in detail LLM-based\nagents in Section IV.\nIn the rest of this paper, Section II presents an overview of\nstate of the art of LLMs, focusing on three LLM families (GPT,\nLLaMA and PaLM) and other representative models. Section\nIII discusses how LLMs are built. Section IV discusses how\nLLMs are used, and augmented for real-world applications\nSections V and VI review popular datasets and benchmarks for\nevaluating LLMs, and summarize the reported LLM evaluation\nresults. Finally, Section VII concludes the paper by summa-\nrizing the challenges and future research directions.\nII.\nLARGE LANGUAGE MODELS\nIn this section we start with a review of early pre-trained\nneural language models as they are the base of LLMs, and\n1Recently, several very promising non-transformer LLMs have been pro-\nposed, such as the LLMs based on structured state space models [29], [30].\nSee Section VII for more details.\nthen focus our discussion on three families of LLMs: GPT,\nLlaMA, and PaLM. Table I provides an overview of some of\nthese models and their characteristics.\nA. Early Pre-trained Neural Language Models\nLanguage modeling using neural networks was pioneered\nby [38], [39], [40]. Bengio et al. [13] developed one of the first\nneural language models (NLMs) that are comparable to n-gram\nmodels. Then, [14] successfully applied NLMs to machine\ntranslation. The release of RNNLM (an open source NLM\ntoolkit) by Mikolov [41], [42] helped significantly popularize\nNLMs. Afterwards, NLMs based on recurrent neural networks\n(RNNs) and their variants, such as long short-term memory\n(LSTM) [19] and gated recurrent unit (GRU) [20], were widely\nused for many natural language applications including machine\ntranslation, text generation and text classification [43].\nThen, the invention of the Transformer architecture [44]\nmarks another milestone in the development of NLMs. By\napplying self-attention to compute in parallel for every word\nin a sentence or document an “attention score” to model the\ninfluence each word has on another, Transformers allow for\nmuch more parallelization than RNNs, which makes it possible\nto efficiently pre-train very big language models on large\namounts of data on GPUs. These pre-trained language models\n(PLMs) can be fine-tuned for many downstream tasks.\nWe group early popular Transformer-based PLMs, based on\ntheir neural architectures, into three main categories: encoder-\nonly, decoder-only, and encoder-decoder models. Comprehen-\nsive surveys of early PLMs are provided in [43], [28].\n1) Encoder-only PLMs: As the name suggests, the encoder-\nonly models only consist of an encoder network. These models\nare originally developed for language understanding tasks,\nsuch as text classification, where the models need to predict a\nclass label for an input text. Representative encoder-only mod-\nels include BERT and its variants, e.g., RoBERTa, ALBERT,\nDeBERTa, XLM, XLNet, UNILM, as to be described below.\nBERT (Birectional Encoder Representations from Trans-\nformers) [24] is one of the most widely used encoder-only\nlanguage models. BERT consists of three modules: (1) an\nembedding module that converts input text into a sequence\nof embedding vectors, (2) a stack of Transformer encoders\nthat converts embedding vectors into contextual representation\nvectors, and (3) a fully connected layer that converts the\nrepresentation vectors (at the final layer) to one-hot vectors.\nBERT is pre-trained uses two objectives: masked language\nmodeling (MLM) and next sentence prediction. The pre-trained\nBERT model can be fine-tuned by adding a classifier layer\nfor many language understanding tasks, ranging from text\nclassification, question answering to language inference. A\nhigh-level overview of BERT framework is shown in Fig 3. As\nBERT significantly improved state of the art on a wide range\nof language understanding tasks when it was published, the AI\ncommunity was inspired to develop many similar encoder-only\nlanguage models based on BERT.\nRoBERTa [25] significantly improves the robustness of\nBERT using a set of model design choices and training strate-\ngies, such as modifying a few key hyperparameters, removing\nthe next-sentence pre-training objective and training with much\n",
      "word_count": 1017,
      "char_count": 6695,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "NimbusRomNo9L-Regu (7.0pt)",
        "NimbusRomNo9L-Regu (6.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 3,
      "text": "Emerging\nBasic\nAugmented\nLLM Capabilities\nReasoning\nCoding\nComprehension\nMultilingual\nTool\nutilization\nWorld\nknowledge\nInstruction\nfollowing\nIn-context\nlearning\nInteracting\nwith users\nSelf-improvement\nMulti choice QA\nWikipedia QA\nXNLI\nCrosslingual QA\nCrosslingual Tasks\nTranslation\nReading Comprehension\nMulti choice QA\nBoolean QA\nSimplification\nSummarization\nFunction Calling\nAPI calling\nLogical\nSymbolic\nCommon Sense\nArithmetic\nTurn based\nCompletion\nTask definition\nFew-shot\nSymbolic\nreference\nPos/Neg example\nStep by step\nsolving\nTool planning\nTask\ndecomposition\nVirtual acting\nPhysical acting\nKnowledge base\nutilization\nAssignment\nplanning\nSelf-cirtisim\nSelf-refinement\nFig. 1: LLM Capabilities.\nlarger mini-batches and learning rates. ALBERT [45] uses two\nparameter-reduction techniques to lower memory consumption\nand increase the training speed of BERT: (1) splitting the\nembedding matrix into two smaller matrices, and (2) using\nrepeating layers split among groups. DeBERTa (Decoding-\nenhanced BERT with disentangled attention) [26] improves the\nBERT and RoBERTa models using two novel techniques. The\nfirst is the disentangled attention mechanism, where each word\nis represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words\nare computed using disentangled matrices on their contents and\nrelative positions, respectively. Second, an enhanced mask de-\ncoder is used to incorporate absolute positions in the decoding\nlayer to predict the masked tokens in model pre-training. In\naddition, a novel virtual adversarial training method is used for\nfine-tuning to improve models’ generalization. ELECTRA [46]\nuses a new pre-training task, known as replaced token detection\n(RTD), which is empirically proven to be more sample-efficient\nthan MLM. Instead of masking the input, RTD corrupts it by\nreplacing some tokens with plausible alternatives sampled from\na small generator network. Then, instead of training a model\nthat predicts the original identities of the corrupted tokens, a\ndiscriminative model is trained to predict whether a token in\nthe corrupted input was replaced by a generated sample or not.\nRTD is more sample-efficient than MLM because the former\nis defined over all input tokens rather than just the small subset\nbeing masked out, as illustrated in Fig 4.\nXLMs [47] extended BERT to cross-lingual language\nmodels using two methods: (1) a unsupervised method that\nonly relies on monolingual data, and (2) a supervised method\nthat leverages parallel data with a new cross-lingual language\nmodel objective, as illustrated in Fig 5. XLMs had obtained\nstate-of-the-art results on cross-lingual classification, unsuper-\nvised and supervised machine translation, at the time they were\nproposed.\nThere are also encoder-only language models that leverage\nthe advantages of auto-regressive (decoder) models for model\ntraining and inference. Two examples are XLNet and UNILM.\nXLNet [48] is based on Transformer-XL, pre-trained using a\ngeneralized autoregressive method that enables learning bidi-\nrectional contexts by maximizing the expected likelihood over\nall permutations of the factorization order. UNILM (UNIfied\npre-trained Language Model) [49] is pre-trained using three\ntypes of language modeling tasks: unidirectional, bidirectional,\nand sequence-to-sequence prediction. This is achieved by\nemploying a shared Transformer network and utilizing specific\nself-attention masks to control what context the prediction is\nconditioned on, as illustrated in Fig 6. The pre-trained model\ncan be fine-tuned for both natural language understanding and\ngeneration tasks.\n2) Decoder-only PLMs: Two of the most widely used\ndecoder-only PLMs are GPT-1 and GPT-2, developed by\nOpenAI. These models lay the foundation to more powerful\nLLMs subsequently, i.e., GPT-3 and GPT-4.\nGPT-1 [50] demonstrates for the first time that good\nperformance over a wide range of natural language tasks can be\nobtained by Generative Pre-Training (GPT) of a decoder-only\nTransformer model on a diverse corpus of unlabeled text in a\nself-supervised learning fashion (i.e., next word/token predic-\ntion), followed by discriminative fine-tuning on each specific\ndownstream task (with much fewer samples), as illustrated in\nFig 7. GPT-1 paves the way for subsequent GPT models, with\neach version improving upon the architecture and achieving\nbetter performance on various language tasks.\nGPT-2 [51] shows that language models are able to learn\nto perform specific natural language tasks without any explicit\nsupervision when trained on a large WebText dataset consisting\nof millions of webpages. The GPT-2 model follows the model\ndesigns of GPT-1 with a few modifications: Layer normal-\nization is moved to the input of each sub-block, additional\nlayer normalization is added after the final self-attention block,\n",
      "word_count": 695,
      "char_count": 4858,
      "fonts": [
        "Arial-BoldMT (3.7pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "Arial-BoldMT (2.9pt)",
        "Arial-BoldMT (4.1pt)",
        "Arial-BoldMT (3.3pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "Arial-BoldMT (4.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 4,
      "text": "Paper Strcuture\nEarly Pre-trained\nLanguage Models\nII\nLarge Language Models\nA\nIII\nHOW LLMS ARE BUILT\nA\nData Cleaning\nB\nLarge Language\nModel Families\nB\nOther Representative\nLLMs\nC\nDominant LLM\nArchitectures\nTokenizations\nC\nPositional Encoding\nD\nModel Pre-training\nE\nFine-tuning and\nInstruction Tuning\nF\nAlignment\nG\nDecoding Strategies\nH\nI\nHOW LLMS ARE USED AND AUGMENTED\nA\nB\nLLM limitations\nCost-Effective Training/Inference,\nAdaptation & Compression\nI\nUsing LLMs: Prompt Design\nand Engineering\nC\nAugmenting LLMs through\nexternal knowledge - RAG\nD\nUsing External Tools\nE\nLLM Agents\nV\n POPULAR DATASETS FOR LLMS\nA\nDatasets for Basic Tasks: language\nmodeling/understanding/generation\nB\n Datasets for Emergent: ICL, reasoning,\ninstruction following\nC\nDatasets for Augmented: using\nexternal knowledge/tools\nVI\n PROMINENT LLMS’ PERFORMANCE\nON BENCHMARKS\nA\nB\nVII\nCHALLENGES AND FUTURE DIRECTIONS\nA\nSmaller and more efficient\nLanguage Models\nLLMs’ Performance on Different Tasks\nPopular Metrics for Evaluating LLMs\nB\nNew Post-attention\nArchitectural Paradigms\nC\nMulti-modal Models\nD\nImproved LLM Usage and\nAugmentation techniques\nD\nSecurity and\nEthical/Responsible AI\nFig. 2: The paper structure.\ninitialization is modified to account for the accumulation on\nthe residual path and scaling the weights of residual layers,\nvocabulary size is expanded to 50,25, and context size is\nincreased from 512 to 1024 tokens.\n3) Encoder-Decoder PLMs: In [52], Raffle et al. shows that\nalmost all NLP tasks can be cast as a sequence-to-sequence\ngeneration task. Thus, an encoder-decoder language model, by\ndesign, is a unified model in that it can perform all natural\nlanguage understanding and generation tasks. Representative\nencoder-decoder PLMs we will review below are T5, mT5,\nMASS, and BART.\nT5 [52] is a Text-to-Text Transfer Transformer (T5) model,\nwhere transfer learning is effectively exploited for NLP via an\nintroduction of a unified framework in which all NLP tasks are\ncast as a text-to-text generation task. mT5 [53] is a multilingual\nvariant of T5, which is pre-trained on a new Common Crawl-\nbased dataset consisting of texts in 101 languages.\nMASS (MAsked Sequence to Sequence pre-training) [54]\nadopts the encoder-decoder framework to reconstruct a sen-\ntence fragment given the remaining part of the sentence. The\nencoder takes a sentence with randomly masked fragment\n(several consecutive tokens) as input, and the decoder predicts\nthe masked fragment. In this way, MASS jointly trains the\n",
      "word_count": 375,
      "char_count": 2491,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "Arial-BoldMT (4.5pt)",
        "Arial-BoldMT (6.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "Arial-BoldMT (5.2pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 5,
      "text": "TABLE I: High-level Overview of Popular Language Models\nType\nModel Name\n#Parameters\nRelease\nBase Models\nOpen\nSource\n#Tokens\nTraining dataset\nBERT\n110M, 340M\n2018\n-\n✓\n137B\nBooksCorpus, English Wikipedia\nRoBERTa\n355M\n2019\n-\n✓\n2.2T\nBooksCorpus,\nEnglish\nWikipedia,\nCC-NEWS,\nSTORIES (a subset of Common Crawl), Reddit\nEncoder-Only\nALBERT\n12M,\n18M,\n60M,\n235M\n2019\n-\n✓\n137B\nBooksCorpus, English Wikipedia\nDeBERTa\n-\n2020\n-\n✓\n-\nBooksCorpus, English Wikipedia, STORIES, Red-\ndit content\nXLNet\n110M, 340M\n2019\n-\n✓\n32.89B\nBooksCorpus, English Wikipedia, Giga5, Com-\nmon Crawl, ClueWeb 2012-B\nDecoder-only\nGPT-1\n120M\n2018\n-\n✓\n1.3B\nBooksCorpus\nGPT-2\n1.5B\n2019\n-\n✓\n10B\nReddit outbound\nT5 (Base)\n223M\n2019\n-\n✓\n156B\nCommon Crawl\nEncoder-Decoder\nMT5 (Base)\n300M\n2020\n-\n✓\n-\nNew Common Crawl-based dataset in 101 lan-\nguages (m Common Crawl)\nBART (Base)\n139M\n2019\n-\n✓\n-\nCorrupting text\nGPT-3\n125M,\n350M,\n760M, 1.3B, 2.7B,\n6.7B, 13B, 175B\n2020\n×\n300B\nCommon Crawl (filtered), WebText2, Books1,\nBooks2, Wikipedia\nGPT Family\nCODEX\n12B\n2021\nGPT\n✓\n-\nPublic GitHub software repositories\nWebGPT\n760M, 13B, 175B\n2021\nGPT-3\n×\n-\nELI5\nGPT-4\n1.76T\n2023\n-\n×\n13T\n-\nLLaMA1\n7B, 13B, 33B, 65B\n2023\n-\n✓\n1T, 1.4T\nOnline sources\nLLaMA2\n7B, 13B, 34B, 70B\n2023\n-\n✓\n2T\nOnline sources\nAlpaca\n7B\n2023\nLLaMA1\n✓\n-\nGPT-3.5\nVicuna-13B\n13B\n2023\nLLaMA1\n✓\n-\nGPT-3.5\nLLaMA Family\nKoala\n13B\n2023\nLLaMA\n✓\n-\nDialogue data\nMistral-7B\n7.3B\n2023\n✓\n-\n-\nCode Llama\n34\n2023\nLLaMA2\n✓\n500B\nPublicly available code\nLongLLaMA\n3B, 7B\n2023\nOpenLLaMA\n✓\n1T\n-\nLLaMA-Pro-8B\n8.3B\n2024\nLLaMA2-7B\n✓\n80B\nCode and math corpora\nTinyLlama-1.1B\n1.1B\n2024\nLLaMA1.1B\n✓\n3T\nSlimPajama, Starcoderdata\nPaLM\n8B, 62B, 540B\n2022\n-\n×\n780B\nWeb documents, books, Wikipedia, conversations,\nGitHub code\nU-PaLM\n8B, 62B, 540B\n2022\n-\n×\n1.3B\nWeb documents, books, Wikipedia, conversations,\nGitHub code\nPaLM Family\nPaLM-2\n340B\n2023\n-\n✓\n3.6T\nWeb documents, books, code, mathematics, con-\nversational data\nMed-PaLM\n540B\n2022\nPaLM\n×\n780B\nHealthSearchQA, MedicationQA, LiveQA\nMed-PaLM 2\n-\n2023\nPaLM 2\n×\n-\nMedQA, MedMCQA, HealthSearchQA, LiveQA,\nMedicationQA\nFLAN\n137B\n2021\nLaMDA-PT\n✓\n-\nWeb documents, code, dialog data, Wikipedia\nGopher\n280B\n2021\n-\n×\n300B\nMassiveText\nERNIE 4.0\n10B\n2023\n-\n×\n4TB\nChinese text\nRetro\n7.5B\n2021\n-\n×\n600B\nMassiveText\nLaMDA\n137B\n2022\n-\n×\n168B\npublic dialog data and web documents\nChinChilla\n70B\n2022\n-\n×\n1.4T\nMassiveText\nGalactia-120B\n120B\n2022\n-\n450B\nOther Popular LLMs\nCodeGen\n16.1B\n2022\n-\n✓\n-\nTHE PILE, BIGQUERY, BIGPYTHON\nBLOOM\n176B\n2022\n-\n✓\n366B\nROOTS\nZephyr\n7.24B\n2023\nMistral-7B\n✓\n800B\nSynthetic data\nGrok-0\n33B\n2023\n-\n×\n-\nOnline source\nORCA-2\n13B\n2023\nLLaMA2\n-\n2001B\n-\nStartCoder\n15.5B\n2023\n-\n✓\n35B\nGitHub\nMPT\n7B\n2023\n-\n✓\n1T\nRedPajama, m Common Crawl, S2ORC, Common\nCrawl\nMixtral-8x7B\n46.7B\n2023\n-\n✓\n-\nInstruction dataset\nFalcon 180B\n180B\n2023\n-\n✓\n3.5T\nRefinedWeb\nGemini\n1.8B, 3.25B\n2023\n✓\n-\nWeb documents, books, and code, image data,\naudio data, video data\nDeepSeek-Coder\n1.3B, 6.7B, 33B\n2024\n-\n✓\n2T\nGitHub’s Markdown and StackExchange\nDocLLM\n1B,7B\n2024\n-\n×\n2T\nIIT-CDIP Test Collection 1.0, DocBank\nencoder and decoder for language embedding and generation,\nrespectively.\nBART [55] uses a standard sequence-to-sequence transla-\ntion model architecture. It is pre-trained by corrupting text with\nan arbitrary noising function, and then learning to reconstruct\nthe original text.\nB. Large Language Model Families\nLarge\nlanguage\nmodels\n(LLMs)\nmainly\nrefer\nto\ntransformer-based\nPLMs\nthat\ncontain\ntens\nto\nhundreds\nof billions of parameters. Compared to PLMs reviewed above,\nLLMs are not only much larger in model size, but also exhibit\nstronger language understanding and generation and emergent\nabilities that are not present in smaller-scale models. In what\nfollows, we review three LLM families: GPT, LLaMA, and\nPaLM, as illustrated in Fig 8.\n1) The GPT Family: Generative Pre-trained Transform-\ners (GPT) are a family of decoder-only Transformer-based\nlanguage models, developed by OpenAI. This family con-\nsists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4,\n",
      "word_count": 665,
      "char_count": 3998,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "NimbusRomNo9L-Regu (7.0pt)",
        "NimbusRomNo9L-Medi (7.0pt)",
        "MSAM7 (7.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "CMSY7 (7.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 6,
      "text": "Fig. 3: Overall pre-training and fine-tuning procedures for\nBERT. Courtesy of [24]\nFig. 4: A comparison between replaced token detection and\nmasked language modeling. Courtesy of [46].\nCODEX, and WebGPT. Although early GPT models, such as\nGPT-1 and GPT-2, are open-source, recent models, such as\nGPT-3 and GPT-4, are close-source and can only be accessed\nvia APIs. GPT-1 and GPT-2 models have been discussed in\nthe early PLM subsection. We start with GPT-3 below.\nGPT-3 [56] is a pre-trained autoregressive language model\nwith 175 billion parameters. GPT-3 is widely considered as\nthe first LLM in that not only it is much larger than previous\nPLMs, but also for the first time demonstrates emergent\nabilities that are not observed in previous smaller PLMs. GPT-\n3 shows the emergent ability of in-context learning, which\nmeans GPT-3 can be applied to any downstream tasks without\nany gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the\nmodel. GPT-3 achieved strong performance on many NLP\ntasks, including translation, question-answering, and the cloze\ntasks, as well as several ones that require on-the-fly reasoning\nor domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, 3-digit arithmetic. Fig 9 plots the\nperformance of GPT-3 as a function of the number of examples\nin in-context prompts.\nCODEX [57], released by OpenAI in March 2023, is a\ngeneral-purpose programming model that can parse natural\nlanguage and generate code in response. CODEX is a de-\nscendant of GPT-3, fine-tuned for programming applications\non code corpora collected from GitHub. CODEX powers\nMicrosoft’s GitHub Copilot.\nWebGPT [58] is another descendant of GPT-3, fine-tuned to\nanswer open-ended questions using a text-based web browser,\nfacilitating users to search and navigate the web. Specifically,\nFig. 5: Cross-lingual language model pretraining. The MLM\nobjective is similar to BERT, but with continuous streams\nof text as opposed to sentence pairs. The TLM objective\nextends MLM to pairs of parallel sentences. To predict a\nmasked English word, the model can attend to both the English\nsentence and its French translation, and is encouraged to align\nEnglish and French representations. Courtesy of [47].\nFig. 6: Overview of unified LM pre-training. The model\nparameters are shared across the LM objectives (i.e., bidirec-\ntional LM, unidirectional LM, and sequence-to-sequence LM).\nCourtesy of [49].\nWebGPT is trained in three steps. The first is for WebGPT\nto learn to mimic human browsing behaviors using human\ndemonstration data. Then, a reward function is learned to\npredict human preferences. Finally, WebGPT is refined to\noptimize the reward function via reinforcement learning and\nrejection sampling.\nTo enable LLMs to follow expected human instructions,\nInstructGPT [59] is proposed to align language models with\nuser intent on a wide range of tasks by fine-tuning with\nhuman feedback. Starting with a set of labeler-written prompts\nand prompts submitted through the OpenAI API, a dataset\nof labeler demonstrations of the desired model behavior is\ncollected. Then GPT-3 is fine-tuned on this dataset. Then, a\ndataset of human-ranked model outputs is collected to further\nfine-tune the model using reinforcement learning. The method\nis known Reinforcement Learning from Human Feedback\n(RLHF), as shown in 10. The resultant InstructGPT models\nhave shown improvements in truthfulness and reductions in\ntoxic output generation while having minimal performance\n",
      "word_count": 542,
      "char_count": 3544,
      "fonts": [
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 937,
          "height": 385,
          "ext": "png",
          "size_bytes": 61739
        },
        {
          "index": 1,
          "width": 1090,
          "height": 493,
          "ext": "png",
          "size_bytes": 45688
        },
        {
          "index": 2,
          "width": 965,
          "height": 533,
          "ext": "png",
          "size_bytes": 32350
        },
        {
          "index": 3,
          "width": 825,
          "height": 608,
          "ext": "png",
          "size_bytes": 51565
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 7,
      "text": "Fig. 7: High-level overview of GPT pretraining, and fine-tuning\nsteps. Courtesy of OpenAI.\nregressions on public NLP datasets.\nThe most important milestone of LLM development is the\nlaunch of ChatGPT (Chat Generative Pre-trained Transformer)\n[60] on November 30, 2022. ChatGPT is chatbot that enables\nusers to steer a conversation to complete a wide range of\ntasks such as question answering, information seeking, text\nsummarization, and more. ChatGPT is powered by GPT-3.5\n(and later by GPT-4), a sibling model to InstructGPT, which\nis trained to follow an instruction in a prompt and provide a\ndetailed response.\nGPT-4 [33] is the latest and most powerful LLM in the\nGPT family. Launched in March, 2023, GPT-4 is a multi-\nmodal LLM in that it can take image and text as inputs and\nproduce text outputs. While still less capable than humans\nin some of the most challenging real-world scenarios, GPT-4\nexhibits human-level performance on various professional and\nacademic benchmarks, including passing a simulated bar exam\nwith a score around the top 10% of test takers, as shown in\nFig 11. Like early GPT models, GPT-4 was first pre-trained to\npredict next tokens on large text corpora, and then fine-tuned\nwith RLHF to align model behaviors with human-desired ones.\n2) The LLaMA Family: LLaMA is a collection of founda-\ntion language models, released by Meta. Unlike GPT models,\nLLaMA models are open-source, i.e., model weights are\nreleased to the research community under a noncommercial\nlicense. Thus, the LLaMA family grows rapidly as these\nmodels are widely used by many research groups to develop\nbetter open-source LLMs to compete the closed-source ones or\nto develop task-specific LLMs for mission-critical applications.\nThe first set of LLaMA models [32] was released in Febru-\nary 2023, ranging from 7B to 65B parameters. These models\nare pre-trained on trillions of tokens, collected from publicly\navailable datasets. LLaMA uses the transformer architecture of\nGPT-3, with a few minor architectural modifications, including\n(1) using a SwiGLU activation function instead of ReLU,\n(2) using rotary positional embeddings instead of absolute\npositional embedding, and (3) using root-mean-squared layer-\nnormalization instead of standard layer-normalization. The\nopen-source LLaMA-13B model outperforms the proprietary\nGPT-3 (175B) model on most benchmarks, making it a good\nbaseline for LLM research.\nIn July 2023, Meta, in partnership with Microsoft, released\nthe LLaMA-2 collection [61], which include both foundation\nlanguage models and Chat models finetuned for dialog, known\nas LLaMA-2 Chat. The LLaMA-2 Chat models were reported\nto outperform other open-source models on many public\nbenchmarks. Fig 12 shows the training process of LLaMA-2\nChat. The process begins with pre-training LLaMA-2 using\npublicly available online data. Then, an initial version of\nLLaMA-2 Chat is built via supervised fine-tuning. Subse-\nquently, the model is iteratively refined using RLHF, rejection\nsampling and proximal policy optimization. In the RLHF stage,\nthe accumulation of human feedback for revising the reward\nmodel is crucial to prevent the reward model from being\nchanged too much, which could hurt the stability of LLaMA\nmodel training.\nAlpaca [62] is fine-tuned from the LLaMA-7B model using\n52K instruction-following demonstrations generated in the\nstyle of self-instruct using GPT-3.5 (text-davinci-003). Alpaca\nis very cost-effective for training, especially for academic\nresearch. On the self-instruct evaluation set, Alpaca performs\nsimilarly to GPT-3.5, despite that Alpaca is much smaller.\nThe Vicuna team has developed a 13B chat model, Vicuna-\n13B, by fine-tuning LLaMA on user-shared conversations\ncollected from ShareGPT. Preliminary evaluation using GPT-\n4 as a evaluator shows that Vicuna-13B achieves more than\n90% quality of OpenAI’s ChatGPT, and Google’s Bard while\noutperforming other models like LLaMA and Stanford Alpaca\nin more than 90% of cases. 13 shows the relative response\nquality of Vicuna and a few other well-known models by\nGPT-4. Another advantage of Vicuna-13B is its relative limited\ncomputational demand for model training. The training cost of\nVicuna-13B is merely $300.\nLike Alpaca and Vicuna, the Guanaco models [63] are also\nfinetuned LLaMA models using instruction-following data. But\nthe finetuning is done very efficiently using QLoRA such\nthat finetuning a 65B parameter model can be done on a\nsingle 48GB GPU. QLoRA back-propagates gradients through\na frozen, 4-bit quantized pre-trained language model into Low\nRank Adapters (LoRA). The best Guanaco model outperforms\nall previously released models on the Vicuna benchmark,\nreaching 99.3% of the performance level of ChatGPT while\nonly requiring 24 hours of fine-tuning on a single GPU.\nKoala [64] is yet another instruction-following language\nmodel built on LLaMA, but with a specific focus on interaction\ndata that include user inputs and responses generated by highly\ncapable closed-source chat models such as ChatGPT. The\nKoala-13B model performs competitively with state-of-the-art\nchat models according to human evaluation based on real-\nworld user prompts.\nMistral-7B [65] is a 7B-parameter language model engi-\nneered for superior performance and efficiency. Mistral-7B\noutperforms the best open-source 13B model (LLaMA-2-13B)\nacross all evaluated benchmarks, and the best open-source\n34B model (LLaMA-34B) in reasoning, mathematics, and code\ngeneration. This model leverages grouped-query attention for\nfaster inference, coupled with sliding window attention to\neffectively handle sequences of arbitrary length with a reduced\ninference cost.\nThe LLaMA family is growing rapidly, as more instruction-\nfollowing models have been built on LLaMA or LLaMA-\n2, including Code LLaMA [66], Gorilla [67], Giraffe [68],\n",
      "word_count": 871,
      "char_count": 5821,
      "fonts": [
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 976,
          "height": 419,
          "ext": "png",
          "size_bytes": 85448
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 8,
      "text": "GPT Family\nPaLM Family\n   LLaMA 1/2 Family\nGPT\nGPT1\nGPT2\nGPT3\nGPT4\nGPT3.5 Turbo\ntext-davinci\ncode-davinci\nCODEX\nInstructGPT\nWebGPT\nGPT4 Vision\nGPT4 Turbo\nGorilla\nMistral\nVigogne\nStable Beluga2\nKoala\nCode LLaMA\nVicuna\nAlpaca\nBaize\nLong LLaMA\nGiraffe\nGuanaco\nTulu\nWizardLM\nMed-PaLM\nPaLM-E\nMed-PaLM2\nFLAN-PaLM\nU-PaLM\nPaLM2\nPaLM\nFig. 8: Popular LLM Families.\nFig. 9: GPT-3 shows that larger models make increasingly\nefficient use of in-context information. It shows in-context\nlearning performance on a simple task requiring the model to\nremove random symbols from a word, both with and without\na natural language task description. Courtesy of [56].\nFig. 10: The high-level overview of RLHF. Courtesy of [59].\nVigogne [69], Tulu 65B [70], Long LLaMA [71], and Stable\nBeluga2 [72], just to name a few.\n3) The PaLM Family: The PaLM (Pathways Language\nModel) family are developed by Google. The first PaLM\nmodel [31] was announced in April 2022 and remained private\nFig. 11: GPT-4 performance on academic and professional\nexams, compared with GPT 3.5. Courtesy of [33].\nuntil March 2023. It is a 540B parameter transformer-based\nLLM. The model is pre-trained on a high-quality text corpus\nconsisting of 780 billion tokens that comprise a wide range\nof natural language tasks and use cases. PaLM is pre-trained\non 6144 TPU v4 chips using the Pathways system, which\nenables highly efficient training across multiple TPU Pods.\nPaLM demonstrates continued benefits of scaling by achiev-\ning state-of-the-art few-shot learning results on hundreds of\nlanguage understanding and generation benchmarks. PaLM-\n540B outperforms not only state-of-the-art fine-tuned models\non a suite of multi-step reasoning tasks, but also on par with\nhumans on the recently released BIG-bench benchmark.\nThe U-PaLM models of 8B, 62B, and 540B scales are\ncontinually trained on PaLM with UL2R, a method of continue\ntraining LLMs on a few steps with UL2’s mixture-of-denoiser\nobjective [73]. An approximately 2x computational savings\nrate is reported.\n",
      "word_count": 311,
      "char_count": 2017,
      "fonts": [
        "ComicSansMS (6.5pt)",
        "ArialMT (5.4pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "ArialMT (6.5pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 914,
          "height": 542,
          "ext": "png",
          "size_bytes": 62523
        },
        {
          "index": 1,
          "width": 879,
          "height": 786,
          "ext": "png",
          "size_bytes": 43734
        },
        {
          "index": 2,
          "width": 901,
          "height": 507,
          "ext": "png",
          "size_bytes": 62117
        },
        {
          "index": 3,
          "width": 258,
          "height": 295,
          "ext": "png",
          "size_bytes": 17546
        },
        {
          "index": 4,
          "width": 192,
          "height": 192,
          "ext": "png",
          "size_bytes": 4245
        },
        {
          "index": 5,
          "width": 432,
          "height": 117,
          "ext": "png",
          "size_bytes": 7074
        },
        {
          "index": 6,
          "width": 1200,
          "height": 675,
          "ext": "png",
          "size_bytes": 94213
        },
        {
          "index": 7,
          "width": 2000,
          "height": 676,
          "ext": "png",
          "size_bytes": 68065
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 9,
      "text": "Fig. 12: Training of LLaMA-2 Chat. Courtesy of [61].\nFig. 13: Relative Response Quality of Vicuna and a few other\nwell-known models by GPT-4. Courtesy of Vicuna Team.\nU-PaLM is later instruction-finetuned as Flan-PaLM [74].\nCompared to other instruction finetuning work mentioned\nabove, Flan-PaLM’s finetuning is performed using a much\nlarger number of tasks, larger model sizes, and chain-of-\nthought data. As a result, Flan-PaLM substantially outperforms\nprevious instruction-following models. For instance, Flan-\nPaLM-540B, which is instruction-finetuned on 1.8K tasks,\noutperforms PaLM-540B by a large margin (+9.4% on av-\nerage). The finetuning data comprises 473 datasets, 146 task\ncategories, and 1,836 total tasks, as illustrated in Fig 14.\nFig. 14: Flan-PaLM finetuning consist of 473 datasets in above\ntask categories. Courtesy of [74].\nPaLM-2 [75] is a more compute-efficient LLM with bet-\nter multilingual and reasoning capabilities, compared to its\npredecessor PaLM. PaLM-2 is trained using a mixture of\nobjectives. Through extensive evaluations on English, multi-\nlingual, and reasoning tasks, PaLM-2 significantly improves\nthe model performance on downstream tasks across different\nmodel sizes, while simultaneously exhibiting faster and more\nefficient inference than PaLM.\nMed-PaLM [76] is a domain-specific PaLM, and is de-\nsigned to provide high-quality answers to medical questions.\nMed-PaLM is finetuned on PaLM using instruction prompt\ntuning, a parameter-efficient method for aligning LLMs to\nnew domains using a few exemplars. Med-PaLM obtains very\nencouraging results on many healthcare tasks, although it is\nstill inferior to human clinicians. Med-PaLM 2 improves Med-\nPaLM via med-domain finetuning and ensemble prompting\n[77]. Med-PaLM 2 scored up to 86.5% on the MedQA\ndataset (i.e., a benchmark combining six existing open ques-\ntion answering datasets spanning professional medical exams,\nresearch, and consumer queries), improving upon Med-PaLM\nby over 19% and setting a new state-of-the-art.\nC. Other Representative LLMs\nIn addition to the models discussed in the previous sub-\nsections, there are other popular LLMs which do not belong\nto those three model families, yet they have achieved great\nperformance and have pushed the LLMs field forward. We\nbriefly describe these LLMs in this subsection.\nFLAN: In [78], Wei et al. explored a simple method for\nimproving the zero-shot learning abilities of language models.\nThey showed that instruction tuning language models on a\ncollection of datasets described via instructions substantially\nimproves zero-shot performance on unseen tasks. They take\na 137B parameter pretrained language model and instruction\ntune it on over 60 NLP datasets verbalized via natural language\ninstruction templates. They call this instruction-tuned model\nFLAN. Fig 15 provides a comparison of instruction tuning\nwith pretrain–finetune and prompting.\nFig.\n15:\ncomparison\nof\ninstruction\ntuning\nwith\npre-\ntrain–finetune and prompting. Courtesy of [78].\nGopher: In [79], Rae et al. presented an analysis of\nTransformer-based language model performance across a wide\nrange of model scales — from models with tens of millions of\nparameters up to a 280 billion parameter model called Gopher.\nThese models were evaluated on 152 diverse tasks, achieving\nstate-of-the-art performance across the majority. The number\nof layers, the key/value size, and other hyper-parameters of\ndifferent model sizes are shown in Fig 16.\nT0: In [80], Sanh et al. developed T0, a system for easily\nmapping any natural language tasks into a human-readable\nprompted form. They converted a large set of supervised\n",
      "word_count": 538,
      "char_count": 3641,
      "fonts": [
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1069,
          "height": 497,
          "ext": "png",
          "size_bytes": 65769
        },
        {
          "index": 1,
          "width": 708,
          "height": 309,
          "ext": "png",
          "size_bytes": 13659
        },
        {
          "index": 2,
          "width": 990,
          "height": 708,
          "ext": "png",
          "size_bytes": 86617
        },
        {
          "index": 3,
          "width": 886,
          "height": 296,
          "ext": "png",
          "size_bytes": 30249
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 10,
      "text": "Fig. 16: Model architecture details of Gopher with different\nnumber of parameters. Courtesy of [78].\ndatasets, each with multiple prompts with diverse wording.\nThese prompted datasets allow for benchmarking the ability\nof a model to perform completely held-out tasks. Then, a\nT0 encoder-decoder model is developed to consume textual\ninputs and produces target responses. The model is trained on\na multitask mixture of NLP datasets partitioned into different\ntasks.\nERNIE 3.0: In [81], Sun et al. proposed a unified frame-\nwork named ERNIE 3.0 for pre-training large-scale knowledge\nenhanced models. It fuses auto-regressive network and auto-\nencoding network, so that the trained model can be easily tai-\nlored for both natural language understanding and generation\ntasks using zero-shot learning, few-shot learning or fine-tuning.\nThey have trained ERNIE 3.0 with 10 billion parameters\non a 4TB corpus consisting of plain texts and a large-scale\nknowledge graph. Fig 17 illustrates the model architecture of\nErnie 3.0.\nFig. 17: High-level model architecture of ERNIE 3.0. Courtesy\nof [81].\nRETRO: In [82], Borgeaud et al. enhanced auto-regressive\nlanguage models by conditioning on document chunks re-\ntrieved from a large corpus, based on local similarity with pre-\nceding tokens. Using a 2-trillion-token database, the Retrieval-\nEnhanced Transformer (Retro) obtains comparable perfor-\nmance to GPT-3 and Jurassic-1 [83] on the Pile, despite using\n25% fewer parameters. As shown in Fig 18, Retro combines\na frozen Bert retriever, a differentiable encoder and a chunked\ncross-attention mechanism to predict tokens based on an order\nof magnitude more data than what is typically consumed\nduring training.\nGLaM: In [84], Du et al. proposed a family of LLMs\nnamed GLaM (Generalist Language Model), which use a\nsparsely activated mixture-of-experts architecture to scale the\nFig. 18: Retro architecture. Left: simplified version where a\nsequence of length n = 12 is split into l = 3 chunks of size\nm = 4. For each chunk, we retrieve k = 2 neighbours of r =\n5 tokens each. The retrieval pathway is shown on top. Right:\nDetails of the interactions in the CCA operator. Causality is\nmaintained as neighbours of the first chunk only affect the last\ntoken of the first chunk and tokens from the second chunk.\nCourtesy of [82].\nmodel capacity while also incurring substantially less training\ncost compared to dense variants. The largest GLaM has 1.2\ntrillion parameters, which is approximately 7x larger than GPT-\n3. It consumes only 1/3 of the energy used to train GPT-3 and\nrequires half of the computation flops for inference, while still\nachieving better overall zero, one and few-shot performance\nacross 29 NLP tasks. Fig 19 shows the high-level architecture\nof GLAM.\nFig. 19: GLaM model architecture. Each MoE layer (the\nbottom block) is interleaved with a Transformer layer (the\nupper block). Courtesy of [84].\nLaMDA: In [85], Thoppilan et al. presented LaMDA, a\nfamily of Transformer-based neural language models special-\nized for dialog, which have up to 137B parameters and are\npre-trained on 1.56T words of public dialog data and web text.\nThey showed that fine-tuning with annotated data and enabling\nthe model to consult external knowledge sources can lead to\nsignificant improvements towards the two key challenges of\nsafety and factual grounding.\nOPT: In [86], Zhang et al. presented Open Pre-trained\nTransformers (OPT), a suite of decoder-only pre-trained trans-\nformers ranging from 125M to 175B parameters, which they\n",
      "word_count": 556,
      "char_count": 3528,
      "fonts": [
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1009,
          "height": 277,
          "ext": "png",
          "size_bytes": 27451
        },
        {
          "index": 1,
          "width": 967,
          "height": 525,
          "ext": "png",
          "size_bytes": 130675
        },
        {
          "index": 2,
          "width": 1137,
          "height": 446,
          "ext": "png",
          "size_bytes": 58367
        },
        {
          "index": 3,
          "width": 451,
          "height": 553,
          "ext": "png",
          "size_bytes": 47529
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 11,
      "text": "share with researchers. The OPT models’ parameters are\nshown in 20\nFig. 20: Different OPT Models’ architecture details. Courtesy\nof [86].\nChinchilla: In [2], Hoffmann et al. investigated the optimal\nmodel size and number of tokens for training a transformer\nlanguage model under a given compute budget. By training\nover 400 language models ranging from 70 million to over\n16 billion parameters on 5 to 500 billion tokens, they found\nthat for compute-optimal training, the model size and the\nnumber of training tokens should be scaled equally: for every\ndoubling of model size the number of training tokens should\nalso be doubled. They tested this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the\nsame compute budget as Gopher but with 70B parameters and\n4% more more data.\nGalactica: In [87], Taylor et al. introduced Galactica, a\nlarge language model that can store, combine and reason about\nscientific knowledge. They trained on a large scientific corpus\nof papers, reference material, knowledge bases and many other\nsources. Galactica performed well on reasoning, outperforming\nChinchilla on mathematical MMLU by 41.3% to 35.7%, and\nPaLM 540B on MATH with a score of 20.4% versus 8.8%.\nCodeGen: In [88], Nijkamp et al. trained and released\na family of large language models up to 16.1B parameters,\ncalled CODEGEN, on natural language and programming\nlanguage data, and open sourced the training library JAX-\nFORMER. They showed the utility of the trained model by\ndemonstrating that it is competitive with the previous state-of-\nthe-art on zero-shot Python code generation on HumanEval.\nThey further investigated the multi-step paradigm for program\nsynthesis, where a single program is factorized into multi-\nple prompts specifying sub-problems. They also constructed\nan open benchmark, Multi-Turn Programming Benchmark\n(MTPB), consisting of 115 diverse problem sets that are\nfactorized into multi-turn prompts.\nAlexaTM: In [89], Soltan et al. demonstrated that mul-\ntilingual large-scale sequence-to-sequence (seq2seq) models,\npre-trained on a mixture of denoising and Causal Language\nModeling (CLM) tasks, are more efficient few-shot learners\nthan decoder-only models on various task. They trained a\n20 billion parameter multilingual seq2seq model called Alexa\nTeacher Model (AlexaTM 20B) and showed that it achieves\nstate-of-the-art (SOTA) performance on 1-shot summarization\ntasks, outperforming a much larger 540B PaLM decoder\nmodel. AlexaTM consist of 46 encoder layers, 32 decoder\nlayers, 32 attention heads, and dmodel = 4096.\nSparrow: In [90], Glaese et al. presented Sparrow, an\ninformation-seeking dialogue agent trained to be more helpful,\ncorrect, and harmless compared to prompted language model\nbaselines. They used reinforcement learning from human feed-\nback to train their models with two new additions to help\nhuman raters judge agent behaviour. The high-level pipeline\nof Sparrow model is shown in Fig 21.\nFig. 21: Sparrow pipeline relies on human participation to\ncontinually expand a training set. Courtesy of [90].\nMinerva: In [91], Lewkowycz et al. introduced Minerva,\na large language model pretrained on general natural language\ndata and further trained on technical content, to tackle previous\nLLM struggle with quantitative reasoning (such as solving\nmathematics, science, and engineering problems).\nMoD: In [92], Tay et al. presented a generalized and\nunified perspective for self-supervision in NLP and show how\ndifferent pre-training objectives can be cast as one another\nand how interpolating between different objectives can be\neffective. They proposed Mixture-of-Denoisers (MoD), a pre-\ntraining objective that combines diverse pre-training paradigms\ntogether. This framework is known as Unifying Language\nLearning (UL2). An overview of UL2 pretraining paradigm\nis shown in Fig 21.\nFig. 22: An overview of UL2 pretraining paradigm. Courtesy\nof [92].\nBLOOM: In [93], Scao et al. presented BLOOM, a 176B-\nparameter open-access language model designed and built\nthanks to a collaboration of hundreds of researchers. BLOOM\nis a decoder-only Transformer language model trained on the\nROOTS corpus, a dataset comprising hundreds of sources in\n46 natural and 13 programming languages (59 in total). An\noverview of BLOOM architecture is shown in Fig 23.\n",
      "word_count": 649,
      "char_count": 4318,
      "fonts": [
        "CMMI10 (10.0pt)",
        "CMR10 (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "CMMI7 (7.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 480,
          "height": 335,
          "ext": "png",
          "size_bytes": 28340
        },
        {
          "index": 1,
          "width": 821,
          "height": 429,
          "ext": "png",
          "size_bytes": 19577
        },
        {
          "index": 2,
          "width": 819,
          "height": 519,
          "ext": "png",
          "size_bytes": 44503
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 12,
      "text": "Fig. 23: An overview of BLOOM architecture. Courtesy of\n[93].\nGLM: In [94], Zeng et al. introduced GLM-130B, a\nbilingual (English and Chinese) pre-trained language model\nwith 130 billion parameters. It was an attempt to open-source\na 100B-scale model at least as good as GPT-3 (davinci) and\nunveil how models of such a scale can be successfully pre-\ntrained.\nPythia: In [95], Biderman et al. introduced Pythia, a suite\nof 16 LLMs all trained on public data seen in the exact same\norder and ranging in size from 70M to 12B parameters. We\nprovide public access to 154 checkpoints for each one of the\n16 models, alongside tools to download and reconstruct their\nexact training dataloaders for further study.\nOrca: In [96], Mukherjee et al. develop Orca, a 13-billion\nparameter model that learns to imitate the reasoning process\nof large foundation models. Orca learns from rich signals\nfrom GPT-4 including explanation traces; step-by-step thought\nprocesses; and other complex instructions, guided by teacher\nassistance from ChatGPT.\nStarCoder: In [97], Li et al. introduced StarCoder and\nStarCoderBase. They are 15.5B parameter models with 8K\ncontext length, infilling capabilities and fast large-batch in-\nference enabled by multi-query attention. StarCoderBase is\ntrained on one trillion tokens sourced from The Stack, a\nlarge collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. They fine-tuned\nStarCoderBase on 35B Python tokens, resulting in the creation\nof StarCoder. They performed the most comprehensive evalu-\nation of Code LLMs to date and showed that StarCoderBase\noutperforms every open Code LLM that supports multiple pro-\ngramming languages and matches or outperforms the OpenAI\ncode-cushman-001 model.\nKOSMOS: In [98], Huang et al. introduced KOSMOS-1,\na Multimodal Large Language Model (MLLM) that can per-\nceive general modalities, learn in context (i.e., few-shot), and\nfollow instructions (i.e. zero-shot). Specifically, they trained\nKOSMOS-1 from scratch on web-scale multi-modal corpora,\nincluding arbitrarily interleaved text and images, image-caption\npairs, and text data. Experimental results show that KOSMOS-\n1 achieves impressive performance on (i) language understand-\ning, generation, and even OCR-free NLP (directly fed with\ndocument images), (ii) perception-language tasks, including\nmultimodal dialogue, image captioning, visual question an-\nswering, and (iii) vision tasks, such as image recognition with\ndescriptions (specifying classification via text instructions).\nGemini: In [99], Gemini team introduced a new family of\nmultimodal models, that exhibit promising capabilities across\nimage, audio, video, and text understanding. Gemini family\nincludes three versions: Ultra for highly-complex tasks, Pro\nfor enhanced performance and deployability at scale, and Nano\nfor on-device applications. Gemini architecture is built on top\nof Transformer decoders, and is trained to support 32k context\nlength (via using efficient attention mechanisms).\nSome of the other popular LLM frameworks (or techniques\nused for efficient developments of LLMs) includes Inner-\nMonologue [100], Megatron-Turing NLG [101], LongFormer\n[102], OPT-IML [103], MeTaLM [104], Dromedary [105],\nPalmyra [106], Camel [107], Yalm [108], MPT [109], ORCA-\n2 [110], Gorilla [67], PAL [111], Claude [112], CodeGen 2\n[113], Zephyr [114], Grok [115], Qwen [116], Mamba [30],\nMixtral-8x7B [117], DocLLM [118], DeepSeek-Coder [119],\nFuseLLM-7B [120], TinyLlama-1.1B [121], LLaMA-Pro-8B\n[122].\nFig 24 provides an overview of some of the most repre-\nsentative LLM frameworks, and the relevant works that have\ncontributed to the success of LLMs and helped to push the\nlimits of LLMs.\nIII.\nHOW LLMS ARE BUILT\nIn this section, we first review the popular architectures\nused for LLMs, and then discuss data and modeling techniques\nranging from data preparation, tokenization, to pre-training,\ninstruction tuning, and alignment.\nOnce the model architecture is chosen, the major steps\ninvolved in training an LLM includes: data preparation (col-\nlection, cleaning, deduping, etc.), tokenization, model pre-\ntraining (in a self-supervised learning fashion), instruction\ntuning, and alignment. We will explain each of them in a\nseparate subsection below. These steps are also illustrated in\nFig 25.\nA. Dominant LLM Architectures\nThe most widely used LLM architectures are encoder-only,\ndecoder-only, and encoder-decoder. Most of them are based on\nTransformer (as the building block). Therefore we also review\nthe Transformer architecture here.\n1) Transformer: in a ground-breaking work [44], Vaswani\net al. proposed the Transformer framework, which was orig-\ninally designed for effective parallel computing using GPUs.\nThe heart of Transformer is the (self-)attention mechanism,\nwhich can capture long-term contextual information much\nmore effectively using GPUs than the recurrence and convo-\nlution mechanisms. Fig 26 provides a high-level overview of\ntransformer work. In this section we provide an overview of the\nmain elements and variants, see [44], [123] for more details.\nThe Transformer language model architecture, originally\nproposed for machine translation, consists of an encoder and\na decoder. The encoder is composed of a stack of N = 6\nidentical Transformer layers. Each layer has two sub-layers.\nThe first one is a multi-head self-attention layer, and the other\none is a simple position-wise fully connected feed-forward\nnetwork. The decoder is composed of a stack of 6 identical\nlayers. In addition to the two sub-layers in each encoder layer,\n",
      "word_count": 821,
      "char_count": 5596,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 947,
          "height": 459,
          "ext": "png",
          "size_bytes": 35506
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 13,
      "text": "Fig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our\n#parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way\nfor their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models. ♣ shows entities that serve\nnot only as models but also as approaches. ♦ shows only approaches.\nthe decoder has a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. The attention\nfunction can be described as mapping a query and a set of key-\nvalue pairs to an output, where the query, keys, values, and\noutput are all vectors. The output is computed as a weighted\nsum of the values, where the weight assigned to each value\nis computed by a compatibility function of the query with the\ncorresponding key. Instead of performing a single attention\nfunction with dmodel dimensional keys, values and queries,\nit is found to be beneficial to linearly project the queries,\nkeys and values h with different, learned linear projections to\ndk, dk and dv dimensions, respectively. Positional encoding is\nincorporated to fuse information about the relative or absolute\nposition of the tokens in the sequence.\n2) Encoder-Only: For this family, at each stage, the atten-\ntion layers can access all the words in the initial sentence.\nThe pre-training of these models usually consist of some-\nhow corrupting a given sentence (for instance, by masking\nrandom words in it) and tasking the model with finding or\nreconstructing the initial sentence. Encoder models are great\nfor tasks requiring an understanding of the full sequence,\nsuch as sentence classification, named entity recognition, and\nextractive question answering. One prominent encoder only\nmodel is BERT (Bidirectional Encoder Representations from\nTransformers), proposed in [24].\n3) Decoder-Only: For these models, at each stage, for any\nword, the attention layers can only access the words positioned\nbefore that in the sentence. These models are also sometimes\ncalled auto-regressive models. The pretraining of these models\nis usually formulated as predicting the next word (or token)\nin the sequence. The decoder-only models are best suited for\ntasks involving text generation. GPT models are prominent\nexample of this model category.\n4) Encoder-Decoder: These models use both encoder and\ndecoder, and are sometimes called sequence-to-sequence mod-\nels. At each stage, the attention layers of the encoder can access\nall the words in the initial sentence, whereas the attention\nlayers of the decoder only accesses the words positioned before\na given word in the input. These models are usually pre-\ntrained using the objectives of encoder or decoder models, but\nusually involve something a bit more complex. For instance,\nsome models are pretrained by replacing random spans of text\n(that can contain several words) with a single mask special\nword, and the objective is then to predict the text that this\nmask word replaces. Encoder-decoder models are best suited\nfor tasks about generating new sentences conditioned on a\ngiven input, such as summarization, translation, or generative\nquestion answering.\nB. Data Cleaning\nData quality is crucial to the performance of language\nmodels trained on them. Data cleaning techniques such as\nfiltering, deduplication, are shown to have a big impact on\nthe model performance.\nAs an example, in Falcon40B [124], Penedo et al. showed\nthat properly filtered and deduplicated web data alone can lead\nto powerful models; even significantly outperforming models\nfrom the state-of-the-art trained on The Pile. Despite extensive\nfiltering, they were able to obtain five trillion tokens from\n",
      "word_count": 590,
      "char_count": 3759,
      "fonts": [
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMMI10 (10.0pt)",
        "CMSY10 (10.0pt)",
        "MSAM10 (10.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "CMMI7 (7.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1930,
          "height": 918,
          "ext": "png",
          "size_bytes": 132465
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 14,
      "text": "How LLMs Are Built?\nData Cleaning\nTokenizations\nBytePairEncoding\nWordPieceEncoding\nSentencePieceEncoding\nPositional Encoding\nAbsolute Positional Embeddings\nRelative Positional Embeddings\nRotary Position Embeddings\nRelative Positional Bias\nModel Pre-training\nMasked Language Modeling\nCausal Language Modeling\nNext Sentence Prediction\nMixture of Experts\nFine-tuning and Instruction Tuning\nAlignment\nSupervised learning\nReinforcement Learning from Human Feedback\nDirect Preference Optimization\nKahneman-Tversky Optimization\nDecoding Strategies\nGreedy Search\nBeam Search\nTop-k Sampling\nTop-p Sampling\nCost-Effective Training/Inference,\nAdaptation & Compression\nOptimized Training\nZero Redundancy Optimizer\nReceptance Weighted Key Value\nLow-Rank Adaption\nKnowledge Distillation\nQuantization\nData Filtering\nRemoving Noise\nHandling Outliers\nAddressing Imbalances\nText Preprocessing\nDeduplication\nLLM Architectures\nEncoder-Only\nDecoder-Only\nEncoder-Decoder\n...\nSupervised Fine-tuning\nGeneral Fine-tuning\nMulti-turn Instructions\nInstruction Following\nFig. 25: This figure shows different components of LLMs.\n",
      "word_count": 118,
      "char_count": 1099,
      "fonts": [
        "Arial-BoldMT (6.9pt)",
        "Arial-BoldItalicMT (6.9pt)",
        "ArialMT (6.9pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 15,
      "text": "Fig. 26: High-level overview of transformer work. Courtesy of\n[44].\nCommonCrawl. They also released an extract of 600 billion\ntokens from our REFINEDWEB dataset, and 1.3/7.5B param-\neters language models trained on it. 27 shows the Refinement\nprocess of CommonCrawl data by this work.\nFig. 27: Subsequent stages of Macrodata Refinement remove\nnearly 90% of the documents originally in CommonCrawl.\nCourtesy of [124].\n1) Data Filtering: Data filtering aims to enhance the qual-\nity of training data and the effectiveness of the trained LLMs.\nCommon data filtering techniques include:\nRemoving Noise: refers to eliminating irrelevant or noisy\ndata that might impact the model’s ability to generalize well.\nAs an example, one can think of removing false information\nfrom the training data, to lower the chance of model generating\nfalse responses. Two mainstream approaches for quality filter-\ning includes: classifier-based, and heuristic-based frameworks.\nHandling Outliers: Identifying and handling outliers or\nanomalies in the data to prevent them from disproportionately\ninfluencing the model.\nAddressing Imbalances: Balancing the distribution of\nclasses or categories in the dataset to avoid biases and ensure\nfair representation. This is specially useful for responsible\nmodel training and evaluation.\nText Preprocessing: Cleaning and standardizing text data\nby removing stop words, punctuation, or other elements that\nmay not contribute significantly to the model’s learning.\nDealing with Ambiguities: Resolving or excluding am-\nbiguous or contradictory data that might confuse the model\nduring training. This can help the model to provide more\ndefinite and reliable answers.\n2) Deduplication: De-duplication refers to the process of\nremoving duplicate instances or repeated occurrences of the\nsame data in a dataset. Duplicate data points can introduce\nbiases in the model training process and reduce the diversity, as\nthe model may learn from the same examples multiple times,\npotentially leading to overfitting on those particular instances.\nSome works [125] have shown that de-duplication improves\nmodels’ ability to generalize to new, unseen data.\nThe de-duplication process is particularly important when\ndealing with large datasets, as duplicates can unintentionally\ninflate the importance of certain patterns or characteristics.\nThis is especially relevant in NLP tasks, where diverse and\nrepresentative training data is crucial for building robust lan-\nguage models.\nThe specific de-duplication method can vary based on\nthe nature of the data and the requirements of the particular\nlanguage model being trained. It may involve comparing entire\ndata points or specific features to identify and eliminate du-\nplicates. At the document level, existing works mainly rely on\nthe overlap ratio of high-level features (e.g. n-grams overlap)\nbetween documents to detect duplicate samples.\nC. Tokenizations\nTokenization referes to the process of converting a se-\nquence of text into smaller parts, known as tokens. While\nthe simplest tokenization tool simply chops text into tokens\nbased on white space, most tokenization tools rely on a word\ndictionary. However, out-of-vocabulary (OOV) is a problem\nin this case because the tokenizer only knows words in its\ndictionary. To increase the coverage of dictionaries, popular\ntokenizers used for LLMs are based on sub-words, which can\nbe combined to form a large number of words, including the\nwords unseen in training data or words in different languages.\nIn what follows, we describe three popular tokenizers.\n1) BytePairEncoding: BytePairEncoding is originally a\ntype of data compression algorithm that uses frequent patterns\nat byte level to compress the data. By definition, this algorithm\nmainly tries to keep the frequent words in their original form\nand break down ones that are not common. This simple\nparadigm keeps the vocabulary not very large, but also good\nenough to represent common words at the same time. Also\nmorphological forms of the frequent words can be represented\nvery well if suffix or prefix is also commonly presented in the\ntraining data of the algorithm.\n",
      "word_count": 621,
      "char_count": 4134,
      "fonts": [
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 575,
          "height": 771,
          "ext": "png",
          "size_bytes": 109107
        },
        {
          "index": 1,
          "width": 898,
          "height": 497,
          "ext": "png",
          "size_bytes": 22215
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 16,
      "text": "2) WordPieceEncoding: This algorithm is mainly used for\nvery well-known models such as BERT and Electra. At the\nbeginning of training, the algorithm takes all the alphabet from\nthe training data to make sure that nothing will be left as UNK\nor unknown from the training dataset. This case happens when\nthe model is given an input that can not be tokenized by the\ntokenizer. It mostly happens in cases where some characters are\nnot tokenizable by it. Similar to BytePairEncoding, it tries to\nmaximize the likelihood of putting all the tokens in vocabulary\nbased on their frequency.\n3) SentencePieceEncoding: Although both tokenizers de-\nscribed before are strong and have many advantages compared\nto white-space tokenization, they still take assumption of\nwords being always separated by white-space as granted. This\nassumption is not always true, in fact in some languages, words\ncan be corrupted by many noisy elements such as unwanted\nspaces or even invented words. SentencePieceEncoding tries\nto address this issue.\nD. Positional Encoding\n1) Absolute Positional Embeddings: (APE) [44] has been\nused in the original Transformer model to preserve the infor-\nmation of sequence order. Therefore, the positional information\nof words is added to the input embeddings at the bottom of\nboth the encoder and decoder stacks. There are various options\nfor positional encodings, either learned or fixed. In the vanilla\nTransformer, sine and cosine functions are employed for this\npurpose. The main drawback of using APE in Transformers\nis the restriction to a certain number of tokens. Additionally,\nAPE fails to account for the relative distances between tokens.\n2) Relative Positional Embeddings: (RPE) [126] involves\nextending self-attention to take into account the pairwise links\nbetween input elements. RPE is added to the model at two\nlevels: first as an additional component to the keys, and\nsubsequently as a sub-component of the values matrix. This\napproach looks at the input as a fully-connected graph with\nlabels and directed edges. In the case of linear sequences, edges\ncan capture information about the relative position differences\nbetween input elements. A clipping distance, represented as k\n2 ≤ k ≤ n − 4, specifies the maximum limit on relative lo-\ncations. This allows the model to make reasonable predictions\nfor sequence lengths that are not part of the training data.\n3) Rotary Position Embeddings: Rotary Positional Em-\nbedding (RoPE) [127] tackles problems with existing ap-\nproaches. Learned absolute positional encodings can lack gen-\neralizability and meaningfulness, particularly when sentences\nare short. Moreover, current methods like T5’s positional\nembedding face challenges with constructing a full attention\nmatrix between positions. RoPE uses a rotation matrix to\nencode the absolute position of words and simultaneously in-\ncludes explicit relative position details in self-attention. RoPE\nbrings useful features like flexibility with sentence lengths, a\ndecrease in word dependency as relative distances increase,\nand the ability to improve linear self-attention with relative\nposition encoding. GPT-NeoX-20B, PaLM, CODEGEN, and\nLLaMA are among models that take advantage of RoPE in\ntheir architectures.\n4) Relative Positional Bias: The concept behind this type\nof positional embedding is to facilitate extrapolation during\ninference for sequences longer than those encountered in train-\ning. In [128] Press et al. proposed Attention with Linear Biases\n(ALiBi). Instead of simply adding positional embeddings to\nword embeddings, they introduced a bias to the attention scores\nof query-key pairs, imposing a penalty proportional to their\ndistance. In the BLOOM model, ALiBi is leveraged.\nE. Model Pre-training\nPre-training is the very first step in large language model\ntraining pipeline, and it helps LLMs to acquire fundamental\nlanguage understanding capabilities, which can be useful in a\nwide range of language related tasks. During pre-training, the\nLLM is trained on a massive amount of (usually) unlabeled\ntexts, usually in a self-supervised manner. There are different\napproaches used for pre-training like next sentence prediction\n[24], two most common ones include, next token prediction\n(autoregressive language modeling), and masked language\nmodeling.\nIn Autoregressive Language Modeling framework, given\na sequence of n tokens x1, ..., xn, the model tries to predict\nnext token xn+1 (and sometimes next sequence of tokens) in\nan auto-regressive fashion. One popular loss function in this\ncase is the log-likelihood of predicted tokens as shown in Eq\n2\nLALM(x) =\nN\nX\ni=1\np(xi+n|xi, ..., xi+n−1)\n(1)\nGiven the auto-regressive nature of this framework, the\ndecoder-only models are naturally better suited to learn how\nto accomplish these task.\nIn Masked Language Modeling, some words are masked\nin a sequence and the model is trained to predict the masked\nwords based on the surrounding context. Sometimes people\nrefer to this approach as denoising autoencoding, too. If we\ndenote the masked/corrupted samples in the sequence x, as ˜x,\nthen the training objective of this approach can be written as:\nLMLM(x) =\nN\nX\ni=1\np(˜x|x\\˜x)\n(2)\nAnd more recently, Mixture of Experts (MoE) [130],\n[131] have become very popular in LLM space too. MoEs\nenable models to be pre-trained with much less compute,\nwhich means one can dramatically scale up the model or\ndataset size with the same compute budget as a dense model.\nMoE consists of two main elements: Sparse MoE layers,\nwhich are used instead of dense feed-forward network (FFN)\nlayers, and have a certain number of “experts” (e.g. 8), in\nwhich each expert is a neural network. In practice, the experts\nare FFNs, but they can also be more complex networks. A gate\nnetwork or router, that determines which tokens are sent to\nwhich expert. It is worth noting that, one can send a token\nto more than one expert. How to route a token to an expert\nis one of the big decisions when working with MoEs - the\nrouter is composed of learned parameters and is pretrained at\nthe same time as the rest of the network. Fig 29 provides an\nillustration of a Switch Transformer encoder block, which are\nused in MoE.\n",
      "word_count": 977,
      "char_count": 6182,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "rsfs10 (10.0pt)",
        "CMMI10 (10.0pt)",
        "CMSY10 (10.0pt)",
        "CMEX10 (10.0pt)",
        "CMR10 (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "CMMI7 (7.0pt)",
        "CMR7 (7.0pt)",
        "CMSY7 (7.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 17,
      "text": "(a) Absolute Positional Embeddings [129]\n(b) Relative Positional Embeddings\n(c) Rotary Positional Embedding [127]\n(d) Relative Positional Bias [128]\nFig. 28: Various positional encodings are employed in LLMs.\nFig. 29: : Illustration of a Switch Transformer encoder block.\nThey replaced the dense feed forward network (FFN) layer\npresent in the Transformer with a sparse Switch FFN layer\n(light blue). . Courtesy of [131].\nF. Fine-tuning and Instruction Tuning\nEarly language models such as BERT trained using self-\nsupervision as explained in section III-E were not able to\nperform specific tasks. In order for the foundation model to be\nuseful it needed to be fine-tuned to a specific task with labeled\ndata (so-called supervised fine-tuning or SFT for short). For\nexample, in the original BERT paper [24], the model was fine-\ntuned to 11 different tasks. While more recent LLMs no longer\nrequire fine-tuning to be used, they can still benefit from task\nor data-specific fine-tuning. For example, OpenAI reports that\nthe much smaller GPT-3.5 Turbo model can outperform GPT-4\nwhen fine-tuned with task specific data 2.\nFine-tuning does not need to be performed to a single\ntask though, and there are different approaches to multi-task\nfine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one\nor more tasks is known to improve results and reduce the\ncomplexity of prompt engineering, and it can serve as an\nalternative to retrieval augmented generation. Furthermore,\nthere are other reasons why it might be advisable to fine-tune.\nFor example, one might want to fine-tune to expose the model\nto new or proprietary data that it has not been exposed to\nduring pre-training.\nAn important reason to fine-tune LLMs is to align the\nresponses to the expectations humans will have when providing\ninstructions through prompts. This is the so-called instruction\ntuning [133]. We dive into the details of how to design\nand engineer prompts in section IV-B, but in the context\nof instruction tuning, it is important to understand that the\ninstruction is a prompt that specifies the task that the LLM\nshould accomplish. Instruction tuning datasets such as Natural\nInstructions [134] include not only the task definition but other\ncomponents such as positive/negative examples or things to\navoid.\nThe specific approach and instruction datasets used to\ninstruction-tune an LLM varies, but, generally speaking, in-\nstruction tuned models outperform their original foundation\nmodels they are based on. For example, InstructGPT [59]\noutperforms GPT-3 on most benchmarks. The same is true\nfor Alpaca [62] when compared to LLaMA.\nSelf-Instruct [135], proposed by Wang et al. is also a\n2https://platform.openai.com/docs/guides/fine-tuning\n",
      "word_count": 423,
      "char_count": 2723,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "NimbusRomNo9L-Regu (7.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Regu (6.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 283,
          "height": 412,
          "ext": "png",
          "size_bytes": 46990
        },
        {
          "index": 1,
          "width": 593,
          "height": 421,
          "ext": "png",
          "size_bytes": 120650
        },
        {
          "index": 2,
          "width": 1204,
          "height": 696,
          "ext": "png",
          "size_bytes": 90278
        },
        {
          "index": 3,
          "width": 1084,
          "height": 476,
          "ext": "png",
          "size_bytes": 38229
        },
        {
          "index": 4,
          "width": 943,
          "height": 481,
          "ext": "png",
          "size_bytes": 65982
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 18,
      "text": "popular approach along this line, in which they introduced a\nframework for improving the instruction-following capabilities\nof pre-trained language models by bootstrapping their own\ngenerations. Their pipeline generates instructions, input, and\noutput samples from a language model, then filters invalid or\nsimilar ones before using them to fine tune the original model.\nG. Alignment\nAI Alignment is the process of steering AI systems towards\nhuman goals, preferences, and principles. LLMs, pre-trained\nfor word prediction, often exhibit unintended behaviors. For\nexample, they might generate contents that are toxic, harmful,\nmisleading and biased.\nInstruction tuning, discussed above, gets LLMs a step\ncloser to being aligned. However, in many cases, it is important\nto include further steps to improve the alignment of the model\nand avoid unintended behaviors 3. We review the most popular\napproaches to alignment in this subsection.\nRLHF (reinforcement learning from human feedback) and\nRLAIF (reinforcement learning from AI feedback) are two\npopular approaches. RLHF uses a reward model to learn\nalignment from human feedback. This reward model, after\nbeing tuned, is able to rate different outputs and score them\naccording to their alignment preferences given by humans. The\nreward model gives feedback to the original LLM and this\nfeedback is used to tune the LLM further [137]. Reinforcement\nlearning from AI feedback on the other hand, directly connects\na pretrained and well-aligned model to the LLM and helps it\nto learn from larger and more aligned models [138].\nIn another recent work (known as DPO) [139], Rafailov\net al. discussed that RLHF is a complex and often unstable\nprocedure, and tried to address this with a new approach. They\nleveraged a mapping between reward functions and optimal\npolicies to show that this constrained reward maximization\nproblem can be optimized exactly with a single stage of policy\ntraining, essentially solving a classification problem on the\nhuman preference data. The resulting algorithm, which they\ncalled Direct Preference Optimization (DPO), is stable, per-\nformant, and computationally lightweight, eliminating the need\nfor fitting a reward model, sampling from the LM during fine-\ntuning, or performing significant hyperparameter tuning. They\nobserved that fine-tuning with DPO exceeds RLHF’s ability to\ncontrol sentiment of generations and improves response quality\nin summarization. Fig 30 shows the high-level comparison\nbetween DPO vs RLHF.\nEven more recently Ethayarajh et al. proposed a new align-\nment approach called the Kahneman-Tversky Optimization\n(KTO) [136]. Unlike existing state-of-the-art approaches, KTO\ndoes not require paired preference data (x, yw, yl), and it\nonly needs (x,y) and knowledge of whether y is desirable or\nundesirable. KTO-aligned models are shown to be good or\nbetter than DPO-aligned models at scales from 1B to 30B,\ndespite not using paired preferences. KTO is also far easier to\nuse in the real world than preference optimization methods, as\nthe kind of data it needs is far more abundant. As an example,\n3According to very recent research by Ethayarajh et al. [136], further\nalignment besides SFT mainly improves models of at least 7B parameters.\nFor smaller models, SFT is sufficient.\nFig. 30: DPO optimizes for human preferences while avoiding\nreinforcement learning. Existing methods for fine-tuning lan-\nguage models with human feedback first fit a reward model\nto a dataset of prompts and human preferences over pairs of\nresponses, and then use RL to find a policy that maximizes\nthe learned reward. In contrast, DPO directly optimizes for\nthe policy best satisfying the preferences with a simple classi-\nfication objective, without an explicit reward function or RL.\nCourtesy of [139].\nevery retail company has a lot of customer interaction data and\nwhether that interaction was successful (e.g., purchase made)\nor unsuccessful (e.g., no purchase made). However, they have\nlittle to no counterfactual data (i.e., what would have made\nan unsuccessful customer interaction yl into a successful one\nyw). Fig 31 shows a high-level comparison between KTO and\nother alignment approaches discussed above.\nFig. 31: LLM alignment involves supervised finetuning fol-\nlowed by optimizing a human-centered loss (HALO). How-\never, the paired preferences that existing approaches need are\nhard-to-obtain. In contrast, KTO uses a far more abundant\nkind of data, making it much easier to use in the real world.\nCourtesy of [136].\nH. Decoding Strategies\nDecoding refers to the process of text generation using pre-\ntrained LLMs. Given an input prompt, the tokenizer translates\neach token in the input text into a corresponding token ID.\nThen, the language model uses these token IDs as input and\npredicts the next most likely token (or a sequence of tokens).\nFinally, the model generates logits, which are converted to\nprobabilities using a softmax function. Different decoding\nstrategies have been proposed. Some of the most popular ones\nare greedy search, beam search, as well as different sample\ntechniques such as top-K, top-P (Nucleus sampling).\n1) Greedy Search: Greedy search takes the most probable\ntoken at each step as the next token in the sequence, discarding\nall other potential options. As you can imagine, this is a simple\napproach and can loose a lot of temporal consistency and\ncoherency. It only considers the most probable token at each\n",
      "word_count": 843,
      "char_count": 5441,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "NimbusRomNo9L-Regu (7.0pt)",
        "CMMI10 (10.0pt)",
        "NimbusRomNo9L-Regu (6.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "CMMI7 (7.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1073,
          "height": 226,
          "ext": "png",
          "size_bytes": 87592
        },
        {
          "index": 1,
          "width": 811,
          "height": 329,
          "ext": "png",
          "size_bytes": 115313
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 19,
      "text": "step, without considering the overall effect on the sequence.\nThis property makes it fast, but it also means that it can miss\nout on better sequences that might have appeared with slightly\nless probable next tokens.\n2) Beam Search: Unlike greedy search that only considers\nthe next most probable token, beam search takes into account\nthe N most likely tokens, where N denotes the number of\nbeams. This procedure is repeated until a predefined maxi-\nmum sequence length is reached or an end-of-sequence token\nappears. At this point, the sequence of tokens (AKA “beam”)\nwith the highest overall score is chosen as the output. For\nexample for beam size of 2 and maximum length of 5,\nthe beam search needs to keep track of 25 = 32 possible\nsequences. So it is more computationally intensive than greedy\nsearch.\n3) Top-k Sampling: Top-k sampling is a technique that\nuses the probability distribution generated by the language\nmodel to select a token randomly from the k most likely\noptions.\nSuppose we have 6 tokens (A, B, C, D, E, F) and k=2,\nand P(A)= 30%, and P(B)= 20%, P(C)= P(D)= P(E)= P(F)=\n12.5%. In top-k sampling, tokens C, D, E, F are disregarded,\nand the model outputs A 60% of the time, and B, 40% of\nthe time. This approach ensures that we prioritize the most\nprobable tokens while introducing an element of randomness\nin the selection process.\nThe randomness is usually introduced via the concept of\ntemperature. The temperature T is a parameter that ranges from\n0 to 1, which affects the probabilities generated by the softmax\nfunction, making the most likely tokens more influential. In\npractice, it simply consists of dividing the input logits by\ntemperature value:\nsoftmax(xi) =\nexi/T\nP\nj exj/T\n(3)\nA low temperature setting significantly alters the proba-\nbility distribution (and is commonly used in text generation\nto control the level of “creativity” in the generated output),\nwhile a large temperature prioritizes the tokens with higher\nprobabilities. Top-k is a creative way of sampling, and can be\nused along with beam search. The sequence chosen by top-\nk sampling may not be the sequence with highest probability\nin beam search. But it’s important to remember that highest\nscores do not always lead to more realistic or meaningful\nsequences.\n4) Top-p Sampling: Top-p sampling, also known as Nu-\ncleus sampling, takes a slightly different approach from top-k\nsampling. Instead of selecting the top k most probable tokens,\nnucleus sampling chooses a cutoff value p such that the sum of\nthe probabilities of the selected tokens exceeds p. This forms\na “nucleus” of tokens from which to randomly choose the next\ntoken. In other words, in top-p sampling the language model\nexamines the most probable tokens in descending order and\nkeeps adding them to the list until the sum of probabilities\nsurpasses the threshold p. As you can imagine, this could be\nbetter specially for scenarios in which top-k tokens do not have\na large probability mass. Unlike top-k sampling, the number\nof tokens included in the nucleus sampling is not fixed. This\nvariability often results in a more diverse and creative output,\nmaking nucleus sampling popular for text generation related\ntasks.\nI.\nCost-Effective Training/Inference/Adaptation/Compression\nIn this part, we review some of the popular approaches\nused for more cost-friendly (and compute-friendly) training\nand usage of LLMs.\n1) Optimized Training: There are many frameworks de-\nveloped for optimized training of LLMs, here we introduce\nsome of the prominent ones.\nZeRO:\nIn [140], Rajbhandari et al. developed a novel\nsolution, Zero Redundancy Optimizer (ZeRO), to optimize\nmemory, vastly improving training speed of LLMs while\nincreasing the model size that can be efficiently trained. ZeRO\neliminates memory redundancies in data- and model-parallel\ntraining while retaining low communication volume and high\ncomputational granularity, allowing one to scale the model\nsize proportional to the number of devices with sustained high\nefficiency.\nRWKV: In [141], Peng et al. proposed a novel model\narchitecture, Receptance Weighted Key Value (RWKV), that\ncombines the efficient parallelizable training of Transformers\nwith the efficient inference of RNNs. Their approach leverages\na linear attention mechanism and allows them to formulate the\nmodel as either a Transformer or an RNN, which parallelizes\ncomputations during training and maintains constant compu-\ntational and memory complexity during inference, leading to\nthe first non-transformer architecture to be scaled to tens of\nbillions of parameters. RWKV architecture is shown in Fig\n32. The Time Complexity comparison of RWKV with different\nFig. 32: RWKV architecture. Courtesy of [141].\nTransformers are provided in Fig 33.\n",
      "word_count": 757,
      "char_count": 4740,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMMI10 (10.0pt)",
        "CMMI5 (5.0pt)",
        "CMEX10 (10.0pt)",
        "CMR10 (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "CMMI7 (7.0pt)",
        "CMR7 (7.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 552,
          "height": 543,
          "ext": "png",
          "size_bytes": 35629
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 20,
      "text": "Fig. 33: Time Complexity comparison of RWKV with different\nTransformers. Here T denotes the sequence length, d the\nfeature dimension, and c is MEGA’s chunk size of quadratic\nattention. Courtesy of [141].\n2) Low-Rank Adaption (LoRA): Low-Rank Adaptation is\na popular and lightweight training technique that significantly\nreduces the number of trainable parameters, and is based\non a crucial insight that the difference between the fine-\ntuned weights for a specialized task and the initial pre-trained\nweights often exhibits “low intrinsic rank” - meaning that\nit can be approximated well by a low rank matrix [142].\nTraining with LoRA is much faster, memory-efficient, and\nproduces smaller model weights (a few hundred MBs), that are\neasier to store and share. One property of low-rank matrices\nis that they can be represented as the product of two smaller\nmatrices. This realization leads to the hypothesis that this delta\nbetween fine-tuned weights and initial pre-trained weights can\nbe represented as the matrix product of two much smaller\nmatrices. By focusing on updating these two smaller matrices\nrather than the entire original weight matrix, computational\nefficiency can be substantially improved.\nSpecifically, for a pre-trained weight matrix W0 ∈ Rd×k,\nLoRA constrains its update by representing the latter with\na low-rank decomposition W0 + ∆W = W0 + BA, where\nB ∈ Rd×r , A ∈ Rr×k, and the rank r ≪ min(d, k). During\ntraining, W0 is frozen and does not receive gradient updates,\nwhile A and B contain trainable parameters. It is worth\nmentioning that both W0 and ∆W = BA are multiplied with\nthe same input, and their respective output vectors are summed\ncoordinate-wise. For h = W0x, their modified forward pass\nyields: h = W0x + ∆Wx = W0x + BAx. Usually a random\nGaussian initialization is used for A, and zero initialization\nfor B, so ∆W = BA is zero at the beginning of training.\nThey then scale ∆Wx by αr, where α is a constant in r. This\nreparametrization is illustrated in Figure 34\nIt is worth mentioning that LoRA can be applied to a subset\nof weight matrices in a neural network to reduce the number\nof trainable parameters. In the Transformer architecture, there\nare four weight matrices in the self-attention module (Wq ,\nWk, Wv , Wo), and two in the MLP module. Most of the\ntime, LoRA is focused on adapting the attention weights only\nfor downstream tasks, and freezes the MLP modules, so they\nare not trained in downstream tasks both for simplicity and\nparameter-efficiency.\n3) Knowledge Distillation: Knowledge distillation is the\nprocess of learning from a larger model [143]. Earlier days of\nbest-performing models release have proven that this approach\nis very useful even if it is used in an API distillation approach.\nFig. 34: An illustration of LoRA reparametrizan. Only A and\nB trained during this process. Courtesy of [142].\nIt is also referred to as an approach to distill the knowledge of\nnot a single model but in fact multiple models into a smaller\none. Creating smaller models by this approach yields smaller\nmodel sizes that can be used even on edge devices. Knowledge\ndistillation as shown in Fig 35, illustrates a general setup of\nthis training scheme.\nFig. 35: A generic knowledge distillation framework with\nstudent and teacher (Courtesy of [144]).\nKnowledge can be transferred by different forms of learn-\ning: response distillation, feature distillation, and API distilla-\ntion. Response distillation is concerned only with the outputs\nof the teacher model and tries to teach the student model\nhow to exactly or at least similarly perform (in the sense of\nprediction) as the teacher. Feature distillation not only uses\nthe last layer but also intermediate layers as well to create a\nbetter inner representation for the student model. This helps the\nsmaller model to have a similar representation as the teacher\nmodel.\nAPI distillation is the process of using an API (typically\nfrom an LLM provider such as OpenAI) to train smaller\nmodels. In the case of LLMs, it is used to train the model\nfrom the direct output of the larger model which makes it very\nsimilar to response distillation. Many concerns are raised by\nthis type of distillation because in cases where the model itself\nis not openly available, a (usually) paid API is exposed for end\nusers. On the other hand, while users pay for each call, how to\nuse the predictions is limited, for example, OpenAI prohibits\nusage of its API to create LLMs that later will be used to\ncompete with it. The main value in such case is training data.\n4) Quantization: deep learning in its core, is a set of\nmathematical functions applied to matrices, with a specific\n",
      "word_count": 784,
      "char_count": 4657,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMMI10 (10.0pt)",
        "CMSY10 (10.0pt)",
        "CMR10 (10.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "CMMI7 (7.0pt)",
        "CMR7 (7.0pt)",
        "CMSY7 (7.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 544,
          "height": 259,
          "ext": "png",
          "size_bytes": 23200
        },
        {
          "index": 1,
          "width": 362,
          "height": 347,
          "ext": "png",
          "size_bytes": 10357
        },
        {
          "index": 2,
          "width": 1257,
          "height": 557,
          "ext": "jpeg",
          "size_bytes": 86678
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 21,
      "text": "precision for model weights. Reducing the precision of the\nweights can be used to reduce the size of the model and also\nmake it faster. As an example, Float-32 operations compared\nto Int-8 operations are slower. This process, which is called\nquantization, can be applied in different phases. Main ap-\nproaches for model quantization can be categorized as: post\ntraining quantization and quantization-aware training. Post-\ntraining quantization is concerned with quantized trained mod-\nels in two well-known methods: dynamic and static. Dynamic\npost-training quantization computes the range of quantization\non the runtime and is slower compared to static. Quantization-\naware training adds quantization criteria into training, and\na quantized model is trained and optimized during training\nprocess. This approach ensures that the end model will have\ngood performance and also does not need to be quantized after\ntraining.\nIV.\nHOW LLMS ARE USED AND AUGMENTED\nOnce the LLMs are trained, we can use them to generate\ndesired outputs for a variety of tasks. LLMs can be used\ndirectly through basic prompting. However, in order to exploit\ntheir full potential or to address some of the shortcomings,\nwe need to augment the models through some external means.\nIn this section we first provide a brief overview of the main\nshortcoming of LLMs, with a deeper look at the issue of\nhallucination. We then describe how prompting and some aug-\nmentation approaches can not only address those limitations\nbut also be used to augment the capabilities of LLMs going\nas far as turning an LLM into a full-blown AI agent with the\nability to interface with the external world.\nA. LLM limitations\nIt is important to remember that LLMs are trained to predict\na token. While fine-tuning and alignment improves their per-\nformance and adds different dimensions to their abilities, there\nare still some important limitations that come up, particularly\nif they are used naively. Some of them include the following:\n•\nThey don’t have state/memory. LLMs on their own\ncannot remember even what was sent to them in the\nprevious prompt. That is an important limitation for\nmany of the use cases that require some form of state.\n•\nThey are stochastic/probabilistic. If you send the same\nprompt to an LLM several times, you are likely to get\ndifferent responses. While there are parameters, and\nin particular the temperature, to limit the variability\nin the response, this is an inherent property of their\ntraining that can create issues.\n•\nThey have stale information and, on their own, don’t\nhave access to external data. An LLM on its own does\nnot even know about the current time or day and does\nnot have access to any information that was not present\nin its training set.\n•\nThey are generally very large. This means that many\ncostly GPU machines are needed for training and\nserving. In some cases, largest models have poor\nSLAs, particularly in terms of latency.\n•\nThey hallucinate. LLMs do not have a notion of\n”truth” and they have usually been trained on a mix\nof good and bad content. They can produce very\nplausible but untruthful answers.\nWhile the previous limitations can all become important\nfor some applications, it is worth for us to dive a bit into the\nlast one, hallucinations, since it has gathered a lot of interest\nover the past few months and it has also sparked many of the\nprompt approaches and LLM augmentation methods we will\nlater describe.\nHallucination: In the realm of Large Language Models\n(LLMs), the phenomenon of ”hallucinations” has garnered\nsignificant attention. Defined in the literature, notably in the\n”Survey of Hallucination in Natural Language Generation”\npaper [145], hallucination in an LLM is characterized as\n”the generation of content that is nonsensical or unfaithful\nto the provided source.” This terminology, although rooted in\npsychological parlance, has been appropriated within the field\nof artificial intelligence.\nHallucinations in LLMs can be broadly categorized into\ntwo types:\n1)\nIntrinsic Hallucinations: These directly conflict with\nthe source material, introducing factual inaccuracies\nor logical inconsistencies.\n2)\nExtrinsic Hallucinations: These, while not contra-\ndicting, are unverifiable against the source, encom-\npassing speculative or unconfirmable elements.\nThe definition of ’source’ in LLM contexts varies with the\ntask. In dialogue-based tasks, it refers to ’world knowledge’,\nwhereas in text summarization, it pertains to the input text\nitself. This distinction plays a crucial role in evaluating and\ninterpreting hallucinations. The impact of hallucinations is also\nhighly context-dependent. For instance, in creative endeavors\nlike poem writing, hallucinations might be deemed acceptable\nor even beneficial.\nLLMs, trained on diverse datasets including the internet,\nbooks, and Wikipedia, generate text based on probabilistic\nmodels without an inherent understanding of truth or falsity.\nRecent advancements like instruct tuning and Reinforcement\nLearning from Human Feedback (RLHF) have attempted to\nsteer LLMs towards more factual outputs, but the fundamental\nprobabilistic nature and its inherent limitations remain. A\nrecent study, “Sources of Hallucination by Large Language\nModels on Inference Tasks” [146], highlights two key aspects\ncontributing to hallucinations in LLMs: the veracity prior and\nthe relative frequency heuristic, underscoring the complexities\ninherent in LLM training and output generation.\nEffective automated measurement of hallucinations in\nLLMs requires a combination of statistical and model-based\nmetrics.\nStatistical Metrics:\n•\nMetrics like ROUGE [147] and BLEU [148] are com-\nmon for assessing text similarity, focusing on intrinsic\nhallucinations.\n•\nAdvanced metrics such as PARENT [149], PARENT-\nT [150], and Knowledge F1 [151] are utilized when\nstructured knowledge sources are available. These\n",
      "word_count": 909,
      "char_count": 5875,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMSY10 (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 22,
      "text": "B) Augmenting LLMs through\nexternal knowledge - RAG\nHow LLMs Are Used and Augmented\nC) Using External Tools\nD) LLM Agents\nFunctionality of an LLM-based agent\nTool Access and Utilization\nDecision Making\nPrompt engineering techniques for agents\nReasoning without Observation\nReason and Act\nDialog-Enabled Resolving Agents\na) RAG-aware prompting techniques\na) Tool-aware prompting techniques\nA) LLM limitations\nHallucination\nHallucination Quantification\nAutomated metrics\nHuman judgment\nStatistical Metrics\nModel-Based Metrics\nScoring\nComparative Analysis\nIE-Based Metrics\nQA-Based Metrics\nNLI-Based Metrics\nB) Using LLMs\n Prompt Design and Engineering\n1) Chain of Thought\nZero-Shot CoT\nManual CoT\n5) Expert Prompting\n6) Chains\n2) Tree of Thought\n7) Rails\nTopical Rails\nFact-Checking Rails\nJailbreaking Rails\n8) Automatic Prompt Engineering\nPrompt Generation\nPrompt Scoring\nRefinement and Iteration\n3) Self-Consistency\n4) Reflection\nComponents of a RAG\nRetrieval \nGeneration \nAugmentation\nRAG Tools\nLangChain \nLlamaIndex\nHayStack\nMeltano\nCohere Coral\nFlowise AI\nFig. 36: How LLMs Are Used and Augmented.\nmetrics, while effective, have limitations in capturing\nsyntactic and semantic nuances.\nModel-Based Metrics:\n•\nIE-Based Metrics: Utilize Information Extraction\nmodels to simplify knowledge into relational tuples,\nthen compare these with the source.\n•\nQA-Based Metrics: Assess the overlap between gen-\nerated content and the source through a question-\nanswering framework (see [152]).\n•\nNLI-Based Metrics: Use Natural Language Inference\ndatasets to evaluate the truthfulness of a generated\nhypothesis based on a given premise (see [153]).\n•\nFaithfulness Classification Metrics: Offer a refined\nassessment by creating task-specific datasets for a\nnuanced evaluation (see [154]).\nDespite advances in automated metrics, human judgment\nremains a vital piece. It typically involves two methodologies:\n1)\nScoring: Human evaluators rate the level of halluci-\nnation within a predefined scale.\n2)\nComparative Analysis: Evaluators compare gener-\nated content against baseline or ground-truth refer-\nences, adding an essential layer of subjective assess-\nment.\nFactScore [155] is a recent example of a metric that can be\nused both for human and model-based evaluation. The metric\nbreaks an LLM generation into “atomic facts”. The final score\nis computed as the sum of the accuracy of each atomic fact,\ngiving each of them equal weight. Accuracy is a binary number\nthat simply states whether the atomic fact is supported by the\nsource. The authors implement different automation strategies\nthat use LLMs to estimate this metric.\nFinally, mitigating hallucinations in LLMs is a multifaceted\nchallenge, requiring tailored strategies to suit various applica-\ntions. Those include:\n•\nProduct Design and User Interaction Strategies such\nas use case design, structuring the input/output, or\nproviding mechanisms for user feedback.\n•\nData Management and Continuous Improvement.\nMaintaining and analyzing a tracking set of hallucina-\ntions is essential for ongoing model improvement.\n",
      "word_count": 434,
      "char_count": 3065,
      "fonts": [
        "Arial-BoldMT (9.8pt)",
        "ArialMT (3.2pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "ArialMT (5.2pt)",
        "CMSY10 (10.0pt)",
        "Arial-BoldMT (5.7pt)",
        "Arial-BoldMT (6.3pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "ArialMT (3.8pt)",
        "Arial-BoldMT (5.2pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 512,
          "height": 512,
          "ext": "png",
          "size_bytes": 17348
        },
        {
          "index": 1,
          "width": 512,
          "height": 512,
          "ext": "png",
          "size_bytes": 863
        },
        {
          "index": 2,
          "width": 282,
          "height": 114,
          "ext": "png",
          "size_bytes": 13277
        },
        {
          "index": 3,
          "width": 2052,
          "height": 1155,
          "ext": "png",
          "size_bytes": 190497
        },
        {
          "index": 4,
          "width": 850,
          "height": 646,
          "ext": "png",
          "size_bytes": 20555
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 23,
      "text": "•\nPrompt Engineering and Metaprompt Design. Many\nof the advanced prompt techniques described in IV-B\nsuch as Retrieval Augmented Generation directly ad-\ndress hallucination risks.\n•\nModel Selection and Configuration for Hallucination\nMitigation. For exemple, larger models with lower\ntemperature settings usually perform better. Also,\ntechniques such as RLHF or domain-sepcific fine-\ntuning can mitigate hallucination risks.\nB. Using LLMs: Prompt Design and Engineering\nA prompt in generative AI models is the textual input\nprovided by users to guide the model’s output. This could\nrange from simple questions to detailed descriptions or specific\ntasks. Prompts generally consist of instructions, questions,\ninput data, and examples. In practice, to elicit a desired\nresponse from an AI model, a prompt must contain either\ninstructions or questions, with other elements being optional.\nAdvanced prompts involve more complex structures, such as\n”chain of thought” prompting, where the model is guided to\nfollow a logical reasoning process to arrive at an answer.\nPrompt engineering is a rapidly evolving discipline that\nshapes the interactions and outputs of LLMs and other gen-\nerative AI models. The essence of prompt engineering lies in\ncrafting the optimal prompt to achieve a specific goal with\na generative model. This process is not only about instructing\nthe model but also involves some understanding of the model’s\ncapabilities and limitations, and the context within which it\noperates.\nPrompt engineering transcends the mere construction of\nprompts; it requires a blend of domain knowledge, understand-\ning of the AI model, and a methodical approach to tailor\nprompts for different contexts. This might involve creating\ntemplates that can be programmatically modified based on a\ngiven dataset or context. For example, generating personalized\nresponses based on user data might use a template that is\ndynamically filled with relevant user information.\nFurthermore, prompt engineering is an iterative and ex-\nploratory process, akin to traditional machine learning prac-\ntices such as model evaluation or hyperparameter tuning. The\nrapid growth of this field suggests its potential to revolutionize\ncertain aspects of machine learning, moving beyond traditional\nmethods like feature or architecture engineering. On the other\nhand, traditional engineering practices such as version con-\ntrol and regression testing need to be adapted to this new\nparadigm just like they were adapted to other machine learning\napproaches [156].\nIn the following paragraphs we detail some of the most\ninteresting and popular prompt engineering approaches.\n1) Chain of Thought (CoT): The Chain of Thought (CoT)\ntechnique, initially described in the paper “Chain-of-Thought\nPrompting Elicits Reasoning in Large Language Models”[34]\nby Google researchers, represents a pivotal advancement in\nprompt engineering for Large Language Models (LLMs).\nThis approach hinges on the understanding that LLMs, while\nproficient in token prediction, are not inherently designed for\nexplicit reasoning. CoT addresses this by guiding the model\nthrough essential reasoning steps.\nCoT is based on making the implicit reasoning process of\nLLMs explicit. By outlining the steps required for reasoning,\nthe model is directed closer to a logical and reasoned output,\nespecially in scenarios demanding more than simple informa-\ntion retrieval or pattern recognition.\nCoT prompting manifests in two primary forms:\n1)\nZero-Shot CoT: This form involves instructing the\nLLM to “think step by step”, prompting it to de-\nconstruct the problem and articulate each stage of\nreasoning.\n2)\nManual CoT: A more complex variant, it requires\nproviding step-by-step reasoning examples as tem-\nplates for the model. While yielding more effective\nresults, it poses challenges in scalability and mainte-\nnance.\nManual CoT is more effective than zero-shot. However,\nthe effectiveness of this example-based CoT depends on the\nchoice of diverse examples, and constructing prompts with\nsuch examples of step by step reasoning by hand is hard and\nerror prone. That is where automatic CoT [157] comes into\nplay.\n2) Tree of Thought (ToT): The Tree of Thought (ToT)\n[158] prompting technique is inspired by the concept of\nconsidering various alternative solutions or thought processes\nbefore converging on the most plausible one. ToT is based\non the idea of branching out into multiple ”thought trees”\nwhere each branch represents a different line of reasoning.\nThis method allows the LLM to explore various possibilities\nand hypotheses, much like human cognitive processes where\nmultiple scenarios are considered before determining the most\nlikely one.\nA critical aspect of ToT is the evaluation of these reasoning\npaths. As the LLM generates different branches of thought,\neach is assessed for its validity and relevance to the query.\nThis process involves real-time analysis and comparison of\nthe branches, leading to a selection of the most coherent and\nlogical outcome.\nToT is particularly useful in complex problem-solving\nscenarios where a single line of reasoning might not suffice.\nIt allows LLMs to mimic a more human-like problem-solving\napproach, considering a range of possibilities before arriving\nat a conclusion. This technique enhances the model’s ability\nto handle ambiguity, complexity, and nuanced tasks, making it\na valuable tool in advanced AI applications.\n3) Self-Consistency:\nSelf-Consistency [159] utilizes an\nensemble-based method, where the LLM is prompted to gen-\nerate multiple responses to the same query. The consistency\namong these responses serves as an indicator of their accuracy\nand reliability.\nThe Self-Consistency approach is grounded in the principle\nthat if an LLM generates multiple, similar responses to the\nsame prompt, it is more likely that the response is accurate.\nThis method involves asking the LLM to tackle a query mul-\ntiple times, each time analyzing the response for consistency.\nThis technique is especially useful in scenarios where factual\naccuracy and precision are paramount.\n",
      "word_count": 911,
      "char_count": 6073,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMSY10 (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 24,
      "text": "The consistency of responses can be measured using vari-\nous methods. One common approach is to analyze the overlap\nin the content of the responses. Other methods may include\ncomparing the semantic similarity of responses or employing\nmore sophisticated techniques like BERT-scores or n-gram\noverlaps. These measures help in quantifying the level of\nagreement among the responses generated by the LLM.\nSelf-Consistency has significant applications in fields\nwhere the veracity of information is critical. It is particularly\nrelevant in scenarios like fact-checking, where ensuring the\naccuracy of information provided by AI models is essential.\nBy employing this technique, prompt engineers can enhance\nthe trustworthiness of LLMs, making them more reliable for\ntasks that require high levels of factual accuracy.\n4) Reflection: Reflection [160] involves prompting LLMs\nto assess and potentially revise their own outputs based on\nreasoning about the correctness and coherence of their re-\nsponses. The concept of Reflection centers on the ability of\nLLMs to engage in a form of self-evaluation. After generating\nan initial response, the model is prompted to reflect on its\nown output, considering factors like factual accuracy, logical\nconsistency, and relevance. This introspective process can lead\nto the generation of revised or improved responses.\nA key aspect of Reflection is the LLM’s capacity for\nself-editing. By evaluating its initial response, the model can\nidentify potential errors or areas of improvement. This iterative\nprocess of generation, reflection, and revision enables the LLM\nto refine its output, enhancing the overall quality and reliability\nof its responses.\n5) Expert Prompting: Expert Prompting [161] enhances the\ncapabilities of Large Language Models (LLMs) by simulating\nthe responses of experts in various fields. This method involves\nprompting the LLMs to assume the role of an expert and re-\nspond accordingly, providing high-quality, informed answers.\nA key strategy within Expert Prompting is the multi-expert\napproach. The LLM is prompted to consider responses from\nmultiple expert perspectives, which are then synthesized to\nform a comprehensive and well-rounded answer. This tech-\nnique not only enhances the depth of the response but also\nincorporates a range of viewpoints, reflecting a more holistic\nunderstanding of the subject matter.\n6) Chains: Chains refer to the method of linking multiple\ncomponents in a sequence to handle complex tasks with Large\nLanguage Models (LLMs). This approach involves creating a\nseries of interconnected steps or processes, each contributing\nto the final outcome. The concept of Chains is based on\nthe idea of constructing a workflow where different stages\nor components are sequentially arranged. Each component in\na Chain performs a specific function, and the output of one\nserves as the input for the next. This end-to-end arrangement\nallows for more complex and nuanced processing, as each\nstage can be tailored to handle a specific aspect of the task.\nChains can vary in complexity and structure, depending on\nthe requirements. In “PromptChainer: Chaining Large Lan-\nguage Model Prompts through Visual Programming” [162],\nthe authors not only describe the main challenges in designing\nchains, but also describe a visual tool to support those tasks.\n7) Rails: Rails in advanced prompt engineering refer to\na method of guiding and controlling the output of Large\nLanguage Models (LLMs) through predefined rules or tem-\nplates. This approach is designed to ensure that the model’s\nresponses adhere to certain standards or criteria, enhancing the\nrelevance, safety, and accuracy of the output. The concept of\nRails involves setting up a framework or a set of guidelines\nthat the LLM must follow while generating responses. These\nguidelines are typically defined using a modeling language or\ntemplates known as Canonical Forms, which standardize the\nway natural language sentences are structured and delivered.\nRails can be designed for various purposes, depending on\nthe specific needs of the application:\n•\nTopical Rails: Ensure that the LLM sticks to a\nparticular topic or domain.\n•\nFact-Checking Rails: Aimed at minimizing the gen-\neration of false or misleading information.\n•\nJailbreaking Rails: Prevent the LLM from generating\nresponses that attempt to bypass its own operational\nconstraints or guidelines.\n8) Automatic\nPrompt\nEngineering\n(APE):\nAutomatic\nPrompt Engineering (APE) [163] focuses on automating the\nprocess of prompt creation for Large Language Models\n(LLMs). APE seeks to streamline and optimize the prompt\ndesign process, leveraging the capabilities of LLMs themselves\nto generate and evaluate prompts. APE involves using LLMs\nin a self-referential manner where the model is employed\nto generate, score, and refine prompts. This recursive use of\nLLMs enables the creation of high-quality prompts that are\nmore likely to elicit the desired response or outcome.\nThe methodology of APE can be broken down into several\nkey steps:\n•\nPrompt Generation: The LLM generates a range of\npotential prompts based on a given task or objective.\n•\nPrompt Scoring: Each generated prompt is then\nevaluated for its effectiveness, often using criteria\nlike clarity, specificity, and likelihood of eliciting the\ndesired response.\n•\nRefinement and Iteration: Based on these evalua-\ntions, prompts can be refined and iterated upon, further\nenhancing their quality and effectiveness.\nC. Augmenting LLMs through external knowledge - RAG\nOne of the main limitations of pre-trained LLMs is their\nlack of up-to-date knowledge or access to private or use-\ncase-specific information. This is where retrieval augmented\ngeneration (RAG) comes into the picture [164]. RAG, illus-\ntrated in figure 37, involves extracting a query from the input\nprompt and using that query to retrieve relevant information\nfrom an external knowledge source (e.g. a search engine or a\nknowledge graph, see figure 38 ). The relevant information is\nthen added to the original prompt and fed to the LLM in order\nfor the model to generate the final response. A RAG system\nincludes three important components: Retrieval, Generation,\nAugmentation [165].\n",
      "word_count": 945,
      "char_count": 6194,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMSY10 (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 25,
      "text": "Fig. 37: An example of synthesizing RAG with LLMs for question answering application [166].\nFig. 38: This is one example of synthesizing the KG as a\nretriever with LLMs [167].\na) RAG-aware prompting techniques: Because of the\nimportance of RAG to build advanced LLM systems, several\nRAG-aware prompting techniques have been developed re-\ncently. One such technique is Forward-looking Active Retrieval\nAugmented Generation (FLARE)\nForward-looking Active Retrieval Augmented Generation\n(FLARE) [168] enhances the capabilities of Large Language\nModels (LLMs) by iteratively combining prediction and in-\nformation retrieval. FLARE represents an evolution in the\nuse of retrieval-augmented generation, aimed at improving the\naccuracy and relevance of LLM responses.\nFLARE involves an iterative process where the LLM\nactively predicts upcoming content and uses these predictions\nas queries to retrieve relevant information. This method con-\ntrasts with traditional retrieval-augmented models that typically\nretrieve information once and then proceed with generation. In\nFLARE, this process is dynamic and ongoing throughout the\ngeneration phase. In FLARE, each sentence or segment gener-\nated by the LLM is evaluated for confidence. If the confidence\nlevel is below a certain threshold, the model uses the generated\ncontent as a query to retrieve relevant information, which is\nthen used to regenerate or refine the sentence. This iterative\nprocess ensures that each part of the response is informed by\nthe most relevant and current information available.\nFor more details on RAG framework and its relevant works,\nwe refer the readers to this survey of retrieval augmented\ngenerations [165].\nD. Using External Tools\nRetrieving information from an external knowledge source\nas described above is only one of the potential ways to augment\nan LLM. More generally, an LLM can access any number\nof external tools (e.g. an API to a service) to augment its\nfunctionality. In that regards, RAG can be seen as a specific\ninstance of the broader category of the so called ”tools”.\nTools in this context are external functions or services that\nLLMs can utilize. These tools extend the range of tasks an\nLLM can perform, from basic information retrieval to complex\ninteractions with external databases or APIs.\nIn the paper ”Toolformer: Language Models Can Teach\nThemselves to Use Tools” [169], the authors go beyond simple\ntool usage by training an LLM to decide what tool to use\nwhen, and even what parameters the API needs. Tools include\ntwo different search engines, or a calculator. In the following\nexamples, the LLM decides to call an external Q&A tool,\na calculator, and a Wikipedia Search Engine More recently,\nresearchers at Berkeley have trained a new LLM called Gorilla\n[67] that beats GPT-4 at the use of APIs, a specific but quite\ngeneral tool.\na) Tool-aware prompting techniques: Similarly to what\nwas described with RAG, several tool-aware prompting ap-\nproaches have been developed to make usage of tools more\nscalable. A popular technique is the so called Automatic Multi-\nstep Reasoning and Tool-use (ART).\nAutomatic Multi-step Reasoning and Tool-use (ART) [170]\nis a prompt engineering technique that combines automated\nchain of thought prompting with the use of external tools.\nART represents a convergence of multiple prompt engineering\nstrategies, enhancing the ability of Large Language Models\n",
      "word_count": 525,
      "char_count": 3400,
      "fonts": [
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 898,
          "height": 532,
          "ext": "png",
          "size_bytes": 130613
        },
        {
          "index": 1,
          "width": 1000,
          "height": 460,
          "ext": "png",
          "size_bytes": 48400
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 26,
      "text": "(LLMs) to handle complex tasks that require both reasoning\nand interaction with external data sources or tools.\nART involves a systematic approach where, given a task\nand input, the system first identifies similar tasks from a task\nlibrary. These tasks are then used as examples in the prompt,\nguiding the LLM on how to approach and execute the current\ntask. This method is particularly effective when tasks require a\ncombination of internal reasoning and external data processing\nor retrieval.\nE. LLM Agents\nThe idea of AI agents has been well-explored in the history\nof AI. An agent is typically an autonomous entity that can\nperceive the environment using its sensors, make a judgment\nbased on the state it currently is, and accordingly act based on\nthe actions that are available to it.\nIn the context of LLMs, an agent refers to a system based\non a specialized instantiation of an (augmented) LLM that\nis capable of performing specific tasks autonomously. These\nagents are designed to interact with users and environment to\nmake decisions based on the input and the intended goal of\nthe interaction. Agents are based on LLMs equipped with the\nability to access and use tools, and to make decisions based on\nthe given input. They are designed to handle tasks that require\na degree of autonomy and decision-making, typically beyond\nsimple response generation.\nThe functionalities of a generic LLM-based agent include:\n•\nTool Access and Utilization: Agents have the capabil-\nity to access external tools and services, and to utilize\nthese resources effectively to accomplish tasks.\n•\nDecision Making: They can make decisions based on\nthe input, context, and the tools available to them,\noften employing complex reasoning processes.\nAs an example, an LLM that has access to a function (or\nan API) such as weather API, can answer any question related\nto the weather of the specific place. In other words, it can use\nAPIs to solve problems. Furthermore, if that LLM has access\nto an API that allows to make purchases, a purchasing agent\ncan be built to not only have capabilities to read information\nfrom the external world, but also act on it [171].\nFig. 40 shows another example of LLM-based agents for\nconversational information seeking [36], where an LLM is\naugmented with a set of plug-and-play modules, including\na working memory that tracks the dialog state, a policy that\nmakes an execution plan for the task and selects next system\naction, an action executor that performs an action selected by\nthe policy (consolidating evidence from external knowledge,\nor prompting the LLM to generate responses), and a utility\nthat accesses the alignment of the LLM’s responses with user\nexpectations or specific business requirements, and generate\nfeedback to improve agent performance.\nFor more details on LLM-based AI agents see recent survey\n[172], [173], [174].\na) Prompt engineering techniques for agents:\nLike\nRAG and Tools, prompt engineering techniques that specif-\nically address the needs of LLM-based agents have been\ndeveloped. Three such examples are Reasoning without Ob-\nservation (ReWOO), Reason and Act (ReAct), and Dialog-\nEnabled Resolving Agents (DERA).\nReasoning without Observation (ReWOO) [175] aims to\ndecouple reasoning from direct observations. ReWOO operates\nby enabling LLMs to formulate comprehensive reasoning plans\nor meta-plans without immediate reliance on external data\nor tools. This approach allows the agent to create a struc-\ntured framework for reasoning that can be executed once the\nnecessary data or observations are available. In ReWOO, the\nLLM initially develops a plan (a series of steps) that outlines\nhow to approach and solve a given problem. This meta-\nplanning phase is crucial as it sets the stage for the agent to\nprocess information once it becomes available. The execution\nphase then involves integrating actual data or observations into\nthe pre-specified plan, leading to coherent and contextually\nrelevant responses. ReWOO offers significant advantages in\nterms of token efficiency and robustness to tool failure. It\nenables LLMs to handle tasks where immediate access to\nexternal data is not available, relying instead on a well-\nstructured reasoning framework. This method is particularly\nadvantageous in scenarios where data retrieval is costly, slow,\nor uncertain, allowing the LLM-based agent to maintain a high\nlevel of performance and reliability.\nReason and Act (ReAct)[176] prompts LLMs to generate\nnot only verbal reasoning but also actionable steps, thus\nenhancing the model’s dynamic problem-solving capabilities.\nReAct is grounded in the principle of integrating reasoning\nwith action. In this approach, the LLM is prompted to alternate\nbetween generating reasoning traces (explanations) and taking\nactions (steps or commands) in an interleaved manner. This\napproach allows the model to dynamically reason about a prob-\nlem, and propose and take concrete actions simultaneously.\nDialog-Enabled Resolving Agents (DERA) [177] are spe-\ncialized AI agents that can engage in dialogue, resolve queries,\nand make decisions based on interactive exchanges. DERA\nis developed based on the idea of utilizing multiple agents\nwithin a dialog context, each with specific roles and functions.\nThese agents can include Researchers, who gather and analyze\ninformation, and Deciders, who make final judgments based\non the information provided. This division of roles allows for\na well-organized and efficient approach to problem-solving\nand decision-making. DERA is particularly advantageous in\nscenarios requiring complex decision-making and problem-\nsolving, such as those in medical diagnostics or customer ser-\nvice. The collaborative and interactive nature of DERA agents\nallows them to handle intricate queries with a level of depth\nand nuance that single-agent systems might struggle with.\nMoreover, this approach aligns well with human decision-\nmaking processes, making AI reasoning more relatable and\ntrustworthy.\nV.\nPOPULAR DATASETS FOR LLMS\nLarge language models exhibit promising accomplish-\nments, but the main question that arises is how effectively\nthey function and how their performance can be assessed in\nspecific tasks or applications.\n",
      "word_count": 958,
      "char_count": 6204,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMSY10 (10.0pt)",
        "NimbusRomNo9L-MediItal (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 27,
      "text": "Fig. 39: HuggingGPT: An agent-based approach to use tools and planning [image courtesy of [171]]\nFig. 40: A LLM-based agent for conversational information\nseeking. Courtesy of [36].\nThe evaluation of LLMs poses particular challenges due\nto the evolving landscape of their applications. The original\nintent behind developing LLMs was to boost the performance\nof NLP tasks such as translation, summarization, question-\nanswering, and so on [178]. However, it is evident today\nthat these models are finding utility across diverse domains\nincluding code generation and finance. Moreover, the eval-\nuation of LLMs encompasses several critical considerations\nsuch as fairness and bias, fact-checking, and reasoning. In\nthis section, we outline the commonly used benchmarks for\nassessing LLMs. These benchmarks are categorized based on\ntraining or evaluating the LLM Capabilities.\nA. Datasets\nfor\nBasic\nTasks:\nlanguage\nmodel-\ning/understanding/generation\nThis section provides an overview of the benchmarks and\ndatasets suited to evaluate the basic abilities of LLMs.\n•\nNatural Questions [179] is a QA dataset that consists\nof real anonymized, aggregated queries submitted to\nthe Google search engine as questions. An annotator\nis presented with a question along with a Wikipedia\npage from the top 5 search results, and annotates a\nlong answer (typically a paragraph) and a short answer\n(one or more entities) if present on the page, or marks\nnull if no long/short answer is present.\n•\nMMLU [180] is intended to evaluate the knowl-\nedge gained in zero-shot and few-shot scenarios. That\nmeans that MMLU assesses both the general knowl-\nedge and problem-solving ability of a model. It covers\n57 subjects in STEM, humanities, social sciences,\nand other areas. The benchmark varies in complexity,\nranging from elementary to advanced professional.\nIt is worth mentioning that the main contribution of\nthis dataset is for multi-task language understanding,\nquestion answering, and arithmetic reasoning.\n•\nMBPP [181] stands for “Mostly Basic Python Prob-\nlems” and provides a benchmark for evaluating the\nperformance of models designed for code generation.\nThe benchmark encompasses 974 short Python pro-\ngrams including a wide range of topics, including\nfundamental programming concepts and standard li-\nbrary usage, and more. Each challenge comprises a\ntask description, a code solution, and three automated\ntest cases.\n•\nHumanEval [182] is a dataset for code generation\ntask. This dataset consists of 164 hand-crafted pro-\ngramming challenges. Each challenge is accompanied\nby a function signature, docstring, code body, and mul-\ntiple unit tests. The main intuition behind developing\nthis dataset is to guarantee the exclusion of its contents\nfrom training datasets for code generation models.\n•\nAPPS [183] is designed for code generation task\nfocusing on the Python programming language. The\nAPPS dataset contains a collection of 232, 444 Python\nprograms. Each program in the dataset has an average\nof 18 lines of Python code. Additionally, APPS offers\naccess to a repository of 10, 000 unique programming\n",
      "word_count": 472,
      "char_count": 3097,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMMI10 (10.0pt)",
        "CMSY10 (10.0pt)",
        "CMR10 (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1589,
          "height": 701,
          "ext": "jpeg",
          "size_bytes": 606591
        },
        {
          "index": 1,
          "width": 594,
          "height": 422,
          "ext": "png",
          "size_bytes": 57825
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 28,
      "text": "Fig. 41: Dataset applications.\nexercises, each with text-based problem descriptions.\nThe final aspect to highlight is that the it includes test\ncases.\n•\nWikiSQL [184] is crafted for code generation task and\nit has 87,726 carefully labeled pairs of SQL queries\nand corresponding natural language questions from\nWikipedia tables. The SQL queries comprise three\nsubsets: test sets (17, 284 examples), development\n(9, 145 examples), and training (61, 297 examples).\n•\nTriviaQA [185] is designed for QA task. This\ndataset\ncomprises\nmore\nthan\n650, 000\nquestion-\nanswer-evidence triples. There are 95, 000 question-\nanswer pairs in this dataset, each authored by trivia en-\nthusiasts and supported by an average of six indepen-\ndently sourced evidence documents. These documents\nare automatically acquired from Wikipedia or broader\nweb search results. The dataset is categorized into\ntwo segments, including those with authentic answers\nfrom Wikipedia and web domains, and verified sets\nembody the accurately answered questions along with\ntheir associated documents from both Wikipedia and\nonline.\n•\nRACE [186] suits for reading comprehension task.\n",
      "word_count": 170,
      "char_count": 1142,
      "fonts": [
        "CMMI10 (10.0pt)",
        "CMSY10 (10.0pt)",
        "CMR10 (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1024,
          "height": 1024,
          "ext": "png",
          "size_bytes": 179532
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 29,
      "text": "Fig. 42: Datasets licensed under different licenses.\nThis dataset is based on English tests completed by\nChinese students from middle school and high school,\naged 12 to 18, and it contains roughly 28, 000 texts\nand 100, 000 questions rigorously prepared by human\nspecialists, primarily English instructors. This dataset\ncontains a wide range of subjects that were purpose-\nfully chosen to assess students’ comprehension and\nreasoning abilities. This dataset is available in three\nsubgroups: RACE-M, RACE-H, and RACE. RACE-\nM refers to the middle school examinations, whereas\nRACE-H denotes the high school tests. Finally, RACE\nis the synthesis of RACE-M and RACE-H.\n•\nSQuAD [187] stands for “Stanford Question Answer-\ning Dataset” and is a crowdsourced reading compre-\nhension dataset based on Wikipedia articles. It has\napproximately 100, 000 question-answer pairs con-\nnected to more than 500 articles. The answers to\nthese questions are typically text fragments or spans\ntaken from the corresponding reading passages. The\nquestions may be unanswerable in some cases. The\ndataset is divided into three sets: an 80% training set,\na 10% development set, and a 10% hidden test set.\n•\nBoolQ [188] is a yes/no question-answering dataset\nwhere the goal is reading comprehension task. BoolQ\nincludes 15, 942 examples. Each example is a triplet\nthat includes a question, a relevant paragraph, and\nthe solution. Although the main intuition behind\nthis dataset is for reading comprehension, it can be\nused for reasoning, natural language inference, and\nquestion-answering tasks.\n•\nMultiRC [189] is another dataset that fits reading\ncomprehension task. MultiRC contains brief para-\ngraphs as well as multi-sentence questions that can\nbe answered using the information in the paragraph.\nThe paragraphs in this dataset come from a variety\nof sources, including news, fiction, historical texts,\nWikipedia articles, discussions on society and law,\nelementary school science textbooks, and 9/11 re-\nports. Each question has many response choices, with\none or more of them being correct. Answering the\nquestions requires reasoning across several sentences.\nMultiRC dataset encompasses around 6, 000 multi-\nsentence questions gathered from over 800 paragraphs.\nOn average, each question offers about two valid\nanswer alternatives out of a total of five.\nB. Datasets for Emergent: ICL, reasoning (CoT), instruction\nfollowing\nThis section centers on the benchmarks and datasets em-\nployed to evaluate the emergent abilities of LLMs.\n•\nGSM8K [190] is designed to evaluate the model’s\nability for multi-step mathematical reasoning. GSM8K\nincludes 8.5K linguistically diverse grade school math\nword problems written by humans. The dataset is split\ninto two sets: a training set with 7.5K problems,\nand a test set with 1K problems. These problems\nneed 2 to 8 steps to be solved. Solutions mainly\n",
      "word_count": 441,
      "char_count": 2874,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMMI10 (10.0pt)",
        "CMSY10 (10.0pt)",
        "CMR10 (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 562,
          "height": 511,
          "ext": "png",
          "size_bytes": 24277
        }
      ],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 30,
      "text": "are a series of elementary calculations using basic\narithmetic operations.\n•\nMATH [191] enables to assess how well models can\nsolve math problems. MATH dataset hast 12, 500\nproblems from high school math competitions. Each\nproblem in the dataset has a step-by-step solution and\na final answer enclosed in a box. The problems cover\na wide range of topics and have different levels of\ncomplexity. There are seven subjects in total. Further-\nmore, the difficulty of each problem is rated based\non the AoPS standards on a scale from ′1′ to ′5′. A\n′1′ shows the easiest problems in a subject, while ′5′\nrepresents the most difficult. In terms of formatting,\nall problems and solutions are presented using LATEX\nand the Asymptote vector graphics language.\n•\nHellaSwag [192] is designed to assess commonsense\nreasoning in LLMs. This benchmark includes 70, 000\nmultiple-choice questions. Each question is derived\nfrom one of two domains: ActivityNet or WikiHow,\nand presents four answer choices regarding what\nmight happen in the following situation. The correct\nanswer provides an actual statement describing the\nupcoming event, but the three wrong answers are\ncreated to confuse machines.\n•\nAI2 Reasoning Challenge (ARC) [193] is used\nfor commonsense reasoning. This benchmark encom-\npasses 7, 787 science examination questions. These\nquestions are in English, and most of them are set\nup in a multiple-choice format. The questions have\nbeen divided into two groups: a Challenge Set with\n2, 590 difficult questions and an Easy Set with 5,197\nquestions. Each collection has also been pre-divided\ninto Train, Development, and Test subsets.\n•\nPIQA [194] is intended to evaluate the language\nrepresentations on their knowledge of physical com-\nmonsense. In this dataset, the focus is on everyday\nsituations with a preference for uncommon solutions.\nThe central task is a multiple-choice question answer-\ning, where a question (q) is provided along with two\npotential solutions (s1, s2). Then, the best solution is\nchosen by whether a model or a human. For each\nquestion, only one of the solutions is the correct\nanswer.\n•\nSIQA [195] provides a framework for evaluating mod-\nels’ ability for commonsense reasoning about social\nsituations. SIQA dataset has 38, 000 multiple-choice\nquestions designed to assess emotional and social\nintelligence in everyday circumstances. This dataset\ncovers a wide variety of social scenarios. In SIQA,\nthe potential answers is a mixture of human-selected\nresponses and machine-generated ones that have been\nfiltered through adversarial processes.\n•\nOpenBookQA (OBQA) [196] is a new kind of\nquestion-answering dataset where answering its ques-\ntions requires additional common and commonsense\nknowledge not contained in the book and rich text\ncomprehension. This dataset includes around 6,000\nmultiple-choice questions. Each question is linked to\none core fact, as well as an additional collection\nof over 6000 facts. The questions were developed\nusing a multi-stage crowdsourcing and expert filter-\ning procedure. OpenBookQA questions are difficult\nbecause they need multi-hop reasoning with limited\nbackground.\n•\nTruthfulQA [197] is designed specifically to eval-\nuate the truthfulness of language models in gen-\nerating answers to questions. This dataset includes\n817 questions, written by authors, from 38 different\ncategories, including health, law, finance, and politics.\nThese questions are purposefully designed to chal-\nlenge human responders, as they may contain common\nmisunderstandings that lead to incorrect answers.\n•\nOPT-IML Bench [103] is a comprehensive bench-\nmark for Instruction Meta-Learning. It covers 2000\nNLP tasks from 8 existing benchmarks. The OPT-IML\nBench consists of a training set with 17.9 M examples,\na dev set with 145K samples, and a test set with 321K\nsamples.\nC. Datasets for Augmented: using external knowledge/tools\nThis section focuses on datasets designed for the aug-\nmented abilities of LLMs.\n•\nHotpotQA [198] is designed to cover a diverse and\nexplainable question-answering dataset that necessi-\ntates multi-hop reasoning. This dataset is derived from\nthe English Wikipedia. It consists of roughly 113, 000\nquestions. Each question in the dataset comes with\ntwo paragraphs, called gold paragraphs, from two\nWikipedia articles. Also, there is a list of sentences\nin those paragraphs that crowdworkers have picked as\nimportant for answering the question.\n•\nToolQA [199] is a question answering benchmark\nto evaluate LLMs’ ability to use external tools for\nanswering questions.\n•\nGPT4Tools serves as an instructional dataset, gener-\nated by instructing advanced teachers (such as Chat-\nGPT), with instructions conditioned on visual content\nand tool descriptions. This process results in the\ngeneration of instructions related to the use of tools.\nThere are three versions of this dataset. The first\nversion comprises 71,000 instruction-following data\npoints utilized to fine-tune the GPT4Tools model. The\nnext version consists of manually cleaned instruction\ndata used for validation, covering instructions related\nto the tools from the first version. The last version is\ncleaned instruction data used for testing and includes\ninstructions related to some tools that are not present\nin the first version.\nVI.\nPROMINENT LLMS’ PERFORMANCE ON\nBENCHMARKS\nIn this section we first provide an overview of some of\npopular metrics used for evaluating the performance of LLMs\nunder different scenarios. We then look at the performance\nof prominent large language models on some of the popular\ndatasets and benchmarks.\n",
      "word_count": 853,
      "char_count": 5572,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "CMMI10 (10.0pt)",
        "CMSY10 (10.0pt)",
        "CMR10 (10.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)",
        "CMSY7 (7.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 31,
      "text": "TABLE II: LLM Datasets Overview.\nBenchmark Name\nEvaluation Metric\nLeaderboard\nSource\npaperswithcode\nHumanEval\nPASS@k\nLink\nLink\nLink\nMBPP\nPASS@k, Accuracy\n-\nLink\nLink\nAPPS\nPASS@k, Accuracy\n-\nLink\nLink\nWikiSQL\nAccuracy\n-\nLink\nLink\nCoNaLa\nBLEU\nLink\nLink\nCodeParrot\nPASS@k\n-\nLink\n-\nHellaSwag\nAccuracy\nLink\nLink\nLink\nAI2\nReasoning\nChallenge (ARC)\nAccuracy\nLink\nLink\nLink\nBoolQ\nAccuracy\n-\nLink\nLink\nMultiRC\nF1-score, Accuracy\n-\nLink\nLink\nCNN/Daily Mail [200]\nAccuracy\n-\nLink\n-\nSQuAD\nF1-score, EM\nLink\nLink\nLink\nRACE\nAccuracy\n-\nLink\nLink\nCNN/Daily Mail [201]\nROUGE\n-\nLink\nLink\nDrop\nF1-score, EM\nLink\nLink\nLink\nQuAC\nF1-score, HEQ-Q, HEQ-D\nLink\nLink\nLink\nTriviaQA\nEM, F1-score, Accuracy\nLink\nLink\nLink\nNatural Questions\nEM, F1-score, Accuracy\nLink\nLink\nLink\nStrategyQA\nAccuracy, Recall@10, SARI\nLink\nLink\nLink\nCoQA\nF1-score\nLink\nLink\nLink\nXSum\nROUGE\n-\nLink\nLink\nSAMSum\nROUGE\n-\n-\nLink\nWikiSum\nROUGE\n-\nLink\n-\nDialogSum\nROUGE\n-\nLink\nLink\nTruthfulQA\nMC1 , MC2, % true, % info, BLEURT\nLink\nLink\nLink\nMMLU\nAccuracy\nLink\nLink\nLink\nGSM8K\nAccuracy\nLink\nLink\nLink\nPIQA\nAccuracy\nLink\nLink\nLink\nSIQA\nAccuracy\nLink\nLink\nLink\nOpenBookQA (OBQA)\nAccuracy\nLink\nLink\nLink\nHotpotQA\nEM, F1-score, Joint EM, Joint F1-score,\nLink\nLink\nLink\nMATH\nAccuracy\n-\nLink\nLink\nCommonsenseQA\nAccuracy\nLink\nLink\nLink\nNatural Instructions\nROUGE-L, Human\nLink\nLink\nLink\nBIG-bench\nAccuracy, Average\n-\nLink\nLink\nToolTalk\nSuccess rate, Precision, Recall, Incorrect\naction rate, Percent of failing error types\n-\nLink\nLink\nMetaTool\nAccuracy, Precision, Recall, F1-score\n-\nLink\nLink\nGPT4Tools\nSuccessful Rate of Thought, Successful\nRate of Action, Successful Rate of Ar-\nguments, Success Rate\n-\nLink\nLink\nAPI-Bank\nCorrectness, ROUGE, Error(API Hallu-\ncination, Has Exception, Invalid Input\nParameters, False API Call Format, API\nCall, Miss Input Parameters)\n-\nLink\nLink\nAlpaca-CoT\n-\n-\nLink\nLink\nA. Popular Metrics for Evaluating LLMs\nEvaluating the performance of generative language models\ndepends on the underlying task they are going to be used for.\nTasks that are mostly about selecting a choice out of given\nones (such as sentiment analysis), can be seen as simple as\nclassification and their performance can be evaluated using\nclassification metrics. Metrics such as accuracy, precision,\nrecall, F1, etc are applicable in this case. It is also important to\nnote that the answers generated by the model for specific tasks\nsuch as multi-choice question answering are always either True\nor False. If the answer is not in a set of options, it can be seen\nas False as well.\nHowever, some tasks that are purely open-ended text gener-\nation cannot be evaluated in the same way as for categorization.\nDifferent metrics are required for the specific purpose of the\nevaluation. Code generation is a very different case in open-\nended generative evaluations. The generated code must pass\nthe test suite but on the other hand, it is also important\nto understand if a model is capable of generating different\nsolutions as a code, what is the probability of selecting the\ncorrect one among them. Pass@k is a very good metric in this\ncase. It works in this manner that given a problem, different\nsolutions as code are generated. They are tested for correctness\nusing different functionality tests. Afterward, from generated\nn solutions, and the respective c number of them being correct\nequation 4 provides the final value.\npass@k :=\nE\nProblems\n\"\n1 −\n\u0000n−c\nk\n\u0001\n\u0000n\nk\n\u0001\n#\n(4)\nExact match (EM) is another metric that is mostly con-\ncerned with exact matches from (pre-defined) answers. It\ncounts a prediction as correct if it exactly matches one of\nmore than one desired reference text token by token. In some\ncases, it can be the same as accuracy and the equation 5 shows\nthe mathematical definition. Here M is total number of correct\nanswers and N is the total number of questions [202].\nEM = M\nN\n(5)\n",
      "word_count": 640,
      "char_count": 3838,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "NimbusRomNo9L-Regu (7.0pt)",
        "CMMI10 (10.0pt)",
        "NimbusRomNo9L-Medi (7.0pt)",
        "CMEX10 (10.0pt)",
        "CMSY10 (10.0pt)",
        "MSBM10 (10.0pt)",
        "CMR10 (10.0pt)",
        "CMMI7 (7.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "CMSY7 (7.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 32,
      "text": "Human equivalence score (HEQ) on the other hand, is an\nalternative to F1 score [203]. HEQ-Q represents the precision\nof individual questions, wherein an answer is deemed correct\nif the model’s F1 score surpasses the average human F1 score.\nLikewise, HEQ-D denotes the precision of each dialogue; it is\ndeemed accurate when all questions within the dialogue meet\nthe criteria of HEQ [182].\nEvaluation of other generative tasks such as machine trans-\nlation are based on metrics such as Rouge and BLEU. These\nscores work well when there is a reference text as ground\ntruth (such as translation) and a hypothesis that is generated\nby the generative model, in our case the LLM. These scores\nare mostly used for cases where the goal is to detect the\nsimilarity of the answer and ground truth in a computation\nmanner. In a computation manner, it meant that nothing more\nthan N-Grams would be used. However, metrics such as BERT-\nScore are also good for these cases but they are also heavily\nerroneous because another model is used to judge. Still, even\ntoday, evaluating purely generated content is very hard and\nno completely fitting metric is not found, metrics are either\nlooking for simplistic features such as N-Gram, SkipGram,\netc, or they are models with unknown accuracy and preciseness\n[204].\nGenerative evaluation metrics are also another type of eval-\nuation metric for LLMs that use another LLM for evaluating\nthe answer. However, depending on the task itself, evaluation\ncan be possible in this way or not. Another dependency\nthat makes generative evaluation error-prone is reliance on\nthe prompt itself. RAGAS is one of the good examples that\nincorporate the usage of generative evaluation.\nVarious benchmarks and leaderboards have been proposed\nto address the most challenging question in the world of\nlarge language models: Which one is better? However not\na simple answer can address this question. The answer de-\npends on various aspects of large language models. Section V\nshows the categorical presentation of different tasks and the\nmost important datasets in each category. We will follow the\nsame categorization and provide a comparison based on each\ncategory. After providing comparison for each category, we\nwill provide a broad overview of aggregated performance by\naveraging the reported performance metric on different tasks.\nEvaluating different LLMs can be seen also from different\nperspectives. For example, a LLM with a drastically fewer\nnumber of parameters is not completely comparable to one\nwith a larger number of parameters. From this perspective, we\nwill categorize LLMs in four categories as well: small (less\nthan or equal to 1 billion parameters), medium (between 1 and\n10 billion), large (between 10 and 100 billion), and very large\n(more than 100 billion). Another classification for the LLMs\nwe use is their primary use case. We consider each LLM to\nbe either: Foundation model (pretrained language model with\nno instruction fine-tuning and chat fine-tuning), Instruction\nmodel (pretrained language model with only instruction fine-\ntuning), and Chat model (pretrained language model with\ninstruction and chat fine-tuning). Apart from all the catego-\nrization described, another category is required to distinguish\nbetween original models and tuned ones. Original models are\nthose that have been released as a foundation model or a fine-\ntuned one. Tuned models are those that grasped the original\nmodel and tuned it with different datasets or even different\ntraining approaches. It is also good to note that original models\nare usually foundation models that have been fine-tuned on\nspecific datasets or even different approaches. Availability of\nthe model weights regardless of the license is another category\nin our classification. Models that have their weights publicly\navailable (even through request) are noted as Public models\nwhile others are noted as Private. Table III shows all of these\ndefinitions and abbreviations used in the rest of the article.\nFigure 43 illustrate these visually.\nAccording to the provided categorizations, we can catego-\nrize and label each notable LLM as shown in table IV. As can\nbe seen from this table, models categorized as very large are\nalso unavailable as well.\nB. LLMs’ Performance on Different Tasks\nCommonsense reasoning is one of the important capabili-\nties each model can obtain. This capability denotes the ability\nof the model to use prior knowledge in combination with\nreasoning skills. In the case of HellaSwag for example, finding\nthe continuation of text is challenging because the given text\ncontains a partial part of the story while the given choices\nas continuation are tricky to select, and without having prior\nknowledge about the world it is not possible. This specific kind\nof reasoning deserves high attention because it is related to\nutilizing previous knowledge with open text-described scenes\nor facts. As can be seen from table V not just Unavailable\nmodels but also Public ones can achieve good results on\nvarious tests.\nTABLE V: Commonsense reasoning comparison.\nModel\nOBQA\nHellaSwag\nDavinci-003\n51\n83.4\nFalcon 7B\n44.4\n76.3\nAlpaca\n43.4\n73.9\nPythia 7B\n37.2\n64\nPythia 12B\n43.2\n68.1\nLLAMA 7B\n42.4\n73\nDolly 6B\n41.2\n67.6\nDolly 12B\n40.4\n71\nAlpaca 7B\n43.4\n73.9\nAlpaca Lora 7B\n42.6\n74\nGPT-J 6.7B\n38.2\n66.2\nLLama 7B\n42.4\n73\nLLama 13B\n42.2\n76.2\nPythia 6.7B\n37.2\n64\nPythia 12B\n38\n67.3\nStableLM Tuned\n33.4\n53.6\nKoala 13B\n42.8\n72.6\nMosaic mpt-7B\n42.6\n76.3\nLLAMA 2 70B\n-\n87.33\nLLAMA 65B\n-\n86.09\nFalcon 40B\n-\n85.3\nFalcon 180B\n-\n88.86\nMPT Instruct 30B\n-\n84.31\nMPT Instruct 7B\n-\n77.91\nYi 6B\n-\n76.42\nYi 34B\n-\n85.69\nGPT-4\n-\n95.3\nGemini Ultra\n-\n87.8\nFrom the results presented in Table V it is clear that GPT-4\nachieves best results for HellaSwag while Davinci-003 is best\nmodel for OBQA. It is also good to note that results for OBQA\nare not reported for all of the models and possibly davinci-003\nis not the best model achieving highest results on OBQA.\n",
      "word_count": 980,
      "char_count": 5946,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "NimbusRomNo9L-Regu (7.0pt)",
        "NimbusRomNo9L-Medi (7.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 33,
      "text": "TABLE III: LLM categories and respective definitions.\nClassification\nCategory\nDescription\nSize\nSmall\nNumber of parameters ≤ 1B\nMedium\n1B < Number of parameters ≤ 10B\nLarge\n10B < Number of parameters ≤ 100B\nVery Large\n100B < Number of parameters\nType\nFoundation model\nPretrained language model\nInstruction model\nPretrained and instruction fine-tuned language model\nChat model\nPretrained, instruction fine-tuned, and chat fine-tuned language model\nOrigin\nOriginal model\nAn original model released with either Foundation, Instruction, or Chat model\nTuned model\nFine-tuned version of an original model\nAvailability\nPublicly available\nModel and weights are available due to request to without request\nPublicly unavailable\nModel and weights are not publicly available\nTABLE IV: Different LLM categorization.\nModel\nSize\n#Params (B)\nType\nAvailability\nOrigin\nDavinci-002\nVery Large\n175\nInstruction\nUnavailable\nTuned\nDavinci-003\nVery Large\n175\nInstruction\nUnavailable\nTuned\nGPT 3.5-turbo\nLarge\n20\nChat\nUnavailable\nTuned\nFalcon 7B\nMedium\n7\nFoundation\nPublic\nOriginal\nAlpaca\nLarge\n13\nChat\nPublic\nTuned\nPythia 7B\nMedium\n7\nFoundation\nPublic\nOriginal\nPythia 12B\nLarge\n12\nFoundation\nPublic\nOriginal\nLLAMA 7B\nMedium\n7\nChat\nPublic\nOriginal\nLLAMA 2 7B\nMedium\n7\nChat\nPublic\nTuned\nLLAMA 2 7B\nMedium\n7\nFoundation\nPublic\nOriginal\nVicuna 13B\nLarge\n13\nFoundation\nPublic\nTuned\nVicuna 7B\nMedium\n7\nFoundation\nPublic\nTuned\nClaude\nLarge\n93\nChat\nUnavailable\nOriginal\nClaude 2\nVery Large\n137\nChat\nUnavailable\nOriginal\nNot all models report their performance on all datasets, and\nbecause of that, the number of models for which performance\nis reported in different tables varies.\nTABLE VI: Symbolic reasoning comparison.\nModel\nCobjects\nPenguins\nGPT-NeoX\n26\n33.56\nOPT 66B\n31.2\n28.08\nBloomberg GPT\n34.8\n37.67\nBLOOM 176B\n36.8\n40.41\nPaLM 540B\n38\n44.5\nGopher-280B\n49.2\n40.6\nChinchilla-70B\n59.7\n48.7\nPaLM 2\n61.2\n65.8\nWorld knowledge is mostly about general knowledge ques-\ntions, for example, in Wikifact dataset questions such as ”Who\nis the author of a specific well-known book” can be found and\nreferences are also provided. Table VII shows the results.\nTABLE VII: World knowledge comparison.\nModel\nTriviaQA\nNaturalQ\nWebQ\nARC\nBLOOM\n-\n-\n-\n32.9\nBLOOM 176B\n-\n-\n-\n50.85\nBloomberg GPT\n-\n-\n-\n48.63\nChinchilla\n-\n35.5\n-\n-\nCodex + REPLUG\n76.8\n44.7\n-\n-\nGAL 120B\n-\n-\n-\n67.9\nGLaM 62B/64E\n75.8\n32.5\n15.5\n50.3\nGopher\n-\n28.2\n-\n-\nGPT-3 175B\n71.2\n29.9\n41.5\n85.2\nGPT-4\n-\n-\n-\n96.4\nGPT-NeoX\n-\n-\n-\n45.39\nLLaMA 13B\n-\n-\n-\n52.7\nLLaMA 2 70B\n85\n33\n-\n-\nLLaMA 33B\n-\n24.9\n-\n57.8\nLLaMA 65B\n72.6\n39.9\n-\n-\nLLaMA 7B\n-\n-\n-\n47.6\nMistral 7B\n69.9\n28.8\n-\n55.5\nNeo-6B\n-\n13.7\n-\n-\nOPT\n-\n-\n-\n31.1\nOPT 66B\n-\n-\n-\n44.54\nOPT-175B\n-\n-\n-\n43.94\nOPT-175B\n-\n-\n-\n25.6\nPaLM 2-L\n86.1\n37.5\n28.2\n95.1\nPaLM 2-M\n81.7\n32\n26.9\n64.9\nPaLM 2-S\n75.2\n25.3\n21.8\n59.6\nPaLM-540B\n81.4\n39.6\n43.5\n87.1\nphi-1.5-web 1.3B\n-\n-\n-\n44.9\nSparseGPT\n-\n-\n-\n38.99\nSparseGPT\n-\n-\n-\n39.85\nSparseGPT\n-\n-\n-\n41.3\nFor some specific use-case models, it is highly demanded to\nhave coding and code-generation capability. Table VIII shows\nthe results of different models on coding capability.\n",
      "word_count": 527,
      "char_count": 3063,
      "fonts": [
        "NimbusRomNo9L-Regu (7.0pt)",
        "CMMI7 (7.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "CMSY7 (7.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 34,
      "text": "Large\nLanguage\nModels\nParameters\nAvailability\nOriginality\nType\nSmall LM\n# of params <1B\nMedium LM\n1B < # of params <10B\nLarge LM\n10B < # of params <100B\nVery Large LM\n100B < # of params\nTuned\nFine tuning\nOriginal\nPublic\nPrivate\nFoundation\nInstruction\nChat\nFine tuned models that are originally\nbased on original models.\nExample: Alpaca (based on LLaMA)\nOriginal models that are not fine\ntuned or based on any other\npretrained model.\nExample: LLaMA\nModel weights are publicly released\nand is available.\nExample: LLaMA\nModel weights are NOT publicly\nreleased and is NOT available.\nExample: GPT-4\nPretrained model with no instruction\nor chat fine-tuning.\nExample: MPT-7B\nPretrained model that is\nalso fine-tuned on\ninstruction following.\nExample: MPT-7B-instruct\nPretrained model that is\nalso fine-tuned on chat.\nExample: MPT-7B-chat\nFig. 43: LLM categorizations.\nTABLE VIII: Coding capability comparison.\nModel\nHumanEval\nGemini Ultra\n74.4\nGemini Pro\n67.7\nGPT-4\n67\nWizardCoder 15B\n57.3\nphi-1 1.3B\n50.6\nCode Llama\n48.8\nGPT-3.5\n48.1\nOctoCoder\n46.2\nphi-1-small\n45\nPaLM 2-S\n37.6\nInstructCodeT5+ 16B\n35\nMistral 7B\n30.5\nLLaMA 2\n29.9\nphi-1-base\n29\nCodex-12B\n28.81\nPaLM 540B\n26.2\nCodeT5+ 2B\n24.2\nLLaMA 65B\n23.7\nLLaMA 33B\n21.7\nPaLM 62B\n15.9\nLLaMA 13B\n15.8\nLaMDA 137B\n14\nMIM-350M\n13.7\nLLaMA 7B\n10.5\nPaLM 8B\n3.6\nArithmetic reasoning is another challenging reasoning ca-\npability to achieve. GSM8K for example contains grade school\nmathematical questions with respect to their answers. Table IX\nprovides an insight for different model comparisons.\nTABLE IX: Arithmetic reasoning comparison.\nModel\nGSM8k\nMATH\nGemini Ultra\n94.4\n53.2\nGPT-4\n87.1\n42.5\nGemini Pro\n86.5\n32.6\nToRA 70B\n84.3\n49.7\nMathCoder-L-70B\n83.9\n-\nMetaMath 70B\n82.3\n26\nMuggleMATH 70B\n82.3\n-\nMathCoder-CL-34B\n81.7\n45.2\nToRA-Code 34B\n80.7\n50.8\nMetaMath-Mistral-7B\n77.7\n-\nArithmo2-Mistral-7B\n76.4\n-\nToRA-Code 13B\n75.8\n48.1\nArithmo-Mistral-7B\n74.7\n-\nMathCoder-CL-13B\n74.1\n35.9\nMuggleMATH 13B\n74\n-\nCodeT5+\n73.8\n-\nKwaiYiiMath 13B\n73.3\n-\nToRA-Code 7B\n72.6\n44.6\nMathCoder-L-13B\n72.6\n29.9\nMetaMath 13B\n71\n22.5\nLLaMA 65B\n69.7\n10.6\nMuggleMATH 7B\n68.4\n-\nMathCoder-CL-7B\n67.8\n23.3\nMetaMath 7B\n66.4\n19.4\nRFT 70B\n64.8\n-\nMathCoder-L-7B\n64.2\n-\nOrca 2-13B\n59.14\n-\nU-PaLM\n58.5\n-\nPaLM-540B\n58.1\n8.8\nLLaMA 2 70B\n56.8\n-\nRFT 13B\n55.3\n-\nLLaMA 33B\n53.1\n7.1\nMistral 7B\n52.2\n13.1\nRFT 7B\n51.2\n-\nLLaMA 65B\n50.9\n20.5\nOrca 2-7B\n47.23\n-\nText-davinci-002\n40.7\n19.1\nLLaMA 33B\n35.6\n3.9\nGPT-Neo-2.7B\n19.5\n-\nLLaMA 7B\n18.1\n2.9\nPaLM 540B\n17.9\n8.8\nLLaMA 13B\n17.8\n3.9\nLLaMA 7B\n11\n2.9\nGPT-Neo-125M\n7.5\n-\nP LM 8B\n4 1\n1 5\n",
      "word_count": 417,
      "char_count": 2525,
      "fonts": [
        "ArialMT (4.1pt)",
        "ArialMT (4.8pt)",
        "NimbusRomNo9L-Regu (7.0pt)",
        "ArialMT (8.2pt)",
        "Arial-BoldMT (8.2pt)",
        "ArialMT (7.5pt)",
        "Arial-BoldMT (4.8pt)",
        "NimbusRomNo9L-Regu (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 35,
      "text": "Large language models in some cases are hallucinating an-\nswers simply because they are next-token prediction machines.\nHallucination is one of the important factors in measuring\nhow much a large language model is trustworthy and reliable.\nMeasuring hallucination on the other hand is also not easy as it\nseems because each fact can be written in different styles and\neven the smallest changes in writing make it hard to detect.\nIt is fair to assume if any particular LLM is more capable\nto detect hallucination of false information in text, it is also\nmore trustworthy. HaluEval is one of the datasets that aims to\nmeasure hallucination in this field [205]. Evaluation can also be\nperformed by another model judging the response with regard\nto the actual answer [206]. Table X shows the evaluation of\ndifferent models based on these datasets.\nVII.\nCHALLENGES AND FUTURE DIRECTIONS\nAs we have seen in the previous sections, large language\nmodels have achieved impressive results in the past 1-2 years.\nAt the same time this is still a new and extremely active\nresearch area where the pace of innovation is increasing rather\nthan slowing down. As in any other evolving area though, there\nare still numerous challenges ahead. Here we briefly mention\nsome of the challenges and main active areas which are known\nso far. It is worth noting that LLM challenges are discussed\nin details in a work by Kaddour et al. [207].\nA. Smaller and more efficient Language Models\nThis is a survey on large language models, and there\nhas been an initial push towards ”larger is better” that has\nclearly been rewarded with ever larger models like GPT-\n4 getting better accuracy and performance in benchmarks.\nHowever, those large models are costly and inefficient in\nseveral dimensions (e.g. high latency). In response to all of\nthis, there is a current research trend to come up with Small\nLanguage Models (SLMs) as a cost-effective alternative to\nLLMs, particularly when used on specific tasks that might not\nrequire the full generality of larger models. Prominent works\nin this direction include Phi-1 [208], Phi-1.5 [209], and Phi-2\nfrom Microsoft.\nMore generally, we should expect many research efforts in\nthis area of how to train smaller and more efficient models.\nTechniques such as parameter-efficient fine-tuning (PEFT),\nteacher/student, and other forms of distillation – see section\nIII-I – will continue to be used to build a smaller model out\nof larger ones.\nB. New Post-attention Architectural Paradigms\nTransformer blocks have been a crucial and constant part of\nmost of current LLM frameworks, and it’s a big question mark\nhow much longer this architecture will be in vogue, and what\nwill be the next big architectural break-through in the field of\ndeep learning (and NLP). Since AlexNet in 2012, we have seen\nmany architectures go in and out of fashion, including LSTM,\nGRU, seq2seq, but Transformers have been the dominant\napproach since its inception. As described earlier, attention is\nthe main mechanism driving transformers. More recently, there\nhas been promising research in alternative approaches that are\nbeing labelled as post-attention.\nAn important class of such class of post-attention models\nare the so called State Space Models (SSMs). While the notion\nof State Space Models has a long history in machine learning,\nit should be noted that in the context of language models, SSM\nis usually used in reference to the newer Structure State Space\nModel architecture or S4 for short (see Gu et al. [29]). Some\nrecent models in this category are Mamba [30], Hyena [210],\nand Striped Hyena [211].\nWhile all of those models are very competitive in terms of\nperformance in leaderboards and efficiency, they also address\nan important challenge in more traditional attention-based\narchitectures: the lack of support for larger context windows.\nHaving a good answer to many prompts requires context.\nFor example, the response to ”Recommend some good movies\nfor me” requires a lot of context about ”me” as well as what\nmovies are available and which ones I have not watched.\nContext length is especially important for RAG, where large\nportions of text might be retrieved and injected into the prompt\nfor generation (see section IV-C.\nThe longer the context length, the more tokens we can\nsqueeze into the context. The more information the model has\naccess to, the better its response will be. But on the other\nhand, with very long context, it would be hard for the model\nto remember everything and efficiently process all the informa-\ntion. Attention-based models are highly inefficient for longer\ncontexts and that is why we should expect more research in\ndifferent mechanisms that enable processing longer contexts\nand generally come up with more efficient architectures.\nThat being said, new architectures might not only propose\nalternatives for the attention mechanism but rather rethink the\nwhole Transformer architecture. As an early example of this,\nMonarch Mixer [212] proposes a new architecture that uses\nthe same sub-quadratic primitive that achieves high hardware\nefficiency on GPUs – Monarch matrices – along both sequence\nlength and model dimension.\nOn the other end of the spectrum, it is worth mentioning\nthat there are some attention-compatible architectural mecha-\nnisms that have been recently gaining steam and proving their\nvalue in creating better and more powerful LLMs. Probably\nthe best example of such mechanism is Mixture of Experts\n(MoE). MoEs have been around in machine learning for years,\neven before the Deep Learning Era [213], but they have been\ngaining popularity since then, and particularly in the context\nof Transformer models and LLMs.\nIn LLMs, MoEs allow to train an extremely large model\nthan is then only partially instantiated during inference\nwhen some of the experts are turned off wherever the gat-\ning/weighting function has a low weight assigned to them. As\nan example, the GLaM model has 1.2 trillion parameters, but\nduring inference only 2 out of the 64 experts are used [84].\nMoEs are nowadays an important component of the so-\ncalled frontier LLMs (i.e. the most advanced and capable\nmodels). GPT-4 itself is rumored to be based on a MoE\narchitecture, and some of the best performing LLMs such as\nMixtral [117], are basically an MoE version of pre-existing\nLLMs.\nFinally, it is important to note that MoEs can be used as a\ncomponent of any architecture regardless of whether it is based\n",
      "word_count": 1053,
      "char_count": 6437,
      "fonts": [
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 36,
      "text": "TABLE X: Hallucination evaluation\nModel\nHHEM\nHaluEval QA\nHaluEval Dialogue\nHaluEval Sum.\nHaluEval General\nGPT 4\n97\n-\n-\n-\n-\nGPT 4 Turbo\n97\n-\n-\n-\n-\nGPT 3.5 Turbo\n96.5\n62.59\n72.4\n58.53\n79.44\nDavinci002\n-\n60.05\n60.81\n47.77\n80.42\nDavinci003\n-\n49.65\n68.37\n48.07\n80.4\nGPT-3\n-\n49.21\n50.02\n51.23\n72.72\nGoogle Gemini Pro\n95.2\n-\n-\n-\n-\nLlama 2 70B\n94.9\n-\n-\n-\n-\nLlama 2 7B\n94.4\n49.6\n43.99\n49.55\n20.46\nLlama 2 13B\n94.1\n-\n-\n-\n-\nCohere-Chat\n92.5\n-\n-\n-\n-\nCohere\n91.5\n-\n-\n-\n-\nClaude 2\n91.5\n69.78\n64.73\n57.75\n75\nClaude 1\n67.6\n64.83\n53.76\n73.88\nMicrosoft Phi 2\n91.5\n-\n-\n-\n-\nGoogle Palm 2 (beta)\n91.4\n-\n-\n-\n-\nMixtral 8x7B\n90.7\n-\n-\n-\n-\nAmazon Titan Express\n90.6\n-\n-\n-\n-\nMistral 7B\n90.6\n-\n-\n-\n-\nGoogle Palm 2 Chat (beta)\n90\n-\n-\n-\n-\nGoogle Palm 2\n87.9\n-\n-\n-\n-\nGoogle Palm 2 Chat\n72.8\n-\n-\n-\n-\nChatGLM\n-\n47.93\n44.41\n48.57\n30.92\nFalcon\n-\n39.66\n29.08\n42.71\n18.98\nVicuna\n-\n60.34\n46.35\n45.62\n19.48\nAlpaca\n-\n6.68\n17.55\n20.63\n9.54\non attention or not. In fact, MoEs have also been applied to\nSSM-based LLMs like Mamba citepioro2024moemamba. We\nshould continue to see MoE-driven improvements in the future\nregardless of the underlying architecture.\nC. Multi-modal Models\nFuture LLMs are expected to be multi-modal and handle\na variety of data types, such as text, images, and videos,\naudio, in a unified manner. This opens up possibilities for\nmore diverse applications in fields like question answering,\ncontent generation, creative arts, and healthcare, robotics, and\nbeyond. There are already several prominent multi-modal\nLLMs out there, including: LLAVA [214], LLAVA-Plus [215],\nGPT-4 [33], Qwen-vl [116], Next-GPT [216], but the trend is\nexpected to be continued. Evaluation of these models also is a\nnew research topic, especially conversational generative vision\nmodels [217]. Multi-modal LLMs can unlock huge potentials\nin a variety of tasks, and there has already been a descent\nprogress in this direction, which needs a dedicated paper to\ndiscuss all its details.\nD. Improved LLM Usage and Augmentation techniques\nAs we described in sectionIV, many of the shortcomings\nand limitations of LLMs such as hallucination can be ad-\ndressed through advanced prompt engineering, use of tools,\nor other augmentation techniques. We should expect not only\ncontinued, but accelerated research in this area. It is worth\nmentioning that, in the specific case of software engineering,\nsome works ([218]) tried to automatically eliminate this issue\nfrom the overall software engineering workflow\nLLM-based systems are already starting to replace ma-\nchine learning systems that were until recently using other\napproaches. As a clear example of this, LLMs are now being\ndeployed to better understand people preference and interests,\nand provide more personalized interactions, whether in cus-\ntomer service, content recommendation, or other applications.\nThis involves better understanding of user preferences, and\nanalyzing their past interactions and using them as the context.\nWe will continue to see research in the application and usage\nof LLMs for not only personalization and recommendations,\nbut many other application areas using other machine learning\ntechniques.\nFinally, another important area of research we expect to\ngather increased attention is that of LLM-based agents and\nmulti-agent systems [172], [173], [174]. The development of\nLLM systems with access to external tools and decision-\nmaking capabilities is both exciting and challenging. We will\nsee continued research and progress in this important area that\nsome argue could lead to Artificial General Intelligence (AGI).\nE. Security and Ethical/Responsible AI\nEnsuring the robustness and security of LLMs against\nadversarial attacks and other vulnerabilities is a critical area\nof research [219]. As LLMs are increasingly deployed in real-\nworld applications, they need to be protected from potential\nthreats, to prevent them being used to manipulate people or\nspread mis-information. Improving the reasoning capabilities\nof these model [220], would help them to better detect potential\nadversarial attacks.\nAddressing ethical concerns and biases in LLMs is another\nactive area of research. Efforts are being made to ensure that\nLLMs are fair, unbiased, and capable of handling sensitive\ninformation responsibly. As LLMs are being used more and\nmore by a large number of people on a daily basis, making\nsure they are unbiased and behave responsibly is crucial.\nVIII.\nCONCLUSION\nThis paper present a survey of LLMs developed in the\npast few years. We first provide an overview of early pre-\n",
      "word_count": 750,
      "char_count": 4526,
      "fonts": [
        "NimbusRomNo9L-Regu (7.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 37,
      "text": "trained language models (e.g., as BERT), then review three\npopular LLM families (GPT, LLaMA, PaLM), and other\nrepresentative LLMs. We then survey methods and techniques\nof building, augmenting, and using LLMs. We review popular\nLLM datasets and benchmarks, and compare performance of\na set of prominent models on public benchmarks. Finally, we\npresent open challenges and future research directions.\nREFERENCES\n[1]\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\nfor neural language models,” arXiv preprint arXiv:2001.08361, 2020.\n[2]\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark\net al., “Training compute-optimal large language models,” arXiv\npreprint arXiv:2203.15556, 2022.\n[3]\nC. E. Shannon, “Prediction and entropy of printed english,” Bell system\ntechnical journal, vol. 30, no. 1, pp. 50–64, 1951.\n[4]\nF. Jelinek, Statistical methods for speech recognition.\nMIT press,\n1998.\n[5]\nC. Manning and H. Schutze, Foundations of statistical natural lan-\nguage processing.\nMIT press, 1999.\n[6]\nC. D. Manning, An introduction to information retrieval.\nCambridge\nuniversity press, 2009.\n[7]\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\nB. Zhang, J. Zhang, Z. Dong et al., “A survey of large language\nmodels,” arXiv preprint arXiv:2303.18223, 2023.\n[8]\nC. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\nL. He et al., “A comprehensive survey on pretrained foundation mod-\nels: A history from bert to chatgpt,” arXiv preprint arXiv:2302.09419,\n2023.\n[9]\nP. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods\nin natural language processing,” ACM Computing Surveys, vol. 55,\nno. 9, pp. 1–35, 2023.\n[10]\nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\nJ. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint\narXiv:2301.00234, 2022.\n[11]\nJ. Huang and K. C.-C. Chang, “Towards reasoning in large language\nmodels: A survey,” arXiv preprint arXiv:2212.10403, 2022.\n[12]\nS. F. Chen and J. Goodman, “An empirical study of smoothing\ntechniques for language modeling,” Computer Speech & Language,\nvol. 13, no. 4, pp. 359–394, 1999.\n[13]\nY. Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic\nlanguage model,” Advances in neural information processing systems,\nvol. 13, 2000.\n[14]\nH. Schwenk, D. D´echelotte, and J.-L. Gauvain, “Continuous space\nlanguage models for statistical machine translation,” in Proceedings\nof the COLING/ACL 2006 Main Conference Poster Sessions, 2006,\npp. 723–730.\n[15]\nT. Mikolov, M. Karafi´at, L. Burget, J. Cernock`y, and S. Khudanpur,\n“Recurrent neural network based language model.” in Interspeech,\nvol. 2, no. 3.\nMakuhari, 2010, pp. 1045–1048.\n[16]\nA. Graves, “Generating sequences with recurrent neural networks,”\narXiv preprint arXiv:1308.0850, 2013.\n[17]\nP.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, “Learning\ndeep structured semantic models for web search using clickthrough\ndata,” in Proceedings of the 22nd ACM international conference on\nInformation & Knowledge Management, 2013, pp. 2333–2338.\n[18]\nJ. Gao, C. Xiong, P. Bennett, and N. Craswell, Neural Approaches to\nConversational Information Retrieval. Springer Nature, 2023, vol. 44.\n[19]\nI. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning\nwith neural networks,” Advances in neural information processing\nsystems, vol. 27, 2014.\n[20]\nK. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio, “On\nthe properties of neural machine translation: Encoder-decoder ap-\nproaches,” arXiv preprint arXiv:1409.1259, 2014.\n[21]\nH. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Doll´ar,\nJ. Gao, X. He, M. Mitchell, J. C. Platt et al., “From captions to\nvisual concepts and back,” in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2015, pp. 1473–1482.\n[22]\nO. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell:\nA neural image caption generator,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2015, pp.\n3156–3164.\n[23]\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations. corr\nabs/1802.05365 (2018),” arXiv preprint arXiv:1802.05365, 2018.\n[24]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[25]\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n[26]\nP. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert\nwith disentangled attention,” arXiv preprint arXiv:2006.03654, 2020.\n[27]\nX. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao,\nA. Zhang, L. Zhang et al., “Pre-trained models: Past, present and\nfuture,” AI Open, vol. 2, pp. 225–250, 2021.\n[28]\nX. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained\nmodels for natural language processing: A survey,” Science China\nTechnological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\n[29]\nA. Gu, K. Goel, and C. R´e, “Efficiently modeling long sequences with\nstructured state spaces,” 2022.\n[30]\nA. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\nselective state spaces,” arXiv preprint arXiv:2312.00752, 2023.\n[31]\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al.,\n“Palm: Scaling language modeling with pathways,” arXiv preprint\narXiv:2204.02311, 2022.\n[32]\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., “Llama:\nOpen and efficient foundation language models,” arXiv preprint\narXiv:2302.13971, 2023.\n[33]\nOpenAI,\n“GPT-4\nTechnical\nReport,”\nhttps://arxiv.org/pdf/2303.\n08774v3.pdf, 2023.\n[34]\nJ.\nWei,\nX.\nWang,\nD.\nSchuurmans,\nM.\nBosma,\nb.\nichter,\nF.\nXia,\nE.\nChi,\nQ.\nV.\nLe,\nand\nD.\nZhou,\n“Chain-of-thought\nprompting\nelicits\nreasoning\nin\nlarge\nlanguage\nmodels,”\nin\nAdvances in Neural Information Processing Systems, S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\nEds., vol. 35.\nCurran Associates, Inc., 2022, pp. 24 824–24 837.\n[Online]. Available: https://proceedings.neurips.cc/paper files/paper/\n2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\n[35]\nG. Mialon, R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,\nR. Raileanu, B. Rozi`ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\nmaz et al., “Augmented language models: a survey,” arXiv preprint\narXiv:2302.07842, 2023.\n[36]\nB. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang,\nL. Liden, Z. Yu, W. Chen, and J. Gao, “Check your facts and try\nagain: Improving large language models with external knowledge and\nautomated feedback,” arXiv preprint arXiv:2302.12813, 2023.\n[37]\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\n“React: Synergizing reasoning and acting in language models,” arXiv\npreprint arXiv:2210.03629, 2022.\n[38]\nD. E. Rumelhart, G. E. Hinton, R. J. Williams et al., “Learning internal\nrepresentations by error propagation,” 1985.\n[39]\nJ. L. Elman, “Finding structure in time,” Cognitive science, vol. 14,\nno. 2, pp. 179–211, 1990.\n[40]\nM. V. Mahoney, “Fast text compression with neural networks.” in\nFLAIRS conference, 2000, pp. 230–234.\n[41]\nT. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ˇCernock`y, “Strate-\ngies for training large scale neural network language models,” in 2011\nIEEE Workshop on Automatic Speech Recognition & Understanding.\nIEEE, 2011, pp. 196–201.\n",
      "word_count": 1204,
      "char_count": 7805,
      "fonts": [
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)",
        "NimbusRomNo9L-ReguItal (8.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 38,
      "text": "[42]\ntmikolov.\nrnnlm.\n[Online].\nAvailable:\nhttps://www.fit.vutbr.cz/\n∼imikolov/rnnlm/\n[43]\nS. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,\nand J. Gao, “Deep learning–based text classification: a comprehensive\nreview,” ACM computing surveys (CSUR), vol. 54, no. 3, pp. 1–40,\n2021.\n[44]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[45]\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n“Albert: A lite bert for self-supervised learning of language represen-\ntations,” arXiv preprint arXiv:1909.11942, 2019.\n[46]\nK. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-\ntraining text encoders as discriminators rather than generators,” arXiv\npreprint arXiv:2003.10555, 2020.\n[47]\nG. Lample and A. Conneau, “Cross-lingual language model pretrain-\ning,” arXiv preprint arXiv:1901.07291, 2019.\n[48]\nZ. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V. Le, “Xlnet: Generalized autoregressive pretraining for language\nunderstanding,” Advances in neural information processing systems,\nvol. 32, 2019.\n[49]\nL. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao,\nM. Zhou, and H.-W. Hon, “Unified language model pre-training for\nnatural language understanding and generation,” Advances in neural\ninformation processing systems, vol. 32, 2019.\n[50]\nA. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improv-\ning language understanding by generative pre-training,” 2018.\n[51]\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[52]\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\nwith a unified text-to-text transformer,” The Journal of Machine\nLearning Research, vol. 21, no. 1, pp. 5485–5551, 2020.\n[53]\nL. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\nA. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained\ntext-to-text transformer,” arXiv preprint arXiv:2010.11934, 2020.\n[54]\nK. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, “Mass: Masked\nsequence to sequence pre-training for language generation,” arXiv\npreprint arXiv:1905.02450, 2019.\n[55]\nM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and\ncomprehension,” arXiv preprint arXiv:1910.13461, 2019.\n[56]\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[57]\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka-\nplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al.,\n“Evaluating large language models trained on code,” arXiv preprint\narXiv:2107.03374, 2021.\n[58]\nR. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., “Webgpt: Browser-\nassisted question-answering with human feedback,” arXiv preprint\narXiv:2112.09332, 2021.\n[59]\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\nmodels to follow instructions with human feedback,” Advances in\nNeural Information Processing Systems, vol. 35, pp. 27 730–27 744,\n2022.\n[60]\nOpenAI. (2022) Introducing chatgpt. [Online]. Available: https:\n//openai.com/blog/chatgpt\n[61]\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\n2: Open foundation and fine-tuned chat models,” arXiv preprint\narXiv:2307.09288, 2023.\n[62]\nR. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,\nand T. B. Hashimoto, “Alpaca: A strong, replicable instruction-\nfollowing model,” Stanford Center for Research on Foundation Mod-\nels. https://crfm. stanford. edu/2023/03/13/alpaca. html, vol. 3, no. 6,\np. 7, 2023.\n[63]\nT. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef-\nficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314,\n2023.\n[64]\nX. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\nand D. Song, “Koala: A dialogue model for academic research,” Blog\npost, April, vol. 1, 2023.\n[65]\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,\nD. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al.,\n“Mistral 7b,” arXiv preprint arXiv:2310.06825, 2023.\n[66]\nB. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,\nJ. Liu, T. Remez, J. Rapin et al., “Code llama: Open foundation models\nfor code,” arXiv preprint arXiv:2308.12950, 2023.\n[67]\nS. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Large\nlanguage model connected with massive apis,” 2023.\n[68]\nA. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and\nS. Naidu, “Giraffe: Adventures in expanding context lengths in llms,”\narXiv preprint arXiv:2308.10882, 2023.\n[69]\nB. Huang, “Vigogne: French instruction-following and chat models,”\nhttps://github.com/bofenghuang/vigogne, 2023.\n[70]\nY. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu,\nD. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al., “How far can\ncamels go? exploring the state of instruction tuning on open resources,”\narXiv preprint arXiv:2306.04751, 2023.\n[71]\nS. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski,\nand P. Miło´s, “Focused transformer: Contrastive training for context\nscaling,” arXiv preprint arXiv:2307.03170, 2023.\n[72]\nD.\nMahan,\nR.\nCarlow,\nL.\nCastricato,\nN.\nCooper,\nand\nC.\nLaforte,\n“Stable\nbeluga\nmodels.”\n[Online].\nAvailable:\n[https://huggingface.co/stabilityai/StableBeluga2](https://\nhuggingface.co/stabilityai/StableBeluga2)\n[73]\nY. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Gar-\ncia, H. S. Zheng, J. Rao, A. Chowdhery et al., “Transcending scaling\nlaws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399,\n2022.\n[74]\nH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus,\nY. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-\nfinetuned language models,” arXiv preprint arXiv:2210.11416, 2022.\n[75]\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., “Palm 2 technical\nreport,” arXiv preprint arXiv:2305.10403, 2023.\n[76]\nK. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language\nmodels encode clinical knowledge,” arXiv preprint arXiv:2212.13138,\n2022.\n[77]\nK. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou,\nK. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al., “Towards expert-\nlevel medical question answering with large language models,” arXiv\npreprint arXiv:2305.09617, 2023.\n[78]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,\nA. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot\nlearners,” arXiv preprint arXiv:2109.01652, 2021.\n[79]\nJ. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language\nmodels: Methods, analysis & insights from training gopher,” arXiv\npreprint arXiv:2112.11446, 2021.\n[80]\nV. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., “Multi-\ntask prompted training enables zero-shot task generalization,” arXiv\npreprint arXiv:2110.08207, 2021.\n[81]\nY. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\nY. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced pre-\ntraining for language understanding and generation,” arXiv preprint\narXiv:2107.02137, 2021.\n[82]\nS. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Mil-\nlican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark\net al., “Improving language models by retrieving from trillions of\ntokens,” in International conference on machine learning.\nPMLR,\n2022, pp. 2206–2240.\n",
      "word_count": 1275,
      "char_count": 8284,
      "fonts": [
        "NimbusRomNo9L-Regu (8.0pt)",
        "CMSY6 (6.0pt)",
        "NimbusRomNo9L-ReguItal (8.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 39,
      "text": "[83]\nO. Lieber, O. Sharir, B. Lenz, and Y. Shoham, “Jurassic-1: Technical\ndetails and evaluation,” White Paper. AI21 Labs, vol. 1, p. 9, 2021.\n[84]\nN. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,\nY. Zhou, A. W. Yu, O. Firat et al., “Glam: Efficient scaling of\nlanguage models with mixture-of-experts,” in International Conference\non Machine Learning.\nPMLR, 2022, pp. 5547–5569.\n[85]\nR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-\nT. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Language\nmodels for dialog applications,” arXiv preprint arXiv:2201.08239,\n2022.\n[86]\nS. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\nC. Dewan, M. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained\ntransformer language models,” arXiv preprint arXiv:2205.01068, 2022.\n[87]\nR. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar-\navia, A. Poulton, V. Kerkez, and R. Stojnic, “Galactica: A large\nlanguage model for science,” arXiv preprint arXiv:2211.09085, 2022.\n[88]\nE. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,\nS. Savarese, and C. Xiong, “Codegen: An open large language\nmodel for code with multi-turn program synthesis,” arXiv preprint\narXiv:2203.13474, 2022.\n[89]\nS. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al.,\n“Alexatm 20b: Few-shot learning using a large-scale multilingual\nseq2seq model,” arXiv preprint arXiv:2208.01448, 2022.\n[90]\nA. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu,\nT. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al.,\n“Improving alignment of dialogue agents via targeted human judge-\nments,” arXiv preprint arXiv:2209.14375, 2022.\n[91]\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski,\nV. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al.,\n“Solving quantitative reasoning problems with language models,”\nAdvances in Neural Information Processing Systems, vol. 35, pp.\n3843–3857, 2022.\n[92]\nY. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster,\nH. S. Zheng, N. Houlsby, and D. Metzler, “Unifying language learning\nparadigms,” arXiv preprint arXiv:2205.05131, 2022.\n[93]\nT. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow,\nR. Castagn´e, A. S. Luccioni, F. Yvon, M. Gall´e et al., “Bloom: A 176b-\nparameter open-access multilingual language model,” arXiv preprint\narXiv:2211.05100, 2022.\n[94]\nA. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\nW. Zheng, X. Xia et al., “Glm-130b: An open bilingual pre-trained\nmodel,” arXiv preprint arXiv:2210.02414, 2022.\n[95]\nS. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien,\nE. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al.,\n“Pythia: A suite for analyzing large language models across train-\ning and scaling,” in International Conference on Machine Learning.\nPMLR, 2023, pp. 2397–2430.\n[96]\nS. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and\nA. Awadallah, “Orca: Progressive learning from complex explanation\ntraces of gpt-4,” arXiv preprint arXiv:2306.02707, 2023.\n[97]\nR. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\nM. Marone, C. Akiki, J. Li, J. Chim et al., “Starcoder: may the source\nbe with you!” arXiv preprint arXiv:2305.06161, 2023.\n[98]\nS. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv,\nL. Cui, O. K. Mohammed, Q. Liu et al., “Language is not all you\nneed: Aligning perception with language models,” arXiv preprint\narXiv:2302.14045, 2023.\n[99]\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,\nJ. Schalkwyk, A. M. Dai, A. Hauth et al., “Gemini: a family of highly\ncapable multimodal models,” arXiv preprint arXiv:2312.11805, 2023.\n[100]\nW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\nJ. Tompson, I. Mordatch, Y. Chebotar et al., “Inner monologue:\nEmbodied reasoning through planning with language models,” arXiv\npreprint arXiv:2207.05608, 2022.\n[101]\nS. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti\net al., “Using deepspeed and megatron to train megatron-turing\nnlg 530b, a large-scale generative language model,” arXiv preprint\narXiv:2201.11990, 2022.\n[102]\nI. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\ndocument transformer,” arXiv preprint arXiv:2004.05150, 2020.\n[103]\nS. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-\nter, T. Wang, Q. Liu, P. S. Koura et al., “Opt-iml: Scaling language\nmodel instruction meta learning through the lens of generalization,”\narXiv preprint arXiv:2212.12017, 2022.\n[104]\nY. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma,\nand F. Wei, “Language models are general-purpose interfaces,” arXiv\npreprint arXiv:2206.06336, 2022.\n[105]\nZ. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang,\nand C. Gan, “Principle-driven self-alignment of language mod-\nels from scratch with minimal human supervision,” arXiv preprint\narXiv:2305.03047, 2023.\n[106]\nW. E. team, “Palmyra-base Parameter Autoregressive Language\nModel,” https://dev.writer.com, 2023.\n[107]\n——, “Camel-5b instructgpt,” https://dev.writer.com, 2023.\n[108]\nYandex.\nYalm.\n[Online].\nAvailable:\nhttps://github.com/yandex/\nYaLM-100B\n[109]\nM. Team et al., “Introducing mpt-7b: a new standard for open-source,\ncommercially usable llms,” 2023.\n[110]\nA. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal,\nX. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi,\nG. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, “Orca 2:\nTeaching small language models how to reason,” 2023.\n[111]\nL. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and\nG. Neubig, “Pal: Program-aided language models,” in International\nConference on Machine Learning.\nPMLR, 2023, pp. 10 764–10 799.\n[112]\nAnthropic. claude. [Online]. Available: https://www.anthropic.com/\nnews/introducing-claude\n[113]\nE. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou,\n“Codegen2: Lessons for training llms on programming and natural\nlanguages,” arXiv preprint arXiv:2305.02309, 2023.\n[114]\nL. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada,\nS. Huang, L. von Werra, C. Fourrier, N. Habib et al., “Zephyr: Direct\ndistillation of lm alignment,” arXiv preprint arXiv:2310.16944, 2023.\n[115]\nX. team. Grok. [Online]. Available: https://grok.x.ai/\n[116]\nJ. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou,\nand J. Zhou, “Qwen-vl: A frontier large vision-language model with\nversatile abilities,” arXiv preprint arXiv:2308.12966, 2023.\n[117]\nmixtral.\nmixtral.\n[Online].\nAvailable:\nhttps://mistral.ai/news/\nmixtral-of-experts/\n[118]\nD. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y. Pei,\nA. Nourbakhsh, and X. Liu, “Docllm: A layout-aware generative\nlanguage model for multimodal document understanding,” 2023.\n[119]\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,\nY. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder:\nWhen the large language model meets programming – the rise of code\nintelligence,” 2024.\n[120]\nF. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, “Knowledge\nfusion of large language models,” 2024.\n[121]\nP. Zhang, G. Zeng, T. Wang, and W. Lu, “Tinyllama: An open-source\nsmall language model,” 2024.\n[122]\nC. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan,\n“Llama pro: Progressive llama with block expansion,” 2024.\n[123]\nX. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and\nM. Kazi, “Transformer models: an introduction and catalog,” 2023.\n[124]\nG. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\nH. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The refined-\nweb dataset for falcon llm: outperforming curated corpora with web\ndata, and web data only,” arXiv preprint arXiv:2306.01116, 2023.\n[125]\nD. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-\nShowk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al.,\n“Scaling laws and interpretability of learning from repeated data,”\narXiv preprint arXiv:2205.10487, 2022.\n[126]\nP. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\nposition representations,” arXiv preprint arXiv:1803.02155, 2018.\n[127]\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, “Roformer: En-\n",
      "word_count": 1303,
      "char_count": 8318,
      "fonts": [
        "NimbusRomNo9L-Regu (8.0pt)",
        "NimbusRomNo9L-ReguItal (8.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 40,
      "text": "hanced transformer with rotary position embedding,” arXiv preprint\narXiv:2104.09864, 2021.\n[128]\nO. Press, N. A. Smith, and M. Lewis, “Train short, test long: Attention\nwith linear biases enables input length extrapolation,” arXiv preprint\narXiv:2108.12409, 2021.\n[129]\nG. Ke, D. He, and T.-Y. Liu, “Rethinking positional encoding in\nlanguage pre-training,” arXiv preprint arXiv:2006.15595, 2020.\n[130]\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017.\n[131]\nW. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling\nto trillion parameter models with simple and efficient sparsity,” The\nJournal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270,\n2022.\n[132]\nR. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\n“Parameter-efficient multi-task fine-tuning for transformers via shared\nhypernetworks,” 2021.\n[133]\nS. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\nT. Zhang, F. Wu, and G. Wang, “Instruction tuning for large language\nmodels: A survey,” 2023.\n[134]\nS. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross-task\ngeneralization via natural language crowdsourcing instructions,” arXiv\npreprint arXiv:2104.08773, 2021.\n[135]\nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,\nand H. Hajishirzi, “Self-instruct: Aligning language model with self\ngenerated instructions,” arXiv preprint arXiv:2212.10560, 2022.\n[136]\nK. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online].\nAvailable: https://github.com/ContextualAI/HALOs/blob/main/assets/\nreport.pdf\n[137]\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and\nD. Amodei, “Deep reinforcement learning from human preferences,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[138]\nH. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Car-\nbune, and A. Rastogi, “Rlaif: Scaling reinforcement learning from\nhuman feedback with ai feedback,” arXiv preprint arXiv:2309.00267,\n2023.\n[139]\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and\nC. Finn, “Direct preference optimization: Your language model is\nsecretly a reward model,” arXiv preprint arXiv:2305.18290, 2023.\n[140]\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory\noptimizations toward training trillion parameter models,” in SC20: In-\nternational Conference for High Performance Computing, Networking,\nStorage and Analysis.\nIEEE, 2020, pp. 1–16.\n[141]\nB. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,\nX. Cheng, M. Chung, M. Grella, K. K. GV et al., “Rwkv: Reinventing\nrnns for the transformer era,” arXiv preprint arXiv:2305.13048, 2023.\n[142]\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\nand W. Chen, “Lora: Low-rank adaptation of large language models,”\narXiv preprint arXiv:2106.09685, 2021.\n[143]\nG. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\nneural network,” arXiv preprint arXiv:1503.02531, 2015.\n[144]\nJ. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation:\nA survey,” International Journal of Computer Vision, vol. 129, pp.\n1789–1819, 2021.\n[145]\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J.\nBang, A. Madotto, and P. Fung, “Survey of hallucination in natural\nlanguage generation,” ACM Comput. Surv., vol. 55, no. 12, mar 2023.\n[Online]. Available: https://doi.org/10.1145/3571730\n[146]\nN. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and\nM. Steedman, “Sources of hallucination by large language models on\ninference tasks,” 2023.\n[147]\nC.-Y.\nLin,\n“ROUGE:\nA\npackage\nfor\nautomatic\nevaluation\nof\nsummaries,” in Text Summarization Branches Out.\nBarcelona, Spain:\nAssociation for Computational Linguistics, Jul. 2004, pp. 74–81.\n[Online]. Available: https://aclanthology.org/W04-1013\n[148]\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\nautomatic evaluation of machine translation,” in Proceedings of the\n40th Annual Meeting of the Association for Computational Linguistics,\nP. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania,\nUSA: Association for Computational Linguistics, Jul. 2002, pp. 311–\n318. [Online]. Available: https://aclanthology.org/P02-1040\n[149]\nB. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and\nW. Cohen, “Handling divergent reference texts when evaluating\ntable-to-text generation,” in Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics, A. Korhonen,\nD. Traum, and L. M`arquez, Eds.\nFlorence, Italy: Association\nfor Computational Linguistics, Jul. 2019, pp. 4884–4895. [Online].\nAvailable: https://aclanthology.org/P19-1483\n[150]\nZ. Wang, X. Wang, B. An, D. Yu, and C. Chen, “Towards faithful\nneural table-to-text generation with content-matching constraints,”\nin Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter,\nand J. Tetreault, Eds.\nOnline: Association for Computational\nLinguistics, Jul. 2020, pp. 1072–1086. [Online]. Available: https:\n//aclanthology.org/2020.acl-main.101\n[151]\nH. Song, W.-N. Zhang, J. Hu, and T. Liu, “Generating persona consis-\ntent dialogues by exploiting natural language inference,” Proceedings\nof the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, pp.\n8878–8885, Apr. 2020.\n[152]\nO. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor,\nand O. Abend, “q2: Evaluating factual consistency in knowledge-\ngrounded dialogues via question generation and question answering,”\nin Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, M.-F. Moens, X. Huang, L. Specia,\nand S. W.-t. Yih, Eds.\nOnline and Punta Cana, Dominican Republic:\nAssociation for Computational Linguistics, Nov. 2021, pp. 7856–7870.\n[Online]. Available: https://aclanthology.org/2021.emnlp-main.619\n[153]\nN. Dziri, H. Rashkin, T. Linzen, and D. Reitter, “Evaluating attribution\nin dialogue systems: The BEGIN benchmark,” Transactions of the\nAssociation for Computational Linguistics, vol. 10, pp. 1066–1083,\n2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.62\n[154]\nS. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim,\nY. Liu, and D. Z. Hakkani-T¨ur, “Rome was built in 1776: A case study\non factual correctness in knowledge-grounded response generation,”\nArXiv, vol. abs/2110.05456, 2021.\n[155]\nS. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer,\nL. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomic\nevaluation of factual precision in long form text generation,” 2023.\n[156]\nD. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,\nV. Chaudhary, and M. Young, “Machine learning: The high interest\ncredit card of technical debt,” in SE4ML: Software Engineering for\nMachine Learning (NIPS 2014 Workshop), 2014.\n[157]\nZ. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought\nprompting in large language models,” 2022.\n[158]\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving with\nlarge language models,” 2023.\n[159]\nP. Manakul, A. Liusie, and M. J. F. Gales, “Selfcheckgpt: Zero-\nresource black-box hallucination detection for generative large lan-\nguage models,” 2023.\n[160]\nN. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan,\nand S. Yao, “Reflexion: Language agents with verbal reinforcement\nlearning,” 2023.\n[161]\nS. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang,\nK. Tyser, Z. Chin, Y. Hicke, N. Singh, M. Udell, Y. Kim, T. Buonassisi,\nA. Solar-Lezama, and I. Drori, “Exploring the mit mathematics and\neecs curriculum using large language models,” 2023.\n[162]\nT. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J.\nCai, “Promptchainer: Chaining large language model prompts through\nvisual programming,” 2022.\n[163]\nY. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and\nJ. Ba, “Large language models are human-level prompt engineers,”\n2023.\n[164]\nP. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin,\nN. Goyal, H. K¨uttler, M. Lewis, W. Yih, T. Rockt¨aschel, S. Riedel, and\nD. Kiela, “Retrieval-augmented generation for knowledge-intensive\nNLP tasks,” CoRR, vol. abs/2005.11401, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2005.11401\n[165]\nY. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and\n",
      "word_count": 1231,
      "char_count": 8453,
      "fonts": [
        "CMMI8 (8.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)",
        "CMR6 (6.0pt)",
        "NimbusRomNo9L-ReguItal (8.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 41,
      "text": "H. Wang, “Retrieval-augmented generation for large language models:\nA survey,” arXiv preprint arXiv:2312.10997, 2023.\n[166]\nA. W. Services. (Year of publication, e.g., 2023) Question answering\nusing retrieval augmented generation with foundation models in\namazon\nsagemaker\njumpstart.\nAccessed:\nDate\nof\naccess,\ne.g.,\nDecember 5, 2023. [Online]. Available: https://shorturl.at/dSV47\n[167]\nS. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying large\nlanguage models and knowledge graphs: A roadmap,” arXiv preprint\narXiv:2306.08302, 2023.\n[168]\nZ. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\n2023.\n[169]\nT. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle-\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\ncan teach themselves to use tools,” 2023.\n[170]\nB. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer,\nand M. T. Ribeiro, “Art: Automatic multi-step reasoning and tool-use\nfor large language models,” 2023.\n[171]\nY. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface,” arXiv\npreprint arXiv:2303.17580, 2023.\n[172]\nZ. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,\nS. Jin, E. Zhou et al., “The rise and potential of large language model\nbased agents: A survey,” arXiv preprint arXiv:2309.07864, 2023.\n[173]\nL. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\nJ. Tang, X. Chen, Y. Lin et al., “A survey on large language model\nbased autonomous agents,” arXiv preprint arXiv:2308.11432, 2023.\n[174]\nZ. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar,\nR. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-\nFei, and J. Gao, “Agent ai: Surveying the horizons of multimodal\ninteraction,” arXiv preprint arXiv:2401.03568, 2024.\n[175]\nB. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, “Rewoo:\nDecoupling reasoning from observations for efficient augmented lan-\nguage models,” 2023.\n[176]\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\n“React: Synergizing reasoning and acting in language models,” 2023.\n[177]\nV. Nair, E. Schumacher, G. Tso, and A. Kannan, “Dera: Enhanc-\ning large language model completions with dialog-enabled resolving\nagents,” 2023.\n[178]\nY. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\nC. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang,\nand X. Xie, “A survey on evaluation of large language models,” 2023.\n[179]\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit,\nQ.\nLe,\nand\nS.\nPetrov,\n“Natural\nquestions:\nA\nbenchmark\nfor\nquestion answering research,” Transactions of the Association for\nComputational Linguistics, vol. 7, pp. 452–466, 2019. [Online].\nAvailable: https://aclanthology.org/Q19-1026\n[180]\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\nJ. Steinhardt, “Measuring massive multitask language understanding,”\n2021.\n[181]\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\nE. Jiang, C. Cai, M. Terry, Q. Le et al., “Program synthesis with large\nlanguage models,” arXiv preprint arXiv:2108.07732, 2021.\n[182]\nE. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang,\nand L. Zettlemoyer, “QuAC: Question answering in context,” in\nProceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, E. Riloff, D. Chiang, J. Hockenmaier, and\nJ. Tsujii, Eds.\nBrussels, Belgium: Association for Computational\nLinguistics, Oct.-Nov. 2018, pp. 2174–2184. [Online]. Available:\nhttps://aclanthology.org/D18-1241\n[183]\nD. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\nC. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, “Measuring\ncoding challenge competence with apps,” NeurIPS, 2021.\n[184]\nV. Zhong, C. Xiong, and R. Socher, “Seq2sql: Generating structured\nqueries from natural language using reinforcement learning,” arXiv\npreprint arXiv:1709.00103, 2017.\n[185]\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\nA large scale distantly supervised challenge dataset for reading\ncomprehension,” in Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\nR. Barzilay and M.-Y. Kan, Eds.\nVancouver, Canada: Association\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online].\nAvailable: https://aclanthology.org/P17-1147\n[186]\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “RACE: Large-scale\nReAding comprehension dataset from examinations,” in Proceedings\nof the 2017 Conference on Empirical Methods in Natural Language\nProcessing, M. Palmer, R. Hwa, and S. Riedel, Eds.\nCopenhagen,\nDenmark: Association for Computational Linguistics, Sep. 2017, pp.\n785–794. [Online]. Available: https://aclanthology.org/D17-1082\n[187]\nP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000+\nquestions for machine comprehension of text,” in Proceedings of\nthe 2016 Conference on Empirical Methods in Natural Language\nProcessing, J. Su, K. Duh, and X. Carreras, Eds.\nAustin, Texas:\nAssociation for Computational Linguistics, Nov. 2016, pp. 2383–2392.\n[Online]. Available: https://aclanthology.org/D16-1264\n[188]\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and\nK. Toutanova, “Boolq: Exploring the surprising difficulty of natural\nyes/no\nquestions,”\nCoRR,\nvol.\nabs/1905.10044,\n2019.\n[Online].\nAvailable: http://arxiv.org/abs/1905.10044\n[189]\nD. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth,\n“Looking beyond the surface:a challenge set for reading compre-\nhension over multiple sentences,” in Proceedings of North American\nChapter of the Association for Computational Linguistics (NAACL),\n2018.\n[190]\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and\nJ. Schulman, “Training verifiers to solve math word problems,”\nCoRR,\nvol.\nabs/2110.14168,\n2021.\n[Online].\nAvailable:\nhttps:\n//arxiv.org/abs/2110.14168\n[191]\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\nD. Song, and J. Steinhardt, “Measuring mathematical problem solving\nwith the MATH dataset,” CoRR, vol. abs/2103.03874, 2021. [Online].\nAvailable: https://arxiv.org/abs/2103.03874\n[192]\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag:\nCan a machine really finish your sentence?” 2019.\n[193]\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\nand O. Tafjord, “Think you have solved question answering? try\narc, the AI2 reasoning challenge,” CoRR, vol. abs/1803.05457, 2018.\n[Online]. Available: http://arxiv.org/abs/1803.05457\n[194]\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA:\nreasoning about physical commonsense in natural language,” CoRR,\nvol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/\n1911.11641\n[195]\nM. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, “Socialiqa:\nCommonsense reasoning about social interactions,” CoRR, vol.\nabs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904.\n09728\n[196]\nT. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of\narmor conduct electricity? A new dataset for open book question\nanswering,” CoRR, vol. abs/1809.02789, 2018. [Online]. Available:\nhttp://arxiv.org/abs/1809.02789\n[197]\nS. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models\nmimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021.\n[198]\nZ. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov,\nand C. D. Manning, “Hotpotqa: A dataset for diverse, explainable\nmulti-hop question answering,” CoRR, vol. abs/1809.09600, 2018.\n[Online]. Available: http://arxiv.org/abs/1809.09600\n[199]\nY. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: A\ndataset for llm question answering with external tools,” arXiv preprint\narXiv:2306.13304, 2023.\n[200]\nD. Chen, J. Bolton, and C. D. Manning, “A thorough examination\nof the cnn/daily mail reading comprehension task,” in Association for\nComputational Linguistics (ACL), 2016.\n[201]\nR. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al., “Abstractive text\nsummarization using sequence-to-sequence rnns and beyond,” arXiv\npreprint arXiv:1602.06023, 2016.\n[202]\nY. Bai and D. Z. Wang, “More than reading comprehension: A survey\n",
      "word_count": 1240,
      "char_count": 8429,
      "fonts": [
        "NimbusRomNo9L-Regu (8.0pt)",
        "NimbusRomNo9L-ReguItal (8.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 42,
      "text": "on datasets and metrics of textual question answering,” arXiv preprint\narXiv:2109.12264, 2021.\n[203]\nH.-Y. Huang, E. Choi, and W.-t. Yih, “Flowqa: Grasping flow in\nhistory for conversational machine comprehension,” arXiv preprint\narXiv:1810.06683, 2018.\n[204]\nS. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, “A\nsurvey on evaluation metrics for machine translation,” Mathematics,\nvol. 11, no. 4, p. 1006, 2023.\n[205]\nJ. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval:\nA large-scale hallucination evaluation benchmark for large language\nmodels,” in Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, 2023, pp. 6449–6464.\n[206]\nSimon\nMark\nHughes,\n“Hughes\nhallucination\nevaluation\nmodel\n(hhem)\nleaderboard,”\n2024,\nhttps://huggingface.co/spaces/vectara/\nHallucination-evaluation-leaderboard, Last accessed on 2024-01-21.\n[207]\nJ. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and\nR. McHardy, “Challenges and applications of large language models,”\narXiv preprint arXiv:2307.10169, 2023.\n[208]\nS. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno,\nS. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al.,\n“Textbooks are all you need,” arXiv preprint arXiv:2306.11644, 2023.\n[209]\nY. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T.\nLee, “Textbooks are all you need ii: phi-1.5 technical report,” arXiv\npreprint arXiv:2309.05463, 2023.\n[210]\nM. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus,\nY. Bengio, S. Ermon, and C. R´e, “Hyena hierarchy: Towards larger\nconvolutional language models,” 2023.\n[211]\nM. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and\nA. Thomas, “StripedHyena: Moving Beyond Transformers with\nHybrid Signal Processing Models,” 12 2023. [Online]. Available:\nhttps://github.com/togethercomputer/stripedhyena\n[212]\nD. Y. Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas,\nB. Spector, M. Poli, A. Rudra, and C. R´e, “Monarch mixer: A simple\nsub-quadratic gemm-based architecture,” 2023.\n[213]\nG. J. McLachlan, S. X. Lee, and S. I. Rathnayake, “Finite mixture\nmodels,” Annual review of statistics and its application, vol. 6, pp.\n355–378, 2019.\n[214]\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” arXiv\npreprint arXiv:2304.08485, 2023.\n[215]\nS. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou,\nJ. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-plus:\nLearning to use tools for creating multimodal agents,” arXiv preprint\narXiv:2311.05437, 2023.\n[216]\nS. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-any\nmultimodal llm,” arXiv preprint arXiv:2309.05519, 2023.\n[217]\nN. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and\nD. Z¨uhlke, “Convgenvismo: Evaluation of conversational generative\nvision models,” 2023.\n[218]\nN. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman,\nI. Harper, A. Marginean, S. Sengupta, and E. Wang, “Automated unit\ntest improvement using large language models at meta,” arXiv preprint\narXiv:2402.09171, 2024.\n[219]\nL. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang,\nW. Lyu, Y. Zhang, X. Li et al., “Trustllm: Trustworthiness in large\nlanguage models,” arXiv preprint arXiv:2401.05561, 2024.\n[220]\nM. Josifoski, L. Klein, M. Peyrard, Y. Li, S. Geng, J. P. Schnitzler,\nY. Yao, J. Wei, D. Paul, and R. West, “Flows: Building blocks of\nreasoning and collaborating ai,” arXiv preprint arXiv:2308.01285,\n2023.\n[221]\nMicrosoft.\nDeepspeed.\n[Online].\nAvailable:\nhttps://github.com/\nmicrosoft/DeepSpeed\n[222]\nHuggingFace. Transformers. [Online]. Available: https://github.com/\nhuggingface/transformers\n[223]\nNvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/\nMegatron-LM\n[224]\nBMTrain. Bmtrain. [Online]. Available: https://github.com/OpenBMB/\nBMTrain\n[225]\nEleutherAI.\ngpt-neox.\n[Online].\nAvailable:\nhttps://github.com/\nEleutherAI/gpt-neox\n[226]\nmicrosoft. Lora. [Online]. Available: https://github.com/microsoft/\nLoRA\n[227]\nColossalAI.\nColossalai.\n[Online].\nAvailable:\nhttps://github.com/\nhpcaitech/ColossalAI\n[228]\nFastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/\nFastChat\n[229]\nskypilot. skypilot. [Online]. Available: https://github.com/skypilot-org/\nskypilot\n[230]\nvllm. vllm. [Online]. Available: https://github.com/vllm-project/vllm\n[231]\nhuggingface. text-generation-inference. [Online]. Available: https:\n//github.com/huggingface/text-generation-inference\n[232]\nlangchain.\nlangchain.\n[Online].\nAvailable:\nhttps://github.com/\nlangchain-ai/langchain\n[233]\nbentoml. Openllm. [Online]. Available: https://github.com/bentoml/\nOpenLLM\n[234]\nembedchain. embedchain. [Online]. Available: https://github.com/\nembedchain/embedchain\n[235]\nmicrosoft. autogen. [Online]. Available: https://github.com/microsoft/\nautogen\n[236]\nbabyagi.\nbabyagi.\n[Online].\nAvailable:\nhttps://github.com/\nyoheinakajima/babyagi\n[237]\nguidance.\nguidance.\n[Online].\nAvailable:\nhttps://github.com/\nguidance-ai/guidance\n[238]\nprompttools. prompttools. [Online]. Available: https://github.com/\nhegelai/prompttools\n[239]\npromptfoo.\npromptfoo.\n[Online].\nAvailable:\nhttps://github.com/\npromptfoo/promptfoo\n[240]\nfacebook.\nfaiss.\n[Online].\nAvailable:\nhttps://github.com/\nfacebookresearch/faiss\n[241]\nmilvus. milvus. [Online]. Available: https://github.com/milvus-io/\nmilvus\n[242]\nqdrant. qdrant. [Online]. Available: https://github.com/qdrant/qdrant\n[243]\nweaviate. weaviate. [Online]. Available: https://github.com/weaviate/\nweaviate\n[244]\nllama index. llama-index. [Online]. Available: https://github.com/\nrun-llama/llama index\nAPPENDIX\n1. Open Source Toolkits For LLM Development and\nDeployment\nThere are various frameworks and libraries developed for\nLLM training, evaluation, and deployment, and covering every\nsingle framework is out of this paper’s scope. But we try to\nprovide a brief introduction of some of the most popular ones,\ngrouped into different categories.\nA. LLM Training/Inference Frameworks\nSome of the popular frameworks which are useful for LLM\ntraining includes (note that some of them can be used beyond\nLLM training too):\nDeepSpeed [221] is a deep learning optimization library\nthat makes distributed training and inference easy, efficient,\nand effective. DeepSpeed enables world’s most powerful lan-\nguage models like MT-530B and BLOOM. It is an easy-\nto-use deep learning optimization software suite that powers\nunprecedented scale and speed for both training and inference.\nWith DeepSpeed you can:\nTransformers [222] is library by HuggingFace which\nprovides thousands of pretrained models to perform tasks on\ndifferent modalities such as text, vision, and audio. Using\npretrained models one can reduce compute costs, carbon\n",
      "word_count": 867,
      "char_count": 6710,
      "fonts": [
        "NimbusRomNo9L-ReguItal (10.0pt)",
        "NimbusRomNo9L-ReguItal (8.0pt)",
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (8.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 43,
      "text": "footprint, and save the time and resources required to train\na model from scratch.\nMegatron-LM [223] is a large, powerful transformer\ndeveloped by the Applied Deep Learning Research team\nat NVIDIA. It contains efficient, model-parallel (tensor, se-\nquence, and pipeline), and multi-node pre-training of trans-\nformer based models such as GPT, BERT, and T5 using mixed\nprecision.\nBMTrain [224] is an efficient large model training toolkit\nthat can be used to train large models with tens of billions of\nparameters. It can train models in a distributed manner while\nkeeping the code as simple as stand-alone training.\nGPT-NeoX [225] leverages many of the same features and\ntechnologies as the popular Megatron-DeepSpeed library but\nwith substantially increased usability and novel optimizations.\nLoRA [226] library provides the support for Low-Rank\nAdaptation of Large Language Models. It reduces the number\nof trainable parameters by learning pairs of rank-decompostion\nmatrices while freezing the original weights. This vastly\nreduces the storage requirement for large language models\nadapted to specific tasks and enables efficient task-switching\nduring deployment all without introducing inference latency.\nLoRA also outperforms several other adaptation methods in-\ncluding adapter, prefix-tuning, and fine-tuning.\nColossalAI library [227] provides a collection of parallel\ncomponents. It aims to support developers to write their\ndistributed deep learning models just like how they write their\nmodel on their laptop. They provide user-friendly tools to\nkickstart distributed training and inference in a few lines. In\nterms of Parallelism strategies, they support: Data Parallelism,\nPipeline Parallelism, Sequence Parallelism, Zero Redundancy\nOptimizer (ZeRO) [140], and Auto-Parallelism.\nB. Deployment Tools\nWe provide an overview of some of the most popular LLM\ndeployment tools here.\nFastChat [228] is an open platform for training, serv-\ning, and evaluating large language model based chatbots.\nFastChat’s core features include: The training and evaluation\ncode for state-of-the-art models (e.g., Vicuna, MT-Bench), and\na distributed multi-model serving system with web UI and\nOpenAI-compatible RESTful APIs.\nSkypilot [229] is a framework for running LLMs, AI,\nand batch jobs on any cloud, offering maximum cost savings,\nhighest GPU availability, and managed execution.\nvLLM [230] is a fast and easy-to-use library for LLM in-\nference and serving. vLLM seamlessly supports many Hugging\nFace models, including the following architectures: Aquila,\nBaichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT Big-\nCode, LLaMA, LLaMA 2, Mistral, Mixtral, MPT, OPT, Qwen,\nYi, and many more.\ntext-generation-inference [231] is a toolkit for deploying\nand serving Large Language Models (LLMs). TGI enables\nhigh-performance text generation for the most popular open-\nsource LLMs, including Llama, Falcon, StarCoder, BLOOM,\nGPT-NeoX, and more.\nLangChain [232] is a framework for developing applica-\ntions powered by language models. It enables applications that:\n•\nAre context-aware: connect a language model to\nsources of context (prompt instructions, few shot ex-\namples, content to ground its response in, etc.)\n•\nReason: rely on a language model to reason (about\nhow to answer based on provided context, what ac-\ntions to take, etc.)\nOpenLLM [233] is an open-source platform designed to\nfacilitate the deployment and operation of large language mod-\nels (LLMs) in real-world applications. With OpenLLM, you\ncan run inference on any open-source LLM, deploy them on\nthe cloud or on-premises, and build powerful AI applications.\nEmbedchain [234] is an Open Source RAG Framework\nthat makes it easy to create and deploy AI apps. Embedchain\nstreamlines the creation of RAG applications, offering a seam-\nless process for managing various types of unstructured data.\nIt efficiently segments data into manageable chunks, generates\nrelevant embeddings, and stores them in a vector database for\noptimized retrieval.\nAutogen [235] is a framework that enables the devel-\nopment of LLM applications using multiple agents that can\nconverse with each other to solve tasks. AutoGen agents\nare customizable, conversable, and seamlessly allow human\nparticipation. They can operate in various modes that employ\ncombinations of LLMs, human inputs, and tools.\nBabyAGI [236] is an autonomous Artificial Intelligence\nagent, that is designed to generate and execute tasks based on\ngiven objectives. It harnesses cutting-edge technologies from\nOpenAI, Pinecone, LangChain, and Chroma to automate tasks\nand achieve specific goals. In this blog post, we will dive\ninto the unique features of BabyAGI and explore how it can\nstreamline task automation.\nC. Prompting Libraries\nGuidance [237] is a programming paradigm that offers\nsuperior control and efficiency compared to conventional\nprompting and chaining. It allows users to constrain generation\n(e.g. with regex and CFGs) as well as to interleave control\n(conditional, loops) and generation seamlessly.\nPromptTools [238] offers a set of open-source, self-\nhostable tools for experimenting with, testing, and evaluating\nLLMs, vector databases, and prompts. The core idea is to\nenable developers to evaluate using familiar interfaces like\ncode, notebooks, and a local playground.\nPromptBench [?] is a Pytorch-based Python package for\nEvaluation of Large Language Models (LLMs). It provides\nuser-friendly APIs for researchers to conduct evaluation on\nLLMs.\nPromptfoo [239] is a tool for testing and evaluating LLM\noutput quality. It systematically test prompts, models, and\nRAGs with predefined test cases.\n",
      "word_count": 824,
      "char_count": 5617,
      "fonts": [
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "CMSY10 (10.0pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    },
    {
      "page_number": 44,
      "text": "D. VectorDB\nFaiss [240] is a library developed by Facebook AI Re-\nsearch that provides efficient similarity search and clustering\nof dense vectors. It is designed for use with large-scale,\nhigh-dimensional data and supports several index types and\nalgorithms for various use cases.\nMilvus [241] is an open-source vector database built to\npower embedding similarity search and AI applications. Mil-\nvus makes unstructured data search more accessible, and pro-\nvides a consistent user experience regardless of the deployment\nenvironment.\nQdrant [242] is a vector similarity search engine and\nvector database. It provides a production-ready service with a\nconvenient API to store, search, and manage points—vectors\nwith an additional payload Qdrant is tailored to extended\nfiltering support. environment.\nWeaviate [243] is an open-source, GraphQL-based vec-\ntor search engine that enables similarity search on high-\ndimensional data. While it is open-source, the commercial ver-\nsion offers additional features, support, and managed services.\nSome of the other popular options includes LlamaIndex\n[244] and Pinecone.\n",
      "word_count": 161,
      "char_count": 1114,
      "fonts": [
        "NimbusRomNo9L-Medi (10.0pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-ReguItal (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        612.0,
        792.0
      ]
    }
  ],
  "extraction_time": 2.3375911712646484,
  "file_size_mb": 4.655871391296387,
  "basic_extraction": {
    "page_count": 44,
    "text_stats": {
      "total_words": 32034,
      "total_chars": 209028
    },
    "fonts_used": [
      "Arial-BoldItalicMT (6.9pt)",
      "Arial-BoldMT (2.9pt)",
      "Arial-BoldMT (3.3pt)",
      "Arial-BoldMT (3.7pt)",
      "Arial-BoldMT (4.1pt)",
      "Arial-BoldMT (4.5pt)",
      "Arial-BoldMT (4.8pt)",
      "Arial-BoldMT (4.9pt)",
      "Arial-BoldMT (5.2pt)",
      "Arial-BoldMT (5.7pt)",
      "Arial-BoldMT (6.0pt)",
      "Arial-BoldMT (6.3pt)",
      "Arial-BoldMT (6.9pt)",
      "Arial-BoldMT (8.2pt)",
      "Arial-BoldMT (9.8pt)",
      "ArialMT (3.2pt)",
      "ArialMT (3.8pt)",
      "ArialMT (4.1pt)",
      "ArialMT (4.8pt)",
      "ArialMT (5.2pt)",
      "ArialMT (5.4pt)",
      "ArialMT (6.5pt)",
      "ArialMT (6.9pt)",
      "ArialMT (7.5pt)",
      "ArialMT (8.2pt)",
      "CMEX10 (10.0pt)",
      "CMMI10 (10.0pt)",
      "CMMI5 (5.0pt)",
      "CMMI7 (7.0pt)",
      "CMMI8 (8.0pt)",
      "CMR10 (10.0pt)",
      "CMR6 (6.0pt)",
      "CMR7 (7.0pt)",
      "CMR8 (8.0pt)",
      "CMSY10 (10.0pt)",
      "CMSY6 (6.0pt)",
      "CMSY7 (7.0pt)",
      "ComicSansMS (6.5pt)",
      "MSAM10 (10.0pt)",
      "MSAM7 (7.0pt)",
      "MSBM10 (10.0pt)",
      "NimbusRomNo9L-Medi (10.0pt)",
      "NimbusRomNo9L-Medi (7.0pt)",
      "NimbusRomNo9L-Medi (9.0pt)",
      "NimbusRomNo9L-MediItal (10.0pt)",
      "NimbusRomNo9L-MediItal (9.0pt)",
      "NimbusRomNo9L-Regu (10.0pt)",
      "NimbusRomNo9L-Regu (11.0pt)",
      "NimbusRomNo9L-Regu (23.9pt)",
      "NimbusRomNo9L-Regu (6.0pt)",
      "NimbusRomNo9L-Regu (7.0pt)",
      "NimbusRomNo9L-Regu (8.0pt)",
      "NimbusRomNo9L-Regu (9.0pt)",
      "NimbusRomNo9L-ReguItal (10.0pt)",
      "NimbusRomNo9L-ReguItal (8.0pt)",
      "Times-Roman (20.0pt)",
      "rsfs10 (10.0pt)"
    ],
    "images_count": 50,
    "metadata": {
      "format": "PDF 1.5",
      "creator": "LaTeX with hyperref",
      "producer": "pdfTeX-1.40.25",
      "creationDate": "D:20250325010006Z",
      "modDate": "D:20250325010006Z"
    },
    "pages": [
      {
        "page_number": 1,
        "text": "Large Language Models: A Survey\nShervin Minaee1, Tomas Mikolov2, Narjes Nikzad3, Meysam Chenaghlu4\nRichard Socher5, Xavier Amatriain6, Jianfeng Gao7\n1 Applied Scientist, Amazon Inc\n2 Senior Researcher, CIIRC CTU\n3 Cologne University of Applied Sciences\n4 Staff Machine Learning Scientist, Ultimate.ai\n5 CEO, You.com\n6 VP of Product, AI and Compute Enablement, Google Inc\n7 VP of Deep Learning Group, Microsoft Research\nAbstract—Large Language Models (LLMs) have drawn a\nlot of attention due to their strong performance on a wide\nrange of natural language tasks, since the release of ChatGPT\nin November 2022. LLMs’ ability of general-purpose language\nunderstanding and generation is acquired by training billions of\nmodel’s parameters on massive amounts of text data, as predicted\nby scaling laws [1], [2]. The research area of LLMs, while very\nrecent, is evolving rapidly in many different ways. In this paper,\nwe review some of the most prominent LLMs, including three\npopular LLM families (GPT, LLaMA, PaLM), and discuss their\ncharacteristics, contributions and limitations. We also give an\noverview of techniques developed to build, and augment LLMs.\nWe then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation\nmetrics, and compare the performance of several popular LLMs\non a set of representative benchmarks. Finally, we conclude\nthe paper by discussing open challenges and future research\ndirections.\nI.\nINTRODUCTION\nLanguage modeling is a long-standing research topic, dat-\ning back to the 1950s with Shannon’s application of informa-\ntion theory to human language, where he measured how well\nsimple n-gram language models predict or compress natural\nlanguage text [3]. Since then, statistical language modeling\nbecame fundamental to many natural language understanding\nand generation tasks, ranging from speech recognition, ma-\nchine translation, to information retrieval [4], [5], [6].\nThe recent advances on transformer-based large language\nmodels (LMs), pretrained on Web-scale text corpora, signifi-\ncantly extended the capabilities of language models (LLMs).\nFor example, OpenAI’s ChatGPT and GPT-4 can be used not\nonly for natural language processing, but also as general task\nsolvers to power Microsoft’s Co-Pilot systems, for instance,\ncan follow human instructions of complex new tasks per-\nforming multi-step reasoning when needed. LLMs are thus\nbecoming the basic building block for the development of\ngeneral-purpose AI agents or artificial general intelligence\n(AGI).\nAs the field of LLMs is moving fast, with new findings,\nmodels and techniques being published in a matter of months\nor weeks [7], [8], [9], [10], [11], AI researchers and practi-\ntioners often find it challenging to figure out the best recipes\nto build LLM-powered AI systems for their tasks. This paper\ngives a timely survey of the recent advances on LLMs. We\nhope this survey will prove a valuable and accessible resource\nfor students, researchers and developers.\nLLMs are large-scale, pre-trained, statistical language mod-\nels based on neural networks. The recent success of LLMs is\nan accumulation of decades of research and development of\nlanguage models, which can be categorized into four waves\nthat have different starting points and velocity: statistical lan-\nguage models, neural language models, pre-trained language\nmodels and LLMs.\nStatistical language models (SLMs) view text as a sequence\nof words, and estimate the probability of text as the product\nof their word probabilities. The dominating form of SLMs\nare Markov chain models known as the n-gram models,\nwhich compute the probability of a word conditioned on its\nimmediate proceeding n − 1 words. Since word probabilities\nare estimated using word and n-gram counts collected from\ntext corpora, the model needs to deal with data sparsity (i.e.,\nassigning zero probabilities to unseen words or n-grams) by\nusing smoothing, where some probability mass of the model\nis reserved for unseen n-grams [12]. N-gram models are\nwidely used in many NLP systems. However, these models\nare incomplete in that they cannot fully capture the diversity\nand variability of natural language due to data sparsity.\nEarly neural language models (NLMs) [13], [14], [15], [16]\ndeal with data sparsity by mapping words to low-dimensional\ncontinuous vectors (embedding vectors) and predict the next\nword based on the aggregation of the embedding vectors of\nits proceeding words using neural networks. The embedding\nvectors learned by NLMs define a hidden space where the\nsemantic similarity between vectors can be readily computed\nas their distance. This opens the door to computing semantic\nsimilarity of any two inputs regardless their forms (e.g., queries\nvs. documents in Web search [17], [18], sentences in different\nlanguages in machine translation [19], [20]) or modalities (e.g.,\nimage and text in image captioning [21], [22]). Early NLMs are\ntask-specific models, in that they are trained on task-specific\ndata and their learned hidden space is task-specific.\nPre-trained language models (PLMs), unlike early NLMs,\nare task-agnostic. This generality also extends to the learned\narXiv:2402.06196v3  [cs.CL]  23 Mar 2025\n",
        "word_count": 791,
        "char_count": 5225,
        "fonts": [
          "NimbusRomNo9L-MediItal (9.0pt)",
          "NimbusRomNo9L-Regu (11.0pt)",
          "NimbusRomNo9L-Medi (9.0pt)",
          "NimbusRomNo9L-Regu (23.9pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMMI10 (10.0pt)",
          "Times-Roman (20.0pt)",
          "CMSY10 (10.0pt)",
          "CMR8 (8.0pt)",
          "CMR10 (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 2,
        "text": "hidden embedding space. The training and inference of PLMs\nfollows the pre-training and fine-tuning paradigm, where lan-\nguage models with recurrent neural networks [23] or trans-\nformers [24], [25], [26] are pre-trained on Web-scale unlabeled\ntext corpora for general tasks such as word prediction, and then\nfinetuned to specific tasks using small amounts of (labeled)\ntask-specific data. Recent surveys on PLMs include [8], [27],\n[28].\nLarge language models mainly refer to transformer-based\nneural language models 1 that contain tens to hundreds of\nbillions of parameters, which are pre-trained on massive text\ndata, such as PaLM [31], LLaMA [32], and GPT-4 [33], as\nsummarized in Table III. Compared to PLMs, LLMs are not\nonly much larger in model size, but also exhibit stronger\nlanguage understanding and generation abilities, and more\nimportantly, emergent abilities that are not present in smaller-\nscale language models. As illustrated in Fig. 1, these emergent\nabilities include (1) in-context learning, where LLMs learn\na new task from a small set of examples presented in the\nprompt at inference time, (2) instruction following, where\nLLMs, after instruction tuning, can follow the instructions\nfor new types of tasks without using explicit examples, and\n(3) multi-step reasoning, where LLMs can solve a complex\ntask by breaking down that task into intermediate reasoning\nsteps as demonstrated in the chain-of-thought prompt [34].\nLLMs can also be augmented by using external knowledge\nand tools [35], [36] so that they can effectively interact with\nusers and environment [37], and continually improve itself\nusing feedback data collected through interactions (e.g. via\nreinforcement learning with human feedback (RLHF)).\nThrough advanced usage and augmentation techniques,\nLLMs can be deployed as so-called AI agents: artificial entities\nthat sense their environment, make decisions, and take actions.\nPrevious research has focused on developing agents for specific\ntasks and domains. The emergent abilities demonstrated by\nLLMs make it possible to build general-purpose AI agents\nbased on LLMs. While LLMs are trained to produce responses\nin static settings, AI agents need to take actions to interact with\ndynamic environment. Therefore, LLM-based agents often\nneed to augment LLMs to e.g., obtain updated information\nfrom external knowledge bases, verify whether a system action\nproduces the expected result, and cope with when things do\nnot go as expected, etc. We will discuss in detail LLM-based\nagents in Section IV.\nIn the rest of this paper, Section II presents an overview of\nstate of the art of LLMs, focusing on three LLM families (GPT,\nLLaMA and PaLM) and other representative models. Section\nIII discusses how LLMs are built. Section IV discusses how\nLLMs are used, and augmented for real-world applications\nSections V and VI review popular datasets and benchmarks for\nevaluating LLMs, and summarize the reported LLM evaluation\nresults. Finally, Section VII concludes the paper by summa-\nrizing the challenges and future research directions.\nII.\nLARGE LANGUAGE MODELS\nIn this section we start with a review of early pre-trained\nneural language models as they are the base of LLMs, and\n1Recently, several very promising non-transformer LLMs have been pro-\nposed, such as the LLMs based on structured state space models [29], [30].\nSee Section VII for more details.\nthen focus our discussion on three families of LLMs: GPT,\nLlaMA, and PaLM. Table I provides an overview of some of\nthese models and their characteristics.\nA. Early Pre-trained Neural Language Models\nLanguage modeling using neural networks was pioneered\nby [38], [39], [40]. Bengio et al. [13] developed one of the first\nneural language models (NLMs) that are comparable to n-gram\nmodels. Then, [14] successfully applied NLMs to machine\ntranslation. The release of RNNLM (an open source NLM\ntoolkit) by Mikolov [41], [42] helped significantly popularize\nNLMs. Afterwards, NLMs based on recurrent neural networks\n(RNNs) and their variants, such as long short-term memory\n(LSTM) [19] and gated recurrent unit (GRU) [20], were widely\nused for many natural language applications including machine\ntranslation, text generation and text classification [43].\nThen, the invention of the Transformer architecture [44]\nmarks another milestone in the development of NLMs. By\napplying self-attention to compute in parallel for every word\nin a sentence or document an “attention score” to model the\ninfluence each word has on another, Transformers allow for\nmuch more parallelization than RNNs, which makes it possible\nto efficiently pre-train very big language models on large\namounts of data on GPUs. These pre-trained language models\n(PLMs) can be fine-tuned for many downstream tasks.\nWe group early popular Transformer-based PLMs, based on\ntheir neural architectures, into three main categories: encoder-\nonly, decoder-only, and encoder-decoder models. Comprehen-\nsive surveys of early PLMs are provided in [43], [28].\n1) Encoder-only PLMs: As the name suggests, the encoder-\nonly models only consist of an encoder network. These models\nare originally developed for language understanding tasks,\nsuch as text classification, where the models need to predict a\nclass label for an input text. Representative encoder-only mod-\nels include BERT and its variants, e.g., RoBERTa, ALBERT,\nDeBERTa, XLM, XLNet, UNILM, as to be described below.\nBERT (Birectional Encoder Representations from Trans-\nformers) [24] is one of the most widely used encoder-only\nlanguage models. BERT consists of three modules: (1) an\nembedding module that converts input text into a sequence\nof embedding vectors, (2) a stack of Transformer encoders\nthat converts embedding vectors into contextual representation\nvectors, and (3) a fully connected layer that converts the\nrepresentation vectors (at the final layer) to one-hot vectors.\nBERT is pre-trained uses two objectives: masked language\nmodeling (MLM) and next sentence prediction. The pre-trained\nBERT model can be fine-tuned by adding a classifier layer\nfor many language understanding tasks, ranging from text\nclassification, question answering to language inference. A\nhigh-level overview of BERT framework is shown in Fig 3. As\nBERT significantly improved state of the art on a wide range\nof language understanding tasks when it was published, the AI\ncommunity was inspired to develop many similar encoder-only\nlanguage models based on BERT.\nRoBERTa [25] significantly improves the robustness of\nBERT using a set of model design choices and training strate-\ngies, such as modifying a few key hyperparameters, removing\nthe next-sentence pre-training objective and training with much\n",
        "word_count": 1017,
        "char_count": 6695,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "NimbusRomNo9L-Regu (7.0pt)",
          "NimbusRomNo9L-Regu (6.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 3,
        "text": "Emerging\nBasic\nAugmented\nLLM Capabilities\nReasoning\nCoding\nComprehension\nMultilingual\nTool\nutilization\nWorld\nknowledge\nInstruction\nfollowing\nIn-context\nlearning\nInteracting\nwith users\nSelf-improvement\nMulti choice QA\nWikipedia QA\nXNLI\nCrosslingual QA\nCrosslingual Tasks\nTranslation\nReading Comprehension\nMulti choice QA\nBoolean QA\nSimplification\nSummarization\nFunction Calling\nAPI calling\nLogical\nSymbolic\nCommon Sense\nArithmetic\nTurn based\nCompletion\nTask definition\nFew-shot\nSymbolic\nreference\nPos/Neg example\nStep by step\nsolving\nTool planning\nTask\ndecomposition\nVirtual acting\nPhysical acting\nKnowledge base\nutilization\nAssignment\nplanning\nSelf-cirtisim\nSelf-refinement\nFig. 1: LLM Capabilities.\nlarger mini-batches and learning rates. ALBERT [45] uses two\nparameter-reduction techniques to lower memory consumption\nand increase the training speed of BERT: (1) splitting the\nembedding matrix into two smaller matrices, and (2) using\nrepeating layers split among groups. DeBERTa (Decoding-\nenhanced BERT with disentangled attention) [26] improves the\nBERT and RoBERTa models using two novel techniques. The\nfirst is the disentangled attention mechanism, where each word\nis represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words\nare computed using disentangled matrices on their contents and\nrelative positions, respectively. Second, an enhanced mask de-\ncoder is used to incorporate absolute positions in the decoding\nlayer to predict the masked tokens in model pre-training. In\naddition, a novel virtual adversarial training method is used for\nfine-tuning to improve models’ generalization. ELECTRA [46]\nuses a new pre-training task, known as replaced token detection\n(RTD), which is empirically proven to be more sample-efficient\nthan MLM. Instead of masking the input, RTD corrupts it by\nreplacing some tokens with plausible alternatives sampled from\na small generator network. Then, instead of training a model\nthat predicts the original identities of the corrupted tokens, a\ndiscriminative model is trained to predict whether a token in\nthe corrupted input was replaced by a generated sample or not.\nRTD is more sample-efficient than MLM because the former\nis defined over all input tokens rather than just the small subset\nbeing masked out, as illustrated in Fig 4.\nXLMs [47] extended BERT to cross-lingual language\nmodels using two methods: (1) a unsupervised method that\nonly relies on monolingual data, and (2) a supervised method\nthat leverages parallel data with a new cross-lingual language\nmodel objective, as illustrated in Fig 5. XLMs had obtained\nstate-of-the-art results on cross-lingual classification, unsuper-\nvised and supervised machine translation, at the time they were\nproposed.\nThere are also encoder-only language models that leverage\nthe advantages of auto-regressive (decoder) models for model\ntraining and inference. Two examples are XLNet and UNILM.\nXLNet [48] is based on Transformer-XL, pre-trained using a\ngeneralized autoregressive method that enables learning bidi-\nrectional contexts by maximizing the expected likelihood over\nall permutations of the factorization order. UNILM (UNIfied\npre-trained Language Model) [49] is pre-trained using three\ntypes of language modeling tasks: unidirectional, bidirectional,\nand sequence-to-sequence prediction. This is achieved by\nemploying a shared Transformer network and utilizing specific\nself-attention masks to control what context the prediction is\nconditioned on, as illustrated in Fig 6. The pre-trained model\ncan be fine-tuned for both natural language understanding and\ngeneration tasks.\n2) Decoder-only PLMs: Two of the most widely used\ndecoder-only PLMs are GPT-1 and GPT-2, developed by\nOpenAI. These models lay the foundation to more powerful\nLLMs subsequently, i.e., GPT-3 and GPT-4.\nGPT-1 [50] demonstrates for the first time that good\nperformance over a wide range of natural language tasks can be\nobtained by Generative Pre-Training (GPT) of a decoder-only\nTransformer model on a diverse corpus of unlabeled text in a\nself-supervised learning fashion (i.e., next word/token predic-\ntion), followed by discriminative fine-tuning on each specific\ndownstream task (with much fewer samples), as illustrated in\nFig 7. GPT-1 paves the way for subsequent GPT models, with\neach version improving upon the architecture and achieving\nbetter performance on various language tasks.\nGPT-2 [51] shows that language models are able to learn\nto perform specific natural language tasks without any explicit\nsupervision when trained on a large WebText dataset consisting\nof millions of webpages. The GPT-2 model follows the model\ndesigns of GPT-1 with a few modifications: Layer normal-\nization is moved to the input of each sub-block, additional\nlayer normalization is added after the final self-attention block,\n",
        "word_count": 695,
        "char_count": 4858,
        "fonts": [
          "Arial-BoldMT (3.7pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "Arial-BoldMT (2.9pt)",
          "Arial-BoldMT (4.1pt)",
          "Arial-BoldMT (3.3pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "Arial-BoldMT (4.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 4,
        "text": "Paper Strcuture\nEarly Pre-trained\nLanguage Models\nII\nLarge Language Models\nA\nIII\nHOW LLMS ARE BUILT\nA\nData Cleaning\nB\nLarge Language\nModel Families\nB\nOther Representative\nLLMs\nC\nDominant LLM\nArchitectures\nTokenizations\nC\nPositional Encoding\nD\nModel Pre-training\nE\nFine-tuning and\nInstruction Tuning\nF\nAlignment\nG\nDecoding Strategies\nH\nI\nHOW LLMS ARE USED AND AUGMENTED\nA\nB\nLLM limitations\nCost-Effective Training/Inference,\nAdaptation & Compression\nI\nUsing LLMs: Prompt Design\nand Engineering\nC\nAugmenting LLMs through\nexternal knowledge - RAG\nD\nUsing External Tools\nE\nLLM Agents\nV\n POPULAR DATASETS FOR LLMS\nA\nDatasets for Basic Tasks: language\nmodeling/understanding/generation\nB\n Datasets for Emergent: ICL, reasoning,\ninstruction following\nC\nDatasets for Augmented: using\nexternal knowledge/tools\nVI\n PROMINENT LLMS’ PERFORMANCE\nON BENCHMARKS\nA\nB\nVII\nCHALLENGES AND FUTURE DIRECTIONS\nA\nSmaller and more efficient\nLanguage Models\nLLMs’ Performance on Different Tasks\nPopular Metrics for Evaluating LLMs\nB\nNew Post-attention\nArchitectural Paradigms\nC\nMulti-modal Models\nD\nImproved LLM Usage and\nAugmentation techniques\nD\nSecurity and\nEthical/Responsible AI\nFig. 2: The paper structure.\ninitialization is modified to account for the accumulation on\nthe residual path and scaling the weights of residual layers,\nvocabulary size is expanded to 50,25, and context size is\nincreased from 512 to 1024 tokens.\n3) Encoder-Decoder PLMs: In [52], Raffle et al. shows that\nalmost all NLP tasks can be cast as a sequence-to-sequence\ngeneration task. Thus, an encoder-decoder language model, by\ndesign, is a unified model in that it can perform all natural\nlanguage understanding and generation tasks. Representative\nencoder-decoder PLMs we will review below are T5, mT5,\nMASS, and BART.\nT5 [52] is a Text-to-Text Transfer Transformer (T5) model,\nwhere transfer learning is effectively exploited for NLP via an\nintroduction of a unified framework in which all NLP tasks are\ncast as a text-to-text generation task. mT5 [53] is a multilingual\nvariant of T5, which is pre-trained on a new Common Crawl-\nbased dataset consisting of texts in 101 languages.\nMASS (MAsked Sequence to Sequence pre-training) [54]\nadopts the encoder-decoder framework to reconstruct a sen-\ntence fragment given the remaining part of the sentence. The\nencoder takes a sentence with randomly masked fragment\n(several consecutive tokens) as input, and the decoder predicts\nthe masked fragment. In this way, MASS jointly trains the\n",
        "word_count": 375,
        "char_count": 2491,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "Arial-BoldMT (4.5pt)",
          "Arial-BoldMT (6.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "Arial-BoldMT (5.2pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 5,
        "text": "TABLE I: High-level Overview of Popular Language Models\nType\nModel Name\n#Parameters\nRelease\nBase Models\nOpen\nSource\n#Tokens\nTraining dataset\nBERT\n110M, 340M\n2018\n-\n✓\n137B\nBooksCorpus, English Wikipedia\nRoBERTa\n355M\n2019\n-\n✓\n2.2T\nBooksCorpus,\nEnglish\nWikipedia,\nCC-NEWS,\nSTORIES (a subset of Common Crawl), Reddit\nEncoder-Only\nALBERT\n12M,\n18M,\n60M,\n235M\n2019\n-\n✓\n137B\nBooksCorpus, English Wikipedia\nDeBERTa\n-\n2020\n-\n✓\n-\nBooksCorpus, English Wikipedia, STORIES, Red-\ndit content\nXLNet\n110M, 340M\n2019\n-\n✓\n32.89B\nBooksCorpus, English Wikipedia, Giga5, Com-\nmon Crawl, ClueWeb 2012-B\nDecoder-only\nGPT-1\n120M\n2018\n-\n✓\n1.3B\nBooksCorpus\nGPT-2\n1.5B\n2019\n-\n✓\n10B\nReddit outbound\nT5 (Base)\n223M\n2019\n-\n✓\n156B\nCommon Crawl\nEncoder-Decoder\nMT5 (Base)\n300M\n2020\n-\n✓\n-\nNew Common Crawl-based dataset in 101 lan-\nguages (m Common Crawl)\nBART (Base)\n139M\n2019\n-\n✓\n-\nCorrupting text\nGPT-3\n125M,\n350M,\n760M, 1.3B, 2.7B,\n6.7B, 13B, 175B\n2020\n×\n300B\nCommon Crawl (filtered), WebText2, Books1,\nBooks2, Wikipedia\nGPT Family\nCODEX\n12B\n2021\nGPT\n✓\n-\nPublic GitHub software repositories\nWebGPT\n760M, 13B, 175B\n2021\nGPT-3\n×\n-\nELI5\nGPT-4\n1.76T\n2023\n-\n×\n13T\n-\nLLaMA1\n7B, 13B, 33B, 65B\n2023\n-\n✓\n1T, 1.4T\nOnline sources\nLLaMA2\n7B, 13B, 34B, 70B\n2023\n-\n✓\n2T\nOnline sources\nAlpaca\n7B\n2023\nLLaMA1\n✓\n-\nGPT-3.5\nVicuna-13B\n13B\n2023\nLLaMA1\n✓\n-\nGPT-3.5\nLLaMA Family\nKoala\n13B\n2023\nLLaMA\n✓\n-\nDialogue data\nMistral-7B\n7.3B\n2023\n✓\n-\n-\nCode Llama\n34\n2023\nLLaMA2\n✓\n500B\nPublicly available code\nLongLLaMA\n3B, 7B\n2023\nOpenLLaMA\n✓\n1T\n-\nLLaMA-Pro-8B\n8.3B\n2024\nLLaMA2-7B\n✓\n80B\nCode and math corpora\nTinyLlama-1.1B\n1.1B\n2024\nLLaMA1.1B\n✓\n3T\nSlimPajama, Starcoderdata\nPaLM\n8B, 62B, 540B\n2022\n-\n×\n780B\nWeb documents, books, Wikipedia, conversations,\nGitHub code\nU-PaLM\n8B, 62B, 540B\n2022\n-\n×\n1.3B\nWeb documents, books, Wikipedia, conversations,\nGitHub code\nPaLM Family\nPaLM-2\n340B\n2023\n-\n✓\n3.6T\nWeb documents, books, code, mathematics, con-\nversational data\nMed-PaLM\n540B\n2022\nPaLM\n×\n780B\nHealthSearchQA, MedicationQA, LiveQA\nMed-PaLM 2\n-\n2023\nPaLM 2\n×\n-\nMedQA, MedMCQA, HealthSearchQA, LiveQA,\nMedicationQA\nFLAN\n137B\n2021\nLaMDA-PT\n✓\n-\nWeb documents, code, dialog data, Wikipedia\nGopher\n280B\n2021\n-\n×\n300B\nMassiveText\nERNIE 4.0\n10B\n2023\n-\n×\n4TB\nChinese text\nRetro\n7.5B\n2021\n-\n×\n600B\nMassiveText\nLaMDA\n137B\n2022\n-\n×\n168B\npublic dialog data and web documents\nChinChilla\n70B\n2022\n-\n×\n1.4T\nMassiveText\nGalactia-120B\n120B\n2022\n-\n450B\nOther Popular LLMs\nCodeGen\n16.1B\n2022\n-\n✓\n-\nTHE PILE, BIGQUERY, BIGPYTHON\nBLOOM\n176B\n2022\n-\n✓\n366B\nROOTS\nZephyr\n7.24B\n2023\nMistral-7B\n✓\n800B\nSynthetic data\nGrok-0\n33B\n2023\n-\n×\n-\nOnline source\nORCA-2\n13B\n2023\nLLaMA2\n-\n2001B\n-\nStartCoder\n15.5B\n2023\n-\n✓\n35B\nGitHub\nMPT\n7B\n2023\n-\n✓\n1T\nRedPajama, m Common Crawl, S2ORC, Common\nCrawl\nMixtral-8x7B\n46.7B\n2023\n-\n✓\n-\nInstruction dataset\nFalcon 180B\n180B\n2023\n-\n✓\n3.5T\nRefinedWeb\nGemini\n1.8B, 3.25B\n2023\n✓\n-\nWeb documents, books, and code, image data,\naudio data, video data\nDeepSeek-Coder\n1.3B, 6.7B, 33B\n2024\n-\n✓\n2T\nGitHub’s Markdown and StackExchange\nDocLLM\n1B,7B\n2024\n-\n×\n2T\nIIT-CDIP Test Collection 1.0, DocBank\nencoder and decoder for language embedding and generation,\nrespectively.\nBART [55] uses a standard sequence-to-sequence transla-\ntion model architecture. It is pre-trained by corrupting text with\nan arbitrary noising function, and then learning to reconstruct\nthe original text.\nB. Large Language Model Families\nLarge\nlanguage\nmodels\n(LLMs)\nmainly\nrefer\nto\ntransformer-based\nPLMs\nthat\ncontain\ntens\nto\nhundreds\nof billions of parameters. Compared to PLMs reviewed above,\nLLMs are not only much larger in model size, but also exhibit\nstronger language understanding and generation and emergent\nabilities that are not present in smaller-scale models. In what\nfollows, we review three LLM families: GPT, LLaMA, and\nPaLM, as illustrated in Fig 8.\n1) The GPT Family: Generative Pre-trained Transform-\ners (GPT) are a family of decoder-only Transformer-based\nlanguage models, developed by OpenAI. This family con-\nsists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4,\n",
        "word_count": 665,
        "char_count": 3998,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "NimbusRomNo9L-Regu (7.0pt)",
          "NimbusRomNo9L-Medi (7.0pt)",
          "MSAM7 (7.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "CMSY7 (7.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 6,
        "text": "Fig. 3: Overall pre-training and fine-tuning procedures for\nBERT. Courtesy of [24]\nFig. 4: A comparison between replaced token detection and\nmasked language modeling. Courtesy of [46].\nCODEX, and WebGPT. Although early GPT models, such as\nGPT-1 and GPT-2, are open-source, recent models, such as\nGPT-3 and GPT-4, are close-source and can only be accessed\nvia APIs. GPT-1 and GPT-2 models have been discussed in\nthe early PLM subsection. We start with GPT-3 below.\nGPT-3 [56] is a pre-trained autoregressive language model\nwith 175 billion parameters. GPT-3 is widely considered as\nthe first LLM in that not only it is much larger than previous\nPLMs, but also for the first time demonstrates emergent\nabilities that are not observed in previous smaller PLMs. GPT-\n3 shows the emergent ability of in-context learning, which\nmeans GPT-3 can be applied to any downstream tasks without\nany gradient updates or fine-tuning, with tasks and few-shot\ndemonstrations specified purely via text interaction with the\nmodel. GPT-3 achieved strong performance on many NLP\ntasks, including translation, question-answering, and the cloze\ntasks, as well as several ones that require on-the-fly reasoning\nor domain adaptation, such as unscrambling words, using a\nnovel word in a sentence, 3-digit arithmetic. Fig 9 plots the\nperformance of GPT-3 as a function of the number of examples\nin in-context prompts.\nCODEX [57], released by OpenAI in March 2023, is a\ngeneral-purpose programming model that can parse natural\nlanguage and generate code in response. CODEX is a de-\nscendant of GPT-3, fine-tuned for programming applications\non code corpora collected from GitHub. CODEX powers\nMicrosoft’s GitHub Copilot.\nWebGPT [58] is another descendant of GPT-3, fine-tuned to\nanswer open-ended questions using a text-based web browser,\nfacilitating users to search and navigate the web. Specifically,\nFig. 5: Cross-lingual language model pretraining. The MLM\nobjective is similar to BERT, but with continuous streams\nof text as opposed to sentence pairs. The TLM objective\nextends MLM to pairs of parallel sentences. To predict a\nmasked English word, the model can attend to both the English\nsentence and its French translation, and is encouraged to align\nEnglish and French representations. Courtesy of [47].\nFig. 6: Overview of unified LM pre-training. The model\nparameters are shared across the LM objectives (i.e., bidirec-\ntional LM, unidirectional LM, and sequence-to-sequence LM).\nCourtesy of [49].\nWebGPT is trained in three steps. The first is for WebGPT\nto learn to mimic human browsing behaviors using human\ndemonstration data. Then, a reward function is learned to\npredict human preferences. Finally, WebGPT is refined to\noptimize the reward function via reinforcement learning and\nrejection sampling.\nTo enable LLMs to follow expected human instructions,\nInstructGPT [59] is proposed to align language models with\nuser intent on a wide range of tasks by fine-tuning with\nhuman feedback. Starting with a set of labeler-written prompts\nand prompts submitted through the OpenAI API, a dataset\nof labeler demonstrations of the desired model behavior is\ncollected. Then GPT-3 is fine-tuned on this dataset. Then, a\ndataset of human-ranked model outputs is collected to further\nfine-tune the model using reinforcement learning. The method\nis known Reinforcement Learning from Human Feedback\n(RLHF), as shown in 10. The resultant InstructGPT models\nhave shown improvements in truthfulness and reductions in\ntoxic output generation while having minimal performance\n",
        "word_count": 542,
        "char_count": 3544,
        "fonts": [
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 937,
            "height": 385,
            "ext": "png",
            "size_bytes": 61739
          },
          {
            "index": 1,
            "width": 1090,
            "height": 493,
            "ext": "png",
            "size_bytes": 45688
          },
          {
            "index": 2,
            "width": 965,
            "height": 533,
            "ext": "png",
            "size_bytes": 32350
          },
          {
            "index": 3,
            "width": 825,
            "height": 608,
            "ext": "png",
            "size_bytes": 51565
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 7,
        "text": "Fig. 7: High-level overview of GPT pretraining, and fine-tuning\nsteps. Courtesy of OpenAI.\nregressions on public NLP datasets.\nThe most important milestone of LLM development is the\nlaunch of ChatGPT (Chat Generative Pre-trained Transformer)\n[60] on November 30, 2022. ChatGPT is chatbot that enables\nusers to steer a conversation to complete a wide range of\ntasks such as question answering, information seeking, text\nsummarization, and more. ChatGPT is powered by GPT-3.5\n(and later by GPT-4), a sibling model to InstructGPT, which\nis trained to follow an instruction in a prompt and provide a\ndetailed response.\nGPT-4 [33] is the latest and most powerful LLM in the\nGPT family. Launched in March, 2023, GPT-4 is a multi-\nmodal LLM in that it can take image and text as inputs and\nproduce text outputs. While still less capable than humans\nin some of the most challenging real-world scenarios, GPT-4\nexhibits human-level performance on various professional and\nacademic benchmarks, including passing a simulated bar exam\nwith a score around the top 10% of test takers, as shown in\nFig 11. Like early GPT models, GPT-4 was first pre-trained to\npredict next tokens on large text corpora, and then fine-tuned\nwith RLHF to align model behaviors with human-desired ones.\n2) The LLaMA Family: LLaMA is a collection of founda-\ntion language models, released by Meta. Unlike GPT models,\nLLaMA models are open-source, i.e., model weights are\nreleased to the research community under a noncommercial\nlicense. Thus, the LLaMA family grows rapidly as these\nmodels are widely used by many research groups to develop\nbetter open-source LLMs to compete the closed-source ones or\nto develop task-specific LLMs for mission-critical applications.\nThe first set of LLaMA models [32] was released in Febru-\nary 2023, ranging from 7B to 65B parameters. These models\nare pre-trained on trillions of tokens, collected from publicly\navailable datasets. LLaMA uses the transformer architecture of\nGPT-3, with a few minor architectural modifications, including\n(1) using a SwiGLU activation function instead of ReLU,\n(2) using rotary positional embeddings instead of absolute\npositional embedding, and (3) using root-mean-squared layer-\nnormalization instead of standard layer-normalization. The\nopen-source LLaMA-13B model outperforms the proprietary\nGPT-3 (175B) model on most benchmarks, making it a good\nbaseline for LLM research.\nIn July 2023, Meta, in partnership with Microsoft, released\nthe LLaMA-2 collection [61], which include both foundation\nlanguage models and Chat models finetuned for dialog, known\nas LLaMA-2 Chat. The LLaMA-2 Chat models were reported\nto outperform other open-source models on many public\nbenchmarks. Fig 12 shows the training process of LLaMA-2\nChat. The process begins with pre-training LLaMA-2 using\npublicly available online data. Then, an initial version of\nLLaMA-2 Chat is built via supervised fine-tuning. Subse-\nquently, the model is iteratively refined using RLHF, rejection\nsampling and proximal policy optimization. In the RLHF stage,\nthe accumulation of human feedback for revising the reward\nmodel is crucial to prevent the reward model from being\nchanged too much, which could hurt the stability of LLaMA\nmodel training.\nAlpaca [62] is fine-tuned from the LLaMA-7B model using\n52K instruction-following demonstrations generated in the\nstyle of self-instruct using GPT-3.5 (text-davinci-003). Alpaca\nis very cost-effective for training, especially for academic\nresearch. On the self-instruct evaluation set, Alpaca performs\nsimilarly to GPT-3.5, despite that Alpaca is much smaller.\nThe Vicuna team has developed a 13B chat model, Vicuna-\n13B, by fine-tuning LLaMA on user-shared conversations\ncollected from ShareGPT. Preliminary evaluation using GPT-\n4 as a evaluator shows that Vicuna-13B achieves more than\n90% quality of OpenAI’s ChatGPT, and Google’s Bard while\noutperforming other models like LLaMA and Stanford Alpaca\nin more than 90% of cases. 13 shows the relative response\nquality of Vicuna and a few other well-known models by\nGPT-4. Another advantage of Vicuna-13B is its relative limited\ncomputational demand for model training. The training cost of\nVicuna-13B is merely $300.\nLike Alpaca and Vicuna, the Guanaco models [63] are also\nfinetuned LLaMA models using instruction-following data. But\nthe finetuning is done very efficiently using QLoRA such\nthat finetuning a 65B parameter model can be done on a\nsingle 48GB GPU. QLoRA back-propagates gradients through\na frozen, 4-bit quantized pre-trained language model into Low\nRank Adapters (LoRA). The best Guanaco model outperforms\nall previously released models on the Vicuna benchmark,\nreaching 99.3% of the performance level of ChatGPT while\nonly requiring 24 hours of fine-tuning on a single GPU.\nKoala [64] is yet another instruction-following language\nmodel built on LLaMA, but with a specific focus on interaction\ndata that include user inputs and responses generated by highly\ncapable closed-source chat models such as ChatGPT. The\nKoala-13B model performs competitively with state-of-the-art\nchat models according to human evaluation based on real-\nworld user prompts.\nMistral-7B [65] is a 7B-parameter language model engi-\nneered for superior performance and efficiency. Mistral-7B\noutperforms the best open-source 13B model (LLaMA-2-13B)\nacross all evaluated benchmarks, and the best open-source\n34B model (LLaMA-34B) in reasoning, mathematics, and code\ngeneration. This model leverages grouped-query attention for\nfaster inference, coupled with sliding window attention to\neffectively handle sequences of arbitrary length with a reduced\ninference cost.\nThe LLaMA family is growing rapidly, as more instruction-\nfollowing models have been built on LLaMA or LLaMA-\n2, including Code LLaMA [66], Gorilla [67], Giraffe [68],\n",
        "word_count": 871,
        "char_count": 5821,
        "fonts": [
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 976,
            "height": 419,
            "ext": "png",
            "size_bytes": 85448
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 8,
        "text": "GPT Family\nPaLM Family\n   LLaMA 1/2 Family\nGPT\nGPT1\nGPT2\nGPT3\nGPT4\nGPT3.5 Turbo\ntext-davinci\ncode-davinci\nCODEX\nInstructGPT\nWebGPT\nGPT4 Vision\nGPT4 Turbo\nGorilla\nMistral\nVigogne\nStable Beluga2\nKoala\nCode LLaMA\nVicuna\nAlpaca\nBaize\nLong LLaMA\nGiraffe\nGuanaco\nTulu\nWizardLM\nMed-PaLM\nPaLM-E\nMed-PaLM2\nFLAN-PaLM\nU-PaLM\nPaLM2\nPaLM\nFig. 8: Popular LLM Families.\nFig. 9: GPT-3 shows that larger models make increasingly\nefficient use of in-context information. It shows in-context\nlearning performance on a simple task requiring the model to\nremove random symbols from a word, both with and without\na natural language task description. Courtesy of [56].\nFig. 10: The high-level overview of RLHF. Courtesy of [59].\nVigogne [69], Tulu 65B [70], Long LLaMA [71], and Stable\nBeluga2 [72], just to name a few.\n3) The PaLM Family: The PaLM (Pathways Language\nModel) family are developed by Google. The first PaLM\nmodel [31] was announced in April 2022 and remained private\nFig. 11: GPT-4 performance on academic and professional\nexams, compared with GPT 3.5. Courtesy of [33].\nuntil March 2023. It is a 540B parameter transformer-based\nLLM. The model is pre-trained on a high-quality text corpus\nconsisting of 780 billion tokens that comprise a wide range\nof natural language tasks and use cases. PaLM is pre-trained\non 6144 TPU v4 chips using the Pathways system, which\nenables highly efficient training across multiple TPU Pods.\nPaLM demonstrates continued benefits of scaling by achiev-\ning state-of-the-art few-shot learning results on hundreds of\nlanguage understanding and generation benchmarks. PaLM-\n540B outperforms not only state-of-the-art fine-tuned models\non a suite of multi-step reasoning tasks, but also on par with\nhumans on the recently released BIG-bench benchmark.\nThe U-PaLM models of 8B, 62B, and 540B scales are\ncontinually trained on PaLM with UL2R, a method of continue\ntraining LLMs on a few steps with UL2’s mixture-of-denoiser\nobjective [73]. An approximately 2x computational savings\nrate is reported.\n",
        "word_count": 311,
        "char_count": 2017,
        "fonts": [
          "ComicSansMS (6.5pt)",
          "ArialMT (5.4pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "ArialMT (6.5pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 914,
            "height": 542,
            "ext": "png",
            "size_bytes": 62523
          },
          {
            "index": 1,
            "width": 879,
            "height": 786,
            "ext": "png",
            "size_bytes": 43734
          },
          {
            "index": 2,
            "width": 901,
            "height": 507,
            "ext": "png",
            "size_bytes": 62117
          },
          {
            "index": 3,
            "width": 258,
            "height": 295,
            "ext": "png",
            "size_bytes": 17546
          },
          {
            "index": 4,
            "width": 192,
            "height": 192,
            "ext": "png",
            "size_bytes": 4245
          },
          {
            "index": 5,
            "width": 432,
            "height": 117,
            "ext": "png",
            "size_bytes": 7074
          },
          {
            "index": 6,
            "width": 1200,
            "height": 675,
            "ext": "png",
            "size_bytes": 94213
          },
          {
            "index": 7,
            "width": 2000,
            "height": 676,
            "ext": "png",
            "size_bytes": 68065
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 9,
        "text": "Fig. 12: Training of LLaMA-2 Chat. Courtesy of [61].\nFig. 13: Relative Response Quality of Vicuna and a few other\nwell-known models by GPT-4. Courtesy of Vicuna Team.\nU-PaLM is later instruction-finetuned as Flan-PaLM [74].\nCompared to other instruction finetuning work mentioned\nabove, Flan-PaLM’s finetuning is performed using a much\nlarger number of tasks, larger model sizes, and chain-of-\nthought data. As a result, Flan-PaLM substantially outperforms\nprevious instruction-following models. For instance, Flan-\nPaLM-540B, which is instruction-finetuned on 1.8K tasks,\noutperforms PaLM-540B by a large margin (+9.4% on av-\nerage). The finetuning data comprises 473 datasets, 146 task\ncategories, and 1,836 total tasks, as illustrated in Fig 14.\nFig. 14: Flan-PaLM finetuning consist of 473 datasets in above\ntask categories. Courtesy of [74].\nPaLM-2 [75] is a more compute-efficient LLM with bet-\nter multilingual and reasoning capabilities, compared to its\npredecessor PaLM. PaLM-2 is trained using a mixture of\nobjectives. Through extensive evaluations on English, multi-\nlingual, and reasoning tasks, PaLM-2 significantly improves\nthe model performance on downstream tasks across different\nmodel sizes, while simultaneously exhibiting faster and more\nefficient inference than PaLM.\nMed-PaLM [76] is a domain-specific PaLM, and is de-\nsigned to provide high-quality answers to medical questions.\nMed-PaLM is finetuned on PaLM using instruction prompt\ntuning, a parameter-efficient method for aligning LLMs to\nnew domains using a few exemplars. Med-PaLM obtains very\nencouraging results on many healthcare tasks, although it is\nstill inferior to human clinicians. Med-PaLM 2 improves Med-\nPaLM via med-domain finetuning and ensemble prompting\n[77]. Med-PaLM 2 scored up to 86.5% on the MedQA\ndataset (i.e., a benchmark combining six existing open ques-\ntion answering datasets spanning professional medical exams,\nresearch, and consumer queries), improving upon Med-PaLM\nby over 19% and setting a new state-of-the-art.\nC. Other Representative LLMs\nIn addition to the models discussed in the previous sub-\nsections, there are other popular LLMs which do not belong\nto those three model families, yet they have achieved great\nperformance and have pushed the LLMs field forward. We\nbriefly describe these LLMs in this subsection.\nFLAN: In [78], Wei et al. explored a simple method for\nimproving the zero-shot learning abilities of language models.\nThey showed that instruction tuning language models on a\ncollection of datasets described via instructions substantially\nimproves zero-shot performance on unseen tasks. They take\na 137B parameter pretrained language model and instruction\ntune it on over 60 NLP datasets verbalized via natural language\ninstruction templates. They call this instruction-tuned model\nFLAN. Fig 15 provides a comparison of instruction tuning\nwith pretrain–finetune and prompting.\nFig.\n15:\ncomparison\nof\ninstruction\ntuning\nwith\npre-\ntrain–finetune and prompting. Courtesy of [78].\nGopher: In [79], Rae et al. presented an analysis of\nTransformer-based language model performance across a wide\nrange of model scales — from models with tens of millions of\nparameters up to a 280 billion parameter model called Gopher.\nThese models were evaluated on 152 diverse tasks, achieving\nstate-of-the-art performance across the majority. The number\nof layers, the key/value size, and other hyper-parameters of\ndifferent model sizes are shown in Fig 16.\nT0: In [80], Sanh et al. developed T0, a system for easily\nmapping any natural language tasks into a human-readable\nprompted form. They converted a large set of supervised\n",
        "word_count": 538,
        "char_count": 3641,
        "fonts": [
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1069,
            "height": 497,
            "ext": "png",
            "size_bytes": 65769
          },
          {
            "index": 1,
            "width": 708,
            "height": 309,
            "ext": "png",
            "size_bytes": 13659
          },
          {
            "index": 2,
            "width": 990,
            "height": 708,
            "ext": "png",
            "size_bytes": 86617
          },
          {
            "index": 3,
            "width": 886,
            "height": 296,
            "ext": "png",
            "size_bytes": 30249
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 10,
        "text": "Fig. 16: Model architecture details of Gopher with different\nnumber of parameters. Courtesy of [78].\ndatasets, each with multiple prompts with diverse wording.\nThese prompted datasets allow for benchmarking the ability\nof a model to perform completely held-out tasks. Then, a\nT0 encoder-decoder model is developed to consume textual\ninputs and produces target responses. The model is trained on\na multitask mixture of NLP datasets partitioned into different\ntasks.\nERNIE 3.0: In [81], Sun et al. proposed a unified frame-\nwork named ERNIE 3.0 for pre-training large-scale knowledge\nenhanced models. It fuses auto-regressive network and auto-\nencoding network, so that the trained model can be easily tai-\nlored for both natural language understanding and generation\ntasks using zero-shot learning, few-shot learning or fine-tuning.\nThey have trained ERNIE 3.0 with 10 billion parameters\non a 4TB corpus consisting of plain texts and a large-scale\nknowledge graph. Fig 17 illustrates the model architecture of\nErnie 3.0.\nFig. 17: High-level model architecture of ERNIE 3.0. Courtesy\nof [81].\nRETRO: In [82], Borgeaud et al. enhanced auto-regressive\nlanguage models by conditioning on document chunks re-\ntrieved from a large corpus, based on local similarity with pre-\nceding tokens. Using a 2-trillion-token database, the Retrieval-\nEnhanced Transformer (Retro) obtains comparable perfor-\nmance to GPT-3 and Jurassic-1 [83] on the Pile, despite using\n25% fewer parameters. As shown in Fig 18, Retro combines\na frozen Bert retriever, a differentiable encoder and a chunked\ncross-attention mechanism to predict tokens based on an order\nof magnitude more data than what is typically consumed\nduring training.\nGLaM: In [84], Du et al. proposed a family of LLMs\nnamed GLaM (Generalist Language Model), which use a\nsparsely activated mixture-of-experts architecture to scale the\nFig. 18: Retro architecture. Left: simplified version where a\nsequence of length n = 12 is split into l = 3 chunks of size\nm = 4. For each chunk, we retrieve k = 2 neighbours of r =\n5 tokens each. The retrieval pathway is shown on top. Right:\nDetails of the interactions in the CCA operator. Causality is\nmaintained as neighbours of the first chunk only affect the last\ntoken of the first chunk and tokens from the second chunk.\nCourtesy of [82].\nmodel capacity while also incurring substantially less training\ncost compared to dense variants. The largest GLaM has 1.2\ntrillion parameters, which is approximately 7x larger than GPT-\n3. It consumes only 1/3 of the energy used to train GPT-3 and\nrequires half of the computation flops for inference, while still\nachieving better overall zero, one and few-shot performance\nacross 29 NLP tasks. Fig 19 shows the high-level architecture\nof GLAM.\nFig. 19: GLaM model architecture. Each MoE layer (the\nbottom block) is interleaved with a Transformer layer (the\nupper block). Courtesy of [84].\nLaMDA: In [85], Thoppilan et al. presented LaMDA, a\nfamily of Transformer-based neural language models special-\nized for dialog, which have up to 137B parameters and are\npre-trained on 1.56T words of public dialog data and web text.\nThey showed that fine-tuning with annotated data and enabling\nthe model to consult external knowledge sources can lead to\nsignificant improvements towards the two key challenges of\nsafety and factual grounding.\nOPT: In [86], Zhang et al. presented Open Pre-trained\nTransformers (OPT), a suite of decoder-only pre-trained trans-\nformers ranging from 125M to 175B parameters, which they\n",
        "word_count": 556,
        "char_count": 3528,
        "fonts": [
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1009,
            "height": 277,
            "ext": "png",
            "size_bytes": 27451
          },
          {
            "index": 1,
            "width": 967,
            "height": 525,
            "ext": "png",
            "size_bytes": 130675
          },
          {
            "index": 2,
            "width": 1137,
            "height": 446,
            "ext": "png",
            "size_bytes": 58367
          },
          {
            "index": 3,
            "width": 451,
            "height": 553,
            "ext": "png",
            "size_bytes": 47529
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 11,
        "text": "share with researchers. The OPT models’ parameters are\nshown in 20\nFig. 20: Different OPT Models’ architecture details. Courtesy\nof [86].\nChinchilla: In [2], Hoffmann et al. investigated the optimal\nmodel size and number of tokens for training a transformer\nlanguage model under a given compute budget. By training\nover 400 language models ranging from 70 million to over\n16 billion parameters on 5 to 500 billion tokens, they found\nthat for compute-optimal training, the model size and the\nnumber of training tokens should be scaled equally: for every\ndoubling of model size the number of training tokens should\nalso be doubled. They tested this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the\nsame compute budget as Gopher but with 70B parameters and\n4% more more data.\nGalactica: In [87], Taylor et al. introduced Galactica, a\nlarge language model that can store, combine and reason about\nscientific knowledge. They trained on a large scientific corpus\nof papers, reference material, knowledge bases and many other\nsources. Galactica performed well on reasoning, outperforming\nChinchilla on mathematical MMLU by 41.3% to 35.7%, and\nPaLM 540B on MATH with a score of 20.4% versus 8.8%.\nCodeGen: In [88], Nijkamp et al. trained and released\na family of large language models up to 16.1B parameters,\ncalled CODEGEN, on natural language and programming\nlanguage data, and open sourced the training library JAX-\nFORMER. They showed the utility of the trained model by\ndemonstrating that it is competitive with the previous state-of-\nthe-art on zero-shot Python code generation on HumanEval.\nThey further investigated the multi-step paradigm for program\nsynthesis, where a single program is factorized into multi-\nple prompts specifying sub-problems. They also constructed\nan open benchmark, Multi-Turn Programming Benchmark\n(MTPB), consisting of 115 diverse problem sets that are\nfactorized into multi-turn prompts.\nAlexaTM: In [89], Soltan et al. demonstrated that mul-\ntilingual large-scale sequence-to-sequence (seq2seq) models,\npre-trained on a mixture of denoising and Causal Language\nModeling (CLM) tasks, are more efficient few-shot learners\nthan decoder-only models on various task. They trained a\n20 billion parameter multilingual seq2seq model called Alexa\nTeacher Model (AlexaTM 20B) and showed that it achieves\nstate-of-the-art (SOTA) performance on 1-shot summarization\ntasks, outperforming a much larger 540B PaLM decoder\nmodel. AlexaTM consist of 46 encoder layers, 32 decoder\nlayers, 32 attention heads, and dmodel = 4096.\nSparrow: In [90], Glaese et al. presented Sparrow, an\ninformation-seeking dialogue agent trained to be more helpful,\ncorrect, and harmless compared to prompted language model\nbaselines. They used reinforcement learning from human feed-\nback to train their models with two new additions to help\nhuman raters judge agent behaviour. The high-level pipeline\nof Sparrow model is shown in Fig 21.\nFig. 21: Sparrow pipeline relies on human participation to\ncontinually expand a training set. Courtesy of [90].\nMinerva: In [91], Lewkowycz et al. introduced Minerva,\na large language model pretrained on general natural language\ndata and further trained on technical content, to tackle previous\nLLM struggle with quantitative reasoning (such as solving\nmathematics, science, and engineering problems).\nMoD: In [92], Tay et al. presented a generalized and\nunified perspective for self-supervision in NLP and show how\ndifferent pre-training objectives can be cast as one another\nand how interpolating between different objectives can be\neffective. They proposed Mixture-of-Denoisers (MoD), a pre-\ntraining objective that combines diverse pre-training paradigms\ntogether. This framework is known as Unifying Language\nLearning (UL2). An overview of UL2 pretraining paradigm\nis shown in Fig 21.\nFig. 22: An overview of UL2 pretraining paradigm. Courtesy\nof [92].\nBLOOM: In [93], Scao et al. presented BLOOM, a 176B-\nparameter open-access language model designed and built\nthanks to a collaboration of hundreds of researchers. BLOOM\nis a decoder-only Transformer language model trained on the\nROOTS corpus, a dataset comprising hundreds of sources in\n46 natural and 13 programming languages (59 in total). An\noverview of BLOOM architecture is shown in Fig 23.\n",
        "word_count": 649,
        "char_count": 4318,
        "fonts": [
          "CMMI10 (10.0pt)",
          "CMR10 (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "CMMI7 (7.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 480,
            "height": 335,
            "ext": "png",
            "size_bytes": 28340
          },
          {
            "index": 1,
            "width": 821,
            "height": 429,
            "ext": "png",
            "size_bytes": 19577
          },
          {
            "index": 2,
            "width": 819,
            "height": 519,
            "ext": "png",
            "size_bytes": 44503
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 12,
        "text": "Fig. 23: An overview of BLOOM architecture. Courtesy of\n[93].\nGLM: In [94], Zeng et al. introduced GLM-130B, a\nbilingual (English and Chinese) pre-trained language model\nwith 130 billion parameters. It was an attempt to open-source\na 100B-scale model at least as good as GPT-3 (davinci) and\nunveil how models of such a scale can be successfully pre-\ntrained.\nPythia: In [95], Biderman et al. introduced Pythia, a suite\nof 16 LLMs all trained on public data seen in the exact same\norder and ranging in size from 70M to 12B parameters. We\nprovide public access to 154 checkpoints for each one of the\n16 models, alongside tools to download and reconstruct their\nexact training dataloaders for further study.\nOrca: In [96], Mukherjee et al. develop Orca, a 13-billion\nparameter model that learns to imitate the reasoning process\nof large foundation models. Orca learns from rich signals\nfrom GPT-4 including explanation traces; step-by-step thought\nprocesses; and other complex instructions, guided by teacher\nassistance from ChatGPT.\nStarCoder: In [97], Li et al. introduced StarCoder and\nStarCoderBase. They are 15.5B parameter models with 8K\ncontext length, infilling capabilities and fast large-batch in-\nference enabled by multi-query attention. StarCoderBase is\ntrained on one trillion tokens sourced from The Stack, a\nlarge collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. They fine-tuned\nStarCoderBase on 35B Python tokens, resulting in the creation\nof StarCoder. They performed the most comprehensive evalu-\nation of Code LLMs to date and showed that StarCoderBase\noutperforms every open Code LLM that supports multiple pro-\ngramming languages and matches or outperforms the OpenAI\ncode-cushman-001 model.\nKOSMOS: In [98], Huang et al. introduced KOSMOS-1,\na Multimodal Large Language Model (MLLM) that can per-\nceive general modalities, learn in context (i.e., few-shot), and\nfollow instructions (i.e. zero-shot). Specifically, they trained\nKOSMOS-1 from scratch on web-scale multi-modal corpora,\nincluding arbitrarily interleaved text and images, image-caption\npairs, and text data. Experimental results show that KOSMOS-\n1 achieves impressive performance on (i) language understand-\ning, generation, and even OCR-free NLP (directly fed with\ndocument images), (ii) perception-language tasks, including\nmultimodal dialogue, image captioning, visual question an-\nswering, and (iii) vision tasks, such as image recognition with\ndescriptions (specifying classification via text instructions).\nGemini: In [99], Gemini team introduced a new family of\nmultimodal models, that exhibit promising capabilities across\nimage, audio, video, and text understanding. Gemini family\nincludes three versions: Ultra for highly-complex tasks, Pro\nfor enhanced performance and deployability at scale, and Nano\nfor on-device applications. Gemini architecture is built on top\nof Transformer decoders, and is trained to support 32k context\nlength (via using efficient attention mechanisms).\nSome of the other popular LLM frameworks (or techniques\nused for efficient developments of LLMs) includes Inner-\nMonologue [100], Megatron-Turing NLG [101], LongFormer\n[102], OPT-IML [103], MeTaLM [104], Dromedary [105],\nPalmyra [106], Camel [107], Yalm [108], MPT [109], ORCA-\n2 [110], Gorilla [67], PAL [111], Claude [112], CodeGen 2\n[113], Zephyr [114], Grok [115], Qwen [116], Mamba [30],\nMixtral-8x7B [117], DocLLM [118], DeepSeek-Coder [119],\nFuseLLM-7B [120], TinyLlama-1.1B [121], LLaMA-Pro-8B\n[122].\nFig 24 provides an overview of some of the most repre-\nsentative LLM frameworks, and the relevant works that have\ncontributed to the success of LLMs and helped to push the\nlimits of LLMs.\nIII.\nHOW LLMS ARE BUILT\nIn this section, we first review the popular architectures\nused for LLMs, and then discuss data and modeling techniques\nranging from data preparation, tokenization, to pre-training,\ninstruction tuning, and alignment.\nOnce the model architecture is chosen, the major steps\ninvolved in training an LLM includes: data preparation (col-\nlection, cleaning, deduping, etc.), tokenization, model pre-\ntraining (in a self-supervised learning fashion), instruction\ntuning, and alignment. We will explain each of them in a\nseparate subsection below. These steps are also illustrated in\nFig 25.\nA. Dominant LLM Architectures\nThe most widely used LLM architectures are encoder-only,\ndecoder-only, and encoder-decoder. Most of them are based on\nTransformer (as the building block). Therefore we also review\nthe Transformer architecture here.\n1) Transformer: in a ground-breaking work [44], Vaswani\net al. proposed the Transformer framework, which was orig-\ninally designed for effective parallel computing using GPUs.\nThe heart of Transformer is the (self-)attention mechanism,\nwhich can capture long-term contextual information much\nmore effectively using GPUs than the recurrence and convo-\nlution mechanisms. Fig 26 provides a high-level overview of\ntransformer work. In this section we provide an overview of the\nmain elements and variants, see [44], [123] for more details.\nThe Transformer language model architecture, originally\nproposed for machine translation, consists of an encoder and\na decoder. The encoder is composed of a stack of N = 6\nidentical Transformer layers. Each layer has two sub-layers.\nThe first one is a multi-head self-attention layer, and the other\none is a simple position-wise fully connected feed-forward\nnetwork. The decoder is composed of a stack of 6 identical\nlayers. In addition to the two sub-layers in each encoder layer,\n",
        "word_count": 821,
        "char_count": 5596,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 947,
            "height": 459,
            "ext": "png",
            "size_bytes": 35506
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 13,
        "text": "Fig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our\n#parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way\nfor their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models. ♣ shows entities that serve\nnot only as models but also as approaches. ♦ shows only approaches.\nthe decoder has a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. The attention\nfunction can be described as mapping a query and a set of key-\nvalue pairs to an output, where the query, keys, values, and\noutput are all vectors. The output is computed as a weighted\nsum of the values, where the weight assigned to each value\nis computed by a compatibility function of the query with the\ncorresponding key. Instead of performing a single attention\nfunction with dmodel dimensional keys, values and queries,\nit is found to be beneficial to linearly project the queries,\nkeys and values h with different, learned linear projections to\ndk, dk and dv dimensions, respectively. Positional encoding is\nincorporated to fuse information about the relative or absolute\nposition of the tokens in the sequence.\n2) Encoder-Only: For this family, at each stage, the atten-\ntion layers can access all the words in the initial sentence.\nThe pre-training of these models usually consist of some-\nhow corrupting a given sentence (for instance, by masking\nrandom words in it) and tasking the model with finding or\nreconstructing the initial sentence. Encoder models are great\nfor tasks requiring an understanding of the full sequence,\nsuch as sentence classification, named entity recognition, and\nextractive question answering. One prominent encoder only\nmodel is BERT (Bidirectional Encoder Representations from\nTransformers), proposed in [24].\n3) Decoder-Only: For these models, at each stage, for any\nword, the attention layers can only access the words positioned\nbefore that in the sentence. These models are also sometimes\ncalled auto-regressive models. The pretraining of these models\nis usually formulated as predicting the next word (or token)\nin the sequence. The decoder-only models are best suited for\ntasks involving text generation. GPT models are prominent\nexample of this model category.\n4) Encoder-Decoder: These models use both encoder and\ndecoder, and are sometimes called sequence-to-sequence mod-\nels. At each stage, the attention layers of the encoder can access\nall the words in the initial sentence, whereas the attention\nlayers of the decoder only accesses the words positioned before\na given word in the input. These models are usually pre-\ntrained using the objectives of encoder or decoder models, but\nusually involve something a bit more complex. For instance,\nsome models are pretrained by replacing random spans of text\n(that can contain several words) with a single mask special\nword, and the objective is then to predict the text that this\nmask word replaces. Encoder-decoder models are best suited\nfor tasks about generating new sentences conditioned on a\ngiven input, such as summarization, translation, or generative\nquestion answering.\nB. Data Cleaning\nData quality is crucial to the performance of language\nmodels trained on them. Data cleaning techniques such as\nfiltering, deduplication, are shown to have a big impact on\nthe model performance.\nAs an example, in Falcon40B [124], Penedo et al. showed\nthat properly filtered and deduplicated web data alone can lead\nto powerful models; even significantly outperforming models\nfrom the state-of-the-art trained on The Pile. Despite extensive\nfiltering, they were able to obtain five trillion tokens from\n",
        "word_count": 590,
        "char_count": 3759,
        "fonts": [
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMMI10 (10.0pt)",
          "CMSY10 (10.0pt)",
          "MSAM10 (10.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "CMMI7 (7.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1930,
            "height": 918,
            "ext": "png",
            "size_bytes": 132465
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 14,
        "text": "How LLMs Are Built?\nData Cleaning\nTokenizations\nBytePairEncoding\nWordPieceEncoding\nSentencePieceEncoding\nPositional Encoding\nAbsolute Positional Embeddings\nRelative Positional Embeddings\nRotary Position Embeddings\nRelative Positional Bias\nModel Pre-training\nMasked Language Modeling\nCausal Language Modeling\nNext Sentence Prediction\nMixture of Experts\nFine-tuning and Instruction Tuning\nAlignment\nSupervised learning\nReinforcement Learning from Human Feedback\nDirect Preference Optimization\nKahneman-Tversky Optimization\nDecoding Strategies\nGreedy Search\nBeam Search\nTop-k Sampling\nTop-p Sampling\nCost-Effective Training/Inference,\nAdaptation & Compression\nOptimized Training\nZero Redundancy Optimizer\nReceptance Weighted Key Value\nLow-Rank Adaption\nKnowledge Distillation\nQuantization\nData Filtering\nRemoving Noise\nHandling Outliers\nAddressing Imbalances\nText Preprocessing\nDeduplication\nLLM Architectures\nEncoder-Only\nDecoder-Only\nEncoder-Decoder\n...\nSupervised Fine-tuning\nGeneral Fine-tuning\nMulti-turn Instructions\nInstruction Following\nFig. 25: This figure shows different components of LLMs.\n",
        "word_count": 118,
        "char_count": 1099,
        "fonts": [
          "Arial-BoldMT (6.9pt)",
          "Arial-BoldItalicMT (6.9pt)",
          "ArialMT (6.9pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 15,
        "text": "Fig. 26: High-level overview of transformer work. Courtesy of\n[44].\nCommonCrawl. They also released an extract of 600 billion\ntokens from our REFINEDWEB dataset, and 1.3/7.5B param-\neters language models trained on it. 27 shows the Refinement\nprocess of CommonCrawl data by this work.\nFig. 27: Subsequent stages of Macrodata Refinement remove\nnearly 90% of the documents originally in CommonCrawl.\nCourtesy of [124].\n1) Data Filtering: Data filtering aims to enhance the qual-\nity of training data and the effectiveness of the trained LLMs.\nCommon data filtering techniques include:\nRemoving Noise: refers to eliminating irrelevant or noisy\ndata that might impact the model’s ability to generalize well.\nAs an example, one can think of removing false information\nfrom the training data, to lower the chance of model generating\nfalse responses. Two mainstream approaches for quality filter-\ning includes: classifier-based, and heuristic-based frameworks.\nHandling Outliers: Identifying and handling outliers or\nanomalies in the data to prevent them from disproportionately\ninfluencing the model.\nAddressing Imbalances: Balancing the distribution of\nclasses or categories in the dataset to avoid biases and ensure\nfair representation. This is specially useful for responsible\nmodel training and evaluation.\nText Preprocessing: Cleaning and standardizing text data\nby removing stop words, punctuation, or other elements that\nmay not contribute significantly to the model’s learning.\nDealing with Ambiguities: Resolving or excluding am-\nbiguous or contradictory data that might confuse the model\nduring training. This can help the model to provide more\ndefinite and reliable answers.\n2) Deduplication: De-duplication refers to the process of\nremoving duplicate instances or repeated occurrences of the\nsame data in a dataset. Duplicate data points can introduce\nbiases in the model training process and reduce the diversity, as\nthe model may learn from the same examples multiple times,\npotentially leading to overfitting on those particular instances.\nSome works [125] have shown that de-duplication improves\nmodels’ ability to generalize to new, unseen data.\nThe de-duplication process is particularly important when\ndealing with large datasets, as duplicates can unintentionally\ninflate the importance of certain patterns or characteristics.\nThis is especially relevant in NLP tasks, where diverse and\nrepresentative training data is crucial for building robust lan-\nguage models.\nThe specific de-duplication method can vary based on\nthe nature of the data and the requirements of the particular\nlanguage model being trained. It may involve comparing entire\ndata points or specific features to identify and eliminate du-\nplicates. At the document level, existing works mainly rely on\nthe overlap ratio of high-level features (e.g. n-grams overlap)\nbetween documents to detect duplicate samples.\nC. Tokenizations\nTokenization referes to the process of converting a se-\nquence of text into smaller parts, known as tokens. While\nthe simplest tokenization tool simply chops text into tokens\nbased on white space, most tokenization tools rely on a word\ndictionary. However, out-of-vocabulary (OOV) is a problem\nin this case because the tokenizer only knows words in its\ndictionary. To increase the coverage of dictionaries, popular\ntokenizers used for LLMs are based on sub-words, which can\nbe combined to form a large number of words, including the\nwords unseen in training data or words in different languages.\nIn what follows, we describe three popular tokenizers.\n1) BytePairEncoding: BytePairEncoding is originally a\ntype of data compression algorithm that uses frequent patterns\nat byte level to compress the data. By definition, this algorithm\nmainly tries to keep the frequent words in their original form\nand break down ones that are not common. This simple\nparadigm keeps the vocabulary not very large, but also good\nenough to represent common words at the same time. Also\nmorphological forms of the frequent words can be represented\nvery well if suffix or prefix is also commonly presented in the\ntraining data of the algorithm.\n",
        "word_count": 621,
        "char_count": 4134,
        "fonts": [
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 575,
            "height": 771,
            "ext": "png",
            "size_bytes": 109107
          },
          {
            "index": 1,
            "width": 898,
            "height": 497,
            "ext": "png",
            "size_bytes": 22215
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 16,
        "text": "2) WordPieceEncoding: This algorithm is mainly used for\nvery well-known models such as BERT and Electra. At the\nbeginning of training, the algorithm takes all the alphabet from\nthe training data to make sure that nothing will be left as UNK\nor unknown from the training dataset. This case happens when\nthe model is given an input that can not be tokenized by the\ntokenizer. It mostly happens in cases where some characters are\nnot tokenizable by it. Similar to BytePairEncoding, it tries to\nmaximize the likelihood of putting all the tokens in vocabulary\nbased on their frequency.\n3) SentencePieceEncoding: Although both tokenizers de-\nscribed before are strong and have many advantages compared\nto white-space tokenization, they still take assumption of\nwords being always separated by white-space as granted. This\nassumption is not always true, in fact in some languages, words\ncan be corrupted by many noisy elements such as unwanted\nspaces or even invented words. SentencePieceEncoding tries\nto address this issue.\nD. Positional Encoding\n1) Absolute Positional Embeddings: (APE) [44] has been\nused in the original Transformer model to preserve the infor-\nmation of sequence order. Therefore, the positional information\nof words is added to the input embeddings at the bottom of\nboth the encoder and decoder stacks. There are various options\nfor positional encodings, either learned or fixed. In the vanilla\nTransformer, sine and cosine functions are employed for this\npurpose. The main drawback of using APE in Transformers\nis the restriction to a certain number of tokens. Additionally,\nAPE fails to account for the relative distances between tokens.\n2) Relative Positional Embeddings: (RPE) [126] involves\nextending self-attention to take into account the pairwise links\nbetween input elements. RPE is added to the model at two\nlevels: first as an additional component to the keys, and\nsubsequently as a sub-component of the values matrix. This\napproach looks at the input as a fully-connected graph with\nlabels and directed edges. In the case of linear sequences, edges\ncan capture information about the relative position differences\nbetween input elements. A clipping distance, represented as k\n2 ≤ k ≤ n − 4, specifies the maximum limit on relative lo-\ncations. This allows the model to make reasonable predictions\nfor sequence lengths that are not part of the training data.\n3) Rotary Position Embeddings: Rotary Positional Em-\nbedding (RoPE) [127] tackles problems with existing ap-\nproaches. Learned absolute positional encodings can lack gen-\neralizability and meaningfulness, particularly when sentences\nare short. Moreover, current methods like T5’s positional\nembedding face challenges with constructing a full attention\nmatrix between positions. RoPE uses a rotation matrix to\nencode the absolute position of words and simultaneously in-\ncludes explicit relative position details in self-attention. RoPE\nbrings useful features like flexibility with sentence lengths, a\ndecrease in word dependency as relative distances increase,\nand the ability to improve linear self-attention with relative\nposition encoding. GPT-NeoX-20B, PaLM, CODEGEN, and\nLLaMA are among models that take advantage of RoPE in\ntheir architectures.\n4) Relative Positional Bias: The concept behind this type\nof positional embedding is to facilitate extrapolation during\ninference for sequences longer than those encountered in train-\ning. In [128] Press et al. proposed Attention with Linear Biases\n(ALiBi). Instead of simply adding positional embeddings to\nword embeddings, they introduced a bias to the attention scores\nof query-key pairs, imposing a penalty proportional to their\ndistance. In the BLOOM model, ALiBi is leveraged.\nE. Model Pre-training\nPre-training is the very first step in large language model\ntraining pipeline, and it helps LLMs to acquire fundamental\nlanguage understanding capabilities, which can be useful in a\nwide range of language related tasks. During pre-training, the\nLLM is trained on a massive amount of (usually) unlabeled\ntexts, usually in a self-supervised manner. There are different\napproaches used for pre-training like next sentence prediction\n[24], two most common ones include, next token prediction\n(autoregressive language modeling), and masked language\nmodeling.\nIn Autoregressive Language Modeling framework, given\na sequence of n tokens x1, ..., xn, the model tries to predict\nnext token xn+1 (and sometimes next sequence of tokens) in\nan auto-regressive fashion. One popular loss function in this\ncase is the log-likelihood of predicted tokens as shown in Eq\n2\nLALM(x) =\nN\nX\ni=1\np(xi+n|xi, ..., xi+n−1)\n(1)\nGiven the auto-regressive nature of this framework, the\ndecoder-only models are naturally better suited to learn how\nto accomplish these task.\nIn Masked Language Modeling, some words are masked\nin a sequence and the model is trained to predict the masked\nwords based on the surrounding context. Sometimes people\nrefer to this approach as denoising autoencoding, too. If we\ndenote the masked/corrupted samples in the sequence x, as ˜x,\nthen the training objective of this approach can be written as:\nLMLM(x) =\nN\nX\ni=1\np(˜x|x\\˜x)\n(2)\nAnd more recently, Mixture of Experts (MoE) [130],\n[131] have become very popular in LLM space too. MoEs\nenable models to be pre-trained with much less compute,\nwhich means one can dramatically scale up the model or\ndataset size with the same compute budget as a dense model.\nMoE consists of two main elements: Sparse MoE layers,\nwhich are used instead of dense feed-forward network (FFN)\nlayers, and have a certain number of “experts” (e.g. 8), in\nwhich each expert is a neural network. In practice, the experts\nare FFNs, but they can also be more complex networks. A gate\nnetwork or router, that determines which tokens are sent to\nwhich expert. It is worth noting that, one can send a token\nto more than one expert. How to route a token to an expert\nis one of the big decisions when working with MoEs - the\nrouter is composed of learned parameters and is pretrained at\nthe same time as the rest of the network. Fig 29 provides an\nillustration of a Switch Transformer encoder block, which are\nused in MoE.\n",
        "word_count": 977,
        "char_count": 6182,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "rsfs10 (10.0pt)",
          "CMMI10 (10.0pt)",
          "CMSY10 (10.0pt)",
          "CMEX10 (10.0pt)",
          "CMR10 (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "CMMI7 (7.0pt)",
          "CMR7 (7.0pt)",
          "CMSY7 (7.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 17,
        "text": "(a) Absolute Positional Embeddings [129]\n(b) Relative Positional Embeddings\n(c) Rotary Positional Embedding [127]\n(d) Relative Positional Bias [128]\nFig. 28: Various positional encodings are employed in LLMs.\nFig. 29: : Illustration of a Switch Transformer encoder block.\nThey replaced the dense feed forward network (FFN) layer\npresent in the Transformer with a sparse Switch FFN layer\n(light blue). . Courtesy of [131].\nF. Fine-tuning and Instruction Tuning\nEarly language models such as BERT trained using self-\nsupervision as explained in section III-E were not able to\nperform specific tasks. In order for the foundation model to be\nuseful it needed to be fine-tuned to a specific task with labeled\ndata (so-called supervised fine-tuning or SFT for short). For\nexample, in the original BERT paper [24], the model was fine-\ntuned to 11 different tasks. While more recent LLMs no longer\nrequire fine-tuning to be used, they can still benefit from task\nor data-specific fine-tuning. For example, OpenAI reports that\nthe much smaller GPT-3.5 Turbo model can outperform GPT-4\nwhen fine-tuned with task specific data 2.\nFine-tuning does not need to be performed to a single\ntask though, and there are different approaches to multi-task\nfine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one\nor more tasks is known to improve results and reduce the\ncomplexity of prompt engineering, and it can serve as an\nalternative to retrieval augmented generation. Furthermore,\nthere are other reasons why it might be advisable to fine-tune.\nFor example, one might want to fine-tune to expose the model\nto new or proprietary data that it has not been exposed to\nduring pre-training.\nAn important reason to fine-tune LLMs is to align the\nresponses to the expectations humans will have when providing\ninstructions through prompts. This is the so-called instruction\ntuning [133]. We dive into the details of how to design\nand engineer prompts in section IV-B, but in the context\nof instruction tuning, it is important to understand that the\ninstruction is a prompt that specifies the task that the LLM\nshould accomplish. Instruction tuning datasets such as Natural\nInstructions [134] include not only the task definition but other\ncomponents such as positive/negative examples or things to\navoid.\nThe specific approach and instruction datasets used to\ninstruction-tune an LLM varies, but, generally speaking, in-\nstruction tuned models outperform their original foundation\nmodels they are based on. For example, InstructGPT [59]\noutperforms GPT-3 on most benchmarks. The same is true\nfor Alpaca [62] when compared to LLaMA.\nSelf-Instruct [135], proposed by Wang et al. is also a\n2https://platform.openai.com/docs/guides/fine-tuning\n",
        "word_count": 423,
        "char_count": 2723,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "NimbusRomNo9L-Regu (7.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Regu (6.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 283,
            "height": 412,
            "ext": "png",
            "size_bytes": 46990
          },
          {
            "index": 1,
            "width": 593,
            "height": 421,
            "ext": "png",
            "size_bytes": 120650
          },
          {
            "index": 2,
            "width": 1204,
            "height": 696,
            "ext": "png",
            "size_bytes": 90278
          },
          {
            "index": 3,
            "width": 1084,
            "height": 476,
            "ext": "png",
            "size_bytes": 38229
          },
          {
            "index": 4,
            "width": 943,
            "height": 481,
            "ext": "png",
            "size_bytes": 65982
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 18,
        "text": "popular approach along this line, in which they introduced a\nframework for improving the instruction-following capabilities\nof pre-trained language models by bootstrapping their own\ngenerations. Their pipeline generates instructions, input, and\noutput samples from a language model, then filters invalid or\nsimilar ones before using them to fine tune the original model.\nG. Alignment\nAI Alignment is the process of steering AI systems towards\nhuman goals, preferences, and principles. LLMs, pre-trained\nfor word prediction, often exhibit unintended behaviors. For\nexample, they might generate contents that are toxic, harmful,\nmisleading and biased.\nInstruction tuning, discussed above, gets LLMs a step\ncloser to being aligned. However, in many cases, it is important\nto include further steps to improve the alignment of the model\nand avoid unintended behaviors 3. We review the most popular\napproaches to alignment in this subsection.\nRLHF (reinforcement learning from human feedback) and\nRLAIF (reinforcement learning from AI feedback) are two\npopular approaches. RLHF uses a reward model to learn\nalignment from human feedback. This reward model, after\nbeing tuned, is able to rate different outputs and score them\naccording to their alignment preferences given by humans. The\nreward model gives feedback to the original LLM and this\nfeedback is used to tune the LLM further [137]. Reinforcement\nlearning from AI feedback on the other hand, directly connects\na pretrained and well-aligned model to the LLM and helps it\nto learn from larger and more aligned models [138].\nIn another recent work (known as DPO) [139], Rafailov\net al. discussed that RLHF is a complex and often unstable\nprocedure, and tried to address this with a new approach. They\nleveraged a mapping between reward functions and optimal\npolicies to show that this constrained reward maximization\nproblem can be optimized exactly with a single stage of policy\ntraining, essentially solving a classification problem on the\nhuman preference data. The resulting algorithm, which they\ncalled Direct Preference Optimization (DPO), is stable, per-\nformant, and computationally lightweight, eliminating the need\nfor fitting a reward model, sampling from the LM during fine-\ntuning, or performing significant hyperparameter tuning. They\nobserved that fine-tuning with DPO exceeds RLHF’s ability to\ncontrol sentiment of generations and improves response quality\nin summarization. Fig 30 shows the high-level comparison\nbetween DPO vs RLHF.\nEven more recently Ethayarajh et al. proposed a new align-\nment approach called the Kahneman-Tversky Optimization\n(KTO) [136]. Unlike existing state-of-the-art approaches, KTO\ndoes not require paired preference data (x, yw, yl), and it\nonly needs (x,y) and knowledge of whether y is desirable or\nundesirable. KTO-aligned models are shown to be good or\nbetter than DPO-aligned models at scales from 1B to 30B,\ndespite not using paired preferences. KTO is also far easier to\nuse in the real world than preference optimization methods, as\nthe kind of data it needs is far more abundant. As an example,\n3According to very recent research by Ethayarajh et al. [136], further\nalignment besides SFT mainly improves models of at least 7B parameters.\nFor smaller models, SFT is sufficient.\nFig. 30: DPO optimizes for human preferences while avoiding\nreinforcement learning. Existing methods for fine-tuning lan-\nguage models with human feedback first fit a reward model\nto a dataset of prompts and human preferences over pairs of\nresponses, and then use RL to find a policy that maximizes\nthe learned reward. In contrast, DPO directly optimizes for\nthe policy best satisfying the preferences with a simple classi-\nfication objective, without an explicit reward function or RL.\nCourtesy of [139].\nevery retail company has a lot of customer interaction data and\nwhether that interaction was successful (e.g., purchase made)\nor unsuccessful (e.g., no purchase made). However, they have\nlittle to no counterfactual data (i.e., what would have made\nan unsuccessful customer interaction yl into a successful one\nyw). Fig 31 shows a high-level comparison between KTO and\nother alignment approaches discussed above.\nFig. 31: LLM alignment involves supervised finetuning fol-\nlowed by optimizing a human-centered loss (HALO). How-\never, the paired preferences that existing approaches need are\nhard-to-obtain. In contrast, KTO uses a far more abundant\nkind of data, making it much easier to use in the real world.\nCourtesy of [136].\nH. Decoding Strategies\nDecoding refers to the process of text generation using pre-\ntrained LLMs. Given an input prompt, the tokenizer translates\neach token in the input text into a corresponding token ID.\nThen, the language model uses these token IDs as input and\npredicts the next most likely token (or a sequence of tokens).\nFinally, the model generates logits, which are converted to\nprobabilities using a softmax function. Different decoding\nstrategies have been proposed. Some of the most popular ones\nare greedy search, beam search, as well as different sample\ntechniques such as top-K, top-P (Nucleus sampling).\n1) Greedy Search: Greedy search takes the most probable\ntoken at each step as the next token in the sequence, discarding\nall other potential options. As you can imagine, this is a simple\napproach and can loose a lot of temporal consistency and\ncoherency. It only considers the most probable token at each\n",
        "word_count": 843,
        "char_count": 5441,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "NimbusRomNo9L-Regu (7.0pt)",
          "CMMI10 (10.0pt)",
          "NimbusRomNo9L-Regu (6.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "CMMI7 (7.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1073,
            "height": 226,
            "ext": "png",
            "size_bytes": 87592
          },
          {
            "index": 1,
            "width": 811,
            "height": 329,
            "ext": "png",
            "size_bytes": 115313
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 19,
        "text": "step, without considering the overall effect on the sequence.\nThis property makes it fast, but it also means that it can miss\nout on better sequences that might have appeared with slightly\nless probable next tokens.\n2) Beam Search: Unlike greedy search that only considers\nthe next most probable token, beam search takes into account\nthe N most likely tokens, where N denotes the number of\nbeams. This procedure is repeated until a predefined maxi-\nmum sequence length is reached or an end-of-sequence token\nappears. At this point, the sequence of tokens (AKA “beam”)\nwith the highest overall score is chosen as the output. For\nexample for beam size of 2 and maximum length of 5,\nthe beam search needs to keep track of 25 = 32 possible\nsequences. So it is more computationally intensive than greedy\nsearch.\n3) Top-k Sampling: Top-k sampling is a technique that\nuses the probability distribution generated by the language\nmodel to select a token randomly from the k most likely\noptions.\nSuppose we have 6 tokens (A, B, C, D, E, F) and k=2,\nand P(A)= 30%, and P(B)= 20%, P(C)= P(D)= P(E)= P(F)=\n12.5%. In top-k sampling, tokens C, D, E, F are disregarded,\nand the model outputs A 60% of the time, and B, 40% of\nthe time. This approach ensures that we prioritize the most\nprobable tokens while introducing an element of randomness\nin the selection process.\nThe randomness is usually introduced via the concept of\ntemperature. The temperature T is a parameter that ranges from\n0 to 1, which affects the probabilities generated by the softmax\nfunction, making the most likely tokens more influential. In\npractice, it simply consists of dividing the input logits by\ntemperature value:\nsoftmax(xi) =\nexi/T\nP\nj exj/T\n(3)\nA low temperature setting significantly alters the proba-\nbility distribution (and is commonly used in text generation\nto control the level of “creativity” in the generated output),\nwhile a large temperature prioritizes the tokens with higher\nprobabilities. Top-k is a creative way of sampling, and can be\nused along with beam search. The sequence chosen by top-\nk sampling may not be the sequence with highest probability\nin beam search. But it’s important to remember that highest\nscores do not always lead to more realistic or meaningful\nsequences.\n4) Top-p Sampling: Top-p sampling, also known as Nu-\ncleus sampling, takes a slightly different approach from top-k\nsampling. Instead of selecting the top k most probable tokens,\nnucleus sampling chooses a cutoff value p such that the sum of\nthe probabilities of the selected tokens exceeds p. This forms\na “nucleus” of tokens from which to randomly choose the next\ntoken. In other words, in top-p sampling the language model\nexamines the most probable tokens in descending order and\nkeeps adding them to the list until the sum of probabilities\nsurpasses the threshold p. As you can imagine, this could be\nbetter specially for scenarios in which top-k tokens do not have\na large probability mass. Unlike top-k sampling, the number\nof tokens included in the nucleus sampling is not fixed. This\nvariability often results in a more diverse and creative output,\nmaking nucleus sampling popular for text generation related\ntasks.\nI.\nCost-Effective Training/Inference/Adaptation/Compression\nIn this part, we review some of the popular approaches\nused for more cost-friendly (and compute-friendly) training\nand usage of LLMs.\n1) Optimized Training: There are many frameworks de-\nveloped for optimized training of LLMs, here we introduce\nsome of the prominent ones.\nZeRO:\nIn [140], Rajbhandari et al. developed a novel\nsolution, Zero Redundancy Optimizer (ZeRO), to optimize\nmemory, vastly improving training speed of LLMs while\nincreasing the model size that can be efficiently trained. ZeRO\neliminates memory redundancies in data- and model-parallel\ntraining while retaining low communication volume and high\ncomputational granularity, allowing one to scale the model\nsize proportional to the number of devices with sustained high\nefficiency.\nRWKV: In [141], Peng et al. proposed a novel model\narchitecture, Receptance Weighted Key Value (RWKV), that\ncombines the efficient parallelizable training of Transformers\nwith the efficient inference of RNNs. Their approach leverages\na linear attention mechanism and allows them to formulate the\nmodel as either a Transformer or an RNN, which parallelizes\ncomputations during training and maintains constant compu-\ntational and memory complexity during inference, leading to\nthe first non-transformer architecture to be scaled to tens of\nbillions of parameters. RWKV architecture is shown in Fig\n32. The Time Complexity comparison of RWKV with different\nFig. 32: RWKV architecture. Courtesy of [141].\nTransformers are provided in Fig 33.\n",
        "word_count": 757,
        "char_count": 4740,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMMI10 (10.0pt)",
          "CMMI5 (5.0pt)",
          "CMEX10 (10.0pt)",
          "CMR10 (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "CMMI7 (7.0pt)",
          "CMR7 (7.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 552,
            "height": 543,
            "ext": "png",
            "size_bytes": 35629
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 20,
        "text": "Fig. 33: Time Complexity comparison of RWKV with different\nTransformers. Here T denotes the sequence length, d the\nfeature dimension, and c is MEGA’s chunk size of quadratic\nattention. Courtesy of [141].\n2) Low-Rank Adaption (LoRA): Low-Rank Adaptation is\na popular and lightweight training technique that significantly\nreduces the number of trainable parameters, and is based\non a crucial insight that the difference between the fine-\ntuned weights for a specialized task and the initial pre-trained\nweights often exhibits “low intrinsic rank” - meaning that\nit can be approximated well by a low rank matrix [142].\nTraining with LoRA is much faster, memory-efficient, and\nproduces smaller model weights (a few hundred MBs), that are\neasier to store and share. One property of low-rank matrices\nis that they can be represented as the product of two smaller\nmatrices. This realization leads to the hypothesis that this delta\nbetween fine-tuned weights and initial pre-trained weights can\nbe represented as the matrix product of two much smaller\nmatrices. By focusing on updating these two smaller matrices\nrather than the entire original weight matrix, computational\nefficiency can be substantially improved.\nSpecifically, for a pre-trained weight matrix W0 ∈ Rd×k,\nLoRA constrains its update by representing the latter with\na low-rank decomposition W0 + ∆W = W0 + BA, where\nB ∈ Rd×r , A ∈ Rr×k, and the rank r ≪ min(d, k). During\ntraining, W0 is frozen and does not receive gradient updates,\nwhile A and B contain trainable parameters. It is worth\nmentioning that both W0 and ∆W = BA are multiplied with\nthe same input, and their respective output vectors are summed\ncoordinate-wise. For h = W0x, their modified forward pass\nyields: h = W0x + ∆Wx = W0x + BAx. Usually a random\nGaussian initialization is used for A, and zero initialization\nfor B, so ∆W = BA is zero at the beginning of training.\nThey then scale ∆Wx by αr, where α is a constant in r. This\nreparametrization is illustrated in Figure 34\nIt is worth mentioning that LoRA can be applied to a subset\nof weight matrices in a neural network to reduce the number\nof trainable parameters. In the Transformer architecture, there\nare four weight matrices in the self-attention module (Wq ,\nWk, Wv , Wo), and two in the MLP module. Most of the\ntime, LoRA is focused on adapting the attention weights only\nfor downstream tasks, and freezes the MLP modules, so they\nare not trained in downstream tasks both for simplicity and\nparameter-efficiency.\n3) Knowledge Distillation: Knowledge distillation is the\nprocess of learning from a larger model [143]. Earlier days of\nbest-performing models release have proven that this approach\nis very useful even if it is used in an API distillation approach.\nFig. 34: An illustration of LoRA reparametrizan. Only A and\nB trained during this process. Courtesy of [142].\nIt is also referred to as an approach to distill the knowledge of\nnot a single model but in fact multiple models into a smaller\none. Creating smaller models by this approach yields smaller\nmodel sizes that can be used even on edge devices. Knowledge\ndistillation as shown in Fig 35, illustrates a general setup of\nthis training scheme.\nFig. 35: A generic knowledge distillation framework with\nstudent and teacher (Courtesy of [144]).\nKnowledge can be transferred by different forms of learn-\ning: response distillation, feature distillation, and API distilla-\ntion. Response distillation is concerned only with the outputs\nof the teacher model and tries to teach the student model\nhow to exactly or at least similarly perform (in the sense of\nprediction) as the teacher. Feature distillation not only uses\nthe last layer but also intermediate layers as well to create a\nbetter inner representation for the student model. This helps the\nsmaller model to have a similar representation as the teacher\nmodel.\nAPI distillation is the process of using an API (typically\nfrom an LLM provider such as OpenAI) to train smaller\nmodels. In the case of LLMs, it is used to train the model\nfrom the direct output of the larger model which makes it very\nsimilar to response distillation. Many concerns are raised by\nthis type of distillation because in cases where the model itself\nis not openly available, a (usually) paid API is exposed for end\nusers. On the other hand, while users pay for each call, how to\nuse the predictions is limited, for example, OpenAI prohibits\nusage of its API to create LLMs that later will be used to\ncompete with it. The main value in such case is training data.\n4) Quantization: deep learning in its core, is a set of\nmathematical functions applied to matrices, with a specific\n",
        "word_count": 784,
        "char_count": 4657,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMMI10 (10.0pt)",
          "CMSY10 (10.0pt)",
          "CMR10 (10.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "CMMI7 (7.0pt)",
          "CMR7 (7.0pt)",
          "CMSY7 (7.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 544,
            "height": 259,
            "ext": "png",
            "size_bytes": 23200
          },
          {
            "index": 1,
            "width": 362,
            "height": 347,
            "ext": "png",
            "size_bytes": 10357
          },
          {
            "index": 2,
            "width": 1257,
            "height": 557,
            "ext": "jpeg",
            "size_bytes": 86678
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 21,
        "text": "precision for model weights. Reducing the precision of the\nweights can be used to reduce the size of the model and also\nmake it faster. As an example, Float-32 operations compared\nto Int-8 operations are slower. This process, which is called\nquantization, can be applied in different phases. Main ap-\nproaches for model quantization can be categorized as: post\ntraining quantization and quantization-aware training. Post-\ntraining quantization is concerned with quantized trained mod-\nels in two well-known methods: dynamic and static. Dynamic\npost-training quantization computes the range of quantization\non the runtime and is slower compared to static. Quantization-\naware training adds quantization criteria into training, and\na quantized model is trained and optimized during training\nprocess. This approach ensures that the end model will have\ngood performance and also does not need to be quantized after\ntraining.\nIV.\nHOW LLMS ARE USED AND AUGMENTED\nOnce the LLMs are trained, we can use them to generate\ndesired outputs for a variety of tasks. LLMs can be used\ndirectly through basic prompting. However, in order to exploit\ntheir full potential or to address some of the shortcomings,\nwe need to augment the models through some external means.\nIn this section we first provide a brief overview of the main\nshortcoming of LLMs, with a deeper look at the issue of\nhallucination. We then describe how prompting and some aug-\nmentation approaches can not only address those limitations\nbut also be used to augment the capabilities of LLMs going\nas far as turning an LLM into a full-blown AI agent with the\nability to interface with the external world.\nA. LLM limitations\nIt is important to remember that LLMs are trained to predict\na token. While fine-tuning and alignment improves their per-\nformance and adds different dimensions to their abilities, there\nare still some important limitations that come up, particularly\nif they are used naively. Some of them include the following:\n•\nThey don’t have state/memory. LLMs on their own\ncannot remember even what was sent to them in the\nprevious prompt. That is an important limitation for\nmany of the use cases that require some form of state.\n•\nThey are stochastic/probabilistic. If you send the same\nprompt to an LLM several times, you are likely to get\ndifferent responses. While there are parameters, and\nin particular the temperature, to limit the variability\nin the response, this is an inherent property of their\ntraining that can create issues.\n•\nThey have stale information and, on their own, don’t\nhave access to external data. An LLM on its own does\nnot even know about the current time or day and does\nnot have access to any information that was not present\nin its training set.\n•\nThey are generally very large. This means that many\ncostly GPU machines are needed for training and\nserving. In some cases, largest models have poor\nSLAs, particularly in terms of latency.\n•\nThey hallucinate. LLMs do not have a notion of\n”truth” and they have usually been trained on a mix\nof good and bad content. They can produce very\nplausible but untruthful answers.\nWhile the previous limitations can all become important\nfor some applications, it is worth for us to dive a bit into the\nlast one, hallucinations, since it has gathered a lot of interest\nover the past few months and it has also sparked many of the\nprompt approaches and LLM augmentation methods we will\nlater describe.\nHallucination: In the realm of Large Language Models\n(LLMs), the phenomenon of ”hallucinations” has garnered\nsignificant attention. Defined in the literature, notably in the\n”Survey of Hallucination in Natural Language Generation”\npaper [145], hallucination in an LLM is characterized as\n”the generation of content that is nonsensical or unfaithful\nto the provided source.” This terminology, although rooted in\npsychological parlance, has been appropriated within the field\nof artificial intelligence.\nHallucinations in LLMs can be broadly categorized into\ntwo types:\n1)\nIntrinsic Hallucinations: These directly conflict with\nthe source material, introducing factual inaccuracies\nor logical inconsistencies.\n2)\nExtrinsic Hallucinations: These, while not contra-\ndicting, are unverifiable against the source, encom-\npassing speculative or unconfirmable elements.\nThe definition of ’source’ in LLM contexts varies with the\ntask. In dialogue-based tasks, it refers to ’world knowledge’,\nwhereas in text summarization, it pertains to the input text\nitself. This distinction plays a crucial role in evaluating and\ninterpreting hallucinations. The impact of hallucinations is also\nhighly context-dependent. For instance, in creative endeavors\nlike poem writing, hallucinations might be deemed acceptable\nor even beneficial.\nLLMs, trained on diverse datasets including the internet,\nbooks, and Wikipedia, generate text based on probabilistic\nmodels without an inherent understanding of truth or falsity.\nRecent advancements like instruct tuning and Reinforcement\nLearning from Human Feedback (RLHF) have attempted to\nsteer LLMs towards more factual outputs, but the fundamental\nprobabilistic nature and its inherent limitations remain. A\nrecent study, “Sources of Hallucination by Large Language\nModels on Inference Tasks” [146], highlights two key aspects\ncontributing to hallucinations in LLMs: the veracity prior and\nthe relative frequency heuristic, underscoring the complexities\ninherent in LLM training and output generation.\nEffective automated measurement of hallucinations in\nLLMs requires a combination of statistical and model-based\nmetrics.\nStatistical Metrics:\n•\nMetrics like ROUGE [147] and BLEU [148] are com-\nmon for assessing text similarity, focusing on intrinsic\nhallucinations.\n•\nAdvanced metrics such as PARENT [149], PARENT-\nT [150], and Knowledge F1 [151] are utilized when\nstructured knowledge sources are available. These\n",
        "word_count": 909,
        "char_count": 5875,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMSY10 (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 22,
        "text": "B) Augmenting LLMs through\nexternal knowledge - RAG\nHow LLMs Are Used and Augmented\nC) Using External Tools\nD) LLM Agents\nFunctionality of an LLM-based agent\nTool Access and Utilization\nDecision Making\nPrompt engineering techniques for agents\nReasoning without Observation\nReason and Act\nDialog-Enabled Resolving Agents\na) RAG-aware prompting techniques\na) Tool-aware prompting techniques\nA) LLM limitations\nHallucination\nHallucination Quantification\nAutomated metrics\nHuman judgment\nStatistical Metrics\nModel-Based Metrics\nScoring\nComparative Analysis\nIE-Based Metrics\nQA-Based Metrics\nNLI-Based Metrics\nB) Using LLMs\n Prompt Design and Engineering\n1) Chain of Thought\nZero-Shot CoT\nManual CoT\n5) Expert Prompting\n6) Chains\n2) Tree of Thought\n7) Rails\nTopical Rails\nFact-Checking Rails\nJailbreaking Rails\n8) Automatic Prompt Engineering\nPrompt Generation\nPrompt Scoring\nRefinement and Iteration\n3) Self-Consistency\n4) Reflection\nComponents of a RAG\nRetrieval \nGeneration \nAugmentation\nRAG Tools\nLangChain \nLlamaIndex\nHayStack\nMeltano\nCohere Coral\nFlowise AI\nFig. 36: How LLMs Are Used and Augmented.\nmetrics, while effective, have limitations in capturing\nsyntactic and semantic nuances.\nModel-Based Metrics:\n•\nIE-Based Metrics: Utilize Information Extraction\nmodels to simplify knowledge into relational tuples,\nthen compare these with the source.\n•\nQA-Based Metrics: Assess the overlap between gen-\nerated content and the source through a question-\nanswering framework (see [152]).\n•\nNLI-Based Metrics: Use Natural Language Inference\ndatasets to evaluate the truthfulness of a generated\nhypothesis based on a given premise (see [153]).\n•\nFaithfulness Classification Metrics: Offer a refined\nassessment by creating task-specific datasets for a\nnuanced evaluation (see [154]).\nDespite advances in automated metrics, human judgment\nremains a vital piece. It typically involves two methodologies:\n1)\nScoring: Human evaluators rate the level of halluci-\nnation within a predefined scale.\n2)\nComparative Analysis: Evaluators compare gener-\nated content against baseline or ground-truth refer-\nences, adding an essential layer of subjective assess-\nment.\nFactScore [155] is a recent example of a metric that can be\nused both for human and model-based evaluation. The metric\nbreaks an LLM generation into “atomic facts”. The final score\nis computed as the sum of the accuracy of each atomic fact,\ngiving each of them equal weight. Accuracy is a binary number\nthat simply states whether the atomic fact is supported by the\nsource. The authors implement different automation strategies\nthat use LLMs to estimate this metric.\nFinally, mitigating hallucinations in LLMs is a multifaceted\nchallenge, requiring tailored strategies to suit various applica-\ntions. Those include:\n•\nProduct Design and User Interaction Strategies such\nas use case design, structuring the input/output, or\nproviding mechanisms for user feedback.\n•\nData Management and Continuous Improvement.\nMaintaining and analyzing a tracking set of hallucina-\ntions is essential for ongoing model improvement.\n",
        "word_count": 434,
        "char_count": 3065,
        "fonts": [
          "Arial-BoldMT (9.8pt)",
          "ArialMT (3.2pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "ArialMT (5.2pt)",
          "CMSY10 (10.0pt)",
          "Arial-BoldMT (5.7pt)",
          "Arial-BoldMT (6.3pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "ArialMT (3.8pt)",
          "Arial-BoldMT (5.2pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 512,
            "height": 512,
            "ext": "png",
            "size_bytes": 17348
          },
          {
            "index": 1,
            "width": 512,
            "height": 512,
            "ext": "png",
            "size_bytes": 863
          },
          {
            "index": 2,
            "width": 282,
            "height": 114,
            "ext": "png",
            "size_bytes": 13277
          },
          {
            "index": 3,
            "width": 2052,
            "height": 1155,
            "ext": "png",
            "size_bytes": 190497
          },
          {
            "index": 4,
            "width": 850,
            "height": 646,
            "ext": "png",
            "size_bytes": 20555
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 23,
        "text": "•\nPrompt Engineering and Metaprompt Design. Many\nof the advanced prompt techniques described in IV-B\nsuch as Retrieval Augmented Generation directly ad-\ndress hallucination risks.\n•\nModel Selection and Configuration for Hallucination\nMitigation. For exemple, larger models with lower\ntemperature settings usually perform better. Also,\ntechniques such as RLHF or domain-sepcific fine-\ntuning can mitigate hallucination risks.\nB. Using LLMs: Prompt Design and Engineering\nA prompt in generative AI models is the textual input\nprovided by users to guide the model’s output. This could\nrange from simple questions to detailed descriptions or specific\ntasks. Prompts generally consist of instructions, questions,\ninput data, and examples. In practice, to elicit a desired\nresponse from an AI model, a prompt must contain either\ninstructions or questions, with other elements being optional.\nAdvanced prompts involve more complex structures, such as\n”chain of thought” prompting, where the model is guided to\nfollow a logical reasoning process to arrive at an answer.\nPrompt engineering is a rapidly evolving discipline that\nshapes the interactions and outputs of LLMs and other gen-\nerative AI models. The essence of prompt engineering lies in\ncrafting the optimal prompt to achieve a specific goal with\na generative model. This process is not only about instructing\nthe model but also involves some understanding of the model’s\ncapabilities and limitations, and the context within which it\noperates.\nPrompt engineering transcends the mere construction of\nprompts; it requires a blend of domain knowledge, understand-\ning of the AI model, and a methodical approach to tailor\nprompts for different contexts. This might involve creating\ntemplates that can be programmatically modified based on a\ngiven dataset or context. For example, generating personalized\nresponses based on user data might use a template that is\ndynamically filled with relevant user information.\nFurthermore, prompt engineering is an iterative and ex-\nploratory process, akin to traditional machine learning prac-\ntices such as model evaluation or hyperparameter tuning. The\nrapid growth of this field suggests its potential to revolutionize\ncertain aspects of machine learning, moving beyond traditional\nmethods like feature or architecture engineering. On the other\nhand, traditional engineering practices such as version con-\ntrol and regression testing need to be adapted to this new\nparadigm just like they were adapted to other machine learning\napproaches [156].\nIn the following paragraphs we detail some of the most\ninteresting and popular prompt engineering approaches.\n1) Chain of Thought (CoT): The Chain of Thought (CoT)\ntechnique, initially described in the paper “Chain-of-Thought\nPrompting Elicits Reasoning in Large Language Models”[34]\nby Google researchers, represents a pivotal advancement in\nprompt engineering for Large Language Models (LLMs).\nThis approach hinges on the understanding that LLMs, while\nproficient in token prediction, are not inherently designed for\nexplicit reasoning. CoT addresses this by guiding the model\nthrough essential reasoning steps.\nCoT is based on making the implicit reasoning process of\nLLMs explicit. By outlining the steps required for reasoning,\nthe model is directed closer to a logical and reasoned output,\nespecially in scenarios demanding more than simple informa-\ntion retrieval or pattern recognition.\nCoT prompting manifests in two primary forms:\n1)\nZero-Shot CoT: This form involves instructing the\nLLM to “think step by step”, prompting it to de-\nconstruct the problem and articulate each stage of\nreasoning.\n2)\nManual CoT: A more complex variant, it requires\nproviding step-by-step reasoning examples as tem-\nplates for the model. While yielding more effective\nresults, it poses challenges in scalability and mainte-\nnance.\nManual CoT is more effective than zero-shot. However,\nthe effectiveness of this example-based CoT depends on the\nchoice of diverse examples, and constructing prompts with\nsuch examples of step by step reasoning by hand is hard and\nerror prone. That is where automatic CoT [157] comes into\nplay.\n2) Tree of Thought (ToT): The Tree of Thought (ToT)\n[158] prompting technique is inspired by the concept of\nconsidering various alternative solutions or thought processes\nbefore converging on the most plausible one. ToT is based\non the idea of branching out into multiple ”thought trees”\nwhere each branch represents a different line of reasoning.\nThis method allows the LLM to explore various possibilities\nand hypotheses, much like human cognitive processes where\nmultiple scenarios are considered before determining the most\nlikely one.\nA critical aspect of ToT is the evaluation of these reasoning\npaths. As the LLM generates different branches of thought,\neach is assessed for its validity and relevance to the query.\nThis process involves real-time analysis and comparison of\nthe branches, leading to a selection of the most coherent and\nlogical outcome.\nToT is particularly useful in complex problem-solving\nscenarios where a single line of reasoning might not suffice.\nIt allows LLMs to mimic a more human-like problem-solving\napproach, considering a range of possibilities before arriving\nat a conclusion. This technique enhances the model’s ability\nto handle ambiguity, complexity, and nuanced tasks, making it\na valuable tool in advanced AI applications.\n3) Self-Consistency:\nSelf-Consistency [159] utilizes an\nensemble-based method, where the LLM is prompted to gen-\nerate multiple responses to the same query. The consistency\namong these responses serves as an indicator of their accuracy\nand reliability.\nThe Self-Consistency approach is grounded in the principle\nthat if an LLM generates multiple, similar responses to the\nsame prompt, it is more likely that the response is accurate.\nThis method involves asking the LLM to tackle a query mul-\ntiple times, each time analyzing the response for consistency.\nThis technique is especially useful in scenarios where factual\naccuracy and precision are paramount.\n",
        "word_count": 911,
        "char_count": 6073,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMSY10 (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 24,
        "text": "The consistency of responses can be measured using vari-\nous methods. One common approach is to analyze the overlap\nin the content of the responses. Other methods may include\ncomparing the semantic similarity of responses or employing\nmore sophisticated techniques like BERT-scores or n-gram\noverlaps. These measures help in quantifying the level of\nagreement among the responses generated by the LLM.\nSelf-Consistency has significant applications in fields\nwhere the veracity of information is critical. It is particularly\nrelevant in scenarios like fact-checking, where ensuring the\naccuracy of information provided by AI models is essential.\nBy employing this technique, prompt engineers can enhance\nthe trustworthiness of LLMs, making them more reliable for\ntasks that require high levels of factual accuracy.\n4) Reflection: Reflection [160] involves prompting LLMs\nto assess and potentially revise their own outputs based on\nreasoning about the correctness and coherence of their re-\nsponses. The concept of Reflection centers on the ability of\nLLMs to engage in a form of self-evaluation. After generating\nan initial response, the model is prompted to reflect on its\nown output, considering factors like factual accuracy, logical\nconsistency, and relevance. This introspective process can lead\nto the generation of revised or improved responses.\nA key aspect of Reflection is the LLM’s capacity for\nself-editing. By evaluating its initial response, the model can\nidentify potential errors or areas of improvement. This iterative\nprocess of generation, reflection, and revision enables the LLM\nto refine its output, enhancing the overall quality and reliability\nof its responses.\n5) Expert Prompting: Expert Prompting [161] enhances the\ncapabilities of Large Language Models (LLMs) by simulating\nthe responses of experts in various fields. This method involves\nprompting the LLMs to assume the role of an expert and re-\nspond accordingly, providing high-quality, informed answers.\nA key strategy within Expert Prompting is the multi-expert\napproach. The LLM is prompted to consider responses from\nmultiple expert perspectives, which are then synthesized to\nform a comprehensive and well-rounded answer. This tech-\nnique not only enhances the depth of the response but also\nincorporates a range of viewpoints, reflecting a more holistic\nunderstanding of the subject matter.\n6) Chains: Chains refer to the method of linking multiple\ncomponents in a sequence to handle complex tasks with Large\nLanguage Models (LLMs). This approach involves creating a\nseries of interconnected steps or processes, each contributing\nto the final outcome. The concept of Chains is based on\nthe idea of constructing a workflow where different stages\nor components are sequentially arranged. Each component in\na Chain performs a specific function, and the output of one\nserves as the input for the next. This end-to-end arrangement\nallows for more complex and nuanced processing, as each\nstage can be tailored to handle a specific aspect of the task.\nChains can vary in complexity and structure, depending on\nthe requirements. In “PromptChainer: Chaining Large Lan-\nguage Model Prompts through Visual Programming” [162],\nthe authors not only describe the main challenges in designing\nchains, but also describe a visual tool to support those tasks.\n7) Rails: Rails in advanced prompt engineering refer to\na method of guiding and controlling the output of Large\nLanguage Models (LLMs) through predefined rules or tem-\nplates. This approach is designed to ensure that the model’s\nresponses adhere to certain standards or criteria, enhancing the\nrelevance, safety, and accuracy of the output. The concept of\nRails involves setting up a framework or a set of guidelines\nthat the LLM must follow while generating responses. These\nguidelines are typically defined using a modeling language or\ntemplates known as Canonical Forms, which standardize the\nway natural language sentences are structured and delivered.\nRails can be designed for various purposes, depending on\nthe specific needs of the application:\n•\nTopical Rails: Ensure that the LLM sticks to a\nparticular topic or domain.\n•\nFact-Checking Rails: Aimed at minimizing the gen-\neration of false or misleading information.\n•\nJailbreaking Rails: Prevent the LLM from generating\nresponses that attempt to bypass its own operational\nconstraints or guidelines.\n8) Automatic\nPrompt\nEngineering\n(APE):\nAutomatic\nPrompt Engineering (APE) [163] focuses on automating the\nprocess of prompt creation for Large Language Models\n(LLMs). APE seeks to streamline and optimize the prompt\ndesign process, leveraging the capabilities of LLMs themselves\nto generate and evaluate prompts. APE involves using LLMs\nin a self-referential manner where the model is employed\nto generate, score, and refine prompts. This recursive use of\nLLMs enables the creation of high-quality prompts that are\nmore likely to elicit the desired response or outcome.\nThe methodology of APE can be broken down into several\nkey steps:\n•\nPrompt Generation: The LLM generates a range of\npotential prompts based on a given task or objective.\n•\nPrompt Scoring: Each generated prompt is then\nevaluated for its effectiveness, often using criteria\nlike clarity, specificity, and likelihood of eliciting the\ndesired response.\n•\nRefinement and Iteration: Based on these evalua-\ntions, prompts can be refined and iterated upon, further\nenhancing their quality and effectiveness.\nC. Augmenting LLMs through external knowledge - RAG\nOne of the main limitations of pre-trained LLMs is their\nlack of up-to-date knowledge or access to private or use-\ncase-specific information. This is where retrieval augmented\ngeneration (RAG) comes into the picture [164]. RAG, illus-\ntrated in figure 37, involves extracting a query from the input\nprompt and using that query to retrieve relevant information\nfrom an external knowledge source (e.g. a search engine or a\nknowledge graph, see figure 38 ). The relevant information is\nthen added to the original prompt and fed to the LLM in order\nfor the model to generate the final response. A RAG system\nincludes three important components: Retrieval, Generation,\nAugmentation [165].\n",
        "word_count": 945,
        "char_count": 6194,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMSY10 (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 25,
        "text": "Fig. 37: An example of synthesizing RAG with LLMs for question answering application [166].\nFig. 38: This is one example of synthesizing the KG as a\nretriever with LLMs [167].\na) RAG-aware prompting techniques: Because of the\nimportance of RAG to build advanced LLM systems, several\nRAG-aware prompting techniques have been developed re-\ncently. One such technique is Forward-looking Active Retrieval\nAugmented Generation (FLARE)\nForward-looking Active Retrieval Augmented Generation\n(FLARE) [168] enhances the capabilities of Large Language\nModels (LLMs) by iteratively combining prediction and in-\nformation retrieval. FLARE represents an evolution in the\nuse of retrieval-augmented generation, aimed at improving the\naccuracy and relevance of LLM responses.\nFLARE involves an iterative process where the LLM\nactively predicts upcoming content and uses these predictions\nas queries to retrieve relevant information. This method con-\ntrasts with traditional retrieval-augmented models that typically\nretrieve information once and then proceed with generation. In\nFLARE, this process is dynamic and ongoing throughout the\ngeneration phase. In FLARE, each sentence or segment gener-\nated by the LLM is evaluated for confidence. If the confidence\nlevel is below a certain threshold, the model uses the generated\ncontent as a query to retrieve relevant information, which is\nthen used to regenerate or refine the sentence. This iterative\nprocess ensures that each part of the response is informed by\nthe most relevant and current information available.\nFor more details on RAG framework and its relevant works,\nwe refer the readers to this survey of retrieval augmented\ngenerations [165].\nD. Using External Tools\nRetrieving information from an external knowledge source\nas described above is only one of the potential ways to augment\nan LLM. More generally, an LLM can access any number\nof external tools (e.g. an API to a service) to augment its\nfunctionality. In that regards, RAG can be seen as a specific\ninstance of the broader category of the so called ”tools”.\nTools in this context are external functions or services that\nLLMs can utilize. These tools extend the range of tasks an\nLLM can perform, from basic information retrieval to complex\ninteractions with external databases or APIs.\nIn the paper ”Toolformer: Language Models Can Teach\nThemselves to Use Tools” [169], the authors go beyond simple\ntool usage by training an LLM to decide what tool to use\nwhen, and even what parameters the API needs. Tools include\ntwo different search engines, or a calculator. In the following\nexamples, the LLM decides to call an external Q&A tool,\na calculator, and a Wikipedia Search Engine More recently,\nresearchers at Berkeley have trained a new LLM called Gorilla\n[67] that beats GPT-4 at the use of APIs, a specific but quite\ngeneral tool.\na) Tool-aware prompting techniques: Similarly to what\nwas described with RAG, several tool-aware prompting ap-\nproaches have been developed to make usage of tools more\nscalable. A popular technique is the so called Automatic Multi-\nstep Reasoning and Tool-use (ART).\nAutomatic Multi-step Reasoning and Tool-use (ART) [170]\nis a prompt engineering technique that combines automated\nchain of thought prompting with the use of external tools.\nART represents a convergence of multiple prompt engineering\nstrategies, enhancing the ability of Large Language Models\n",
        "word_count": 525,
        "char_count": 3400,
        "fonts": [
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 898,
            "height": 532,
            "ext": "png",
            "size_bytes": 130613
          },
          {
            "index": 1,
            "width": 1000,
            "height": 460,
            "ext": "png",
            "size_bytes": 48400
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 26,
        "text": "(LLMs) to handle complex tasks that require both reasoning\nand interaction with external data sources or tools.\nART involves a systematic approach where, given a task\nand input, the system first identifies similar tasks from a task\nlibrary. These tasks are then used as examples in the prompt,\nguiding the LLM on how to approach and execute the current\ntask. This method is particularly effective when tasks require a\ncombination of internal reasoning and external data processing\nor retrieval.\nE. LLM Agents\nThe idea of AI agents has been well-explored in the history\nof AI. An agent is typically an autonomous entity that can\nperceive the environment using its sensors, make a judgment\nbased on the state it currently is, and accordingly act based on\nthe actions that are available to it.\nIn the context of LLMs, an agent refers to a system based\non a specialized instantiation of an (augmented) LLM that\nis capable of performing specific tasks autonomously. These\nagents are designed to interact with users and environment to\nmake decisions based on the input and the intended goal of\nthe interaction. Agents are based on LLMs equipped with the\nability to access and use tools, and to make decisions based on\nthe given input. They are designed to handle tasks that require\na degree of autonomy and decision-making, typically beyond\nsimple response generation.\nThe functionalities of a generic LLM-based agent include:\n•\nTool Access and Utilization: Agents have the capabil-\nity to access external tools and services, and to utilize\nthese resources effectively to accomplish tasks.\n•\nDecision Making: They can make decisions based on\nthe input, context, and the tools available to them,\noften employing complex reasoning processes.\nAs an example, an LLM that has access to a function (or\nan API) such as weather API, can answer any question related\nto the weather of the specific place. In other words, it can use\nAPIs to solve problems. Furthermore, if that LLM has access\nto an API that allows to make purchases, a purchasing agent\ncan be built to not only have capabilities to read information\nfrom the external world, but also act on it [171].\nFig. 40 shows another example of LLM-based agents for\nconversational information seeking [36], where an LLM is\naugmented with a set of plug-and-play modules, including\na working memory that tracks the dialog state, a policy that\nmakes an execution plan for the task and selects next system\naction, an action executor that performs an action selected by\nthe policy (consolidating evidence from external knowledge,\nor prompting the LLM to generate responses), and a utility\nthat accesses the alignment of the LLM’s responses with user\nexpectations or specific business requirements, and generate\nfeedback to improve agent performance.\nFor more details on LLM-based AI agents see recent survey\n[172], [173], [174].\na) Prompt engineering techniques for agents:\nLike\nRAG and Tools, prompt engineering techniques that specif-\nically address the needs of LLM-based agents have been\ndeveloped. Three such examples are Reasoning without Ob-\nservation (ReWOO), Reason and Act (ReAct), and Dialog-\nEnabled Resolving Agents (DERA).\nReasoning without Observation (ReWOO) [175] aims to\ndecouple reasoning from direct observations. ReWOO operates\nby enabling LLMs to formulate comprehensive reasoning plans\nor meta-plans without immediate reliance on external data\nor tools. This approach allows the agent to create a struc-\ntured framework for reasoning that can be executed once the\nnecessary data or observations are available. In ReWOO, the\nLLM initially develops a plan (a series of steps) that outlines\nhow to approach and solve a given problem. This meta-\nplanning phase is crucial as it sets the stage for the agent to\nprocess information once it becomes available. The execution\nphase then involves integrating actual data or observations into\nthe pre-specified plan, leading to coherent and contextually\nrelevant responses. ReWOO offers significant advantages in\nterms of token efficiency and robustness to tool failure. It\nenables LLMs to handle tasks where immediate access to\nexternal data is not available, relying instead on a well-\nstructured reasoning framework. This method is particularly\nadvantageous in scenarios where data retrieval is costly, slow,\nor uncertain, allowing the LLM-based agent to maintain a high\nlevel of performance and reliability.\nReason and Act (ReAct)[176] prompts LLMs to generate\nnot only verbal reasoning but also actionable steps, thus\nenhancing the model’s dynamic problem-solving capabilities.\nReAct is grounded in the principle of integrating reasoning\nwith action. In this approach, the LLM is prompted to alternate\nbetween generating reasoning traces (explanations) and taking\nactions (steps or commands) in an interleaved manner. This\napproach allows the model to dynamically reason about a prob-\nlem, and propose and take concrete actions simultaneously.\nDialog-Enabled Resolving Agents (DERA) [177] are spe-\ncialized AI agents that can engage in dialogue, resolve queries,\nand make decisions based on interactive exchanges. DERA\nis developed based on the idea of utilizing multiple agents\nwithin a dialog context, each with specific roles and functions.\nThese agents can include Researchers, who gather and analyze\ninformation, and Deciders, who make final judgments based\non the information provided. This division of roles allows for\na well-organized and efficient approach to problem-solving\nand decision-making. DERA is particularly advantageous in\nscenarios requiring complex decision-making and problem-\nsolving, such as those in medical diagnostics or customer ser-\nvice. The collaborative and interactive nature of DERA agents\nallows them to handle intricate queries with a level of depth\nand nuance that single-agent systems might struggle with.\nMoreover, this approach aligns well with human decision-\nmaking processes, making AI reasoning more relatable and\ntrustworthy.\nV.\nPOPULAR DATASETS FOR LLMS\nLarge language models exhibit promising accomplish-\nments, but the main question that arises is how effectively\nthey function and how their performance can be assessed in\nspecific tasks or applications.\n",
        "word_count": 958,
        "char_count": 6204,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMSY10 (10.0pt)",
          "NimbusRomNo9L-MediItal (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 27,
        "text": "Fig. 39: HuggingGPT: An agent-based approach to use tools and planning [image courtesy of [171]]\nFig. 40: A LLM-based agent for conversational information\nseeking. Courtesy of [36].\nThe evaluation of LLMs poses particular challenges due\nto the evolving landscape of their applications. The original\nintent behind developing LLMs was to boost the performance\nof NLP tasks such as translation, summarization, question-\nanswering, and so on [178]. However, it is evident today\nthat these models are finding utility across diverse domains\nincluding code generation and finance. Moreover, the eval-\nuation of LLMs encompasses several critical considerations\nsuch as fairness and bias, fact-checking, and reasoning. In\nthis section, we outline the commonly used benchmarks for\nassessing LLMs. These benchmarks are categorized based on\ntraining or evaluating the LLM Capabilities.\nA. Datasets\nfor\nBasic\nTasks:\nlanguage\nmodel-\ning/understanding/generation\nThis section provides an overview of the benchmarks and\ndatasets suited to evaluate the basic abilities of LLMs.\n•\nNatural Questions [179] is a QA dataset that consists\nof real anonymized, aggregated queries submitted to\nthe Google search engine as questions. An annotator\nis presented with a question along with a Wikipedia\npage from the top 5 search results, and annotates a\nlong answer (typically a paragraph) and a short answer\n(one or more entities) if present on the page, or marks\nnull if no long/short answer is present.\n•\nMMLU [180] is intended to evaluate the knowl-\nedge gained in zero-shot and few-shot scenarios. That\nmeans that MMLU assesses both the general knowl-\nedge and problem-solving ability of a model. It covers\n57 subjects in STEM, humanities, social sciences,\nand other areas. The benchmark varies in complexity,\nranging from elementary to advanced professional.\nIt is worth mentioning that the main contribution of\nthis dataset is for multi-task language understanding,\nquestion answering, and arithmetic reasoning.\n•\nMBPP [181] stands for “Mostly Basic Python Prob-\nlems” and provides a benchmark for evaluating the\nperformance of models designed for code generation.\nThe benchmark encompasses 974 short Python pro-\ngrams including a wide range of topics, including\nfundamental programming concepts and standard li-\nbrary usage, and more. Each challenge comprises a\ntask description, a code solution, and three automated\ntest cases.\n•\nHumanEval [182] is a dataset for code generation\ntask. This dataset consists of 164 hand-crafted pro-\ngramming challenges. Each challenge is accompanied\nby a function signature, docstring, code body, and mul-\ntiple unit tests. The main intuition behind developing\nthis dataset is to guarantee the exclusion of its contents\nfrom training datasets for code generation models.\n•\nAPPS [183] is designed for code generation task\nfocusing on the Python programming language. The\nAPPS dataset contains a collection of 232, 444 Python\nprograms. Each program in the dataset has an average\nof 18 lines of Python code. Additionally, APPS offers\naccess to a repository of 10, 000 unique programming\n",
        "word_count": 472,
        "char_count": 3097,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMMI10 (10.0pt)",
          "CMSY10 (10.0pt)",
          "CMR10 (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1589,
            "height": 701,
            "ext": "jpeg",
            "size_bytes": 606591
          },
          {
            "index": 1,
            "width": 594,
            "height": 422,
            "ext": "png",
            "size_bytes": 57825
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 28,
        "text": "Fig. 41: Dataset applications.\nexercises, each with text-based problem descriptions.\nThe final aspect to highlight is that the it includes test\ncases.\n•\nWikiSQL [184] is crafted for code generation task and\nit has 87,726 carefully labeled pairs of SQL queries\nand corresponding natural language questions from\nWikipedia tables. The SQL queries comprise three\nsubsets: test sets (17, 284 examples), development\n(9, 145 examples), and training (61, 297 examples).\n•\nTriviaQA [185] is designed for QA task. This\ndataset\ncomprises\nmore\nthan\n650, 000\nquestion-\nanswer-evidence triples. There are 95, 000 question-\nanswer pairs in this dataset, each authored by trivia en-\nthusiasts and supported by an average of six indepen-\ndently sourced evidence documents. These documents\nare automatically acquired from Wikipedia or broader\nweb search results. The dataset is categorized into\ntwo segments, including those with authentic answers\nfrom Wikipedia and web domains, and verified sets\nembody the accurately answered questions along with\ntheir associated documents from both Wikipedia and\nonline.\n•\nRACE [186] suits for reading comprehension task.\n",
        "word_count": 170,
        "char_count": 1142,
        "fonts": [
          "CMMI10 (10.0pt)",
          "CMSY10 (10.0pt)",
          "CMR10 (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1024,
            "height": 1024,
            "ext": "png",
            "size_bytes": 179532
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 29,
        "text": "Fig. 42: Datasets licensed under different licenses.\nThis dataset is based on English tests completed by\nChinese students from middle school and high school,\naged 12 to 18, and it contains roughly 28, 000 texts\nand 100, 000 questions rigorously prepared by human\nspecialists, primarily English instructors. This dataset\ncontains a wide range of subjects that were purpose-\nfully chosen to assess students’ comprehension and\nreasoning abilities. This dataset is available in three\nsubgroups: RACE-M, RACE-H, and RACE. RACE-\nM refers to the middle school examinations, whereas\nRACE-H denotes the high school tests. Finally, RACE\nis the synthesis of RACE-M and RACE-H.\n•\nSQuAD [187] stands for “Stanford Question Answer-\ning Dataset” and is a crowdsourced reading compre-\nhension dataset based on Wikipedia articles. It has\napproximately 100, 000 question-answer pairs con-\nnected to more than 500 articles. The answers to\nthese questions are typically text fragments or spans\ntaken from the corresponding reading passages. The\nquestions may be unanswerable in some cases. The\ndataset is divided into three sets: an 80% training set,\na 10% development set, and a 10% hidden test set.\n•\nBoolQ [188] is a yes/no question-answering dataset\nwhere the goal is reading comprehension task. BoolQ\nincludes 15, 942 examples. Each example is a triplet\nthat includes a question, a relevant paragraph, and\nthe solution. Although the main intuition behind\nthis dataset is for reading comprehension, it can be\nused for reasoning, natural language inference, and\nquestion-answering tasks.\n•\nMultiRC [189] is another dataset that fits reading\ncomprehension task. MultiRC contains brief para-\ngraphs as well as multi-sentence questions that can\nbe answered using the information in the paragraph.\nThe paragraphs in this dataset come from a variety\nof sources, including news, fiction, historical texts,\nWikipedia articles, discussions on society and law,\nelementary school science textbooks, and 9/11 re-\nports. Each question has many response choices, with\none or more of them being correct. Answering the\nquestions requires reasoning across several sentences.\nMultiRC dataset encompasses around 6, 000 multi-\nsentence questions gathered from over 800 paragraphs.\nOn average, each question offers about two valid\nanswer alternatives out of a total of five.\nB. Datasets for Emergent: ICL, reasoning (CoT), instruction\nfollowing\nThis section centers on the benchmarks and datasets em-\nployed to evaluate the emergent abilities of LLMs.\n•\nGSM8K [190] is designed to evaluate the model’s\nability for multi-step mathematical reasoning. GSM8K\nincludes 8.5K linguistically diverse grade school math\nword problems written by humans. The dataset is split\ninto two sets: a training set with 7.5K problems,\nand a test set with 1K problems. These problems\nneed 2 to 8 steps to be solved. Solutions mainly\n",
        "word_count": 441,
        "char_count": 2874,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMMI10 (10.0pt)",
          "CMSY10 (10.0pt)",
          "CMR10 (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 562,
            "height": 511,
            "ext": "png",
            "size_bytes": 24277
          }
        ],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 30,
        "text": "are a series of elementary calculations using basic\narithmetic operations.\n•\nMATH [191] enables to assess how well models can\nsolve math problems. MATH dataset hast 12, 500\nproblems from high school math competitions. Each\nproblem in the dataset has a step-by-step solution and\na final answer enclosed in a box. The problems cover\na wide range of topics and have different levels of\ncomplexity. There are seven subjects in total. Further-\nmore, the difficulty of each problem is rated based\non the AoPS standards on a scale from ′1′ to ′5′. A\n′1′ shows the easiest problems in a subject, while ′5′\nrepresents the most difficult. In terms of formatting,\nall problems and solutions are presented using LATEX\nand the Asymptote vector graphics language.\n•\nHellaSwag [192] is designed to assess commonsense\nreasoning in LLMs. This benchmark includes 70, 000\nmultiple-choice questions. Each question is derived\nfrom one of two domains: ActivityNet or WikiHow,\nand presents four answer choices regarding what\nmight happen in the following situation. The correct\nanswer provides an actual statement describing the\nupcoming event, but the three wrong answers are\ncreated to confuse machines.\n•\nAI2 Reasoning Challenge (ARC) [193] is used\nfor commonsense reasoning. This benchmark encom-\npasses 7, 787 science examination questions. These\nquestions are in English, and most of them are set\nup in a multiple-choice format. The questions have\nbeen divided into two groups: a Challenge Set with\n2, 590 difficult questions and an Easy Set with 5,197\nquestions. Each collection has also been pre-divided\ninto Train, Development, and Test subsets.\n•\nPIQA [194] is intended to evaluate the language\nrepresentations on their knowledge of physical com-\nmonsense. In this dataset, the focus is on everyday\nsituations with a preference for uncommon solutions.\nThe central task is a multiple-choice question answer-\ning, where a question (q) is provided along with two\npotential solutions (s1, s2). Then, the best solution is\nchosen by whether a model or a human. For each\nquestion, only one of the solutions is the correct\nanswer.\n•\nSIQA [195] provides a framework for evaluating mod-\nels’ ability for commonsense reasoning about social\nsituations. SIQA dataset has 38, 000 multiple-choice\nquestions designed to assess emotional and social\nintelligence in everyday circumstances. This dataset\ncovers a wide variety of social scenarios. In SIQA,\nthe potential answers is a mixture of human-selected\nresponses and machine-generated ones that have been\nfiltered through adversarial processes.\n•\nOpenBookQA (OBQA) [196] is a new kind of\nquestion-answering dataset where answering its ques-\ntions requires additional common and commonsense\nknowledge not contained in the book and rich text\ncomprehension. This dataset includes around 6,000\nmultiple-choice questions. Each question is linked to\none core fact, as well as an additional collection\nof over 6000 facts. The questions were developed\nusing a multi-stage crowdsourcing and expert filter-\ning procedure. OpenBookQA questions are difficult\nbecause they need multi-hop reasoning with limited\nbackground.\n•\nTruthfulQA [197] is designed specifically to eval-\nuate the truthfulness of language models in gen-\nerating answers to questions. This dataset includes\n817 questions, written by authors, from 38 different\ncategories, including health, law, finance, and politics.\nThese questions are purposefully designed to chal-\nlenge human responders, as they may contain common\nmisunderstandings that lead to incorrect answers.\n•\nOPT-IML Bench [103] is a comprehensive bench-\nmark for Instruction Meta-Learning. It covers 2000\nNLP tasks from 8 existing benchmarks. The OPT-IML\nBench consists of a training set with 17.9 M examples,\na dev set with 145K samples, and a test set with 321K\nsamples.\nC. Datasets for Augmented: using external knowledge/tools\nThis section focuses on datasets designed for the aug-\nmented abilities of LLMs.\n•\nHotpotQA [198] is designed to cover a diverse and\nexplainable question-answering dataset that necessi-\ntates multi-hop reasoning. This dataset is derived from\nthe English Wikipedia. It consists of roughly 113, 000\nquestions. Each question in the dataset comes with\ntwo paragraphs, called gold paragraphs, from two\nWikipedia articles. Also, there is a list of sentences\nin those paragraphs that crowdworkers have picked as\nimportant for answering the question.\n•\nToolQA [199] is a question answering benchmark\nto evaluate LLMs’ ability to use external tools for\nanswering questions.\n•\nGPT4Tools serves as an instructional dataset, gener-\nated by instructing advanced teachers (such as Chat-\nGPT), with instructions conditioned on visual content\nand tool descriptions. This process results in the\ngeneration of instructions related to the use of tools.\nThere are three versions of this dataset. The first\nversion comprises 71,000 instruction-following data\npoints utilized to fine-tune the GPT4Tools model. The\nnext version consists of manually cleaned instruction\ndata used for validation, covering instructions related\nto the tools from the first version. The last version is\ncleaned instruction data used for testing and includes\ninstructions related to some tools that are not present\nin the first version.\nVI.\nPROMINENT LLMS’ PERFORMANCE ON\nBENCHMARKS\nIn this section we first provide an overview of some of\npopular metrics used for evaluating the performance of LLMs\nunder different scenarios. We then look at the performance\nof prominent large language models on some of the popular\ndatasets and benchmarks.\n",
        "word_count": 853,
        "char_count": 5572,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "CMMI10 (10.0pt)",
          "CMSY10 (10.0pt)",
          "CMR10 (10.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)",
          "CMSY7 (7.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 31,
        "text": "TABLE II: LLM Datasets Overview.\nBenchmark Name\nEvaluation Metric\nLeaderboard\nSource\npaperswithcode\nHumanEval\nPASS@k\nLink\nLink\nLink\nMBPP\nPASS@k, Accuracy\n-\nLink\nLink\nAPPS\nPASS@k, Accuracy\n-\nLink\nLink\nWikiSQL\nAccuracy\n-\nLink\nLink\nCoNaLa\nBLEU\nLink\nLink\nCodeParrot\nPASS@k\n-\nLink\n-\nHellaSwag\nAccuracy\nLink\nLink\nLink\nAI2\nReasoning\nChallenge (ARC)\nAccuracy\nLink\nLink\nLink\nBoolQ\nAccuracy\n-\nLink\nLink\nMultiRC\nF1-score, Accuracy\n-\nLink\nLink\nCNN/Daily Mail [200]\nAccuracy\n-\nLink\n-\nSQuAD\nF1-score, EM\nLink\nLink\nLink\nRACE\nAccuracy\n-\nLink\nLink\nCNN/Daily Mail [201]\nROUGE\n-\nLink\nLink\nDrop\nF1-score, EM\nLink\nLink\nLink\nQuAC\nF1-score, HEQ-Q, HEQ-D\nLink\nLink\nLink\nTriviaQA\nEM, F1-score, Accuracy\nLink\nLink\nLink\nNatural Questions\nEM, F1-score, Accuracy\nLink\nLink\nLink\nStrategyQA\nAccuracy, Recall@10, SARI\nLink\nLink\nLink\nCoQA\nF1-score\nLink\nLink\nLink\nXSum\nROUGE\n-\nLink\nLink\nSAMSum\nROUGE\n-\n-\nLink\nWikiSum\nROUGE\n-\nLink\n-\nDialogSum\nROUGE\n-\nLink\nLink\nTruthfulQA\nMC1 , MC2, % true, % info, BLEURT\nLink\nLink\nLink\nMMLU\nAccuracy\nLink\nLink\nLink\nGSM8K\nAccuracy\nLink\nLink\nLink\nPIQA\nAccuracy\nLink\nLink\nLink\nSIQA\nAccuracy\nLink\nLink\nLink\nOpenBookQA (OBQA)\nAccuracy\nLink\nLink\nLink\nHotpotQA\nEM, F1-score, Joint EM, Joint F1-score,\nLink\nLink\nLink\nMATH\nAccuracy\n-\nLink\nLink\nCommonsenseQA\nAccuracy\nLink\nLink\nLink\nNatural Instructions\nROUGE-L, Human\nLink\nLink\nLink\nBIG-bench\nAccuracy, Average\n-\nLink\nLink\nToolTalk\nSuccess rate, Precision, Recall, Incorrect\naction rate, Percent of failing error types\n-\nLink\nLink\nMetaTool\nAccuracy, Precision, Recall, F1-score\n-\nLink\nLink\nGPT4Tools\nSuccessful Rate of Thought, Successful\nRate of Action, Successful Rate of Ar-\nguments, Success Rate\n-\nLink\nLink\nAPI-Bank\nCorrectness, ROUGE, Error(API Hallu-\ncination, Has Exception, Invalid Input\nParameters, False API Call Format, API\nCall, Miss Input Parameters)\n-\nLink\nLink\nAlpaca-CoT\n-\n-\nLink\nLink\nA. Popular Metrics for Evaluating LLMs\nEvaluating the performance of generative language models\ndepends on the underlying task they are going to be used for.\nTasks that are mostly about selecting a choice out of given\nones (such as sentiment analysis), can be seen as simple as\nclassification and their performance can be evaluated using\nclassification metrics. Metrics such as accuracy, precision,\nrecall, F1, etc are applicable in this case. It is also important to\nnote that the answers generated by the model for specific tasks\nsuch as multi-choice question answering are always either True\nor False. If the answer is not in a set of options, it can be seen\nas False as well.\nHowever, some tasks that are purely open-ended text gener-\nation cannot be evaluated in the same way as for categorization.\nDifferent metrics are required for the specific purpose of the\nevaluation. Code generation is a very different case in open-\nended generative evaluations. The generated code must pass\nthe test suite but on the other hand, it is also important\nto understand if a model is capable of generating different\nsolutions as a code, what is the probability of selecting the\ncorrect one among them. Pass@k is a very good metric in this\ncase. It works in this manner that given a problem, different\nsolutions as code are generated. They are tested for correctness\nusing different functionality tests. Afterward, from generated\nn solutions, and the respective c number of them being correct\nequation 4 provides the final value.\npass@k :=\nE\nProblems\n\"\n1 −\n\u0000n−c\nk\n\u0001\n\u0000n\nk\n\u0001\n#\n(4)\nExact match (EM) is another metric that is mostly con-\ncerned with exact matches from (pre-defined) answers. It\ncounts a prediction as correct if it exactly matches one of\nmore than one desired reference text token by token. In some\ncases, it can be the same as accuracy and the equation 5 shows\nthe mathematical definition. Here M is total number of correct\nanswers and N is the total number of questions [202].\nEM = M\nN\n(5)\n",
        "word_count": 640,
        "char_count": 3838,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "NimbusRomNo9L-Regu (7.0pt)",
          "CMMI10 (10.0pt)",
          "NimbusRomNo9L-Medi (7.0pt)",
          "CMEX10 (10.0pt)",
          "CMSY10 (10.0pt)",
          "MSBM10 (10.0pt)",
          "CMR10 (10.0pt)",
          "CMMI7 (7.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "CMSY7 (7.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 32,
        "text": "Human equivalence score (HEQ) on the other hand, is an\nalternative to F1 score [203]. HEQ-Q represents the precision\nof individual questions, wherein an answer is deemed correct\nif the model’s F1 score surpasses the average human F1 score.\nLikewise, HEQ-D denotes the precision of each dialogue; it is\ndeemed accurate when all questions within the dialogue meet\nthe criteria of HEQ [182].\nEvaluation of other generative tasks such as machine trans-\nlation are based on metrics such as Rouge and BLEU. These\nscores work well when there is a reference text as ground\ntruth (such as translation) and a hypothesis that is generated\nby the generative model, in our case the LLM. These scores\nare mostly used for cases where the goal is to detect the\nsimilarity of the answer and ground truth in a computation\nmanner. In a computation manner, it meant that nothing more\nthan N-Grams would be used. However, metrics such as BERT-\nScore are also good for these cases but they are also heavily\nerroneous because another model is used to judge. Still, even\ntoday, evaluating purely generated content is very hard and\nno completely fitting metric is not found, metrics are either\nlooking for simplistic features such as N-Gram, SkipGram,\netc, or they are models with unknown accuracy and preciseness\n[204].\nGenerative evaluation metrics are also another type of eval-\nuation metric for LLMs that use another LLM for evaluating\nthe answer. However, depending on the task itself, evaluation\ncan be possible in this way or not. Another dependency\nthat makes generative evaluation error-prone is reliance on\nthe prompt itself. RAGAS is one of the good examples that\nincorporate the usage of generative evaluation.\nVarious benchmarks and leaderboards have been proposed\nto address the most challenging question in the world of\nlarge language models: Which one is better? However not\na simple answer can address this question. The answer de-\npends on various aspects of large language models. Section V\nshows the categorical presentation of different tasks and the\nmost important datasets in each category. We will follow the\nsame categorization and provide a comparison based on each\ncategory. After providing comparison for each category, we\nwill provide a broad overview of aggregated performance by\naveraging the reported performance metric on different tasks.\nEvaluating different LLMs can be seen also from different\nperspectives. For example, a LLM with a drastically fewer\nnumber of parameters is not completely comparable to one\nwith a larger number of parameters. From this perspective, we\nwill categorize LLMs in four categories as well: small (less\nthan or equal to 1 billion parameters), medium (between 1 and\n10 billion), large (between 10 and 100 billion), and very large\n(more than 100 billion). Another classification for the LLMs\nwe use is their primary use case. We consider each LLM to\nbe either: Foundation model (pretrained language model with\nno instruction fine-tuning and chat fine-tuning), Instruction\nmodel (pretrained language model with only instruction fine-\ntuning), and Chat model (pretrained language model with\ninstruction and chat fine-tuning). Apart from all the catego-\nrization described, another category is required to distinguish\nbetween original models and tuned ones. Original models are\nthose that have been released as a foundation model or a fine-\ntuned one. Tuned models are those that grasped the original\nmodel and tuned it with different datasets or even different\ntraining approaches. It is also good to note that original models\nare usually foundation models that have been fine-tuned on\nspecific datasets or even different approaches. Availability of\nthe model weights regardless of the license is another category\nin our classification. Models that have their weights publicly\navailable (even through request) are noted as Public models\nwhile others are noted as Private. Table III shows all of these\ndefinitions and abbreviations used in the rest of the article.\nFigure 43 illustrate these visually.\nAccording to the provided categorizations, we can catego-\nrize and label each notable LLM as shown in table IV. As can\nbe seen from this table, models categorized as very large are\nalso unavailable as well.\nB. LLMs’ Performance on Different Tasks\nCommonsense reasoning is one of the important capabili-\nties each model can obtain. This capability denotes the ability\nof the model to use prior knowledge in combination with\nreasoning skills. In the case of HellaSwag for example, finding\nthe continuation of text is challenging because the given text\ncontains a partial part of the story while the given choices\nas continuation are tricky to select, and without having prior\nknowledge about the world it is not possible. This specific kind\nof reasoning deserves high attention because it is related to\nutilizing previous knowledge with open text-described scenes\nor facts. As can be seen from table V not just Unavailable\nmodels but also Public ones can achieve good results on\nvarious tests.\nTABLE V: Commonsense reasoning comparison.\nModel\nOBQA\nHellaSwag\nDavinci-003\n51\n83.4\nFalcon 7B\n44.4\n76.3\nAlpaca\n43.4\n73.9\nPythia 7B\n37.2\n64\nPythia 12B\n43.2\n68.1\nLLAMA 7B\n42.4\n73\nDolly 6B\n41.2\n67.6\nDolly 12B\n40.4\n71\nAlpaca 7B\n43.4\n73.9\nAlpaca Lora 7B\n42.6\n74\nGPT-J 6.7B\n38.2\n66.2\nLLama 7B\n42.4\n73\nLLama 13B\n42.2\n76.2\nPythia 6.7B\n37.2\n64\nPythia 12B\n38\n67.3\nStableLM Tuned\n33.4\n53.6\nKoala 13B\n42.8\n72.6\nMosaic mpt-7B\n42.6\n76.3\nLLAMA 2 70B\n-\n87.33\nLLAMA 65B\n-\n86.09\nFalcon 40B\n-\n85.3\nFalcon 180B\n-\n88.86\nMPT Instruct 30B\n-\n84.31\nMPT Instruct 7B\n-\n77.91\nYi 6B\n-\n76.42\nYi 34B\n-\n85.69\nGPT-4\n-\n95.3\nGemini Ultra\n-\n87.8\nFrom the results presented in Table V it is clear that GPT-4\nachieves best results for HellaSwag while Davinci-003 is best\nmodel for OBQA. It is also good to note that results for OBQA\nare not reported for all of the models and possibly davinci-003\nis not the best model achieving highest results on OBQA.\n",
        "word_count": 980,
        "char_count": 5946,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "NimbusRomNo9L-Regu (7.0pt)",
          "NimbusRomNo9L-Medi (7.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 33,
        "text": "TABLE III: LLM categories and respective definitions.\nClassification\nCategory\nDescription\nSize\nSmall\nNumber of parameters ≤ 1B\nMedium\n1B < Number of parameters ≤ 10B\nLarge\n10B < Number of parameters ≤ 100B\nVery Large\n100B < Number of parameters\nType\nFoundation model\nPretrained language model\nInstruction model\nPretrained and instruction fine-tuned language model\nChat model\nPretrained, instruction fine-tuned, and chat fine-tuned language model\nOrigin\nOriginal model\nAn original model released with either Foundation, Instruction, or Chat model\nTuned model\nFine-tuned version of an original model\nAvailability\nPublicly available\nModel and weights are available due to request to without request\nPublicly unavailable\nModel and weights are not publicly available\nTABLE IV: Different LLM categorization.\nModel\nSize\n#Params (B)\nType\nAvailability\nOrigin\nDavinci-002\nVery Large\n175\nInstruction\nUnavailable\nTuned\nDavinci-003\nVery Large\n175\nInstruction\nUnavailable\nTuned\nGPT 3.5-turbo\nLarge\n20\nChat\nUnavailable\nTuned\nFalcon 7B\nMedium\n7\nFoundation\nPublic\nOriginal\nAlpaca\nLarge\n13\nChat\nPublic\nTuned\nPythia 7B\nMedium\n7\nFoundation\nPublic\nOriginal\nPythia 12B\nLarge\n12\nFoundation\nPublic\nOriginal\nLLAMA 7B\nMedium\n7\nChat\nPublic\nOriginal\nLLAMA 2 7B\nMedium\n7\nChat\nPublic\nTuned\nLLAMA 2 7B\nMedium\n7\nFoundation\nPublic\nOriginal\nVicuna 13B\nLarge\n13\nFoundation\nPublic\nTuned\nVicuna 7B\nMedium\n7\nFoundation\nPublic\nTuned\nClaude\nLarge\n93\nChat\nUnavailable\nOriginal\nClaude 2\nVery Large\n137\nChat\nUnavailable\nOriginal\nNot all models report their performance on all datasets, and\nbecause of that, the number of models for which performance\nis reported in different tables varies.\nTABLE VI: Symbolic reasoning comparison.\nModel\nCobjects\nPenguins\nGPT-NeoX\n26\n33.56\nOPT 66B\n31.2\n28.08\nBloomberg GPT\n34.8\n37.67\nBLOOM 176B\n36.8\n40.41\nPaLM 540B\n38\n44.5\nGopher-280B\n49.2\n40.6\nChinchilla-70B\n59.7\n48.7\nPaLM 2\n61.2\n65.8\nWorld knowledge is mostly about general knowledge ques-\ntions, for example, in Wikifact dataset questions such as ”Who\nis the author of a specific well-known book” can be found and\nreferences are also provided. Table VII shows the results.\nTABLE VII: World knowledge comparison.\nModel\nTriviaQA\nNaturalQ\nWebQ\nARC\nBLOOM\n-\n-\n-\n32.9\nBLOOM 176B\n-\n-\n-\n50.85\nBloomberg GPT\n-\n-\n-\n48.63\nChinchilla\n-\n35.5\n-\n-\nCodex + REPLUG\n76.8\n44.7\n-\n-\nGAL 120B\n-\n-\n-\n67.9\nGLaM 62B/64E\n75.8\n32.5\n15.5\n50.3\nGopher\n-\n28.2\n-\n-\nGPT-3 175B\n71.2\n29.9\n41.5\n85.2\nGPT-4\n-\n-\n-\n96.4\nGPT-NeoX\n-\n-\n-\n45.39\nLLaMA 13B\n-\n-\n-\n52.7\nLLaMA 2 70B\n85\n33\n-\n-\nLLaMA 33B\n-\n24.9\n-\n57.8\nLLaMA 65B\n72.6\n39.9\n-\n-\nLLaMA 7B\n-\n-\n-\n47.6\nMistral 7B\n69.9\n28.8\n-\n55.5\nNeo-6B\n-\n13.7\n-\n-\nOPT\n-\n-\n-\n31.1\nOPT 66B\n-\n-\n-\n44.54\nOPT-175B\n-\n-\n-\n43.94\nOPT-175B\n-\n-\n-\n25.6\nPaLM 2-L\n86.1\n37.5\n28.2\n95.1\nPaLM 2-M\n81.7\n32\n26.9\n64.9\nPaLM 2-S\n75.2\n25.3\n21.8\n59.6\nPaLM-540B\n81.4\n39.6\n43.5\n87.1\nphi-1.5-web 1.3B\n-\n-\n-\n44.9\nSparseGPT\n-\n-\n-\n38.99\nSparseGPT\n-\n-\n-\n39.85\nSparseGPT\n-\n-\n-\n41.3\nFor some specific use-case models, it is highly demanded to\nhave coding and code-generation capability. Table VIII shows\nthe results of different models on coding capability.\n",
        "word_count": 527,
        "char_count": 3063,
        "fonts": [
          "NimbusRomNo9L-Regu (7.0pt)",
          "CMMI7 (7.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "CMSY7 (7.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 34,
        "text": "Large\nLanguage\nModels\nParameters\nAvailability\nOriginality\nType\nSmall LM\n# of params <1B\nMedium LM\n1B < # of params <10B\nLarge LM\n10B < # of params <100B\nVery Large LM\n100B < # of params\nTuned\nFine tuning\nOriginal\nPublic\nPrivate\nFoundation\nInstruction\nChat\nFine tuned models that are originally\nbased on original models.\nExample: Alpaca (based on LLaMA)\nOriginal models that are not fine\ntuned or based on any other\npretrained model.\nExample: LLaMA\nModel weights are publicly released\nand is available.\nExample: LLaMA\nModel weights are NOT publicly\nreleased and is NOT available.\nExample: GPT-4\nPretrained model with no instruction\nor chat fine-tuning.\nExample: MPT-7B\nPretrained model that is\nalso fine-tuned on\ninstruction following.\nExample: MPT-7B-instruct\nPretrained model that is\nalso fine-tuned on chat.\nExample: MPT-7B-chat\nFig. 43: LLM categorizations.\nTABLE VIII: Coding capability comparison.\nModel\nHumanEval\nGemini Ultra\n74.4\nGemini Pro\n67.7\nGPT-4\n67\nWizardCoder 15B\n57.3\nphi-1 1.3B\n50.6\nCode Llama\n48.8\nGPT-3.5\n48.1\nOctoCoder\n46.2\nphi-1-small\n45\nPaLM 2-S\n37.6\nInstructCodeT5+ 16B\n35\nMistral 7B\n30.5\nLLaMA 2\n29.9\nphi-1-base\n29\nCodex-12B\n28.81\nPaLM 540B\n26.2\nCodeT5+ 2B\n24.2\nLLaMA 65B\n23.7\nLLaMA 33B\n21.7\nPaLM 62B\n15.9\nLLaMA 13B\n15.8\nLaMDA 137B\n14\nMIM-350M\n13.7\nLLaMA 7B\n10.5\nPaLM 8B\n3.6\nArithmetic reasoning is another challenging reasoning ca-\npability to achieve. GSM8K for example contains grade school\nmathematical questions with respect to their answers. Table IX\nprovides an insight for different model comparisons.\nTABLE IX: Arithmetic reasoning comparison.\nModel\nGSM8k\nMATH\nGemini Ultra\n94.4\n53.2\nGPT-4\n87.1\n42.5\nGemini Pro\n86.5\n32.6\nToRA 70B\n84.3\n49.7\nMathCoder-L-70B\n83.9\n-\nMetaMath 70B\n82.3\n26\nMuggleMATH 70B\n82.3\n-\nMathCoder-CL-34B\n81.7\n45.2\nToRA-Code 34B\n80.7\n50.8\nMetaMath-Mistral-7B\n77.7\n-\nArithmo2-Mistral-7B\n76.4\n-\nToRA-Code 13B\n75.8\n48.1\nArithmo-Mistral-7B\n74.7\n-\nMathCoder-CL-13B\n74.1\n35.9\nMuggleMATH 13B\n74\n-\nCodeT5+\n73.8\n-\nKwaiYiiMath 13B\n73.3\n-\nToRA-Code 7B\n72.6\n44.6\nMathCoder-L-13B\n72.6\n29.9\nMetaMath 13B\n71\n22.5\nLLaMA 65B\n69.7\n10.6\nMuggleMATH 7B\n68.4\n-\nMathCoder-CL-7B\n67.8\n23.3\nMetaMath 7B\n66.4\n19.4\nRFT 70B\n64.8\n-\nMathCoder-L-7B\n64.2\n-\nOrca 2-13B\n59.14\n-\nU-PaLM\n58.5\n-\nPaLM-540B\n58.1\n8.8\nLLaMA 2 70B\n56.8\n-\nRFT 13B\n55.3\n-\nLLaMA 33B\n53.1\n7.1\nMistral 7B\n52.2\n13.1\nRFT 7B\n51.2\n-\nLLaMA 65B\n50.9\n20.5\nOrca 2-7B\n47.23\n-\nText-davinci-002\n40.7\n19.1\nLLaMA 33B\n35.6\n3.9\nGPT-Neo-2.7B\n19.5\n-\nLLaMA 7B\n18.1\n2.9\nPaLM 540B\n17.9\n8.8\nLLaMA 13B\n17.8\n3.9\nLLaMA 7B\n11\n2.9\nGPT-Neo-125M\n7.5\n-\nP LM 8B\n4 1\n1 5\n",
        "word_count": 417,
        "char_count": 2525,
        "fonts": [
          "ArialMT (4.1pt)",
          "ArialMT (4.8pt)",
          "NimbusRomNo9L-Regu (7.0pt)",
          "ArialMT (8.2pt)",
          "Arial-BoldMT (8.2pt)",
          "ArialMT (7.5pt)",
          "Arial-BoldMT (4.8pt)",
          "NimbusRomNo9L-Regu (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 35,
        "text": "Large language models in some cases are hallucinating an-\nswers simply because they are next-token prediction machines.\nHallucination is one of the important factors in measuring\nhow much a large language model is trustworthy and reliable.\nMeasuring hallucination on the other hand is also not easy as it\nseems because each fact can be written in different styles and\neven the smallest changes in writing make it hard to detect.\nIt is fair to assume if any particular LLM is more capable\nto detect hallucination of false information in text, it is also\nmore trustworthy. HaluEval is one of the datasets that aims to\nmeasure hallucination in this field [205]. Evaluation can also be\nperformed by another model judging the response with regard\nto the actual answer [206]. Table X shows the evaluation of\ndifferent models based on these datasets.\nVII.\nCHALLENGES AND FUTURE DIRECTIONS\nAs we have seen in the previous sections, large language\nmodels have achieved impressive results in the past 1-2 years.\nAt the same time this is still a new and extremely active\nresearch area where the pace of innovation is increasing rather\nthan slowing down. As in any other evolving area though, there\nare still numerous challenges ahead. Here we briefly mention\nsome of the challenges and main active areas which are known\nso far. It is worth noting that LLM challenges are discussed\nin details in a work by Kaddour et al. [207].\nA. Smaller and more efficient Language Models\nThis is a survey on large language models, and there\nhas been an initial push towards ”larger is better” that has\nclearly been rewarded with ever larger models like GPT-\n4 getting better accuracy and performance in benchmarks.\nHowever, those large models are costly and inefficient in\nseveral dimensions (e.g. high latency). In response to all of\nthis, there is a current research trend to come up with Small\nLanguage Models (SLMs) as a cost-effective alternative to\nLLMs, particularly when used on specific tasks that might not\nrequire the full generality of larger models. Prominent works\nin this direction include Phi-1 [208], Phi-1.5 [209], and Phi-2\nfrom Microsoft.\nMore generally, we should expect many research efforts in\nthis area of how to train smaller and more efficient models.\nTechniques such as parameter-efficient fine-tuning (PEFT),\nteacher/student, and other forms of distillation – see section\nIII-I – will continue to be used to build a smaller model out\nof larger ones.\nB. New Post-attention Architectural Paradigms\nTransformer blocks have been a crucial and constant part of\nmost of current LLM frameworks, and it’s a big question mark\nhow much longer this architecture will be in vogue, and what\nwill be the next big architectural break-through in the field of\ndeep learning (and NLP). Since AlexNet in 2012, we have seen\nmany architectures go in and out of fashion, including LSTM,\nGRU, seq2seq, but Transformers have been the dominant\napproach since its inception. As described earlier, attention is\nthe main mechanism driving transformers. More recently, there\nhas been promising research in alternative approaches that are\nbeing labelled as post-attention.\nAn important class of such class of post-attention models\nare the so called State Space Models (SSMs). While the notion\nof State Space Models has a long history in machine learning,\nit should be noted that in the context of language models, SSM\nis usually used in reference to the newer Structure State Space\nModel architecture or S4 for short (see Gu et al. [29]). Some\nrecent models in this category are Mamba [30], Hyena [210],\nand Striped Hyena [211].\nWhile all of those models are very competitive in terms of\nperformance in leaderboards and efficiency, they also address\nan important challenge in more traditional attention-based\narchitectures: the lack of support for larger context windows.\nHaving a good answer to many prompts requires context.\nFor example, the response to ”Recommend some good movies\nfor me” requires a lot of context about ”me” as well as what\nmovies are available and which ones I have not watched.\nContext length is especially important for RAG, where large\nportions of text might be retrieved and injected into the prompt\nfor generation (see section IV-C.\nThe longer the context length, the more tokens we can\nsqueeze into the context. The more information the model has\naccess to, the better its response will be. But on the other\nhand, with very long context, it would be hard for the model\nto remember everything and efficiently process all the informa-\ntion. Attention-based models are highly inefficient for longer\ncontexts and that is why we should expect more research in\ndifferent mechanisms that enable processing longer contexts\nand generally come up with more efficient architectures.\nThat being said, new architectures might not only propose\nalternatives for the attention mechanism but rather rethink the\nwhole Transformer architecture. As an early example of this,\nMonarch Mixer [212] proposes a new architecture that uses\nthe same sub-quadratic primitive that achieves high hardware\nefficiency on GPUs – Monarch matrices – along both sequence\nlength and model dimension.\nOn the other end of the spectrum, it is worth mentioning\nthat there are some attention-compatible architectural mecha-\nnisms that have been recently gaining steam and proving their\nvalue in creating better and more powerful LLMs. Probably\nthe best example of such mechanism is Mixture of Experts\n(MoE). MoEs have been around in machine learning for years,\neven before the Deep Learning Era [213], but they have been\ngaining popularity since then, and particularly in the context\nof Transformer models and LLMs.\nIn LLMs, MoEs allow to train an extremely large model\nthan is then only partially instantiated during inference\nwhen some of the experts are turned off wherever the gat-\ning/weighting function has a low weight assigned to them. As\nan example, the GLaM model has 1.2 trillion parameters, but\nduring inference only 2 out of the 64 experts are used [84].\nMoEs are nowadays an important component of the so-\ncalled frontier LLMs (i.e. the most advanced and capable\nmodels). GPT-4 itself is rumored to be based on a MoE\narchitecture, and some of the best performing LLMs such as\nMixtral [117], are basically an MoE version of pre-existing\nLLMs.\nFinally, it is important to note that MoEs can be used as a\ncomponent of any architecture regardless of whether it is based\n",
        "word_count": 1053,
        "char_count": 6437,
        "fonts": [
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 36,
        "text": "TABLE X: Hallucination evaluation\nModel\nHHEM\nHaluEval QA\nHaluEval Dialogue\nHaluEval Sum.\nHaluEval General\nGPT 4\n97\n-\n-\n-\n-\nGPT 4 Turbo\n97\n-\n-\n-\n-\nGPT 3.5 Turbo\n96.5\n62.59\n72.4\n58.53\n79.44\nDavinci002\n-\n60.05\n60.81\n47.77\n80.42\nDavinci003\n-\n49.65\n68.37\n48.07\n80.4\nGPT-3\n-\n49.21\n50.02\n51.23\n72.72\nGoogle Gemini Pro\n95.2\n-\n-\n-\n-\nLlama 2 70B\n94.9\n-\n-\n-\n-\nLlama 2 7B\n94.4\n49.6\n43.99\n49.55\n20.46\nLlama 2 13B\n94.1\n-\n-\n-\n-\nCohere-Chat\n92.5\n-\n-\n-\n-\nCohere\n91.5\n-\n-\n-\n-\nClaude 2\n91.5\n69.78\n64.73\n57.75\n75\nClaude 1\n67.6\n64.83\n53.76\n73.88\nMicrosoft Phi 2\n91.5\n-\n-\n-\n-\nGoogle Palm 2 (beta)\n91.4\n-\n-\n-\n-\nMixtral 8x7B\n90.7\n-\n-\n-\n-\nAmazon Titan Express\n90.6\n-\n-\n-\n-\nMistral 7B\n90.6\n-\n-\n-\n-\nGoogle Palm 2 Chat (beta)\n90\n-\n-\n-\n-\nGoogle Palm 2\n87.9\n-\n-\n-\n-\nGoogle Palm 2 Chat\n72.8\n-\n-\n-\n-\nChatGLM\n-\n47.93\n44.41\n48.57\n30.92\nFalcon\n-\n39.66\n29.08\n42.71\n18.98\nVicuna\n-\n60.34\n46.35\n45.62\n19.48\nAlpaca\n-\n6.68\n17.55\n20.63\n9.54\non attention or not. In fact, MoEs have also been applied to\nSSM-based LLMs like Mamba citepioro2024moemamba. We\nshould continue to see MoE-driven improvements in the future\nregardless of the underlying architecture.\nC. Multi-modal Models\nFuture LLMs are expected to be multi-modal and handle\na variety of data types, such as text, images, and videos,\naudio, in a unified manner. This opens up possibilities for\nmore diverse applications in fields like question answering,\ncontent generation, creative arts, and healthcare, robotics, and\nbeyond. There are already several prominent multi-modal\nLLMs out there, including: LLAVA [214], LLAVA-Plus [215],\nGPT-4 [33], Qwen-vl [116], Next-GPT [216], but the trend is\nexpected to be continued. Evaluation of these models also is a\nnew research topic, especially conversational generative vision\nmodels [217]. Multi-modal LLMs can unlock huge potentials\nin a variety of tasks, and there has already been a descent\nprogress in this direction, which needs a dedicated paper to\ndiscuss all its details.\nD. Improved LLM Usage and Augmentation techniques\nAs we described in sectionIV, many of the shortcomings\nand limitations of LLMs such as hallucination can be ad-\ndressed through advanced prompt engineering, use of tools,\nor other augmentation techniques. We should expect not only\ncontinued, but accelerated research in this area. It is worth\nmentioning that, in the specific case of software engineering,\nsome works ([218]) tried to automatically eliminate this issue\nfrom the overall software engineering workflow\nLLM-based systems are already starting to replace ma-\nchine learning systems that were until recently using other\napproaches. As a clear example of this, LLMs are now being\ndeployed to better understand people preference and interests,\nand provide more personalized interactions, whether in cus-\ntomer service, content recommendation, or other applications.\nThis involves better understanding of user preferences, and\nanalyzing their past interactions and using them as the context.\nWe will continue to see research in the application and usage\nof LLMs for not only personalization and recommendations,\nbut many other application areas using other machine learning\ntechniques.\nFinally, another important area of research we expect to\ngather increased attention is that of LLM-based agents and\nmulti-agent systems [172], [173], [174]. The development of\nLLM systems with access to external tools and decision-\nmaking capabilities is both exciting and challenging. We will\nsee continued research and progress in this important area that\nsome argue could lead to Artificial General Intelligence (AGI).\nE. Security and Ethical/Responsible AI\nEnsuring the robustness and security of LLMs against\nadversarial attacks and other vulnerabilities is a critical area\nof research [219]. As LLMs are increasingly deployed in real-\nworld applications, they need to be protected from potential\nthreats, to prevent them being used to manipulate people or\nspread mis-information. Improving the reasoning capabilities\nof these model [220], would help them to better detect potential\nadversarial attacks.\nAddressing ethical concerns and biases in LLMs is another\nactive area of research. Efforts are being made to ensure that\nLLMs are fair, unbiased, and capable of handling sensitive\ninformation responsibly. As LLMs are being used more and\nmore by a large number of people on a daily basis, making\nsure they are unbiased and behave responsibly is crucial.\nVIII.\nCONCLUSION\nThis paper present a survey of LLMs developed in the\npast few years. We first provide an overview of early pre-\n",
        "word_count": 750,
        "char_count": 4526,
        "fonts": [
          "NimbusRomNo9L-Regu (7.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 37,
        "text": "trained language models (e.g., as BERT), then review three\npopular LLM families (GPT, LLaMA, PaLM), and other\nrepresentative LLMs. We then survey methods and techniques\nof building, augmenting, and using LLMs. We review popular\nLLM datasets and benchmarks, and compare performance of\na set of prominent models on public benchmarks. Finally, we\npresent open challenges and future research directions.\nREFERENCES\n[1]\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\nfor neural language models,” arXiv preprint arXiv:2001.08361, 2020.\n[2]\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark\net al., “Training compute-optimal large language models,” arXiv\npreprint arXiv:2203.15556, 2022.\n[3]\nC. E. Shannon, “Prediction and entropy of printed english,” Bell system\ntechnical journal, vol. 30, no. 1, pp. 50–64, 1951.\n[4]\nF. Jelinek, Statistical methods for speech recognition.\nMIT press,\n1998.\n[5]\nC. Manning and H. Schutze, Foundations of statistical natural lan-\nguage processing.\nMIT press, 1999.\n[6]\nC. D. Manning, An introduction to information retrieval.\nCambridge\nuniversity press, 2009.\n[7]\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\nB. Zhang, J. Zhang, Z. Dong et al., “A survey of large language\nmodels,” arXiv preprint arXiv:2303.18223, 2023.\n[8]\nC. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\nL. He et al., “A comprehensive survey on pretrained foundation mod-\nels: A history from bert to chatgpt,” arXiv preprint arXiv:2302.09419,\n2023.\n[9]\nP. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods\nin natural language processing,” ACM Computing Surveys, vol. 55,\nno. 9, pp. 1–35, 2023.\n[10]\nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\nJ. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint\narXiv:2301.00234, 2022.\n[11]\nJ. Huang and K. C.-C. Chang, “Towards reasoning in large language\nmodels: A survey,” arXiv preprint arXiv:2212.10403, 2022.\n[12]\nS. F. Chen and J. Goodman, “An empirical study of smoothing\ntechniques for language modeling,” Computer Speech & Language,\nvol. 13, no. 4, pp. 359–394, 1999.\n[13]\nY. Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic\nlanguage model,” Advances in neural information processing systems,\nvol. 13, 2000.\n[14]\nH. Schwenk, D. D´echelotte, and J.-L. Gauvain, “Continuous space\nlanguage models for statistical machine translation,” in Proceedings\nof the COLING/ACL 2006 Main Conference Poster Sessions, 2006,\npp. 723–730.\n[15]\nT. Mikolov, M. Karafi´at, L. Burget, J. Cernock`y, and S. Khudanpur,\n“Recurrent neural network based language model.” in Interspeech,\nvol. 2, no. 3.\nMakuhari, 2010, pp. 1045–1048.\n[16]\nA. Graves, “Generating sequences with recurrent neural networks,”\narXiv preprint arXiv:1308.0850, 2013.\n[17]\nP.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, “Learning\ndeep structured semantic models for web search using clickthrough\ndata,” in Proceedings of the 22nd ACM international conference on\nInformation & Knowledge Management, 2013, pp. 2333–2338.\n[18]\nJ. Gao, C. Xiong, P. Bennett, and N. Craswell, Neural Approaches to\nConversational Information Retrieval. Springer Nature, 2023, vol. 44.\n[19]\nI. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning\nwith neural networks,” Advances in neural information processing\nsystems, vol. 27, 2014.\n[20]\nK. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio, “On\nthe properties of neural machine translation: Encoder-decoder ap-\nproaches,” arXiv preprint arXiv:1409.1259, 2014.\n[21]\nH. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Doll´ar,\nJ. Gao, X. He, M. Mitchell, J. C. Platt et al., “From captions to\nvisual concepts and back,” in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2015, pp. 1473–1482.\n[22]\nO. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell:\nA neural image caption generator,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2015, pp.\n3156–3164.\n[23]\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations. corr\nabs/1802.05365 (2018),” arXiv preprint arXiv:1802.05365, 2018.\n[24]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[25]\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n[26]\nP. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert\nwith disentangled attention,” arXiv preprint arXiv:2006.03654, 2020.\n[27]\nX. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao,\nA. Zhang, L. Zhang et al., “Pre-trained models: Past, present and\nfuture,” AI Open, vol. 2, pp. 225–250, 2021.\n[28]\nX. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained\nmodels for natural language processing: A survey,” Science China\nTechnological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\n[29]\nA. Gu, K. Goel, and C. R´e, “Efficiently modeling long sequences with\nstructured state spaces,” 2022.\n[30]\nA. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\nselective state spaces,” arXiv preprint arXiv:2312.00752, 2023.\n[31]\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al.,\n“Palm: Scaling language modeling with pathways,” arXiv preprint\narXiv:2204.02311, 2022.\n[32]\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., “Llama:\nOpen and efficient foundation language models,” arXiv preprint\narXiv:2302.13971, 2023.\n[33]\nOpenAI,\n“GPT-4\nTechnical\nReport,”\nhttps://arxiv.org/pdf/2303.\n08774v3.pdf, 2023.\n[34]\nJ.\nWei,\nX.\nWang,\nD.\nSchuurmans,\nM.\nBosma,\nb.\nichter,\nF.\nXia,\nE.\nChi,\nQ.\nV.\nLe,\nand\nD.\nZhou,\n“Chain-of-thought\nprompting\nelicits\nreasoning\nin\nlarge\nlanguage\nmodels,”\nin\nAdvances in Neural Information Processing Systems, S. Koyejo,\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\nEds., vol. 35.\nCurran Associates, Inc., 2022, pp. 24 824–24 837.\n[Online]. Available: https://proceedings.neurips.cc/paper files/paper/\n2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\n[35]\nG. Mialon, R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,\nR. Raileanu, B. Rozi`ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\nmaz et al., “Augmented language models: a survey,” arXiv preprint\narXiv:2302.07842, 2023.\n[36]\nB. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang,\nL. Liden, Z. Yu, W. Chen, and J. Gao, “Check your facts and try\nagain: Improving large language models with external knowledge and\nautomated feedback,” arXiv preprint arXiv:2302.12813, 2023.\n[37]\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\n“React: Synergizing reasoning and acting in language models,” arXiv\npreprint arXiv:2210.03629, 2022.\n[38]\nD. E. Rumelhart, G. E. Hinton, R. J. Williams et al., “Learning internal\nrepresentations by error propagation,” 1985.\n[39]\nJ. L. Elman, “Finding structure in time,” Cognitive science, vol. 14,\nno. 2, pp. 179–211, 1990.\n[40]\nM. V. Mahoney, “Fast text compression with neural networks.” in\nFLAIRS conference, 2000, pp. 230–234.\n[41]\nT. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ˇCernock`y, “Strate-\ngies for training large scale neural network language models,” in 2011\nIEEE Workshop on Automatic Speech Recognition & Understanding.\nIEEE, 2011, pp. 196–201.\n",
        "word_count": 1204,
        "char_count": 7805,
        "fonts": [
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)",
          "NimbusRomNo9L-ReguItal (8.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 38,
        "text": "[42]\ntmikolov.\nrnnlm.\n[Online].\nAvailable:\nhttps://www.fit.vutbr.cz/\n∼imikolov/rnnlm/\n[43]\nS. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,\nand J. Gao, “Deep learning–based text classification: a comprehensive\nreview,” ACM computing surveys (CSUR), vol. 54, no. 3, pp. 1–40,\n2021.\n[44]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[45]\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n“Albert: A lite bert for self-supervised learning of language represen-\ntations,” arXiv preprint arXiv:1909.11942, 2019.\n[46]\nK. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-\ntraining text encoders as discriminators rather than generators,” arXiv\npreprint arXiv:2003.10555, 2020.\n[47]\nG. Lample and A. Conneau, “Cross-lingual language model pretrain-\ning,” arXiv preprint arXiv:1901.07291, 2019.\n[48]\nZ. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V. Le, “Xlnet: Generalized autoregressive pretraining for language\nunderstanding,” Advances in neural information processing systems,\nvol. 32, 2019.\n[49]\nL. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao,\nM. Zhou, and H.-W. Hon, “Unified language model pre-training for\nnatural language understanding and generation,” Advances in neural\ninformation processing systems, vol. 32, 2019.\n[50]\nA. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improv-\ning language understanding by generative pre-training,” 2018.\n[51]\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[52]\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\nwith a unified text-to-text transformer,” The Journal of Machine\nLearning Research, vol. 21, no. 1, pp. 5485–5551, 2020.\n[53]\nL. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\nA. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained\ntext-to-text transformer,” arXiv preprint arXiv:2010.11934, 2020.\n[54]\nK. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, “Mass: Masked\nsequence to sequence pre-training for language generation,” arXiv\npreprint arXiv:1905.02450, 2019.\n[55]\nM. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\nV. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and\ncomprehension,” arXiv preprint arXiv:1910.13461, 2019.\n[56]\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[57]\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka-\nplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al.,\n“Evaluating large language models trained on code,” arXiv preprint\narXiv:2107.03374, 2021.\n[58]\nR. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., “Webgpt: Browser-\nassisted question-answering with human feedback,” arXiv preprint\narXiv:2112.09332, 2021.\n[59]\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\nmodels to follow instructions with human feedback,” Advances in\nNeural Information Processing Systems, vol. 35, pp. 27 730–27 744,\n2022.\n[60]\nOpenAI. (2022) Introducing chatgpt. [Online]. Available: https:\n//openai.com/blog/chatgpt\n[61]\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\n2: Open foundation and fine-tuned chat models,” arXiv preprint\narXiv:2307.09288, 2023.\n[62]\nR. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,\nand T. B. Hashimoto, “Alpaca: A strong, replicable instruction-\nfollowing model,” Stanford Center for Research on Foundation Mod-\nels. https://crfm. stanford. edu/2023/03/13/alpaca. html, vol. 3, no. 6,\np. 7, 2023.\n[63]\nT. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef-\nficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314,\n2023.\n[64]\nX. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\nand D. Song, “Koala: A dialogue model for academic research,” Blog\npost, April, vol. 1, 2023.\n[65]\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,\nD. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al.,\n“Mistral 7b,” arXiv preprint arXiv:2310.06825, 2023.\n[66]\nB. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,\nJ. Liu, T. Remez, J. Rapin et al., “Code llama: Open foundation models\nfor code,” arXiv preprint arXiv:2308.12950, 2023.\n[67]\nS. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Large\nlanguage model connected with massive apis,” 2023.\n[68]\nA. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and\nS. Naidu, “Giraffe: Adventures in expanding context lengths in llms,”\narXiv preprint arXiv:2308.10882, 2023.\n[69]\nB. Huang, “Vigogne: French instruction-following and chat models,”\nhttps://github.com/bofenghuang/vigogne, 2023.\n[70]\nY. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu,\nD. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al., “How far can\ncamels go? exploring the state of instruction tuning on open resources,”\narXiv preprint arXiv:2306.04751, 2023.\n[71]\nS. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski,\nand P. Miło´s, “Focused transformer: Contrastive training for context\nscaling,” arXiv preprint arXiv:2307.03170, 2023.\n[72]\nD.\nMahan,\nR.\nCarlow,\nL.\nCastricato,\nN.\nCooper,\nand\nC.\nLaforte,\n“Stable\nbeluga\nmodels.”\n[Online].\nAvailable:\n[https://huggingface.co/stabilityai/StableBeluga2](https://\nhuggingface.co/stabilityai/StableBeluga2)\n[73]\nY. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Gar-\ncia, H. S. Zheng, J. Rao, A. Chowdhery et al., “Transcending scaling\nlaws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399,\n2022.\n[74]\nH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus,\nY. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-\nfinetuned language models,” arXiv preprint arXiv:2210.11416, 2022.\n[75]\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., “Palm 2 technical\nreport,” arXiv preprint arXiv:2305.10403, 2023.\n[76]\nK. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language\nmodels encode clinical knowledge,” arXiv preprint arXiv:2212.13138,\n2022.\n[77]\nK. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou,\nK. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al., “Towards expert-\nlevel medical question answering with large language models,” arXiv\npreprint arXiv:2305.09617, 2023.\n[78]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,\nA. M. Dai, and Q. V. Le, “Finetuned language models are zero-shot\nlearners,” arXiv preprint arXiv:2109.01652, 2021.\n[79]\nJ. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language\nmodels: Methods, analysis & insights from training gopher,” arXiv\npreprint arXiv:2112.11446, 2021.\n[80]\nV. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., “Multi-\ntask prompted training enables zero-shot task generalization,” arXiv\npreprint arXiv:2110.08207, 2021.\n[81]\nY. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\nY. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced pre-\ntraining for language understanding and generation,” arXiv preprint\narXiv:2107.02137, 2021.\n[82]\nS. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Mil-\nlican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark\net al., “Improving language models by retrieving from trillions of\ntokens,” in International conference on machine learning.\nPMLR,\n2022, pp. 2206–2240.\n",
        "word_count": 1275,
        "char_count": 8284,
        "fonts": [
          "NimbusRomNo9L-Regu (8.0pt)",
          "CMSY6 (6.0pt)",
          "NimbusRomNo9L-ReguItal (8.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 39,
        "text": "[83]\nO. Lieber, O. Sharir, B. Lenz, and Y. Shoham, “Jurassic-1: Technical\ndetails and evaluation,” White Paper. AI21 Labs, vol. 1, p. 9, 2021.\n[84]\nN. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,\nY. Zhou, A. W. Yu, O. Firat et al., “Glam: Efficient scaling of\nlanguage models with mixture-of-experts,” in International Conference\non Machine Learning.\nPMLR, 2022, pp. 5547–5569.\n[85]\nR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-\nT. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Language\nmodels for dialog applications,” arXiv preprint arXiv:2201.08239,\n2022.\n[86]\nS. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\nC. Dewan, M. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained\ntransformer language models,” arXiv preprint arXiv:2205.01068, 2022.\n[87]\nR. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar-\navia, A. Poulton, V. Kerkez, and R. Stojnic, “Galactica: A large\nlanguage model for science,” arXiv preprint arXiv:2211.09085, 2022.\n[88]\nE. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,\nS. Savarese, and C. Xiong, “Codegen: An open large language\nmodel for code with multi-turn program synthesis,” arXiv preprint\narXiv:2203.13474, 2022.\n[89]\nS. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al.,\n“Alexatm 20b: Few-shot learning using a large-scale multilingual\nseq2seq model,” arXiv preprint arXiv:2208.01448, 2022.\n[90]\nA. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu,\nT. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al.,\n“Improving alignment of dialogue agents via targeted human judge-\nments,” arXiv preprint arXiv:2209.14375, 2022.\n[91]\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski,\nV. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al.,\n“Solving quantitative reasoning problems with language models,”\nAdvances in Neural Information Processing Systems, vol. 35, pp.\n3843–3857, 2022.\n[92]\nY. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster,\nH. S. Zheng, N. Houlsby, and D. Metzler, “Unifying language learning\nparadigms,” arXiv preprint arXiv:2205.05131, 2022.\n[93]\nT. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow,\nR. Castagn´e, A. S. Luccioni, F. Yvon, M. Gall´e et al., “Bloom: A 176b-\nparameter open-access multilingual language model,” arXiv preprint\narXiv:2211.05100, 2022.\n[94]\nA. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\nW. Zheng, X. Xia et al., “Glm-130b: An open bilingual pre-trained\nmodel,” arXiv preprint arXiv:2210.02414, 2022.\n[95]\nS. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien,\nE. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al.,\n“Pythia: A suite for analyzing large language models across train-\ning and scaling,” in International Conference on Machine Learning.\nPMLR, 2023, pp. 2397–2430.\n[96]\nS. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and\nA. Awadallah, “Orca: Progressive learning from complex explanation\ntraces of gpt-4,” arXiv preprint arXiv:2306.02707, 2023.\n[97]\nR. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\nM. Marone, C. Akiki, J. Li, J. Chim et al., “Starcoder: may the source\nbe with you!” arXiv preprint arXiv:2305.06161, 2023.\n[98]\nS. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv,\nL. Cui, O. K. Mohammed, Q. Liu et al., “Language is not all you\nneed: Aligning perception with language models,” arXiv preprint\narXiv:2302.14045, 2023.\n[99]\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,\nJ. Schalkwyk, A. M. Dai, A. Hauth et al., “Gemini: a family of highly\ncapable multimodal models,” arXiv preprint arXiv:2312.11805, 2023.\n[100]\nW. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\nJ. Tompson, I. Mordatch, Y. Chebotar et al., “Inner monologue:\nEmbodied reasoning through planning with language models,” arXiv\npreprint arXiv:2207.05608, 2022.\n[101]\nS. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti\net al., “Using deepspeed and megatron to train megatron-turing\nnlg 530b, a large-scale generative language model,” arXiv preprint\narXiv:2201.11990, 2022.\n[102]\nI. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\ndocument transformer,” arXiv preprint arXiv:2004.05150, 2020.\n[103]\nS. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-\nter, T. Wang, Q. Liu, P. S. Koura et al., “Opt-iml: Scaling language\nmodel instruction meta learning through the lens of generalization,”\narXiv preprint arXiv:2212.12017, 2022.\n[104]\nY. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma,\nand F. Wei, “Language models are general-purpose interfaces,” arXiv\npreprint arXiv:2206.06336, 2022.\n[105]\nZ. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang,\nand C. Gan, “Principle-driven self-alignment of language mod-\nels from scratch with minimal human supervision,” arXiv preprint\narXiv:2305.03047, 2023.\n[106]\nW. E. team, “Palmyra-base Parameter Autoregressive Language\nModel,” https://dev.writer.com, 2023.\n[107]\n——, “Camel-5b instructgpt,” https://dev.writer.com, 2023.\n[108]\nYandex.\nYalm.\n[Online].\nAvailable:\nhttps://github.com/yandex/\nYaLM-100B\n[109]\nM. Team et al., “Introducing mpt-7b: a new standard for open-source,\ncommercially usable llms,” 2023.\n[110]\nA. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal,\nX. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi,\nG. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, “Orca 2:\nTeaching small language models how to reason,” 2023.\n[111]\nL. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and\nG. Neubig, “Pal: Program-aided language models,” in International\nConference on Machine Learning.\nPMLR, 2023, pp. 10 764–10 799.\n[112]\nAnthropic. claude. [Online]. Available: https://www.anthropic.com/\nnews/introducing-claude\n[113]\nE. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou,\n“Codegen2: Lessons for training llms on programming and natural\nlanguages,” arXiv preprint arXiv:2305.02309, 2023.\n[114]\nL. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada,\nS. Huang, L. von Werra, C. Fourrier, N. Habib et al., “Zephyr: Direct\ndistillation of lm alignment,” arXiv preprint arXiv:2310.16944, 2023.\n[115]\nX. team. Grok. [Online]. Available: https://grok.x.ai/\n[116]\nJ. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou,\nand J. Zhou, “Qwen-vl: A frontier large vision-language model with\nversatile abilities,” arXiv preprint arXiv:2308.12966, 2023.\n[117]\nmixtral.\nmixtral.\n[Online].\nAvailable:\nhttps://mistral.ai/news/\nmixtral-of-experts/\n[118]\nD. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y. Pei,\nA. Nourbakhsh, and X. Liu, “Docllm: A layout-aware generative\nlanguage model for multimodal document understanding,” 2023.\n[119]\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,\nY. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder:\nWhen the large language model meets programming – the rise of code\nintelligence,” 2024.\n[120]\nF. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, “Knowledge\nfusion of large language models,” 2024.\n[121]\nP. Zhang, G. Zeng, T. Wang, and W. Lu, “Tinyllama: An open-source\nsmall language model,” 2024.\n[122]\nC. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan,\n“Llama pro: Progressive llama with block expansion,” 2024.\n[123]\nX. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and\nM. Kazi, “Transformer models: an introduction and catalog,” 2023.\n[124]\nG. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\nH. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The refined-\nweb dataset for falcon llm: outperforming curated corpora with web\ndata, and web data only,” arXiv preprint arXiv:2306.01116, 2023.\n[125]\nD. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-\nShowk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al.,\n“Scaling laws and interpretability of learning from repeated data,”\narXiv preprint arXiv:2205.10487, 2022.\n[126]\nP. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\nposition representations,” arXiv preprint arXiv:1803.02155, 2018.\n[127]\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, “Roformer: En-\n",
        "word_count": 1303,
        "char_count": 8318,
        "fonts": [
          "NimbusRomNo9L-Regu (8.0pt)",
          "NimbusRomNo9L-ReguItal (8.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 40,
        "text": "hanced transformer with rotary position embedding,” arXiv preprint\narXiv:2104.09864, 2021.\n[128]\nO. Press, N. A. Smith, and M. Lewis, “Train short, test long: Attention\nwith linear biases enables input length extrapolation,” arXiv preprint\narXiv:2108.12409, 2021.\n[129]\nG. Ke, D. He, and T.-Y. Liu, “Rethinking positional encoding in\nlanguage pre-training,” arXiv preprint arXiv:2006.15595, 2020.\n[130]\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017.\n[131]\nW. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling\nto trillion parameter models with simple and efficient sparsity,” The\nJournal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270,\n2022.\n[132]\nR. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\n“Parameter-efficient multi-task fine-tuning for transformers via shared\nhypernetworks,” 2021.\n[133]\nS. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\nT. Zhang, F. Wu, and G. Wang, “Instruction tuning for large language\nmodels: A survey,” 2023.\n[134]\nS. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross-task\ngeneralization via natural language crowdsourcing instructions,” arXiv\npreprint arXiv:2104.08773, 2021.\n[135]\nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,\nand H. Hajishirzi, “Self-instruct: Aligning language model with self\ngenerated instructions,” arXiv preprint arXiv:2212.10560, 2022.\n[136]\nK. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online].\nAvailable: https://github.com/ContextualAI/HALOs/blob/main/assets/\nreport.pdf\n[137]\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and\nD. Amodei, “Deep reinforcement learning from human preferences,”\nAdvances in neural information processing systems, vol. 30, 2017.\n[138]\nH. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Car-\nbune, and A. Rastogi, “Rlaif: Scaling reinforcement learning from\nhuman feedback with ai feedback,” arXiv preprint arXiv:2309.00267,\n2023.\n[139]\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and\nC. Finn, “Direct preference optimization: Your language model is\nsecretly a reward model,” arXiv preprint arXiv:2305.18290, 2023.\n[140]\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memory\noptimizations toward training trillion parameter models,” in SC20: In-\nternational Conference for High Performance Computing, Networking,\nStorage and Analysis.\nIEEE, 2020, pp. 1–16.\n[141]\nB. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,\nX. Cheng, M. Chung, M. Grella, K. K. GV et al., “Rwkv: Reinventing\nrnns for the transformer era,” arXiv preprint arXiv:2305.13048, 2023.\n[142]\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\nand W. Chen, “Lora: Low-rank adaptation of large language models,”\narXiv preprint arXiv:2106.09685, 2021.\n[143]\nG. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\nneural network,” arXiv preprint arXiv:1503.02531, 2015.\n[144]\nJ. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation:\nA survey,” International Journal of Computer Vision, vol. 129, pp.\n1789–1819, 2021.\n[145]\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J.\nBang, A. Madotto, and P. Fung, “Survey of hallucination in natural\nlanguage generation,” ACM Comput. Surv., vol. 55, no. 12, mar 2023.\n[Online]. Available: https://doi.org/10.1145/3571730\n[146]\nN. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and\nM. Steedman, “Sources of hallucination by large language models on\ninference tasks,” 2023.\n[147]\nC.-Y.\nLin,\n“ROUGE:\nA\npackage\nfor\nautomatic\nevaluation\nof\nsummaries,” in Text Summarization Branches Out.\nBarcelona, Spain:\nAssociation for Computational Linguistics, Jul. 2004, pp. 74–81.\n[Online]. Available: https://aclanthology.org/W04-1013\n[148]\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\nautomatic evaluation of machine translation,” in Proceedings of the\n40th Annual Meeting of the Association for Computational Linguistics,\nP. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania,\nUSA: Association for Computational Linguistics, Jul. 2002, pp. 311–\n318. [Online]. Available: https://aclanthology.org/P02-1040\n[149]\nB. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and\nW. Cohen, “Handling divergent reference texts when evaluating\ntable-to-text generation,” in Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics, A. Korhonen,\nD. Traum, and L. M`arquez, Eds.\nFlorence, Italy: Association\nfor Computational Linguistics, Jul. 2019, pp. 4884–4895. [Online].\nAvailable: https://aclanthology.org/P19-1483\n[150]\nZ. Wang, X. Wang, B. An, D. Yu, and C. Chen, “Towards faithful\nneural table-to-text generation with content-matching constraints,”\nin Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter,\nand J. Tetreault, Eds.\nOnline: Association for Computational\nLinguistics, Jul. 2020, pp. 1072–1086. [Online]. Available: https:\n//aclanthology.org/2020.acl-main.101\n[151]\nH. Song, W.-N. Zhang, J. Hu, and T. Liu, “Generating persona consis-\ntent dialogues by exploiting natural language inference,” Proceedings\nof the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, pp.\n8878–8885, Apr. 2020.\n[152]\nO. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor,\nand O. Abend, “q2: Evaluating factual consistency in knowledge-\ngrounded dialogues via question generation and question answering,”\nin Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, M.-F. Moens, X. Huang, L. Specia,\nand S. W.-t. Yih, Eds.\nOnline and Punta Cana, Dominican Republic:\nAssociation for Computational Linguistics, Nov. 2021, pp. 7856–7870.\n[Online]. Available: https://aclanthology.org/2021.emnlp-main.619\n[153]\nN. Dziri, H. Rashkin, T. Linzen, and D. Reitter, “Evaluating attribution\nin dialogue systems: The BEGIN benchmark,” Transactions of the\nAssociation for Computational Linguistics, vol. 10, pp. 1066–1083,\n2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.62\n[154]\nS. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim,\nY. Liu, and D. Z. Hakkani-T¨ur, “Rome was built in 1776: A case study\non factual correctness in knowledge-grounded response generation,”\nArXiv, vol. abs/2110.05456, 2021.\n[155]\nS. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer,\nL. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomic\nevaluation of factual precision in long form text generation,” 2023.\n[156]\nD. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,\nV. Chaudhary, and M. Young, “Machine learning: The high interest\ncredit card of technical debt,” in SE4ML: Software Engineering for\nMachine Learning (NIPS 2014 Workshop), 2014.\n[157]\nZ. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought\nprompting in large language models,” 2022.\n[158]\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving with\nlarge language models,” 2023.\n[159]\nP. Manakul, A. Liusie, and M. J. F. Gales, “Selfcheckgpt: Zero-\nresource black-box hallucination detection for generative large lan-\nguage models,” 2023.\n[160]\nN. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan,\nand S. Yao, “Reflexion: Language agents with verbal reinforcement\nlearning,” 2023.\n[161]\nS. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang,\nK. Tyser, Z. Chin, Y. Hicke, N. Singh, M. Udell, Y. Kim, T. Buonassisi,\nA. Solar-Lezama, and I. Drori, “Exploring the mit mathematics and\neecs curriculum using large language models,” 2023.\n[162]\nT. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J.\nCai, “Promptchainer: Chaining large language model prompts through\nvisual programming,” 2022.\n[163]\nY. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and\nJ. Ba, “Large language models are human-level prompt engineers,”\n2023.\n[164]\nP. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin,\nN. Goyal, H. K¨uttler, M. Lewis, W. Yih, T. Rockt¨aschel, S. Riedel, and\nD. Kiela, “Retrieval-augmented generation for knowledge-intensive\nNLP tasks,” CoRR, vol. abs/2005.11401, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2005.11401\n[165]\nY. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and\n",
        "word_count": 1231,
        "char_count": 8453,
        "fonts": [
          "CMMI8 (8.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)",
          "CMR6 (6.0pt)",
          "NimbusRomNo9L-ReguItal (8.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 41,
        "text": "H. Wang, “Retrieval-augmented generation for large language models:\nA survey,” arXiv preprint arXiv:2312.10997, 2023.\n[166]\nA. W. Services. (Year of publication, e.g., 2023) Question answering\nusing retrieval augmented generation with foundation models in\namazon\nsagemaker\njumpstart.\nAccessed:\nDate\nof\naccess,\ne.g.,\nDecember 5, 2023. [Online]. Available: https://shorturl.at/dSV47\n[167]\nS. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying large\nlanguage models and knowledge graphs: A roadmap,” arXiv preprint\narXiv:2306.08302, 2023.\n[168]\nZ. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\n2023.\n[169]\nT. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettle-\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\ncan teach themselves to use tools,” 2023.\n[170]\nB. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer,\nand M. T. Ribeiro, “Art: Automatic multi-step reasoning and tool-use\nfor large language models,” 2023.\n[171]\nY. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface,” arXiv\npreprint arXiv:2303.17580, 2023.\n[172]\nZ. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,\nS. Jin, E. Zhou et al., “The rise and potential of large language model\nbased agents: A survey,” arXiv preprint arXiv:2309.07864, 2023.\n[173]\nL. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\nJ. Tang, X. Chen, Y. Lin et al., “A survey on large language model\nbased autonomous agents,” arXiv preprint arXiv:2308.11432, 2023.\n[174]\nZ. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar,\nR. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-\nFei, and J. Gao, “Agent ai: Surveying the horizons of multimodal\ninteraction,” arXiv preprint arXiv:2401.03568, 2024.\n[175]\nB. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, “Rewoo:\nDecoupling reasoning from observations for efficient augmented lan-\nguage models,” 2023.\n[176]\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\n“React: Synergizing reasoning and acting in language models,” 2023.\n[177]\nV. Nair, E. Schumacher, G. Tso, and A. Kannan, “Dera: Enhanc-\ning large language model completions with dialog-enabled resolving\nagents,” 2023.\n[178]\nY. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\nC. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang,\nand X. Xie, “A survey on evaluation of large language models,” 2023.\n[179]\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit,\nQ.\nLe,\nand\nS.\nPetrov,\n“Natural\nquestions:\nA\nbenchmark\nfor\nquestion answering research,” Transactions of the Association for\nComputational Linguistics, vol. 7, pp. 452–466, 2019. [Online].\nAvailable: https://aclanthology.org/Q19-1026\n[180]\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\nJ. Steinhardt, “Measuring massive multitask language understanding,”\n2021.\n[181]\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\nE. Jiang, C. Cai, M. Terry, Q. Le et al., “Program synthesis with large\nlanguage models,” arXiv preprint arXiv:2108.07732, 2021.\n[182]\nE. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang,\nand L. Zettlemoyer, “QuAC: Question answering in context,” in\nProceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, E. Riloff, D. Chiang, J. Hockenmaier, and\nJ. Tsujii, Eds.\nBrussels, Belgium: Association for Computational\nLinguistics, Oct.-Nov. 2018, pp. 2174–2184. [Online]. Available:\nhttps://aclanthology.org/D18-1241\n[183]\nD. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\nC. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, “Measuring\ncoding challenge competence with apps,” NeurIPS, 2021.\n[184]\nV. Zhong, C. Xiong, and R. Socher, “Seq2sql: Generating structured\nqueries from natural language using reinforcement learning,” arXiv\npreprint arXiv:1709.00103, 2017.\n[185]\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\nA large scale distantly supervised challenge dataset for reading\ncomprehension,” in Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\nR. Barzilay and M.-Y. Kan, Eds.\nVancouver, Canada: Association\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online].\nAvailable: https://aclanthology.org/P17-1147\n[186]\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “RACE: Large-scale\nReAding comprehension dataset from examinations,” in Proceedings\nof the 2017 Conference on Empirical Methods in Natural Language\nProcessing, M. Palmer, R. Hwa, and S. Riedel, Eds.\nCopenhagen,\nDenmark: Association for Computational Linguistics, Sep. 2017, pp.\n785–794. [Online]. Available: https://aclanthology.org/D17-1082\n[187]\nP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000+\nquestions for machine comprehension of text,” in Proceedings of\nthe 2016 Conference on Empirical Methods in Natural Language\nProcessing, J. Su, K. Duh, and X. Carreras, Eds.\nAustin, Texas:\nAssociation for Computational Linguistics, Nov. 2016, pp. 2383–2392.\n[Online]. Available: https://aclanthology.org/D16-1264\n[188]\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and\nK. Toutanova, “Boolq: Exploring the surprising difficulty of natural\nyes/no\nquestions,”\nCoRR,\nvol.\nabs/1905.10044,\n2019.\n[Online].\nAvailable: http://arxiv.org/abs/1905.10044\n[189]\nD. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth,\n“Looking beyond the surface:a challenge set for reading compre-\nhension over multiple sentences,” in Proceedings of North American\nChapter of the Association for Computational Linguistics (NAACL),\n2018.\n[190]\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and\nJ. Schulman, “Training verifiers to solve math word problems,”\nCoRR,\nvol.\nabs/2110.14168,\n2021.\n[Online].\nAvailable:\nhttps:\n//arxiv.org/abs/2110.14168\n[191]\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\nD. Song, and J. Steinhardt, “Measuring mathematical problem solving\nwith the MATH dataset,” CoRR, vol. abs/2103.03874, 2021. [Online].\nAvailable: https://arxiv.org/abs/2103.03874\n[192]\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag:\nCan a machine really finish your sentence?” 2019.\n[193]\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\nand O. Tafjord, “Think you have solved question answering? try\narc, the AI2 reasoning challenge,” CoRR, vol. abs/1803.05457, 2018.\n[Online]. Available: http://arxiv.org/abs/1803.05457\n[194]\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA:\nreasoning about physical commonsense in natural language,” CoRR,\nvol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/\n1911.11641\n[195]\nM. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, “Socialiqa:\nCommonsense reasoning about social interactions,” CoRR, vol.\nabs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904.\n09728\n[196]\nT. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of\narmor conduct electricity? A new dataset for open book question\nanswering,” CoRR, vol. abs/1809.02789, 2018. [Online]. Available:\nhttp://arxiv.org/abs/1809.02789\n[197]\nS. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models\nmimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021.\n[198]\nZ. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov,\nand C. D. Manning, “Hotpotqa: A dataset for diverse, explainable\nmulti-hop question answering,” CoRR, vol. abs/1809.09600, 2018.\n[Online]. Available: http://arxiv.org/abs/1809.09600\n[199]\nY. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: A\ndataset for llm question answering with external tools,” arXiv preprint\narXiv:2306.13304, 2023.\n[200]\nD. Chen, J. Bolton, and C. D. Manning, “A thorough examination\nof the cnn/daily mail reading comprehension task,” in Association for\nComputational Linguistics (ACL), 2016.\n[201]\nR. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al., “Abstractive text\nsummarization using sequence-to-sequence rnns and beyond,” arXiv\npreprint arXiv:1602.06023, 2016.\n[202]\nY. Bai and D. Z. Wang, “More than reading comprehension: A survey\n",
        "word_count": 1240,
        "char_count": 8429,
        "fonts": [
          "NimbusRomNo9L-Regu (8.0pt)",
          "NimbusRomNo9L-ReguItal (8.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 42,
        "text": "on datasets and metrics of textual question answering,” arXiv preprint\narXiv:2109.12264, 2021.\n[203]\nH.-Y. Huang, E. Choi, and W.-t. Yih, “Flowqa: Grasping flow in\nhistory for conversational machine comprehension,” arXiv preprint\narXiv:1810.06683, 2018.\n[204]\nS. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, “A\nsurvey on evaluation metrics for machine translation,” Mathematics,\nvol. 11, no. 4, p. 1006, 2023.\n[205]\nJ. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval:\nA large-scale hallucination evaluation benchmark for large language\nmodels,” in Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, 2023, pp. 6449–6464.\n[206]\nSimon\nMark\nHughes,\n“Hughes\nhallucination\nevaluation\nmodel\n(hhem)\nleaderboard,”\n2024,\nhttps://huggingface.co/spaces/vectara/\nHallucination-evaluation-leaderboard, Last accessed on 2024-01-21.\n[207]\nJ. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and\nR. McHardy, “Challenges and applications of large language models,”\narXiv preprint arXiv:2307.10169, 2023.\n[208]\nS. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno,\nS. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al.,\n“Textbooks are all you need,” arXiv preprint arXiv:2306.11644, 2023.\n[209]\nY. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T.\nLee, “Textbooks are all you need ii: phi-1.5 technical report,” arXiv\npreprint arXiv:2309.05463, 2023.\n[210]\nM. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus,\nY. Bengio, S. Ermon, and C. R´e, “Hyena hierarchy: Towards larger\nconvolutional language models,” 2023.\n[211]\nM. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and\nA. Thomas, “StripedHyena: Moving Beyond Transformers with\nHybrid Signal Processing Models,” 12 2023. [Online]. Available:\nhttps://github.com/togethercomputer/stripedhyena\n[212]\nD. Y. Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas,\nB. Spector, M. Poli, A. Rudra, and C. R´e, “Monarch mixer: A simple\nsub-quadratic gemm-based architecture,” 2023.\n[213]\nG. J. McLachlan, S. X. Lee, and S. I. Rathnayake, “Finite mixture\nmodels,” Annual review of statistics and its application, vol. 6, pp.\n355–378, 2019.\n[214]\nH. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” arXiv\npreprint arXiv:2304.08485, 2023.\n[215]\nS. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou,\nJ. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-plus:\nLearning to use tools for creating multimodal agents,” arXiv preprint\narXiv:2311.05437, 2023.\n[216]\nS. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-any\nmultimodal llm,” arXiv preprint arXiv:2309.05519, 2023.\n[217]\nN. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and\nD. Z¨uhlke, “Convgenvismo: Evaluation of conversational generative\nvision models,” 2023.\n[218]\nN. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman,\nI. Harper, A. Marginean, S. Sengupta, and E. Wang, “Automated unit\ntest improvement using large language models at meta,” arXiv preprint\narXiv:2402.09171, 2024.\n[219]\nL. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang,\nW. Lyu, Y. Zhang, X. Li et al., “Trustllm: Trustworthiness in large\nlanguage models,” arXiv preprint arXiv:2401.05561, 2024.\n[220]\nM. Josifoski, L. Klein, M. Peyrard, Y. Li, S. Geng, J. P. Schnitzler,\nY. Yao, J. Wei, D. Paul, and R. West, “Flows: Building blocks of\nreasoning and collaborating ai,” arXiv preprint arXiv:2308.01285,\n2023.\n[221]\nMicrosoft.\nDeepspeed.\n[Online].\nAvailable:\nhttps://github.com/\nmicrosoft/DeepSpeed\n[222]\nHuggingFace. Transformers. [Online]. Available: https://github.com/\nhuggingface/transformers\n[223]\nNvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/\nMegatron-LM\n[224]\nBMTrain. Bmtrain. [Online]. Available: https://github.com/OpenBMB/\nBMTrain\n[225]\nEleutherAI.\ngpt-neox.\n[Online].\nAvailable:\nhttps://github.com/\nEleutherAI/gpt-neox\n[226]\nmicrosoft. Lora. [Online]. Available: https://github.com/microsoft/\nLoRA\n[227]\nColossalAI.\nColossalai.\n[Online].\nAvailable:\nhttps://github.com/\nhpcaitech/ColossalAI\n[228]\nFastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/\nFastChat\n[229]\nskypilot. skypilot. [Online]. Available: https://github.com/skypilot-org/\nskypilot\n[230]\nvllm. vllm. [Online]. Available: https://github.com/vllm-project/vllm\n[231]\nhuggingface. text-generation-inference. [Online]. Available: https:\n//github.com/huggingface/text-generation-inference\n[232]\nlangchain.\nlangchain.\n[Online].\nAvailable:\nhttps://github.com/\nlangchain-ai/langchain\n[233]\nbentoml. Openllm. [Online]. Available: https://github.com/bentoml/\nOpenLLM\n[234]\nembedchain. embedchain. [Online]. Available: https://github.com/\nembedchain/embedchain\n[235]\nmicrosoft. autogen. [Online]. Available: https://github.com/microsoft/\nautogen\n[236]\nbabyagi.\nbabyagi.\n[Online].\nAvailable:\nhttps://github.com/\nyoheinakajima/babyagi\n[237]\nguidance.\nguidance.\n[Online].\nAvailable:\nhttps://github.com/\nguidance-ai/guidance\n[238]\nprompttools. prompttools. [Online]. Available: https://github.com/\nhegelai/prompttools\n[239]\npromptfoo.\npromptfoo.\n[Online].\nAvailable:\nhttps://github.com/\npromptfoo/promptfoo\n[240]\nfacebook.\nfaiss.\n[Online].\nAvailable:\nhttps://github.com/\nfacebookresearch/faiss\n[241]\nmilvus. milvus. [Online]. Available: https://github.com/milvus-io/\nmilvus\n[242]\nqdrant. qdrant. [Online]. Available: https://github.com/qdrant/qdrant\n[243]\nweaviate. weaviate. [Online]. Available: https://github.com/weaviate/\nweaviate\n[244]\nllama index. llama-index. [Online]. Available: https://github.com/\nrun-llama/llama index\nAPPENDIX\n1. Open Source Toolkits For LLM Development and\nDeployment\nThere are various frameworks and libraries developed for\nLLM training, evaluation, and deployment, and covering every\nsingle framework is out of this paper’s scope. But we try to\nprovide a brief introduction of some of the most popular ones,\ngrouped into different categories.\nA. LLM Training/Inference Frameworks\nSome of the popular frameworks which are useful for LLM\ntraining includes (note that some of them can be used beyond\nLLM training too):\nDeepSpeed [221] is a deep learning optimization library\nthat makes distributed training and inference easy, efficient,\nand effective. DeepSpeed enables world’s most powerful lan-\nguage models like MT-530B and BLOOM. It is an easy-\nto-use deep learning optimization software suite that powers\nunprecedented scale and speed for both training and inference.\nWith DeepSpeed you can:\nTransformers [222] is library by HuggingFace which\nprovides thousands of pretrained models to perform tasks on\ndifferent modalities such as text, vision, and audio. Using\npretrained models one can reduce compute costs, carbon\n",
        "word_count": 867,
        "char_count": 6710,
        "fonts": [
          "NimbusRomNo9L-ReguItal (10.0pt)",
          "NimbusRomNo9L-ReguItal (8.0pt)",
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (8.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 43,
        "text": "footprint, and save the time and resources required to train\na model from scratch.\nMegatron-LM [223] is a large, powerful transformer\ndeveloped by the Applied Deep Learning Research team\nat NVIDIA. It contains efficient, model-parallel (tensor, se-\nquence, and pipeline), and multi-node pre-training of trans-\nformer based models such as GPT, BERT, and T5 using mixed\nprecision.\nBMTrain [224] is an efficient large model training toolkit\nthat can be used to train large models with tens of billions of\nparameters. It can train models in a distributed manner while\nkeeping the code as simple as stand-alone training.\nGPT-NeoX [225] leverages many of the same features and\ntechnologies as the popular Megatron-DeepSpeed library but\nwith substantially increased usability and novel optimizations.\nLoRA [226] library provides the support for Low-Rank\nAdaptation of Large Language Models. It reduces the number\nof trainable parameters by learning pairs of rank-decompostion\nmatrices while freezing the original weights. This vastly\nreduces the storage requirement for large language models\nadapted to specific tasks and enables efficient task-switching\nduring deployment all without introducing inference latency.\nLoRA also outperforms several other adaptation methods in-\ncluding adapter, prefix-tuning, and fine-tuning.\nColossalAI library [227] provides a collection of parallel\ncomponents. It aims to support developers to write their\ndistributed deep learning models just like how they write their\nmodel on their laptop. They provide user-friendly tools to\nkickstart distributed training and inference in a few lines. In\nterms of Parallelism strategies, they support: Data Parallelism,\nPipeline Parallelism, Sequence Parallelism, Zero Redundancy\nOptimizer (ZeRO) [140], and Auto-Parallelism.\nB. Deployment Tools\nWe provide an overview of some of the most popular LLM\ndeployment tools here.\nFastChat [228] is an open platform for training, serv-\ning, and evaluating large language model based chatbots.\nFastChat’s core features include: The training and evaluation\ncode for state-of-the-art models (e.g., Vicuna, MT-Bench), and\na distributed multi-model serving system with web UI and\nOpenAI-compatible RESTful APIs.\nSkypilot [229] is a framework for running LLMs, AI,\nand batch jobs on any cloud, offering maximum cost savings,\nhighest GPU availability, and managed execution.\nvLLM [230] is a fast and easy-to-use library for LLM in-\nference and serving. vLLM seamlessly supports many Hugging\nFace models, including the following architectures: Aquila,\nBaichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT Big-\nCode, LLaMA, LLaMA 2, Mistral, Mixtral, MPT, OPT, Qwen,\nYi, and many more.\ntext-generation-inference [231] is a toolkit for deploying\nand serving Large Language Models (LLMs). TGI enables\nhigh-performance text generation for the most popular open-\nsource LLMs, including Llama, Falcon, StarCoder, BLOOM,\nGPT-NeoX, and more.\nLangChain [232] is a framework for developing applica-\ntions powered by language models. It enables applications that:\n•\nAre context-aware: connect a language model to\nsources of context (prompt instructions, few shot ex-\namples, content to ground its response in, etc.)\n•\nReason: rely on a language model to reason (about\nhow to answer based on provided context, what ac-\ntions to take, etc.)\nOpenLLM [233] is an open-source platform designed to\nfacilitate the deployment and operation of large language mod-\nels (LLMs) in real-world applications. With OpenLLM, you\ncan run inference on any open-source LLM, deploy them on\nthe cloud or on-premises, and build powerful AI applications.\nEmbedchain [234] is an Open Source RAG Framework\nthat makes it easy to create and deploy AI apps. Embedchain\nstreamlines the creation of RAG applications, offering a seam-\nless process for managing various types of unstructured data.\nIt efficiently segments data into manageable chunks, generates\nrelevant embeddings, and stores them in a vector database for\noptimized retrieval.\nAutogen [235] is a framework that enables the devel-\nopment of LLM applications using multiple agents that can\nconverse with each other to solve tasks. AutoGen agents\nare customizable, conversable, and seamlessly allow human\nparticipation. They can operate in various modes that employ\ncombinations of LLMs, human inputs, and tools.\nBabyAGI [236] is an autonomous Artificial Intelligence\nagent, that is designed to generate and execute tasks based on\ngiven objectives. It harnesses cutting-edge technologies from\nOpenAI, Pinecone, LangChain, and Chroma to automate tasks\nand achieve specific goals. In this blog post, we will dive\ninto the unique features of BabyAGI and explore how it can\nstreamline task automation.\nC. Prompting Libraries\nGuidance [237] is a programming paradigm that offers\nsuperior control and efficiency compared to conventional\nprompting and chaining. It allows users to constrain generation\n(e.g. with regex and CFGs) as well as to interleave control\n(conditional, loops) and generation seamlessly.\nPromptTools [238] offers a set of open-source, self-\nhostable tools for experimenting with, testing, and evaluating\nLLMs, vector databases, and prompts. The core idea is to\nenable developers to evaluate using familiar interfaces like\ncode, notebooks, and a local playground.\nPromptBench [?] is a Pytorch-based Python package for\nEvaluation of Large Language Models (LLMs). It provides\nuser-friendly APIs for researchers to conduct evaluation on\nLLMs.\nPromptfoo [239] is a tool for testing and evaluating LLM\noutput quality. It systematically test prompts, models, and\nRAGs with predefined test cases.\n",
        "word_count": 824,
        "char_count": 5617,
        "fonts": [
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "CMSY10 (10.0pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      },
      {
        "page_number": 44,
        "text": "D. VectorDB\nFaiss [240] is a library developed by Facebook AI Re-\nsearch that provides efficient similarity search and clustering\nof dense vectors. It is designed for use with large-scale,\nhigh-dimensional data and supports several index types and\nalgorithms for various use cases.\nMilvus [241] is an open-source vector database built to\npower embedding similarity search and AI applications. Mil-\nvus makes unstructured data search more accessible, and pro-\nvides a consistent user experience regardless of the deployment\nenvironment.\nQdrant [242] is a vector similarity search engine and\nvector database. It provides a production-ready service with a\nconvenient API to store, search, and manage points—vectors\nwith an additional payload Qdrant is tailored to extended\nfiltering support. environment.\nWeaviate [243] is an open-source, GraphQL-based vec-\ntor search engine that enables similarity search on high-\ndimensional data. While it is open-source, the commercial ver-\nsion offers additional features, support, and managed services.\nSome of the other popular options includes LlamaIndex\n[244] and Pinecone.\n",
        "word_count": 161,
        "char_count": 1114,
        "fonts": [
          "NimbusRomNo9L-Medi (10.0pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-ReguItal (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          612.0,
          792.0
        ]
      }
    ]
  }
}