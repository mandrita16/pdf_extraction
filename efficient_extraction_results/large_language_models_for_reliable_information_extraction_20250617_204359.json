{
  "file_path": "..\\test_pdfs\\large_language_models_for_reliable_information_extraction.pdf",
  "file_hash": "dc67a2db5334b143e89914e41422a91a",
  "timestamp": "2025-06-17T20:43:59.129626",
  "page_count": 76,
  "total_words": 21498,
  "total_chars": 136405,
  "fonts_used": [
    "Arial-BoldMT (8.7pt)",
    "Arial-ItalicMT (6.5pt)",
    "ArialMT (4.9pt)",
    "ArialMT (5.2pt)",
    "ArialMT (6.3pt)",
    "ArialMT (6.5pt)",
    "ArialMT (6.6pt)",
    "ArialMT (7.0pt)",
    "ArialMT (8.7pt)",
    "ArialMT (8.8pt)",
    "ArialMT (9.4pt)",
    "CMEX10 (12.0pt)",
    "CMMI10 (12.0pt)",
    "CMR10 (12.0pt)",
    "CMR10 (9.0pt)",
    "CMSY10 (12.0pt)",
    "CMSY10 (9.0pt)",
    "DejaVuSans (10.6pt)",
    "DejaVuSans (11.4pt)",
    "DejaVuSans (4.5pt)",
    "DejaVuSans (5.4pt)",
    "DejaVuSans (5.5pt)",
    "DejaVuSans (5.7pt)",
    "DejaVuSans (5.8pt)",
    "DejaVuSans (5.9pt)",
    "DejaVuSans (6.1pt)",
    "DejaVuSans (6.2pt)",
    "DejaVuSans (6.3pt)",
    "DejaVuSans (6.6pt)",
    "DejaVuSans (6.8pt)",
    "DejaVuSans (6.9pt)",
    "DejaVuSans (7.1pt)",
    "DejaVuSans (7.3pt)",
    "DejaVuSans (7.7pt)",
    "DejaVuSans (9.1pt)",
    "MSBM10 (12.0pt)",
    "NimbusRomNo9L-Medi (11.8pt)",
    "NimbusRomNo9L-Medi (11.9pt)",
    "NimbusRomNo9L-Medi (12.0pt)",
    "NimbusRomNo9L-Medi (12.1pt)",
    "NimbusRomNo9L-Medi (14.3pt)",
    "NimbusRomNo9L-Medi (17.2pt)",
    "NimbusRomNo9L-Medi (24.8pt)",
    "NimbusRomNo9L-Regu (10.0pt)",
    "NimbusRomNo9L-Regu (10.1pt)",
    "NimbusRomNo9L-Regu (11.8pt)",
    "NimbusRomNo9L-Regu (11.9pt)",
    "NimbusRomNo9L-Regu (12.0pt)",
    "NimbusRomNo9L-Regu (12.1pt)",
    "NimbusRomNo9L-Regu (14.3pt)",
    "NimbusRomNo9L-Regu (7.4pt)",
    "NimbusRomNo9L-Regu (9.0pt)",
    "NimbusRomNo9L-ReguItal (11.8pt)",
    "NimbusRomNo9L-ReguItal (11.9pt)",
    "NimbusRomNo9L-ReguItal (12.0pt)",
    "NimbusRomNo9L-ReguItal (12.1pt)",
    "NimbusRomNo9L-ReguItal (14.3pt)",
    "NimbusRomNo9L-ReguItal (9.0pt)",
    "SFTT1200 (12.0pt)",
    "StandardSymL (12.0pt)",
    "StandardSymL (17.2pt)",
    "StandardSymL-Slant_167 (12.0pt)"
  ],
  "images_count": 2,
  "metadata": {
    "format": "PDF 1.5",
    "title": "Large Language Models for Reliable Information Extraction",
    "author": "Lukas Baliunas",
    "subject": "LaTeX",
    "keywords": "LaTeX Master Thesis Engineering University of Cambridge",
    "creator": "LaTeX with hyperref",
    "producer": "pdfTeX-1.40.24",
    "creationDate": "D:20230817110817Z",
    "modDate": "D:20230817110817Z"
  },
  "pages": [
    {
      "page_number": 1,
      "text": "Large Language Models for Reliable\nInformation Extraction\nLukas Baliunas\nDepartment of Engineering\nUniversity of Cambridge\nThis dissertation is submitted for the degree of\nMaster of Philosophy in Machine Learning and Machine Intelligence\nChurchill College\nAugust 2023\n",
      "word_count": 36,
      "char_count": 268,
      "fonts": [
        "NimbusRomNo9L-ReguItal (14.3pt)",
        "NimbusRomNo9L-Regu (14.3pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Medi (24.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 2,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 3,
      "text": "Declaration\nI, Lukas Baliunas, of Churchill College, being a candidate for the MPhil in Machine Learning\nand Machine Intelligence, hereby declare that this report and the work described in it are my\nown work, unaided except as may be specified below, and that the report does not contain\nmaterial that has already been used to any substantial extent for a comparable purpose.\nAll of the software in this project was developed using Python, and can be found in a\npublic GitHub repository1. This project extended the ELICIT library2 to make use of Large\nLanguage Models and perform speaker-centric information extraction. The Large Language\nModels, used in this project, were retrieved through the Huggingface transformers3 library.\nThe model used in this project, Vicuna4 — a fine-tuned LLaMA model — is a publicly\navailable model with its use subject to the model license of LLaMA. The use of LLaMA was\napproved by Meta AI after filling out a request form. Low-Rank Adaption fine-tuning was\ndone by modifying the code in the Alpaca-LoRA repository5, which is available under an\nApache-2.0 license, and uses the PyTorch6 and transformers libraries. The embedding\nmodel used in the project, ’all-mpnet-base-v2’, came from the SentenceTransformers7\nlibrary. Vector index functionality was provided by the LangChain8 and ChromaDB9 Python\nlibraries. OpenAI’s GPT-4 API10 was used for generating synthetic data under a paid license.\nThis dissertation contains 14993 words, excluding declarations, bibliography, pho-\ntographs and diagrams, but including tables, footnotes, figure captions and appendices.\nLukas Baliunas\nAugust 2023\n1https://github.com/lbaliunas/elicit/tree/lb956-project\n2https://github.com/Bradley-Butcher/elicit\n3https://huggingface.co/docs/transformers\n4https://lmsys.org/blog/2023-03-30-vicuna/\n5https://github.com/tloen/alpaca-lora\n6https://pytorch.org\n7https://www.sbert.net/\n8https://python.langchain.com/\n9https://www.trychroma.com\n10https://openai.com/gpt-4\n",
      "word_count": 257,
      "char_count": 1977,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "SFTT1200 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Regu (7.4pt)",
        "NimbusRomNo9L-Regu (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 4,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 5,
      "text": "Acknowledgements\nI wish to express my sincere thanks to my supervisor Dr. Miri Zilka, and Dr. Jiri Hron for\ntheir guidance and invaluable lessons throughout this project.\nI would also like to thank my girlfriend, Ieva, and my family for their support and\ncontinuous encouragement.\n",
      "word_count": 45,
      "char_count": 281,
      "fonts": [
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 6,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 7,
      "text": "Abstract\nThis project addresses the ongoing challenge of achieving reliable Information Extraction\n(IE), particularly in domains which require near-perfect precision. ELICIT (Butcher et al.,\n2023) introduced a novel approach that combines the processing speed of automated IE tools\nwith the precision of manual annotation through a unique approach of weak supervision\nlabeling and human validation. The existing setup achieved impressive results when it came\nto precision, but the recall of the system can still be greatly improved. Recognizing the\npotential of Large Language Models (LLMs) in tasks that require language understanding,\nthis project focuses on extending ELICIT’s capabilities with LLMs and evaluating their\neffectiveness in the context of structured IE. The research is grounded in the legal domain,\nfocusing on evaluations on UK Crown Court sentencing remarks, and court transcripts. The\nlatter usually contains multiple speakers with different narratives, necessitating the extraction\nof reliable information along with speaker attribution. This project builds upon the existing\nsystem, improving it in two signficant ways: firstly, by introducing LLMs as an extraction\ntool, and secondly, by extending the system’s functionality to extract information attributed\nto specific speakers. The integration of LLMs into ELICIT resulted in improved recall\ncompared to the previous version. Moreover, through the utilization of human-validated\nextractions for fine-tuning, the enhanced system showcased improved recall on both already\nextracted documents and unseen ones. The research demonstrates how fine-tuning with\nlimited data leads to enhanced performance, while also requiring fewer extractions to be\nshown to the user, thereby reducing potential manual efforts.\n",
      "word_count": 250,
      "char_count": 1782,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 8,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 9,
      "text": "Table of contents\n1\nIntroduction\n1\n1.1\nContext and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.3\nThesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2\nBackground\n5\n2.1\nInformation Extraction and Question Answering . . . . . . . . . . . . . . .\n5\n2.2\nMulti-Value Extraction and Source Attribution . . . . . . . . . . . . . . . .\n7\n2.3\nLarge Language Models\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3.1\nAuto-regressive Language Models . . . . . . . . . . . . . . . . . .\n8\n2.3.2\nTransformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.3.3\nLLaMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.3.4\nVicuna\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.3.5\nFine-tuning using Low-Rank Adaptation\n. . . . . . . . . . . . . .\n12\n2.4\nELICIT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.4.1\nApproach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.4.2\nLabelling Functions\n. . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3\nMethodology\n17\n3.1\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.1.1\nSentencing Remarks . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.1.2\nCourt Transcripts . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.2\nInformation Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.2.1\nApproach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.2.2\nPassage Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.2.3\nValue Labelling . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.3\nSource Attribution\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.3.1\nSource Attribution as the Final Component . . . . . . . . . . . . .\n25\n",
      "word_count": 726,
      "char_count": 1946,
      "fonts": [
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)",
        "NimbusRomNo9L-Medi (24.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 10,
      "text": "x\nTable of contents\n3.3.2\nSource Attribution in Passage Retrieval . . . . . . . . . . . . . . .\n25\n3.3.3\nSource Attribution in Value Labelling . . . . . . . . . . . . . . . .\n26\n3.3.4\nImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3.4\nFine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.5\nMetrics\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.5.1\nEnd-to-End Metrics . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.5.2\nSupervised Dataset Metrics . . . . . . . . . . . . . . . . . . . . . .\n30\n4\nResults and Discussion\n35\n4.1\nSentencing Remarks Dataset . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.1.1\nEnd-to-end Performance . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.1.2\nFine-tuned Performance on Unseen Documents . . . . . . . . . . .\n37\n4.1.3\nFine-tuning for Recalibration . . . . . . . . . . . . . . . . . . . . .\n42\n4.2\nCourt Transcripts Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n4.2.1\nPerformance on Unseen Documents . . . . . . . . . . . . . . . . .\n43\n4.2.2\nFine-tuning for Recalibration . . . . . . . . . . . . . . . . . . . . .\n47\n4.2.3\nRecall and Shown Extractions . . . . . . . . . . . . . . . . . . . .\n49\n5\nConclusions\n53\n5.1\nProject Recap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.2\nKey Findings\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.3\nLimitations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n5.4\nFuture Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nReferences\n57\nAppendix A Fine-tuning Details\n63\nA.1\nSentencing Remarks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\nA.1.1\nPassage Retriever . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\nA.1.2\nValue Labellers . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nA.2\nCourt Transcripts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\nA.2.1\nPassage Retriever . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\nA.2.2\nValue Labellers . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n",
      "word_count": 776,
      "char_count": 2165,
      "fonts": [
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 11,
      "text": "Chapter 1\nIntroduction\nIn recent years, Large Language Models (LLMs) have demonstrated exceptional performance\nin tasks which require language understanding (Brown et al., 2020). Among the domains\nstanding to benefit from this advancement is Information Extraction (IE), which concerns\nthe extraction of structured information from unstructured textual data. IE finds applications\nin many fields like healthcare (Javaid et al., 2022; Qayyum et al., 2020), finance (Ghoddusi\net al., 2019), and law (Bansal et al., 2019; Carnaz et al., 2020).\nHowever, these domains often involve sensitive data, the use of which has serious\nconsequences, highlighting the importance of IE with near-perfect precision. The legal sector\nfor instance, is especially sensitive to inaccuracies in IE, which could lead to wasted resources,\nunfair legal outcomes, or the loss of public trust. These issues could be addressed by tasking\na human with IE, but such a solution requires significant time and financial resources.\nConversely, automated methods, while demonstrating rapid processing, compromise on\nprecision of the extracted information (Yang et al., 2022).\nAddressing this challenge, the ELICIT framework, introduced by Butcher et al. (2023),\noffers a novel approach by combining automated IE systems with human validation. This\napproach attains precision comparable to manual annotation, surpassing automated tools,\nand most importantly reduces annotation time relative to manual annotation.\nNevertheless, the limiting element of the system is its imperfect recall, primarily caused\nby the automated component. In light of the promising advancements by LLMs in natural\nlanguage processing, this project extends ELICIT by incorporating LLMs as the automated\nIE element. Furthermore, the objective of this research is to evaluate the effectiveness of\nLLMs in IE.\nSince it is not possible to fit the full documents into LLM prompts due to their context\nwindow limits, the decision was made to divide them into smaller passages, extracting the\ninformation from the most relevant ones. While more commonly embedding models are used\n",
      "word_count": 312,
      "char_count": 2113,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (24.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 12,
      "text": "2\nIntroduction\nfor passage retrieval (Kamalloo et al., 2023), this research also explores the use of prompting\nLLMs for relevant passage retrieval. Moreover, the impact of using validated extractions for\nfine-tuning are evaluated, with the objective of enhancing the system’s performance.\nThe research is grounded in the legal domain through evaluations on UK Crown Court\nsentencing remarks, the same dataset utilized by Butcher et al. (2023). Additionally, a novel\ntask of IE involving source attribution is introduced, focusing on extraction of information\nprovided by the speakers present in the text. Synthetic court transcript, generated using\nGPT-4 (OpenAI, 2023b) based on real sentencing remarks, serve as the basis for evaluation.\nIn the baseline model setting, the developed system exhibits superior recall to the current\nversion of ELICIT. The baseline LLM performs worse than the embedding model in passage\nretrieval, showing the limitation of direct use of general LLMs. However, fine-tuning\non a limited amount of human validated extractions results in significant improvement in\nperformance, matching the embedding model’s performance for sentencing remarks, and\neven exceeding it for court transcripts. This improvement also transfers to the complete IE\nsystem, which includes the human validator. Notably, the fine-tuning results in significant\nincreases in recall in both unseen documents, and already processed ones, requiring fewer\nextractions shown to the human validator.\n1.1\nContext and Motivation\nThe motivation for the project comes from the desire to accomplish structured information\nextraction on various types of documents, specifically those in the legal domain - sentencing\nremarks and transcripts of court proceedings. Sentencing remarks, which are the judge’s\nremarks during the announcement of the sentence to the defendant, follow a consistent\nformat - they are given from the perspective of the judge and directed to the defendant.\nThey encapsulate a lot of information about the case, including the description of the crime,\nsubstantial evidence, and discussions on the mitigating and aggravating factors. Thus, they\npresent a valuable summary of information related to the case, whose extraction would\nbenefit future researchers with more comprehensive datasets from the legal domain.\nThe project is also dictated by the types of variables aimed to be extracted from these\ndocuments. The extraction system is designed to have an open nature, accommodating a\nwide variety of categorical variables. The ability to extract information, regardless if it’s\npresented explicitly, or if it is subjective, is desired. For instance, users should be able to\ndefine simple variables, such as the victim’s sex, but also more complex and subjective one’s\nsuch as the presence of emotional abuse or whether remorse was shown by the defendant.\n",
      "word_count": 428,
      "char_count": 2867,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 13,
      "text": "1.2 Contributions\n3\nTranscripts of court proceedings represent another source of information. Criminal court\nproceedings consist of multiple stages, including counsel’s opening and closing remarks,\nwitness evidence (examination), and others. Thus, these are extensive documents, usually\nrunning into hundreds of pages, presented in dialogue format with each speaker identified by\na dialogue tag. Whereas sentencing remarks are from a single person’s perspective, court\ntranscripts consist of the multiple parties involved in the case. This variety adds a layer of\nsubjectivity to the information presented, since the defense and prosecution often present\nopposing narratives and provide different interpretations of the same facts. The reliability of\nwitness testimonies can also vary, since the examinations are led by the prosecutor or defense\nand will usually consist of questions favouring the respective counsel’s narrative. Therefore,\nit’s evident that court transcripts pose a significantly more challenging setting for information\nextraction.\nThe inherent subjectivity and potential presence of contradictory facts in court transcripts\nnecessitate the need for an alternative information extraction process. Unlike with sentencing\nremarks, where the goal is to extract a single value for a variable, the aim now is to extract all\nvalues presented by each speaker for a given variable. This changes the nature of the problem\nfrom variable-centric to speaker-centric.\nThis type of information could potentially reveal what facts or interpretations of those\nfacts were presented by each counsel. Moreover, what evidence was provided by each witness\nand whether there were any contradictions could be identified. Extraction of data through\nthis novel approach, can lead to a more comprehensive understanding of legal cases, as well\nas, potential practical applications for future cases and policy-making.\n1.2\nContributions\nThe main contributions of this project are the following:\n1. Development of an end-to-end system for structured information extraction employing\nLLMs, and its implementation in the ELICIT codebase. The developed system out-\nperforms the original ELICIT system in terms of recall in sentencing remarks using\nVicuna-13B (Chiang et al., 2023), and with a significant margin on human-validated\ndata after fine-tuning the models.\n2. Introduction of a novel task of information extraction with source attribution, which\nfocuses on the extraction of information for distinct speakers within documents. The\nchallenges inherent in this task are outlined, and a methodology utilizing LLM prompt-\n",
      "word_count": 375,
      "char_count": 2613,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 14,
      "text": "4\nIntroduction\ning is proposed and implemented. The evaluation of this methodology is carried out\non a dataset of synthetic legal court transcripts.\n3. A method of generating synthetic text documents, namely legal court transcripts, is\nproposed, which uses an iterative summarization approach to overcome context window\nlimitations of GPT-4.\n4. Analysis of the impact of human-validated data for LLM-finetuning. This fine-tuning\nenhances recall for already extracted and unseen documents, requiring fewer extractions\nto be shown to the human validator.\n5. Comparison of passage retrieval using a vector index storing passage embeddings\nwith an LLM-based approach. Particularly noteworthy is the demonstration of the\nperformance of Vicuna-13B fine-tuned with limited data, which is comparable to an\nembedding model specifically trained for semantic search.\n1.3\nThesis Outline\nIn Chapter 2, the background pertinent to understanding the concepts utilized in the project\nis introduced. This encompasses the concepts of IE and Question Answering (QA), LLMs,\nand the ELICIT framework.\nIn Chapter 3, the methodology employed to achieve the established research goals is\ndescribed. This includes the selection of datasets, including the generation of synthetic court\ntranscripts, as well as the evaluation metrics. Furthermore, the specific design choices made\nto accomplish information extraction using LLMs are outlined.\nIn Chapter 4, the results of evaluating the different components of the LLM-enhanced\nsystem are presented and discussed, both in a supervised dataset and end-to-end setting.\nIn Chapter 5, an overview of the project is provided, key findings are reiterated, and\npotential future research directions are proposed.\n",
      "word_count": 250,
      "char_count": 1728,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 15,
      "text": "Chapter 2\nBackground\nIn this chapter, I present the background relevant to the project in order to better understand\nthe context and motivation behind the research. I begin by introducing the concepts of\nInformation Extraction and Question Answering, which set the stage for our project task.\nNext, I discuss the idea of extracting multiple variable values and assigning them to a specific\nspeaker. The concepts of Large Language Models, the specific model that was chosen, and\nthe selected framework of fine-tuning are covered. Finally, the ELICIT system (Butcher\net al., 2023), which is the practical setting for our information extraction task, is introduced.\n2.1\nInformation Extraction and Question Answering\nInformation Extraction (IE) is a crucial task in natural language processing, which aims to\nextract structured information from unstructured textual data. In this project, the goal of\nthe system is to create tabular datasets from sentencing remarks and court transcripts with\nvariables consisting of categorical values.\nMany IE tasks are typically focused on explicit information, with a common approach\nbeing Named Entity Recognition (NER) methods. These include the identification of persons,\nlocations, dates, and other information. Additionally, Relationship Extraction methods seek\nto identify the semantic connections between different entities in a document. Conversely,\nQuestion Answering (QA) presents a broader spectrum of tasks since it can accommodate a\nmultitude of different queries.\nQA tasks are usually categorized by the type of context they encompass: closed-book\ntasks, which provide no context, and open-book tasks, where context is given. Within IE, QA\nis considered an open-book task, more specifically a reading comprehension one, where the\ngoal is to find evidence in a provided context. The output of QA tasks also varies, ranging\n",
      "word_count": 278,
      "char_count": 1869,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (24.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 16,
      "text": "6\nBackground\nfrom span prediction, where part of the context serves as the answer, multiple choice QA, or\nfree-form responses.\nThe SQuAD dataset (Rajpurkar et al., 2018, 2016) is one of the most common benchmarks\nfor QA tasks, containing question-answer pairs generated from Wikipedia articles, and\nrequiring that answers form part of the provided context. Currently the best performing\nmethods are pre-trained language models fine-tuned on question answering datasets (Lan\net al., 2020; Raffel et al., 2020). More recently, large language models in a few-shot setting\nhave also demonstrated competitive performance in such tasks (Brown et al., 2020).\nA recent topic of interest is the application of IE and QA in conversational settings. The\nFriendsQA (Yang and Choi, 2019) dataset was introduced to tackle questions asked about\nmulti-party dialogue, utilizing transcripts from the TV show \"Friends\". Its tested outputs\ninclude answer utterance selection and answer span prediction. The paper highlights the\nincreased complexity of QA tasks in a multi-party dialogue scenario compared to conventional\nQA tasks.\nAnother dataset, DREAM (Sun et al., 2019), introduced a multiple-choice QA problem\nfor dialogue-based texts. This dataset presents unique challenges since most answers are not\nextractive and necessitate multi-sentence reasoning. The language in a conversational setting\ncan also include interruptions or reasoning, which spans multiple turns. The Molweni dataset\n(Li et al., 2020) consists of multi-party chat conversations on the topic of Ubuntu. The authors\nshowcase that strong QA models display a significant drop in their performance compared\nto the SQuAD 2.0 dataset. However, Molweni only consists of question answering over\nshort spans of text. QAConv (Wu et al., 2022) introduces a dataset for question answering\nwith long and complex conversations as the context. Furthermore it consists of two testing\nscenarios - where only the relevant chunk is provided, or the full text. This is closer to the\nsetting of this project, where information needs to be extracted over long documents.\nThese recent research undertakings provide the context for the task at hand, which in-\nvolves extracting information from sentencing remarks and court transcripts. Sentencing\nremarks align with standard IE tasks but introduce complexity due to the length of the docu-\nments and the complex variables being extracted, which are not explicit factual information.\nCourt transcripts introduce a conversational setting with multiple parties, a scenario which\nrecent research has identified as particularly complex. This helps in understand how the work\nof this project fits into the larger field of IE and QA, and shows the need for more advanced\nmethods to solve these problems.\n",
      "word_count": 420,
      "char_count": 2781,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 17,
      "text": "2.2 Multi-Value Extraction and Source Attribution\n7\n2.2\nMulti-Value Extraction and Source Attribution\nThe unique context of court transcripts introduces additional challenges to standard Informa-\ntion Extraction (IE) and Question Answering (QA) tasks. One such complication arises from\nthe fact that a single variable within a document can attain multiple values, dependent on the\nspeaker.\nTraditional IE methods, such as Named Entity Recognition (NER), are capable of ex-\ntracting a varying number of values. However, as discussed in the previous section, it is not\nsufficient for the task, which demands extraction of complex, non-explicit information.\nOn the other hand, most QA tasks are set with a context that contains a single answer, or\nno answer at all. This structure is notably different from the setting, where multiple variable\nvalues may exist in the text, derived from multiple speakers.\nThe task of this project holds some resemblance to argument mining (Lawrence and\nReed, 2019) or opinion mining (Sun et al., 2017), since the information about a variable\ncould be thought of as an argument or opinion. While these methods focus on explicit\narguments or opinions, the task of this project involves a wider range of textual nuances,\nsuch as implications, answers to questions, and other forms of language. Thus, it necessitates\na broader approach to capture the richness and diversity of information present in the texts.\nIn addition to dealing with multiple possible values for a variable, the project also aims to\nattribute these values to their respective speakers. While quotation-to-speaker attribution in\ndialogue-based texts of known format can often be completed with pattern matching, which\nis a relatively straightforward approach, research which attempts to assign the extracted\ninformation to one of multiple speakers has not been identified.\n2.3\nLarge Language Models\nThe core of this thesis is Large Language Models (LLMs), which have become very popular\ntools in natural language processing tasks, competing with most specialised models (Radford\net al., 2020; Sun et al., 2020; Yang et al., 2020). LLMs are neural networks with many\nparameters (with state-of-the-art models in the trillions), which are trained on vast amounts\nof unlabeled texts from general sources, such as Wikipedia (Wikimedia Foundation, 2001),\nCommon Crawl (Common Crawl, 2007) and others.\nLLMs are used in this project based on the fact that they have been shown to work well\nin different tasks which require language understanding (Brown et al., 2020; Chung et al.,\n2022), which is the main problem of our thesis - extracting information from unstructured\ntexts.\n",
      "word_count": 414,
      "char_count": 2668,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 18,
      "text": "8\nBackground\nIn this section, the concept of auto-regressive language models, is described. This is\nfollowed by the introduction of the Transformer architecture (Vaswani et al., 2017), which is\nthe most popular architecture of modern LLMs. This leads to the presentation of the LLaMA\nmodels (Touvron et al., 2023) and its extension, Vicuna (Chiang et al., 2023), which is the\nmodel used in this project. Finally, the concept of fine-tuning through Low-Rank Adaption\n(Hu et al., 2021) is introduced, and why it is a suitable fine-tuning approach for this project.\n2.3.1\nAuto-regressive Language Models\nAuto-regressive models, such as (Chiang et al., 2023, introduced in Section 2.3.4), are a\nclass of sequential models that generate each output by conditioning it on all the previously\ngenerated outputs.\nThis process can be formalized by the following formula:\np(sequence) = p(y0:N) = p(y0)\nN\n∏\ni=1\np(yi|y0:i−1)\n(2.1)\nThis distribution can then be used to generate model responses p(response|prompt) using\na decoding strategy (e.g. sampling token by token, or choosing the highest probability token\nat each step). In the context of language models, yi represents a token at the i-th position in\nthe sequence of length N.\nA token is an integer that uniquely represents some text. Tokens are produced by\na tokenizer, which is trained independently, before the language model itself. Various\ntokenization strategies exist, broadly categorized into word-level and subword tokenization.\nThe Vicuna model, which is based on the LLaMA model (Touvron et al., 2023), employs a\nByte-Pair Encoding tokenization strategy (Sennrich et al., 2016). This strategy splits words\ninto subword units merging together frequently occurring pairs, resulting in an efficient\ntokenizer capable of handling words not present in the vocabulary.\nThe significant advancements in auto-regressive and other language models in recent\nyears can be attributed to the introduction of the Transformer architecture by Vaswani et al.\n(2017). This architecture revolutionized the training of models with a large number of\nparameters on vasts amounts of data.\n2.3.2\nTransformer\nThe Transformer addressed several limitations of previous state-of-the-art models, such as\ngated Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) models,\nand enabled effective handling of long-range dependencies.\n",
      "word_count": 355,
      "char_count": 2369,
      "fonts": [
        "NimbusRomNo9L-ReguItal (9.0pt)",
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "CMR10 (9.0pt)",
        "CMSY10 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "StandardSymL (17.2pt)",
        "CMSY10 (9.0pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 19,
      "text": "2.3 Large Language Models\n9\nThe key concept behind the Transformer is the attention mechanism (Bahdanau et al.,\n2016), which allows the model to selectively focus or attend to different parts of the input\ncontext. Unlike sequential processing, the Transformer looks at multiple parts of the context\nsimultaneously and focuses on parts it finds more important. This is achieved through scaled\ndot-product attention and multi-head attention mechanisms.\nThe scaled dot-product attention, shown in the left of Figure 2.1, is computed as:\nAttention = softmax\n\u0012QKT\n√dk\n\u0013\nV\n(2.2)\nwhere Q, K, and V represent the query, key, and value, respectively, and dk is the dimension\nof each key. Q = xW Q, K = xW K, V = xWV are linear projections of the layer input x, itself\na matrix with s (sequence length) rows and d (embedding dimension) columns. The weights\nare trained so that QT\ni Kj expresses how much the i-th input depends on the j-th input.\nTo enhance the attention mechanism, the Transformer employs multi-head attention,\nshown in the right of Figure 2.1, which uses multiple attention functions or \"heads\", allowing\nthe model to focus on multiple segments of the context simultaneously. Each head computes\nthe attention independently, and the results are concatenated and combined as follows:\nMultiHead = Concat(head1,...,headh)W O\n(2.3)\nwhere headi = Attention(xW Q\ni ,xW K\ni ,xWV\ni ) represents the attention computed by the i-th\nhead. W Q\ni , W K\ni , and WV\ni are the weight matrices for each head, and W O is the output weight\nmatrix. Each head is applied to a subset of the embedding dimension. For h heads and\nan embedding dimension dmodel, the weight matrices have dimensions dmodel ×dw, where\ndw = dmodel/h (h is chosen such that dw is an integer).\nThe Transformer architecture follows an encoder-decoder structure (Vaswani et al., 2017).\nBoth the encoder and decoder consist of self-attention blocks. In the encoder self-attention\nmechanism, the queries, keys, and values are derived from the outputs of the previous encoder\nlayer. On the other hand, in the decoder self-attention mechanism, the inputs are obtained\nfrom the outputs of the previous decoder layer, but with masked attention, as illustrated in\nthe left of Figure 2.1, so that it only attends to preceeding tokens. Equations 2.3.2 and 2.3.2\nprovide the formulas for self-attention. Additionally, the decoder includes a cross-attention\nblock, however, it is not utilized in the models employed in this project; further details on\ncross-attention can be found in Vaswani et al. (2017).\nThe absence of recurrence in the Transformer architecture makes it highly suitable for\nparallelization on multiple GPUs, leading to significantly reduced training times. Addition-\n",
      "word_count": 447,
      "char_count": 2734,
      "fonts": [
        "NimbusRomNo9L-ReguItal (11.8pt)",
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-ReguItal (9.0pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "CMSY10 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "CMMI10 (12.0pt)",
        "CMEX10 (12.0pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-ReguItal (12.1pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)",
        "NimbusRomNo9L-Medi (12.1pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 20,
      "text": "10\nBackground\nFig. 2.1 Scaled dot-product attention (left) and Multi-head attention (right). Figures taken\nfrom Vaswani et al. (2017).\nally, Transformers do not suffer from vanishing and exploding gradient problems, which is\nan issue with RNNs and LSTMs when processing extremely long sequences (Pascanu et al.,\n2013). This advantage stems from the Transformer’s ability to attend to all input positions\nsimultaneously.\nIn the upcoming sections, the LLaMA model (Touvron et al., 2023) and its fine-tuned\nversion Vicuna (Chiang et al., 2023), will be introduced, which are pre-trained auto-regressive\nlanguage models based on the Transformer architecture. As mentioned before, these models\nonly use the decoder part (without cross-attention), not the full architecture proposed by\nVaswani et al. (2017).\n2.3.3\nLLaMA\nLLaMA (Touvron et al., 2023) is a collection of auto-regressive language models developed\nby Meta AI, which have showcased competitive performance at a smaller parameter size.\nLLaMA was developed based on recent work in Hoffmann et al. (2022), which showed that\nbetter performance can be achieved not by making the models bigger, but by increasing the\namount of training data. It was trained on 1 trillion tokens for the 7B and 13B models (the\n\"B\" indicates the number of parameters in billions), and 1.4 trillion for the 33B and 65B\n",
      "word_count": 209,
      "char_count": 1349,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 922,
          "height": 508,
          "ext": "png",
          "size_bytes": 66848
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 21,
      "text": "2.3 Large Language Models\n11\nmodels. LLaMA-13B has been shown to outperform GPT-3 (Brown et al., 2020), while\nbeing more than ten times smaller (Touvron et al., 2023).\nLLaMA has become a popular option for fine-tuning for downstream tasks, since the\nsmaller models can be run on a single GPU with lower floating point precision, but still result\nin competitive performance.\n2.3.4\nVicuna\nThe open-source nature of LLaMA has resulted in many publicly available models fine-tuned\non more specific datasets. Vicuna (Chiang et al., 2023) is an open-source language model,\nwhich was trained on 140k (for the 1.3 version, which is used in the project; the version in\nthe article was trained on 70k) user-shared chatbot conversations from ShareGPT (Eccleston,\n2023). Using an automated evaluation framework with GPT-4 (Zheng et al., 2023), the\nauthors found that Vicuna-13B performs on-par with OpenAI’s ChatGPT-3.5 (OpenAI, 2022)\nand Google’s Bard (March 2023 version) (Manyika, 2023).\nVicuna was trained as an autoregressive chatbot model, meaning that it generates output\nin an autoregressive fashion and produces sequences in a chatbot conversation style - user\nmessages are followed by assistant (chatbot) messages. The standard way of using Vicuna\nis to provide a prompt consisting of a series of user and assistant messages, with the final\nassistant message being left empty for the model to generate a response. Figure 2.2 showcases\nthe prompting format of Vicuna.\nA chat between a curious user and an artificial\nintelligence assistant. The assistant gives helpful,\ndetailed, and polite answers to the user's questions.\nUSER:\nHow long would it take for a commercial airline to fly\naround the Earth?\nASSISTANT:\nIt would take a commercial airline approximately 40\nhours to fly around the Earth, assuming a direct flight\nand no stops. However, most commercial flights do not\ntake this route, as it would be both impractical and\ninefficient due to the need to refuel and the fact that\nmost airports are not equipped to handle such long\nflights. Instead, commercial flights typically follow more\ndirect routes that take them over oceans and between\nmajor hubs.\nFig. 2.2 An example of the prompt format in Vicuna. The black bold text represents the\nfixed template, which is required in every prompt for consistent performance. The red text\nhighlights the user input, while the blue text is the completion from Vicuna-13B.\n",
      "word_count": 386,
      "char_count": 2416,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "Arial-BoldMT (8.7pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "ArialMT (8.7pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 22,
      "text": "12\nBackground\n2.3.5\nFine-tuning using Low-Rank Adaptation\nLow-Rank Adaptation (LoRA) (Hu et al., 2021) is a technique for parameter efficient fine-\ntuning of large pre-trained language models. Full fine-tuning large pre-trained models with\nbillions of parameters can be computationally expensive. To address this, the authors propose\nthe LoRA approach.\nIn LoRA, during the fine-tuning process, the update of a pre-trained weight matrix\nW0 ∈ Rd×k can be expressed as W0 +∆W. Typically the weight matrices are full-rank, thus\nthe authors propose expressing the update matrix as a low-rank decomposition W0 +∆W =\nW0 +BA, with matrices B ∈ Rd×r and A ∈ Rr×k, where rank r ≪ min(d,k). By freezing W0\nduring fine-tuning, while B and A are updated, LoRA achieves parameter efficiency.\nThe low-rank assumption is based on the observation that the rank of the update matrix is\noften low when adapting a pre-trained model to a specific task. This is because the fine-tuning\ntask is usually narrower in scope compared to the broad pre-training task. The authors refer\nto Aghajanyan et al. (2020), which shows that large pre-trained models have a low \"intrinsic\ndimension\" when adapting to a task, and thus can be reparameterised in more efficient forms.\nLoRA offers several advantages over other existing fine-tuning methods:\n• LoRA is parameter and memory efficient, resulting in reduced training times and\nlower GPU memory requirements. According to the authors, the number of trainable\nparameters can be reduced by up to 10,000 times, while the GPU memory requirements\ncan be reduced by up to 3 times (Hu et al., 2021).\n• A single pre-trained model can be used with multiple LoRA modules. This flexibility\nallows for task-specific modules to be easily swapped, enabling the use of a single\npre-trained model for multiple downstream tasks. For instance, in this project, where\nthe aim is to utilize user-validated data for fine-tuning for different subtasks, LoRA\nenables the use of just a single pre-trained model and fine-tune multiple modules to the\nspecific subtasks, which eliminates the need of using multiple large models.\n• LoRA introduces less additional inference latency compared to methods such as\nadapters (Houlsby et al., 2019). This is due to the linear of nature of the method, where\nthe existing weight matrices are modified by adding the update matrices, avoiding\nthe need for additional modules. This property makes LoRA an efficient choice for\nreal-time applications.\nIn summary, LoRA provides an efficient fine-tuning process that is suitable for this project.\nIts parameter and memory efficiency, along with the ability to use a single pre-trained model\nfor multiple tasks, make it an appealing choice.\n",
      "word_count": 432,
      "char_count": 2716,
      "fonts": [
        "NimbusRomNo9L-ReguItal (9.0pt)",
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "CMSY10 (12.0pt)",
        "MSBM10 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "CMMI10 (12.0pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "StandardSymL (12.0pt)",
        "CMSY10 (9.0pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 23,
      "text": "2.4 ELICIT\n13\n2.4\nELICIT\nELICIT is an information extraction system, introduced in Butcher et al. (2023), designed\nto enhance extraction efficiency and precision through a combination of weak supervision\nand human validation. The purpose of ELICIT was to address the imperfect accuracy of\nautomated systems, and the slowness of manual annotation by combining the two. By incor-\nporating modern language model capabilities and the ability to identify differing opinions\nand attribute them to specific speakers, we aim to further extend the system’s functionality in\nthis project.\n2.4.1\nApproach\nIn ELICIT, users begin by defining a set of variables they wish to extract, with each variable\nconsisting of a range of possible values. This is called a categorical schema, an example of\nwhich is given in Listing 2.4.1. To handle cases where the desired information is not present\nin the document, an \"Abstain\" value is used in the system.\nListing 2.1 An example of a categorical schema. It defines the possible values that each\nvariable can take.\nr e l a t i o n s h i p :\n#\nv a r i a b l e\nname\n#\nv a r i a b l e\nvalues :\n− family\n− f r i e n d\n− p a r t n e r\n− c o l l e a g u e\n− none\nTo automate the extraction process, the users selects a set of labelling functions. These\nfunctions are automated tools that return a predetermined number of candidates, which\nare excerpts from the document labeled with a particular variable value. Each candidate\nis accompanied by a confidence score, indicating the system’s level of certainty in its\nassignment.\nELICIT utilizes a ranking mechanism to prioritize the candidates and presents the Top-k\ncandidates per variable to the user. This ranking enables the user to focus on the most\nrelevant and potentially accurate extractions. The system also combines excerpts from\ndifferent labelling functions with significant overlap.\n",
      "word_count": 330,
      "char_count": 1867,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 24,
      "text": "14\nBackground\nFinally, the user engages in the validation process by reviewing and validating each\ncandidate. This validation step contributes to the completion of the extraction process,\nensuring that the extracted information is accurate and reliable.\nFig. 2.3 A high-level overview of ELICIT for a single variable extraction.\n2.4.2\nLabelling Functions\nThe recall of ELICIT is mainly determined by the quality of the labelling functions. Each\nselected labelling function is applied to the complete set of documents under examination.\nELICIT is comprised of four main labelling functions, each specifically designed to identify\nand categorize a singular variable within a document. These functions are explained in detail\nbelow.\n1. QA Model Followed by Zero-shot Classification: This labeling function requires the\nuser to define a question schema, which contains supporting questions for each variable.\nSubsequently, the document is split up into smaller contexts, and each question-context\npair is supplied to a RoBERTa model that has been fine-tuned for question-answering\ntasks on the Squad2 (Liu et al., 2019; Rajpurkar et al., 2018). The output from the\nmodel is an extract from the context, where the model believes the answer to be.\nAlong with extract, a relevance score is also computed, interpretable as the conditional\nprobability PQA(excerpt|question). This extracted portion is then fed into a RoBERTa\nNatural Language Inference zero-shot classification model (Yin et al., 2019), with the\nvariable values operating as labels. The model generates a score PNLI(label|excerpt)\nfor every value, and the total confidence score is computed as the product of these two\nprobabilities: PNLI(label|excerpt)·PQA(excerpt|question).\n2. QA Model Followed by Cosine Similarity: This labelling function has the same\nmechanism of the previous one, except for the final classification phase. Here, instead\n",
      "word_count": 279,
      "char_count": 1902,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "CMSY10 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Medi (11.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)",
        "NimbusRomNo9L-Medi (12.1pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 911,
          "height": 271,
          "ext": "png",
          "size_bytes": 17453
        }
      ],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 25,
      "text": "2.4 ELICIT\n15\nof zero-shot classification, the function uses cosine similarity computation among\nRoBERTa embeddings for the final classification. Each text excerpt and variable value\nis embedded in the pattern “this is a {word}”, with \"{word}\" referring to the excerpt or\nvariable value, which is being embedded. Subsequently, the cosine similarity between\nall excerpt embeddings and label (variable value) embeddings is calculated. The\naggregate confidence score in this approach is the product of the cosine similarity and\nthe QA probability: Pcos(label,excerpt)·PQA(excerpt|question).\n3. Semantic Search: This method employs a transformer model to embed every sentence\nfrom the document. The dot product between each sentence embedding and the embed-\ndings of questions from the question schema is calculated. The average score for all\nquestions is taken as Pquestions(sentence,questions). Then these sentence embeddings\nare compared to the embeddings of each label, yielding the score Plabel(sentence,label).\nThe total score for a sentence-label pairing is the product of these two scores, specifi-\ncally, Plabel(sentence,label)·Pquestions(sentence,questions).\n4. Keyword Search: This labelling function requires the user to define a keyword schema,\nwhich contains a set of keywords for each variable. The function employs a phrase\nmatcher from the SpaCy library (Honnibal et al., 2020) to detect all instances of these\nkeywords in the document. Due to the absence of a scoring mechanism in the keyword\nmatching, the extractions are attributed a default value of 0.1.\nIn conclusion, the ELICIT system uses each of these labelling functions to produce\ncandidate excerpts from the text along with the identified labels. The recall of the system\nis dependant on at least one labelling function being able to identify a relevant part of the\ndocument, as well as, to assign it to the correct label. However, the current version of ELICIT\nassumes that only a single variable value corresponds to a document, which is not the case\nfor many documents, for example, court transcripts. In this project, the work is focused on\nextending ELICIT’s capabilities by introducing new labelling functions, which make use of\nLLMs, and which have the ability to attribute information to speakers in the text.\n",
      "word_count": 346,
      "char_count": 2293,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "CMSY10 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "CMMI10 (12.0pt)",
        "NimbusRomNo9L-Medi (11.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 26,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 27,
      "text": "Chapter 3\nMethodology\nIn this chapter, I describe the methodology of the project, which is used to achieve the\nresearch goals. The process of creating a suitable dataset for the project is detailed, and\nthe specific approaches of using LLMs for information extraction in a single-speaker and\nmulti-speaker setting. The technical modifications implemented in ELICIT to accommodate\nthe approach are also discussed. Furthermore, the method of utilizing user-validated data\nfor model fine-tuning is explained. Lastly, the metrics employed to assess and compare the\nimpact of the decisions on the results are described.\n3.1\nDatasets\n3.1.1\nSentencing Remarks\nFor a fair comparison against the extraction methods presented in Butcher et al. (2023), the\nsame extracted sentencing remarks as in the paper are used for the single-speaker information\nextraction. The documents used are part of openly accessible cases published by the United\nKingdom Judiciary for cases of public interest.\nThe authors have selected the sentencing remarks of nineteen murder cases. Each of\nthe documents was manually labelled for eighteen categorical variables. To ensure privacy,\neach named person in the text was anonymized, and the names of the defendant and victim\nwere replaced with generic identifiers \"the defendant\" and \"the victim\", respectively. The\ndocuments averaged 3357 tokens.\nOut of the eighteen variables, thirteen, which occurred five or more times in the docu-\nments, were used for evaluation. These variables cover non-trivial information about the\nvictim, the defendant, and the crime or case. The victim-focused variables include their sex,\nemployment status, pregnancy status, and whether they were considered vulnerable. The\n",
      "word_count": 256,
      "char_count": 1721,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Medi (24.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 28,
      "text": "18\nMethodology\nvariables covering the defendant include prior convictions, remorse, and whether their age\nwas considered a mitigating factor. The remaining variables, which do not strictly fall under\nthe previous two categories, include evidence of physical abuse, domestic abuse, instance\nof emotional abuse, the premeditation of the crime, and whether the crime was sexually\nmotivated.\nIn every document, the victim sex is noted, either through pronouns or gender-specific\nwords. Some information might be explicitly stated, inferred through other facts, or not\nmentioned at all. Most variables require complex reasoning through implied statements by\nthe judge or from the descriptions of the crime.\nThe sentencing remarks dataset comprises 19 documents with 13 distinct variables,\nresulting in a total of 189 non-abstain and 58 abstain data points. This dataset provides a\ngood setting to test the ability of LLMs to comprehend non-trivial information.\n3.1.2\nCourt Transcripts\nFor the analysis of information extraction in a multi-speaker setting, transcripts of UK\ncourt proceedings were used as the document of choice. Although transcripts of UK court\nproceedings are not available to the general public and cannot be used for public research,\ntheir format is known. To overcome this limitation, GPT-4 (OpenAI, 2023b) was used\nto produce synthetic court transcripts based on the information present in real sentencing\nremarks, as modern LLMs have shown to be capable of producing human-like text (Hu et al.,\n2023). Although synthetic text has limitations, it is expected that approaches which do not\nwork here will also be ineffective with the more challenging real data.\nI established the following criteria for the generated documents:\n1. Size: The generated transcripts should be at least a few pages long, totaling at least\na few thousand words and thus exceeding the context window (the number of tokens\nin the generated sequence, including the prompt) of most commonly available LLMs,\nthus, facilitating the need for splitting the text, mimicking the process of extracting\nfrom real, lengthy transcripts. For reference, the Vicuna models were trained on a\ncontext length of 2048 tokens (approximately 1500 words).\n2. Multi-turn dialogue: The transcripts should consist of multi-turn dialogue, including\nspeakers addressing someone, raising questions and answering them.\n3. Multiple variable values: The documents should contain instances of multiple differ-\nent values applying to the same variable, including some speakers indicating different\nvalues to the same variable, i.e. giving contradicting statements.\n",
      "word_count": 391,
      "char_count": 2623,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)",
        "NimbusRomNo9L-Medi (12.1pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 29,
      "text": "3.1 Datasets\n19\n4. Stage structure: The transcript should follow a predefined stage structure, starting\nwith opening remarks by the prosecutor and the defense (counsel), followed by a series\nof cross-examinations of different witnesses, and end with closing remarks by the\ncounsel. This structure, with additional stages, is found in real court transcripts, and\nalso facilitates the inclusion of competing narratives.\nThe chat completion model of GPT-4 was used to generate the transcripts, which consists\nof three types of messages: system, user, and assistant. While the user and assistant messages\nwork the same way as described for Vicuna in Section 2.3.4, the system message allows\nus to preconfigure the model so that it adheres to a certain tone, style or task in its future\nresponses (OpenAI, 2023a). In this case, the system message contained a description of the\ntask, and outlined the structure that the transcript should follow. Specific variable values\nwere included by providing a list of these variables as statements in the system message,\nsuch as \"the victim was unemployed\". Additionally, the real sentencing remarks are provided\nin the system message for better context.\nGenerating a transcript with the desired length using a single model response is challeng-\ning due to the model’s tendency to produce compact responses. Instructing the model to\nproduce longer responses did result in slightly longer generations, but not enough to meet the\nneeds of the project. Similarly, instructing the model to generate text of a defined word count\nwould result in generations significantly below the requested word length. To overcome this,\nan iterative approach is employed. The model is prompted to generate the transcript of a\nsingle stage, and the responses are concatenated to produce the full transcript. This iterative\napproach allows better control over the length of the transcript, and over the content of the\ntranscript.\nTo ensure variation in variable values across speakers, a subset of variables that can\nlogically take different values in the text is established, such as premeditation, remorse,\nvulnerability of the victim, emotional abuse, and defendant’s age as a mitigating factor. For\neach witness, two of these variables were randomly sampled, and the model was instructed\nto include the information during the examination by the prosecutor and defense, with each\ncounsel taking opposing values.\nTo ensure a logically consistent document, the model has to take into account its previous\ngenerations. However, the available model has a context length limit of 8000 tokens, which is\nnot sufficient to include previous generated stages. To address this, an iterative summarization\napproach is implemented. The system message and the opening remarks are included in\nevery generation, and after generating a pair of witness examinations, the model is requested\nto summarize them. These summarizations replace the longer generations in the message\n",
      "word_count": 459,
      "char_count": 2974,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)",
        "NimbusRomNo9L-Medi (12.1pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 30,
      "text": "20\nMethodology\nhistory, circumventing the limited context length problem while ensuring that the model sees\nthe essential information.\nUsing this approach, five transcripts were generated, with an average of 5592 tokens. A\nsingle data point is defined as the tuple (document, variable, value, speaker), representing a\nspeaker indicating a distinct variable value in a document. The transcripts were manually\nannotated to identify the variable values each speaker produces, resulting in a total of 171\nnon-abstain data points. The annotation revealed occasional issues with the synthetic data,\nincluding repeating sentences in the opening and closing remarks, and instances of logical\nincoherence. Nevertheless, the synthetic dataset is a good starting point for analyzing the\nchallenges that would be faced in information extraction from real court transcripts.\n3.2\nInformation Extraction\n3.2.1\nApproach\nIn order to extend ELICIT with the capability of using LLMs for information extraction, a\nnew labelling function had to be created, which would take the document text and variable\nname as input, and return a set of candidate extractions as the output. Each extraction should\ncontain the identified variable value and a corresponding confidence score.\n3.2.2\nPassage Retrieval\nThe first challenge faced was the issue of large document length, which exceeded the context\nlength of most modern LLMs. To address this, the approach of splitting the text into smaller\npassages was taken. Subsequently, extraction on the n most relevant ones identified by the\npassage retrieval part is performed. This technique, visualized in Figure 3.1, is a standard\napproach in IE and QA systems over long documents or multiple documents (Choi et al.,\n2017; Izacard and Grave, 2021; Xu et al., 2011).\nD passages\nDocument\nn relevant\npassages\nPassage Retrieval\nk extractions\nValue Labelling\n(passage, value,\nconfidence score)\nFig. 3.1 High-level overview of the extraction process for the LLM-enhanced system. For\neach variable, the document is split into D passages, which are provided to retrieval system.\nThe passage retrieval system returns n most relevant ones, which are fed to the value labelling\npart. It assigns a confidence score to each variable value for the passage, and top k extractions,\nbased on confidence, are shown to the user.\n",
      "word_count": 356,
      "char_count": 2328,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "ArialMT (7.0pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "Arial-ItalicMT (6.5pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "ArialMT (6.5pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 31,
      "text": "3.2 Information Extraction\n21\nIn the project, experimentation was conducted with two passage retrieval methods,\ndescribed in the following subsections. The first approach involved storing embedded\npassages in a vector index and querying them to identify the n most relevant ones. The\nsecond approach used an LLM in a Yes/No question format to identify whether the passage\ncontains the relevant information.\nVector Index\nIn this approach to passage retrieval, semantic search is performed across all passages from\na single document. The all-mpnet-base-v2 pre-trained model from SentenceTransformers,\nwhich has shown strong performance in semantic search (Reimers and Gurevych, 2019), was\nused to generate vector embeddings of the individual passages.\nThe Chroma vector index (Huber, 2022) was utilized for storing these embeddings and\nperforming the similarity search. This eliminates the need for recomputing the embeddings of\na single document. Additionally, a vector index allows relevant passages to be retrieved using\nefficient approximate nearest neighbour search for high-dimensional vector data (Malkov\nand Yashunin, 2018).\nThe LangChain Python package (Chase, 2022) was employed, providing a simple inter-\nface for storing and querying passages, and directly supporting Chroma and other popular\nvector indices. All embedded passages from the documents are stored in a single index, with\nthe document name assigned as metadata. During the search, filtering is applied to only\ninclude passages from the relevant document, removing the overhead of creating multiple\nindices for every new document and creating the possibility of performing extractions across\na set of documents in the future. The queries are statements describing each variable, which\nthe user defines in an information schema, for example, the statement \"the relationship\nbetween the victim and the defendant\" for the \"relationship\" variable.\nThe embedding vectors are stored as 768-dimensional L2-normalized vectors. The\nsquared L2 (Euclidean) distance is used as the distance metric between the query and passage\nembeddings. The following formula was used to convert it into a cosine similarity score:\nsimilarity = 1− distance\n2\n(3.1)\nLLM-Based Retrieval\nIn this method, for each passage, a binary classification approach is adopted. The LLM is\npresented with the passage and asked whether the context includes information about the\n",
      "word_count": 353,
      "char_count": 2408,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "CMSY10 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-ReguItal (12.1pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 32,
      "text": "22\nMethodology\nspecified variable. The prompt template (the user message in Vicuna), as shown in the left of\nFigure 3.2, is used, with a sample prompt visualized in the right of the figure.\nThe context is an excerpt from a legal\ndocument. Does it contain discussions\nabout or indications to {information}? Yes\nor no.\nContext:\n{context}\nThe context is an excerpt from a legal document.\nDoes it contain discussions about or indications\nto the premeditation of the crime? Yes or no.\nContext:\n6. Text messages sent by you and internet\nsearches done on your mobile phone reveal a\ndegree of premeditation. the victim had two\nhospital admissions prior to her death when\nyou claimed that she had suffered fits.\nFig. 3.2 Prompt template (left) and example prompt (right) for the LLM-based passage\nretriever. The {information} tag is replaced with the description of the variable from the\ninformation schema, as described in Section 3.2.2, while the {context} tag is replaced with\nthe passage.\nPreliminary experiments indicated that the majority of answers start with a \"Yes\" or \"No\",\nthus, decoding is unnecessary, and it is sufficient to check the conditional probability of the\ncompletion being \"Yes\" or \"No\", given the prompt.\nThis is accomplished by concatenating the answer to the prompt and completing a single\nforward pass, which yields the logits for each token in the vocabulary at each sequence\nposition. The logits, representing the unnormalized score for each token in the vocabulary,\nare passed through a softmax function. This results in the (log) probability distribution over\nthe entire vocabulary at that sequence position, given the previous tokens. The conditional\nprobability of the answer given the prompt is obtained by summing the log probabilities of\nthe answer tokens and applying an exponential:\nP(answer|prompt) = P(yN+1:N+T+1|y1:N) = exp\n \nN+T+1\n∑\ni=N+1\nlogP(yi|y1:i−1)\n!\n(3.2)\nwhere N is the number of tokens in the prompt, T is the number of tokens in the answer, and\nyi denotes the token at the i-th position in the sequence.\nUsing a baseline model in this manner leads to some probability mass being lost to other\ngenerations, which have the same meaning but do not start with a \"Yes\" or \"No\", for example,\nthe sequence \"The answer is Yes\". The conditional probabilities of the next token being\n\"Yes\" or \"No\" are used and normalized as follows:\nP(Yes) =\nP(label = Yes|prompt)\nP(label = Yes|prompt)+P(label = No|prompt)\n(3.3)\n",
      "word_count": 399,
      "char_count": 2448,
      "fonts": [
        "NimbusRomNo9L-ReguItal (9.0pt)",
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "SFTT1200 (12.0pt)",
        "CMSY10 (12.0pt)",
        "CMR10 (9.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "ArialMT (8.8pt)",
        "StandardSymL (17.2pt)",
        "CMSY10 (9.0pt)",
        "CMEX10 (12.0pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 33,
      "text": "3.2 Information Extraction\n23\nThe normalized conditional probability of \"Yes\" is used as the confidence score of the\npassage retrieval component, and the top n highest scoring passages are retrieved.\n3.2.3\nValue Labelling\nThe retrieved passages are taken as input to the value labelling component with the objective\nof producing a set of extracted variable values with respective confidence scores. This is\na multi-class classification problem, but with the aim of using a single model and with the\nability to have an arbitrary number of possible variable values.\nIn the prompt to the LLM, given in Figure 3.3, the task is first described, which is to\nidentify the most suitable variable value (referred to as ’label’) based on the context (the\ncandidate passage) and questions from the question schema. The latter are provided as\nadditional context to the model, helping it align better with the specific task. This reduces\npotential confusion during classification. For example, in the classification of the ’victim\nsex’ variable, the questions are \"what sex was the victim?\", \"was the victim a male?\". These\nquestions guide the model to identify the sex of the victim specifically, thus reducing the\nchance of misclassification.\nThe context is an excerpt from a legal document. Identify the\nlabel based on the provided context and questions.\nReturn the label 'unclear' if the text does not provide enough\ninformation to identify a label.\nYour response should be in the following format:\nLabel: '<identified label>'\nHere is an example of the task.\nInput:\nLabels: 'premeditated', 'not premeditated'\nQuestions: \n'was \nthe \ncrime \nplanned?', \n'was \nthere \nany\npremeditation?'\nContext:\nIn your favour is your good character save for the three cannabis\nmatters; your record of\ncontinuous \nemployment; \nthe \nlack \nof \nany \nsignificant\npremeditation; and (to a limited\nextent only for the reasons I have already expressed) that you\nmay not have intended to\nkill her. Additionally, there were no previous indications that you\nharboured violent\nintentions towards your sister.\nOutput:\nLabel: 'not premeditated'\nIdentify the label for the following context:\nLabels: {labels}\nQuestions: {questions}\nContext:\n{context}\nThe context is an excerpt from a legal document. Identify the label\nbased on the provided context and questions.\nReturn the label 'unclear' if the text does not provide enough\ninformation to identify a label.\nYour response should be in the following format:\nLabel: '<identified label>'\nHere is an example of the task.\nInput:\nLabels: 'premeditated', 'not premeditated'\nQuestions: 'was the crime planned?', 'was there any premeditation?'\nContext:\nIn your favour is your good character save for the three cannabis\nmatters; your record of\ncontinuous employment; the lack of any significant premeditation;\nand (to a limited\nextent only for the reasons I have already expressed) that you may\nnot have intended to\nkill her. Additionally, there were no previous indications that you\nharboured violent\nintentions towards your sister.\nOutput:\nLabel: 'not premeditated'\nIdentify the label for the following context:\nLabels: 'mitigate', 'not mitigate'\nQuestions: 'was age a mitigating factor?'\nContext:\n20. Your age and relatively good character afford you mitigation.\nHaving seen you give\nevidence and from all that I have heard about you during the trial, I\nconsider that you\nare immature. \nFig. 3.3 Prompt template (left) and example prompt (right) for the value labelling model. A\ndetailed explanation is given in Section 3.2.3.\n",
      "word_count": 545,
      "char_count": 3530,
      "fonts": [
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "ArialMT (6.6pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 34,
      "text": "24\nMethodology\nAdditionally, the model is instructed to return the label \"unclear\", if the context does not\ncontain sufficient information to make a decision. This acts a safeguard noting the fact that\nthe passage retrieval system is not perfect and may return some irrelevant passages. Thus, the\nmodel does not need to make a decision between two incorrect values. The \"unclear\" label is\nalso used to handle situations where the variable in question is mentioned, but a definitive\nvalue is not provided. For example, the sentence \"We have yet to determine if the victim was\npregnant.\" mentions the victim’s pregnancy, but does not give a clear answer, and thus fits\nthe \"unclear\" label.\nThe label \"unclear\" is not returned to the user as an extraction, but is employed to indicate\nuncertainty and remove some probability mass from the other variable values during model\ninference.\nFinally, the prompt also contains the response format and a single example of the task.\nBoth of these are used to focus more probability onto the exact possible labels, while the\nexample also improves the model’s understanding of the task, as prompts including in-context\nexamples have shown to produce better results in LLMs Brown et al. (2020).\nFor the labelling, the conditional probability of each of the variable values, including\n\"unclear\", being the completion to the prompt is computed, as is done in Section 3.2.2.\nThe probabilities are also normalized as some probability mass is distributed to alternative\ncompletions. This is formalized, for a label x, as:\nP(label = x) =\nP(label = x|prompt)\n∑\ny∈labels\nP(label = y|prompt)\n(3.4)\nThis process results in a set of variable values with confidence scores for each retrieved\npassage. The overall confidence score of the end-to-end extraction system is the product\nof the passage retrieval confidence score and the value labelling confidence score. Finally,\nthe Top-k highest-scoring passage-value pairs for each variable are presented to the user for\nvalidation.\n3.3\nSource Attribution\nIn this section, the process of extending ELICIT to assign source information to extracted\nvariables when there are multiple speakers present, is described. While the primary focus of\nthe project lies in dialogue, the methods and design choices described in this section hold\nthe potential for future extension to other types of attributions, such as indirect quotations or\n",
      "word_count": 381,
      "char_count": 2399,
      "fonts": [
        "NimbusRomNo9L-ReguItal (9.0pt)",
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "CMSY10 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "StandardSymL (12.0pt)",
        "CMSY10 (9.0pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 35,
      "text": "3.3 Source Attribution\n25\ndirect quotations in a non-dialogue context. The modified objective of the extraction process\nis to correctly identify the variable values given by a speaker.\nThe primary design choice lies in the placement of the source attribution component\nwithin the system. Three possible scenarios have been identified:\n1. Integration of the source attribution as a final step after value labelling.\n2. Incorporation of the source attribution within the passage retrieval process.\n3. Embedding of the source attribution into the value labelling component.\n3.3.1\nSource Attribution as the Final Component\nWhen source attribution is considered as the final step in the system, the component would\nreceive the extracted passage-value pairs, and the approach taken would involve instructing\nthe LLM to identify the speaker who provided evidence for a specific value.\nThe advantage of this approach is that the source attribution part would remain decoupled\nfrom the passage retrieval and value labelling parts, and thus could be improved independently\nthrough prompt engineering or fine-tuning. It would also be compatible with other labelling\nfunctions since it only requires an assigned variable value and a context.\nHowever, this approach is limited in that not all possible speaker-value combinations\ncan be extracted. If there are multiple speakers giving the same value in a passage, the\nmodel would have to choose between them, considering the task is formed as a multi-class\nclassification one. If the LLM is used in a multi-label classification fashion, where multiple\nspeakers can be identified for a single value, issues arise with getting the output in an\nexpected format, as well, as getting a representative confidence score, which is speaker-\nspecific. Furthermore, if there are multiples speakers giving different values in the same\npassage, the value labelling part will have distribute the probability to each of the values, or\neven give most of the probability mass to the \"unclear\" label. Hence, the score would not be\nreflective of the confidence of a single speaker giving a single value.\n3.3.2\nSource Attribution in Passage Retrieval\nAn alternative approach could be the incorporation of source attribution into the passage\nretrieval segment, allowing only those passages to be retrieved where the speaker gives some\nindication of the variable. Assuming that a list of speakers for the passage is available, it is\nnecessary to determine the relevance of the passage for each speaker. This would involve\nmodifying the LLM-based passage retriever’s prompt to incorporate speaker information, for\n",
      "word_count": 404,
      "char_count": 2628,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 36,
      "text": "26\nMethodology\ninstance, \"Based on the context, are there discussions or indications given by the {speaker}\nabout {variable}?\".\nWith this methodology, only passages where the speaker discusses the specific variable\nare provided to the value labelling segment. However, this approach precludes the use of\nthe designed strategy for value labelling without additional speaker context. For example,\nin a passage containing two speakers indicating differing variable values, the modified\npassage retrieval system would identify the passage as relevant for both speakers. However,\nproviding the passage to the value labelling segment would pose a challenge for the model in\nidentifying the correct value, given the absence of a distinct value for the passage. Therefore,\nthis approach would also necessitate a modification to the value labelling segment, which\nwould lead to two systems facing more complex tasks, and thus, likely a larger decrease in\nperformance.\n3.3.3\nSource Attribution in Value Labelling\nThe final candidate approach is to include source attribution as part of the value labelling\nphase. Given a set of speakers present in the passage, the model would be prompted to\nidentify the variable value, based on each specific speaker or, in other words, according\nto that speaker’s point of view. This approach has several advantages over the previously\noutlined methods.\nFirstly, it enables the capture of all values identified by a single speaker, since it performs\nlabelling per-speaker, and not per-variable first. As a result, the confidence score is also\nreflective of assigning a distinct variable value to a distinct speaker. In contrast to the previous\ntwo methods, the case where multiple speakers give the same or different values in a single\npassage can be handled, and the speaker-value pairs extracted separately.\nThe only disadvantage of this method is that the model will be prompted for speakers\nthat are irrelevant, i.e. who do not give any indication to any of the variable values. This is\ndue to the fact that labelling is performed for every speaker appearing in the passage, and\nwill have no information if that speaker discusses that variable.\n3.3.4\nImplementation\nTaking into account all of the advantages and disadvantages of the proposed approaches, the\ndecision was made to include the source attribution in the value labelling stage, as it best\naligns with the objective of correctly identifying the variable values given by a speaker, and\nis expected to result in the smallest decrease in performance compared to the single-speaker\napproach.\n",
      "word_count": 402,
      "char_count": 2579,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "SFTT1200 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 37,
      "text": "3.3 Source Attribution\n27\nThe implementation of this involves two main modifications to the single-speaker scenario.\nThe first modification is the extension of the passage retrieval component to store the speakers\npresent in each passage.\nThe context is an excerpt from a legal document. Identify the\nlabel based on the provided questions and the information\nprovided by the specified speaker in the context.\nReturn the label 'unclear' if the speaker does not provide\nenough information to identify a label.\nYour response should be in the following format:\nLabel: '<identified label>'\nHere is an example of the task.\nInput:\nLabels: 'premeditated', 'not premeditated'\nQuestions: 'was the crime planned?', 'was there any\npremeditation?'\nContext:\nProsecutor: Text messages sent by the defendant and internet\nsearches done on her mobile phone prove\npremeditation. Would you agree?\nFirst Witness: No, I would not. She could have been searching\nfor anything.\nSpeaker: 'Prosecutor'\nOutput:\nLabel: 'premeditated'\nInput:\nLabels: 'remorse', 'no remorse', 'unclear'\nQuestions: 'was the defendant remorseful?', 'remorse?'\nContext:\nDefense: Would you say that the defendant showed remorse for\nher actions?\nFirst Witness: I would say so, yes.\nDefense: Why is that?\nFirst Witness: She was crying when she was arrested.\nSpeaker: 'First Witness'\nOutput:\nLabel: 'remorse'\nIdentify the label for the following context and specified speaker:\nLabels: {labels}\nQuestions: {questions}\nContext:\n{context}\nSpeaker: '{speaker}'\nThe context is an excerpt from a legal document. Identify the label\nbased on the provided questions and the information provided by the\nspecified speaker in the context.\nReturn the label 'unclear' if the speaker does not provide enough\ninformation to identify a label.\nYour response should be in the following format:\nLabel: '<identified label>'\nHere is an example of the task.\nInput:\nLabels: 'premeditated', 'not premeditated'\nQuestions: 'was the crime planned?', 'was there any premeditation?'\nContext:\nProsecutor: Text messages sent by the defendant and internet searches\ndone on her mobile phone prove\npremeditation. Would you agree?\nFirst Witness: No, I would not. She could have been searching for\nanything.\nSpeaker: 'Prosecutor'\nOutput:\nLabel: 'premeditated'\nInput:\nLabels: 'remorse', 'no remorse', 'unclear'\nQuestions: 'was the defendant remorseful?', 'remorse?'\nContext:\nDefense: Would you say that the defendant showed remorse for her\nactions?\nFirst Witness: I would say so, yes.\nDefense: Why is that?\nFirst Witness: She was crying when she was arrested.\nSpeaker: 'First Witness'\nOutput:\nLabel: 'remorse'\nIdentify the label for the following context and specified speaker:\nLabels: 'male', 'female'\nQuestions: 'what sex was the victim?', 'was the victim a male?', 'was the\nvictim a female?'\nContext:\nDefense: Good morning, First Witness.\nFirst Witness: Good morning.\nDefense: You mentioned earlier that the victim was a close friend of\nyours. Can you tell us more about her personality?\nFirst Witness: She was a very strong, independent woman. She was\nalways doing things her own way, and she didn't let anyone stop her.\nSpeaker: 'First Witness'\nFig. 3.4 Prompt template (left) and example prompt (right) for the value labelling based on\nthe specified speaker. The prompt is a modified version of the prompt in Figure 3.3, but\ninstructs the model to assign the value based on a specific speaker’s point of view.\nWith every passage, the dialogue tag pattern matching is run to identify the speakers in\nit. If no dialogue tags are present, a check is performed to see if a speaker was assigned to\nthe previous chunk. If it was, this suggests that a single speaker’s speech was split into two\npassages, and thus the same speaker is assigned. If the previous chunk did not have a speaker\nassigned to it, then it means that the passage does not contain direct speech, and it is assigned\nthe default speaker (narrator, or first person case).\n",
      "word_count": 612,
      "char_count": 3947,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "ArialMT (6.3pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 38,
      "text": "28\nMethodology\nIn the passage retriever, all relevant passages for each speaker in the document are\ngathered. In both the vector index and the LLM-based approach this is done by only\nsearching the passages which have been assigned the specific speaker. This results in a\nmapping of \"speaker\" to \"relevant passages\". This mapping is then used to executed the\nvalue labelling for each speaker’s relevant passages.\nIn situations where only a single speaker is present in a passage, the prompt described in\nSection 3.2.3 is used. If there are multiple speakers in a passage, a modified prompt template,\ngiven in Figure 3.4, is used. The prompt is designed for the new task of identifying a variable\nvalue based on information from a specific speaker. The format of the prompt is similar to\nthe prompt in Figure 3.3, with the addition of the speaker as part of the input and instructions\nto identify the label based on it.\nThe probabilities of each variable value are computed and normalized as described in\nSection 3.2.3. The Top-k passage-value pairs for each speaker and variable are presented to\nthe user for validation.\n3.4\nFine-tuning\nThe task of obtaining a complete supervised dataset for full fine-tuning and evaluation in\nthis project’s setup is practically infeasible. The documents are very long, containing many\npassages when split up, and each of these passages would need to be manually labelled for\nevery single variable. However, the human validation component in ELICIT yields a smaller\nsupervised dataset, which can be used for fine-tuning to achieve two goals: improving\nperformance on unseen documents, and presenting new extractions in already analyzed\ndocuments.\nA single human validation run gives two types of information for each passage shown\nto the user - whether its relevant for that variable, and if the assigned value is correct for\nthe passage and speaker. This data can be used to further improve the LLM-based passage\nretriever and value labelling components through fine-tuning.\nIn order to evaluate the impact of fine-tuning on unseen documents, four documents are\nheld for testing in the sentencing remarks dataset, and two in the court transcripts dataset.\nThe remaining data samples are split into training and validations sets using an 80-20 ratio.\nStratified sampling was applied to ensure the proportions of variables and variable values were\nconsistent across the training and validation sets. Hyperparameter tuning was accomplished\nusing a grid search to find the configuration resulting in the lowest validation loss.\nAfter a single human validation run, there are some variables which had to be abstained,\nsince the presented passages did not contain relevant or sufficient information. While some\n",
      "word_count": 436,
      "char_count": 2740,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 39,
      "text": "3.5 Metrics\n29\nof these are true abstain data points, the rest represent imperfect recall and the potential\nfor further improvement. Therefore, the models are fine-tuned on the full set of extractions\npresented to the user, keeping 20% of the samples as validation data for early stopping,\nand the extraction is re-run again with the fine-tuned models. The human validator then\nvalidates the previously abstained variables with new extractions presented. This is expected\nto improve recall, and it also creates the opportunity to iteratively improve the model with\nnewly validated data used as training examples.\nThe model fine-tuning is accomplished using Low-Rank Adaptation (LoRA), as intro-\nduced in Section 2.3.5. A result of this method is that only a small fraction of parameters\nare trainable, resulting in a small LoRA adapter containing the weight differences, which\ncan be applied to the base model. The cross-entropy loss function was used with the input\n(prompt) tokens masked, which means that they were ignored when computing the loss and\nthe model was only trained on the output tokens. This was done, since the model is used in\na sequence-to-sequence fashion, where the objective is to maximise the probability of the\noutput sequence given the input sequence:\nθ ∗\nseq2seq = argmax p(yN+1:N+T+1|y0:N;θ)\n(3.5)\nwhere yi is the i-th token, N is the length of the prompt, and T is the length of the completion.\nThis is in contrast to the default auto-regressive objective of maximising the probability of\nthe next token given the previous tokens:\nθ ∗\nAR = argmax p(y0;θ)\nN\n∏\ni=1\np(yi|y0:i−1;θ)\n(3.6)\n3.5\nMetrics\nThis section is dedicated to discussing the metrics used in the project to assess the perfor-\nmance of the different parts of the system. These metrics can be categorized into two distinct\ngroups: end-to-end metrics and supervised dataset metrics.\n3.5.1\nEnd-to-End Metrics\nEnd-to-end metrics are employed to evaluate the performance of the full information extrac-\ntion system, which includes human validation. They are computed using a manually labelled\ngold standard for a set of documents. These metrics are used to compare the LLM-extended\nsystem against the earlier version of ELICIT, and also to compare between different configu-\n",
      "word_count": 362,
      "char_count": 2261,
      "fonts": [
        "NimbusRomNo9L-ReguItal (9.0pt)",
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "CMR10 (9.0pt)",
        "CMSY10 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "StandardSymL (17.2pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "CMSY10 (9.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "CMR10 (12.0pt)",
        "StandardSymL-Slant_167 (12.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 40,
      "text": "30\nMethodology\nrations of the components, such as the different passage retrievers, and the base model versus\nfine-tuned model.\nPrecision\nPrecision, as defined in Equation 3.7, measures how many non-abstain predictions were\ncorrect. The precision is largely determined by the human validator, hence the system is\nexpected to have near-perfect precision. The only instance where losses in precision may\nappear are when the passages provided to the user are misleading, and do not contain the full\ncontext.\nPrecision =\nTrue Positives\nTrue Positives+False Positives = Correct Non-Abstain Predictions\nTotal Non-Abstain Predictions\n(3.7)\nRecall\nRecall, as defined in Equation 3.8, measures how many of the true non-abstain samples\nwere identified by the system. Assuming a near perfect ability by the human validator to\nidentify the correct variable value given a relevant passage, the recall is largely determined\nby the capability of the automated component to retrieve a relevant passage and label it\naccurately. The Top-k parameter, that is, the number of extractions shown to the human\nvalidator, controls the trade-off between recall and time-efficiency. A large Top-k will require\nthe user to scan through many extractions, increasing the likelihood of identifying the correct\nvalue, but consuming more time in the process. The objective of the project is to enhance the\nrecall of system by improving the automated component.\nRecall =\nTrue Positives\nTrue Positives+False Negatives = Correct Non-Abstain Predictions\nTotal Non-Abstain Data Points\n(3.8)\n3.5.2\nSupervised Dataset Metrics\nThe supervised metrics are a set of metrics used to evaluate the individual components of the\nsystem, specifically to compare the fine-tuned models against the base models. As explained\nin Section 3.4, a single extraction run with human validation results in labelled datasets of\npassages and values. Despite being imbalanced, containing significantly more irrelevant\npassages, these datasets represent a true use case of doing extraction with the system and\nimproving it through human validation and fine-tuning.\n",
      "word_count": 313,
      "char_count": 2100,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 41,
      "text": "3.5 Metrics\n31\nReceiver Operating Curve and Area Under the Receiver Operating Curve\nThe Receiver Operating Characteristic (ROC) curve is used to represent the performance of a\nmodel in a binary classification task. It is a plot of the true positive rate (recall) in the y-axis\nagainst the false positive rate in the x-axis at different classification thresholds applied to the\nmodel’s predicted probability for the positive class.\nThe ROC curve is employed in the passage retriever analysis, since it is a binary classifi-\ncation task - a passage is labeled as relevant or not. The ROC curve gives us an insight into\nthe performance of the retriever at various thresholds.\nThe Area Under the ROC curve (ROC-AUC) gives an aggregate measure of the classifier’s\noverall performance. It quantifies the probability that a randomly chosen true positive sample\nwill be ranked higher, i.e. have a higher confidence score, than a randomly chosen true\nnegative sample (Hanley and McNeil, 1982). This aligns with the passage retrieval task,\nwhere the top-n passages based on their confidence scores are retrieved, thus the aim is for\nthe classifier to rank the true positives higher than the true negatives.\nPrecision-Recall Curve and Average Precision\nThe Precision-Recall (PR) curve illustrates the effectiveness of the binary classifier predic-\ntions. It is a plot of the precision (y-axis) against recall (x-axis) at varying classification\nthresholds. In the context of passage retrieval, the precision measures how many of the\nretrieved passages are truly relevant, while the recall measures how many of the true relevant\npassages are retrieved. The PR curve better reflects a classifier’s performance in heavily\nimbalanced datasets than the ROC curve (Davis and Goadrich, 2006; Saito and Rehmsmeier,\n2015), since the precision is not skewed by a large number of negative samples present in the\ndataset, whereas the ROC curve treats both false negative and false positives equally.\nThe Average Precision (AP) is a summary of the PR curve, which measures the average\nprecision across all recall levels, and is commonly used for classification tasks with imbal-\nanced datasets (Sofaer et al., 2019). In theory, it is computed as the integral of the PR curve,\nbut in practice the following formula is used:\nAP =\n1\n∑N\ni=1 rel(i)\nN\n∑\ni=1\n(P(i)×rel(i))\n(3.9)\nThe term P(i) is the precision at cut-off level i (only including the top-i results), while\nthe term rel(i) is the relevance score for a result at position i, with 1 if the result is relevant,\nand 0 otherwise.\n",
      "word_count": 416,
      "char_count": 2557,
      "fonts": [
        "NimbusRomNo9L-ReguItal (9.0pt)",
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "CMR10 (9.0pt)",
        "CMSY10 (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "StandardSymL (17.2pt)",
        "StandardSymL (12.0pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 42,
      "text": "32\nMethodology\nAP is also employed to evaluate the value labelling system. The computation of AP\nis reflective of the use case of ELICIT. In the value labelling part, each passage receives\na confidence score for each non-abstain (or non-\"unclear\") value. The Top-k of these are\nshown to the user for human validation. The objective is to ensure that most of these results\n(passage-value pairs) have the correct value assigned to them. Therefore, AP is used in the\nsame way as in the binary case, by assigning rel(i) to 1 if the result is assigned the correct\nvalue, and in P(i) computing the amount of correct results present in the result set with\ncut-off i.\nNormalized Discounted Cumulative Gain\nThe Normalized Discounted Cumulative Gain (nDCG), commonly used in information\nretrieval tasks, evaluates the quality of retrieved results by taking into account their relative\nscores or ranking. It is used according to the assumption that relevant results should appear\nhigher in the results list. In the project, this metric is utilized to assess the effectiveness\nof the passage retrieval task, where the objective is to assign higher relevance scores to\ntruly relevant passages. Similarly, in the value labelling task, the aim is to place the correct\nvariable value assignments higher according to their confidence score. Both objectives aim\nto improve the recall of the end-to-end system for smaller Top-k settings, showing fewer but\nrelevant extractions to the human validator. The nDCG score is computed using the same\napproach as AP, but with the following formula:\nnDCGp = DCGp\nIDCGp\nDCGp =\np\n∑\ni=1\nrel(i)\nlog2(i+1)\n(3.10)\nwhere the parameter p defines the number of results under consideration, while rel(i)\nis the relevance score as in Equation 3.5.2. The score is normalized by dividing the score\nwith the ideal DCG score (IDCG), which, in this context, represents the scenario where all\ncorrect documents or values appear at the top of the result list. This normalization enables\nthe comparison of different queries, which, in this task, corresponds to different variables.\nThe nDCG score is related to AP as both take into account the order of the presented\nresults. However, the former emphasizes placing the correct results at the top of the list,\nwhereas the latter focuses on maximising the number of correct results in the list. Thus,\nin the passage retrieval system, AP is a better indicator of performance, given the goal of\nproviding the value labelling part with the maximum amount of truly relevant passages.\nConversely, in the value labelling part, the focus is on displaying the correct extraction to the\nhuman validator as high as possible in the results list, making nDCG a better indicator of\n",
      "word_count": 446,
      "char_count": 2720,
      "fonts": [
        "NimbusRomNo9L-ReguItal (9.0pt)",
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "CMR10 (9.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "StandardSymL (17.2pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "CMR10 (12.0pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 43,
      "text": "3.5 Metrics\n33\nits effectiveness. Both metrics are presented for the two tasks to provide a comprehensive\nperformance evaluation.\n",
      "word_count": 19,
      "char_count": 130,
      "fonts": [
        "NimbusRomNo9L-Medi (12.0pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 44,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 45,
      "text": "Chapter 4\nResults and Discussion\nIn this chapter, I present and discuss the outcomes of several experiments conducted using our\nLLM-enhanced information extraction system on the sentencing remarks and court transcripts\ndatasets. Both sections follow a structured analysis of individual components - the passage\nretriever and value labeller. This is followed by the evaluation of end-to-end performance and\nrecalibration. In Section 4.2, I also address limitations related to speaker-specific information\nextraction, and how fine-tuning impacts overall performance.\n4.1\nSentencing Remarks Dataset\n4.1.1\nEnd-to-end Performance\nIn this section, the results from using ELICIT with all previous labelling functions, referred\nto as ELICIT-1.0, are compared to the results from LLM-enhanced ELICIT with the vector\nindex passage retriever (ELICIT-VI), and the LLM-based passage retriever (ELICIT-LLM).\nIn the human validation step, the validator is presented with the Top-k extractions to\nvalidate if the assigned label is correct based on the passage presented. If none of the\nextractions are valid, then the variable is assigned the \"abstain\" value.\nIt is important to consider that the end-to-end results are validator specific, since the\nhuman validator determines if the extraction contains sufficient information to be assigned a\nspecific label. Nevertheless, they are sufficient to highlight the differences in performance\nbetween the different configurations. In this project, I validated the extractions, however, in\nthe future, a more comprehensive analysis should be accomplished with multiple different\nvalidators.\nAs detailed in Section 3.5.1, precision is primarily determined by the human validator,\nwith losses potentially occurring if the presented passage is misleading. The difference in\n",
      "word_count": 254,
      "char_count": 1799,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Medi (24.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 46,
      "text": "36\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nELICIT-1.0\nELICIT-VI\nELICIT-LLM\nFig. 4.1 Top-3 weighted precision on the complete sentencing remarks dataset.\nobserved precision between different systems is not substantial, as seen in Figure 4.1, nor\ndoes it provide a significant edge for any system. Typically, precision losses result from the\nvalidator indicating a negative value, for example \"no physical abuse\", when the true value\nwas \"abstain\".\nThe recalls of the three configurations are compared in Figure 4.2 for Top-1 and Top-3.\nThe recall is weighted by per-class support to avoid skewing results due to imbalanced\nclass distributions. In Top-3, ELICIT-VI performs better or equally than ELICIT-1.0 in six\nvariables, while ELICIT-LLM outperforms ELICIT-1.0 in seven out of thirteen variables.\nWhen we look at the recall averaged across all variables, both LLM-enhanced systems\noutperform ELICIT-1.0, by 0.06 and 0.11, respectively.\nUsing Top-1 reduces the recall for the LLM configurations by approximately 0.2, as\nseen in Table 4.1, suggesting a weaker automated performance. A performance drop is also\nobserved with ELICIT-1.0, but it is less significant, only by 0.13. The difference in Top-1\nbetween the LLM-enhanced systems and ELICIT-1.0 is less noticeable, by only 0.02.\nTable 4.1 Top-1 and Top-3 mean recall across all variables for different configurations on the\ncomplete sentencing remarks dataset.\nConfiguration\nRecall\nTop-1\nTop-3\nELICIT-1.0\n0.3426\n0.4769\nELICIT-VI\n0.3685\n0.5397\nELICIT-LLM\n0.3687\n0.5892\n",
      "word_count": 255,
      "char_count": 1760,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "DejaVuSans (7.1pt)",
        "DejaVuSans (6.3pt)",
        "NimbusRomNo9L-Medi (12.0pt)",
        "DejaVuSans (5.4pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 47,
      "text": "4.1 Sentencing Remarks Dataset\n37\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nTop-1\nELICIT-1.0\nELICIT-VI\nELICIT-LLM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nTop-3\nELICIT-1.0\nELICIT-VI\nELICIT-LLM\nFig. 4.2 Top-1 and Top-3 weighted recall on the complete sentencing remarks dataset.\n4.1.2\nFine-tuned Performance on Unseen Documents\nIn this part, the performance of the passage retriever and value labeller with fine-tuning is\nanalyzed. Both components are fine-tuned using a supervised dataset, which was created\nby validating the extractions of 15 out of the 19 sentencing remarks with a Top-3 setting,\nresulting in 585 extractions. The components are evaluated on the remaining 4 sentencing\nremarks (156 extractions). The fine-tuning details for this task are presented in Appendix A.1.\nPassage Retriever\nThe different retrievers were tested in a binary classification manner to determine if the\npassage is relevant or not, as labeled by the human validator (the user of ELICIT). For the\nVector Index retriever, the relevance scores between the query and passages were calculated\nas is described in Section 3.2.2. The ROC and PR curves, given in Figure 4.3, computed\nacross all variables together, showcase the general classification capacity of our classifier.\nThe ROC curve and ROC-AUC score demonstrate that the base model retriever struggles\n",
      "word_count": 255,
      "char_count": 1760,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "DejaVuSans (5.9pt)",
        "DejaVuSans (5.5pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "DejaVuSans (6.6pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 48,
      "text": "38\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nReceiver Operating Characteristic (ROC) Curves\nBase (AUC = 0.46)\nFine-tuned (AUC = 0.78)\nVector Index (AUC = 0.67)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall (Sensitivity)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nPrecision-Recall Curves\nBase (AUC-PR = 0.29)\nFine-tuned (AUC-PR = 0.67)\nVector Index (AUC-PR = 0.41)\nFig. 4.3 ROC and Precision-Recall curves for different passage retrievers on supervised\nsentencing remarks data.\nwith this task, performing worse than random guessing1. Despite this, the fine-tuned model\nexcels, even surpassing the embedding model, in terms of ROC-AUC.\nThe PR curve displays a similar performance trend, with the fine-tuned model performing\nthe best, resulting in higher precision over all recall levels. Both metrics indicate the\nimprovement of the retrievers’ ability to classify relevant from irrelevant passages when the\nmodel is fine-tuned on validated data, even when most of the samples are negative.\nThe nDCG score and Average Precision (AP) are utilized to evaluate how well the\nretrievers rank the passages by relevance for each variable. While ROC and PR are computed\n1The classification decision could be reversed in this case. This was tested and did not lead to better\nperformance in other metrics due to the imbalanced dataset.\n",
      "word_count": 221,
      "char_count": 1380,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Regu (7.4pt)",
        "DejaVuSans (6.8pt)",
        "DejaVuSans (4.5pt)",
        "NimbusRomNo9L-Regu (10.1pt)",
        "DejaVuSans (7.7pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 49,
      "text": "4.1 Sentencing Remarks Dataset\n39\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnDCG\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nVulnerable Victim\nPhysical Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nVector Index\nFig. 4.4 Per-variable nDCG score for different passage retrievers on supervised sentencing\nremarks data.\nfor all predictions in the supervised dataset, these are computed per-variable by considering\nthe predictions of each variable separately2. In terms of these ranking scores, as illustrated in\nFigures 4.4 and 4.5, fine-tuning enhances or equals the performance in all variables except\nfor \"Victim Domestic Abuse\".\nThe fine-tuned model has the highest score in 7 out of the 11 variables3, indicating strong\noverall performance. When the scores are averaged across variables, the fine-tuned retriever\nhas lower nDCG and AP than the vector index retriever. This outcome is a result of the\nfine-tuned model marginally improving in most variables and surpassing the vector index,\nbut it performs significantly worse in a few variables, most notably in \"Victim Pregnancy\".\nNo notable correlation was found between the number of positive training samples for a\nvariable and its corresponding scores. Overall, these results indicate that the passage retriever\nresponds positively to fine-tuning from a relatively small amount of human validated data,\nand it is anticipated to perform even better with more validation. Increasing the Top-k setting\nduring the validation of the first few documents could be a viable solution.\nValue Labeller\nIn the value labelling task, the nDCG score increases in six variables, and slightly decreases\nin five for the fine-tuned model compared to the base model, as given in Figure 4.6. Overall,\nthe nDCG score is higher by 0.07 on average across all variables4. Due to the model being\nmore likely to label variables as “unclear”, and the prevalence of it in our dataset, another\nmodel was trained with the non-\"unclear\" data points oversampled to match the number of\n2The PR-AUC in Figure 4.3 is equivalent to AP computed for all predictions.\n3Variables which did not have positive samples were removed when computing nDCG and AP.\n4Uniform averaging is used when computing the average across all variables, i.e. each variable has equal\nweighting.\n",
      "word_count": 363,
      "char_count": 2363,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "DejaVuSans (5.8pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Regu (7.4pt)",
        "DejaVuSans (6.2pt)",
        "DejaVuSans (7.1pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 50,
      "text": "40\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Precision\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nVulnerable Victim\nPhysical Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nVector Index\nFig. 4.5 Per-variable Average Precision for different passage retrievers on supervised test\nsentencing remarks data.\n\"unclear\" samples for each variable. This led to a better overall performance in terms of\nnDCG, outperforming the base model in six variables, and the regularly fine-tuned model\nin seven. Averaged across all variables, the nDCG is higher by 0.124 than the base model.\nThis shows that when dealing with an imbalanced dataset, the model benefits from seeing the\npositive samples more frequently, and thus results in better ranking capabilities.\nThe nDCG score evaluates the ability of the model to place the correct predictions higher.\nAs the fine-tuned models result in higher nDCG, this is expected to transfer to stronger\nperformance in smaller Top-k settings, as the quality of extractions returned will be better.\nIn terms of AP, visualized in Figure 4.7, both fine-tuned models displayed improvements\nin seven variables. On average, the standard fine-tuned model achieved an AP score higher by\napproximately 0.13, while the model trained on oversampled minority data showed a larger\nimprovement of 0.18. AP summarizes the precision of the models at various recall levels,\nthus, this together with the nDCG scores signifies the strength of the fine-tuned automated\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnDCG\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nVulnerable Victim\nPhysical Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nMinority Oversampled\nFig. 4.6 Per-variable nDCG score for different value labellers on supervised test sentencing\nremarks data.\n",
      "word_count": 286,
      "char_count": 1945,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "DejaVuSans (5.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "DejaVuSans (6.2pt)",
        "DejaVuSans (7.1pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 51,
      "text": "4.1 Sentencing Remarks Dataset\n41\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Precision\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nVulnerable Victim\nPhysical Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nMinority Oversampled\nFig. 4.7 Per-variable Average Precision for different value labellers on supervised test\nsentencing remarks data.\ncomponent of ELICIT, and suggests that using human validated data is expected to result in\nbetter value labelling on unseen documents.\nEnd-to-end Performance\nSince the supervised dataset was created from a single human-validation run, it means that\nthe components were evaluated only on a fraction of the passages from the documents.\nBy running a complete extraction process using fine-tuned models, we can see how the\nsystem operates when presented with full unseen documents. As given in Table 4.2, for\nTop-3, ELICIT-1.0 performs worse than ELICIT-VI, but slightly better than ELICIT-LLM.\nHowever, fine-tuning resulted in a significant improvement for the LLM-enhanced systems,\nwith FT-ELICIT-VI improving by 0.09, while FT-ELICIT-LLM improved by 0.084, resulting\nin a higher recall than ELICIT-1.0. Overall, for Top-3, FT-ELICIT-VI has the highest recall,\nconsistent with the stronger per-variable nDCG and AP scores of the vector index retriever.\nAn important observation is the enhanced performance in Top-1, which is the most\nrestrictive setting, requiring the best performance from the automated components in a\nnarrow recall window. FT-ELICIT-VI only improves by 0.04 from its base configuration,\nwhile FT-ELICIT-LLM improves by close to 0.15, even showing better performance than\nFT-ELICIT-VI. The difference in these two configurations indicates the differing performance\nof the passage retrievers for small Top-k settings. FT-ELICIT-LLM performs better in Top-1,\nwhich suggests that the LLM-based retriever is better at positioning the most relevant passage\nat the top, however, in Top-3, FT-ELICIT-VI achieves the highest recall, meaning that it\nplaces better passages at the second and third position. As more and better-quality data are\nused for training, it is expected that FT-ELICIT-LLM would surpass FT-ELICIT-VI across a\nwider Top-k range.\n",
      "word_count": 327,
      "char_count": 2281,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "DejaVuSans (5.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "DejaVuSans (6.2pt)",
        "DejaVuSans (7.1pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 52,
      "text": "42\nResults and Discussion\nTable 4.2 Top-1 and Top-3 recall on unseen sentencing remarks for different configurations.\n\"FT\" refers to fine-tuned configurations.\nConfiguration\nRecall\nTop-1\nTop-3\nELICIT-1.0\n0.3141\n0.4231\nELICIT-VI\n0.3397\n0.4744\nELICIT-LLM\n0.2500\n0.4038\nFT-ELICIT-VI\n0.3782\n0.5641\nFT-ELICIT-LLM\n0.3974\n0.4872\nIn conclusion, these results demonstrate the strength of using human validated data from\nextractions on other documents to fine-tune both the passage retriever and the value labeller.\nThey both learn to position the most relevant passages and values at the top of the result list\nand can extract information better from unseen documents.\n4.1.3\nFine-tuning for Recalibration\nAfter the model is fine-tuned using labeled data from a single human validation run, the\nextraction process could be run again to identify new potential extractions from previously\nabstained variables. This potential improvement in the LLM-enhanced system primarily\noriginates from two sources: the fine-tuned LLM passage retriever returning more relevant\npassages; and the fine-tuned value labeller assigning values with a greater accuracy and\nconfidence. As illustrated in Figure 4.8, the performance of the fine-tuned ELICIT-LLM\nsystem showcases discernible improvements. Specifically, out of 13 variables, enhancements\nwere observed in 10, increasing the average recall by 0.097 to 0.687. Conversely, ELICIT-VI\nexhibited a slightly smaller increase by 0.06, attributable mainly to the enhancement in the\nvalue labeller alone.\nOverall, these results showcase the significance of human-validated data as an invaluable\ninformation source for the models. Using this data not only improves the performance on\nalready extracted documents but also enhances the extraction of previously unseen ones\nthrough fine-tuning the models.\n",
      "word_count": 256,
      "char_count": 1823,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 53,
      "text": "4.2 Court Transcripts Dataset\n43\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nELICIT-LLM\nFT-ELICIT-LLM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nELICIT-VI\nFT-ELICIT-VI\nFig. 4.8 Improvement in recall for ELICIT-LLM and ELICIT-VI from recalibration using\nfine-tuning on extracted sentencing remarks.\n4.2\nCourt Transcripts Dataset\n4.2.1\nPerformance on Unseen Documents\nIn this part, the passage retriever and value labeller were fine-tuned on validated Top-3\nextractions of 3 out of 5 court transcripts, resulting in 663 extractions. The remaining\ntranscripts form the test set containing 485 extractions. The fine-tuning details for this task\nare presented in Appendix A.2.\nPassage Retriever\nTranscripts of court proceedings have a known dialogue structure containing speaker tags,\nand featuring questions followed by answers. This inherent structure provides contextual\ncues that aid passage retrievers in assessing the passage relevance. In contrast, sentencing\nremarks lack these cues and contain more nuanced and indirect language.\nThese differences manifest in the performance of passage retrievers on unseen human\nvalidated court transcript passages. The ROC curve, presented in Figure 4.9, demonstrates\nbetter performance across all passage retrievers compared to the sentencing remarks dataset\nin Figure 4.3. The fine-tuned retriever emerges as the best with an ROC-AUC of 0.91. In\ncomparison, the base retriever and vector index achieve scores of 0.65 and 0.70, respectively.\nWhile ROC evaluates the general performance of the binary classifier, the Precision-\nRecall suggest how the model performs considering class imbalance. The retriever, fine-tuned\non user-validated extractions from court transcripts, shows strong performance with an AUC-\nPR of 0.70, while the lower scores for the base and vector index retriever suggest them\nstruggling with the minority class. The AUC-PR achieved by the retrievers on the court\ntranscripts is similar as on the sentencing remarks, hence, the difference in AUC-ROC might\n",
      "word_count": 317,
      "char_count": 2214,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "DejaVuSans (5.7pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "DejaVuSans (6.1pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "DejaVuSans (6.9pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 54,
      "text": "44\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nReceiver Operating Characteristic (ROC) Curves\nBase (AUC = 0.65)\nFine-tuned (AUC = 0.91)\nVector Index (AUC = 0.70)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall (Sensitivity)\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nPrecision-Recall Curves\nBase (AUC-PR = 0.35)\nFine-tuned (AUC-PR = 0.70)\nVector Index (AUC-PR = 0.37)\nFig. 4.9 ROC and Precision-Recall curves for different passage retrievers on supervised court\ntranscript data.\nbe due to its sensitivity to class imbalanced. The court transcripts dataset contained fewer\npositive samples, specifically 16%, compared to 25% in the sentencing remarks.\nEvaluation using nDCG and AP reveals similar trends. In terms of nDCG, which focuses\non how well the retriever positions relevant passages at the top of the results list, the fine-\ntuned retriever achieves the highest mean score of 0.85, improving from the base model’s\nscore of 0.66, and ahead of the vector index score of 0.79. In terms of AP, which measures\nprecision across varying recall levels, the fine-tuned retriever leads again with a mean score\nof 0.74, while the vector index and base model score 0.63 and 0.46, respectively.\nThe improvements in nDCG and AP across all variables, as seen in Figures 4.10 and\n4.11, highlight the fine-tuned retriever’s effectiveness on court transcripts. Furthermore, it\n",
      "word_count": 226,
      "char_count": 1402,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "DejaVuSans (6.8pt)",
        "DejaVuSans (4.5pt)",
        "DejaVuSans (7.7pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 55,
      "text": "4.2 Court Transcripts Dataset\n45\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnDCG\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nVector Index\nFig. 4.10 Per-variable nDCG score for different passage retrievers on supervised test court\ntranscript data.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Precision\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nVector Index\nFig. 4.11 Per-variable Average Precision for different passage retrievers on supervised test\ncourt transcript data.\nshowcases better performance across most variables compared to the vector index. These\nresults indicate the model’s successful learning from human-validated data, resulting in\nsuperior ranking and precision capabilities on unseen court transcript documents.\nValue Labeller\nThe value labelling component, used for dialogue texts, is responsible for assigning variable\nvalues to passages based on specific speakers. If a speaker does not provide a value, it is\ninstructed to assign the \"unclear\" value. The evaluation of nDCG and AP follows the same\napproach as in the sentencing remarks scenario, by comparing every non-zero probability\nnon-”unclear” prediction with the correct label. As in Section 4.1.2, the model is fine-tuned\nwith two approaches: using the complete imbalanced training set, and with oversampling the\nminority values for each variable.\nIn terms of nDCG, the labeller improves significantly from fine-tuning. On average,\nthe regular fine-tuned model shows an improvement of 0.21, while the oversampled model\n",
      "word_count": 255,
      "char_count": 1814,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "DejaVuSans (5.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "DejaVuSans (6.2pt)",
        "DejaVuSans (7.1pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 56,
      "text": "46\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnDCG\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nMinority Oversampled\nFig. 4.12 Per-variable nDCG score for different value labellers on supervised test court\ntranscript data.\nimproves by 0.13. Furthermore, as illustrated in Figure 4.12, the regular fine-tuned model\nimproves over the base model in 10 out of 12 variables, while the oversampled model\nimproves in 11. Notably, the increases in the regular fine-tuned model are more significant,\nsurpassing the oversampled model’s nDCG scores in all 10 of the improved variables.\nWith AP, as given in Figure 4.13, the same trends are apparent, with the regularly\nfine-tuned model resulting in higher mean AP, and improving in 9 variables over the base\nmodel.\nThe presented results outline the significant benefits of fine-tuning when dealing with\nthe complexity of speaker-based labelling. This fine-tuning results in an improved value\nlabeller, which more effectively places correct values at the top of the result list, and with\ngreater confidence. Interestingly, it was also observed that contrary to the performance on\nsentencing remarks, oversampling the minority classes does not lead to better performance\nthan using the full imbalanced dataset.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Precision\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nMinority Oversampled\nFig. 4.13 Per-variable Average Precision for different value labellers on supervised test court\ntranscript data.\n",
      "word_count": 262,
      "char_count": 1815,
      "fonts": [
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "DejaVuSans (5.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "DejaVuSans (6.2pt)",
        "DejaVuSans (7.1pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 57,
      "text": "4.2 Court Transcripts Dataset\n47\nTable 4.3 Top-1 and Top-3 recall results on unseen court transcripts for different configu-\nrations. \"Per-variable\" is obtained by taking the mean of the weighted recalls across all\nvariables. \"Total\" is obtained by computing the recall considering all data points equally.\nConfiguration\nPer-variable\nTotal\nTop-1\nTop-3\nTop-1\nTop-3\nELICIT-VI\n0.4815\n0.5858\n0.5172\n0.6724\nELICIT-LLM\n0.3321\n0.4595\n0.4310\n0.6207\nFT-ELICIT-VI\n0.5545\n0.6494\n0.6207\n0.7759\nFT-ELICIT-LLM\n0.6390\n0.7386\n0.7586\n0.8621\nEnd-to-end Performance\nThe evaluation of the passage retriever and value labeller on the human validated supervised\ndataset serves as a proxy for the true end-to-end performance. This subsection focuses on the\nevaluation of the different configurations with base and fine-tuned models to determine if the\nperformance improvements in the supervised dataset transfer to improvements on complete\nunseen transcripts with the human validation element.\nAs discussed in Section 4.1.2, precision is predominantly determined by the human\nvalidator rather than the automated component. Consequently, no significant differences in\nprecision between various configurations were observed.\nThe mean recall results, given in Table 4.3, align with the trends identified in the evalu-\nations on the supervised dataset. Among the base model configurations, the system using\nthe vector index passage retriever (ELICIT-VI) performs better due to the better retrieving\ncapabilities using the embedding model compared to the LLM (ELICIT-LLM).\nWith fine-tuning, the model with both LLM components (FT-ELICIT-LLM) achieves the\nhighest recall. Notably, it improves from its base model by a more significant margin (0.31\nin Top-1 and 0.28 in Top-3, per-variable) than FT-ELICIT-VI (0.07 in Top-1 and 0.06 in\nTop-3, per-variable), indicating that the majority of the recall gain comes from the fine-tuned\npassage retriever.\n4.2.2\nFine-tuning for Recalibration\nFine-tuning the models with a complete human validation dataset (consisting of all five\ntranscripts) and running extraction again to find new potential extractions results in similar\ntrends as in previous tasks. As expected, the full LLM-enhanced system benefits significantly\nfrom fine-tuning due to the improvement of the value labeller and passage retriever, resulting\nin a mean improvement in recall of 0.12 per-variable and in total, as given in Table 4.4.\n",
      "word_count": 348,
      "char_count": 2419,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 58,
      "text": "48\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nTop-3\nELICIT-VI\nELICIT-LLM\nFT-ELICIT-VI\nFT-ELICIT-LLM\nFig. 4.14 Per-variable recall for different configurations on the unseen court transcripts with\nTop-3. FT-ELICIT-LLM outperforms its base configuration in 11 out of 12 variables, and\nachieves the highest overall recall in 10 variables.\nTable 4.4 Improvement in mean total and per-variable recall on unseen court transcripts from\nrecalibration through fine-tuning.\nConfiguration\nPer-variable\nTotal\nOriginal\nImproved\nOriginal\nImproved\nELICIT-VI\n0.5245\n0.5828\n0.5945\n0.6667\nELICIT-LLM\n0.5088\n0.6290\n0.5882\n0.7059\nConversely, the configuration using the vector index passage retriever improves by 0.06\nper-variable and 0.07 across all data points. The recalibrated ELICIT-LLM results in the\nhighest overall recall, and it also improves in more variables (eight versus ELICIT-VI’s five),\nas visualized in Figure 4.15.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nELICIT-LLM\nFT-ELICIT-LLM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nELICIT-VI\nFT-ELICIT-VI\nFig. 4.15 Improvement in recall for ELICIT-LLM and ELICIT-VI from recalibration using\nfine-tuning on extracted court transcripts.\n",
      "word_count": 211,
      "char_count": 1564,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "DejaVuSans (5.7pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "DejaVuSans (6.1pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "DejaVuSans (5.8pt)",
        "DejaVuSans (6.9pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "DejaVuSans (6.2pt)",
        "DejaVuSans (7.1pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 59,
      "text": "4.2 Court Transcripts Dataset\n49\n4.2.3\nRecall and Shown Extractions\nWith the objective of finding every variable value indicated by a speaker, the number of\nextractions shown to the user can quickly increase and become cumbersome. Specifically,\nfor a single document, the number of extractions shown the user is the product of the number\nof speakers, the number of variables, and k (in Top-k). While the user defines the number of\nvariables and k, the speaker count is inherent to the document. A limitation of this is that any\nspeaker which appears in the transcript will have extractions associated with it, even though\nin reality, they might not have given a relevant value. To avoid showing the user unnecessary\nextractions, a good automated component is essential.\nThis part details how for the same recall, fine-tuned configurations require fewer extrac-\ntions be shown to the user, and how the fine-tuned labeller better separates relevant from\nirrelevant speakers for given passages.\nFigure 4.16 illustrates the mean recall against the number of extractions shown to the\nuser from a Top-3 validation run on the unseen court transcripts. The maximum number of\nextractions shown to the user is 448, when all top 3 extractions are shown for each speaker\nand variable combination. As fewer examples are shown to the user, based on their confidence\nscores, a logarithmic decrease in recall is observed across all configurations. This logarithmic\ntrend signifies that, for equivalent recall levels, more efficient systems need a significantly\nsmaller number of displayed extractions.\n0\n100\n200\n300\n400\nShown Extractions\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nRecall vs Shown Extractions\nELICIT-VI\nELICIT-LLM\nFT-ELICIT-VI\nFT-ELICIT-LLM\nFig. 4.16 Recall against number of shown extractions in a Top-3 run on unseen court\ntranscripts. FT-ELICIT-LLM reaches the top recall level of ELICIT-LLM (achieved at 404\nextractions) with just 120 extractions; of ELICIT-VI (achieved at 396) with 142; and of\nFT-ELICIT-VI (achieved at 259) with 191 extractions.\n",
      "word_count": 321,
      "char_count": 2041,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "DejaVuSans (7.3pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "DejaVuSans (5.8pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 60,
      "text": "50\nResults and Discussion\nThis outlined setup can be conceptualized as a global Top-k setting, which is applied\nacross the entire extraction set rather than per variable. Thus, users can save manual extraction\ntime while only marginally compromising on recall. This also indicates the benefits of fine-\ntuning on human-validated data, and as the models are trained on more data, they will require\nfewer extractions to be shown to the user.\nIn the analysis of unseen court transcripts, certain passages are repeatedly presented\nas part of extractions attributed to different speakers, however, not all of the speakers are\nrelevant for the specific variable. An important part of our system is for the automated\ncomponent to assign a lower confidence or rank for extractions associated with irrelevant\nspeakers compared to those with relevant speakers.\nTo quantify this disparity in ranking, a comparative approach is employed by measuring\nthe difference in rankings5 between relevant and irrelevant speaker extractions for identical\npassage-variable pairs, which were presented to the user for validation. This evaluation\nfocuses on the configuration using the vector index passage retriever, enabling the isolation\nof the value labeller’s performance.\nIt was found that FT-ELICIT-VI ranked the extractions of the relevant speaker on average\n31.23 positions higher than the irrelevant one’s, while ELICIT-VI only separated them by\n9.18 positions. This suggests that fine-tuning improves the model’s ability in differentiating\nrelevant from irrelevant speakers for an identical passage and variable.\nA specific example of this is the “Justice” speaker, which in the unseen transcripts only\nhas procedural speech, not giving any valuable information. Thus, any extraction associated\nwith it should be deemed irrelevant. However, the \"Justice\" dialogue tag appears in essential\npassages, such as the counsel’s opening or closing remarks, which are full of important\ninformation. The value labeller is required to distinguish that the information in these\npassages is not associated with the \"Justice\" speaker, resulting in low confidence extractions.\nThe distribution of per-variable rankings for each \"Justice\" extraction — where each is ranked\ncompared to other extractions of the same variable — is depicted in Figure 4.17 for ELICIT-\nVI and FT-ELICIT-VI. Notably, both configurations effectively rank these extractions with\nlow priority. However, the fine-tuned system displays a more favorable outcome, with a\nhigher mean rank and reduced variance compared to the base configuration.\n5The difference in rankings is used, since the confidence scores among various configurations are not\ncalibrated, making them unreliable for comparison.\n",
      "word_count": 404,
      "char_count": 2738,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (10.0pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Regu (7.4pt)",
        "NimbusRomNo9L-Regu (10.1pt)",
        "NimbusRomNo9L-Regu (9.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 61,
      "text": "4.2 Court Transcripts Dataset\n51\n5\n10\n15\n20\n25\n30\n35\nRank\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nFrequency\nDistribution of Per-Variable Rankings\nELICIT-VI\nFT-ELICIT-VI\nFig. 4.17 Per-variable ranking distribution of extractions attributed to the \"Justice\" speaker.\n",
      "word_count": 40,
      "char_count": 265,
      "fonts": [
        "DejaVuSans (11.4pt)",
        "DejaVuSans (10.6pt)",
        "DejaVuSans (9.1pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 62,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 63,
      "text": "Chapter 5\nConclusions\nIn this project I delved into the analysis and evaluation of utilizing LLMs for the extraction\nof structured information from long unstructured documents, specifically in the legal domain,\nfocusing on sentencing remarks and court transcripts. This chapter presents a recap of the\nproject, highlights the key findings, and proposes promising future research directions.\n5.1\nProject Recap\nThe primary objective of this project was to extend the capabilities of ELICIT by integrating\nthe use of LLMs. The study focused on two particular tasks with distinct datasets: the\nextraction of single corresponding variable values from sentencing remarks, and the novel\ntask of extracting speaker-specific variable values from court transcripts.\nTo overcome the challenges associated with the length of the documents, the approach of\nsplitting the documents into smaller passages was taken. This resulted in the development\nand analysis of two separate components: (a) the passage retriever, which identifies relevant\npassages, and (b) the value labeller, which assigns variable values to these passages. The\nsubsequent evaluation of these components led to a comprehensive investigation of how\nLLMs perform in the task of IE, the impact of the individual components to the overall\nperformance, and how human validated data gathered through the use of ELICIT could be\nutilized for fine-tuning.\n5.2\nKey Findings\nThe evaluation on sentencing remarks and court trancripts, in supervised data and end-to-end\nsettings, led to several key findings.\n",
      "word_count": 231,
      "char_count": 1553,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (24.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 64,
      "text": "54\nConclusions\nA comparative analysis of two passage retrievers — a vector index using the ’all-mpnet-\nbase-v2’ embedding model, and the Vicuna-13B LLM, which was prompted to identify if the\npassage is relevant or not — revealed that the LLM-based retriever initially underperformed\nbut significantly improved from fine-tuning with human-validated data. In the case of court\ntranscripts, the LLM-based retriever even outperformed the embedding model in both the\nsupervised and end-to-end setting. A possible explanation for this is that the fine-tuned model\naligns with the particular task and user, whereas the embedding model relies on semantic\ninformation and might result in misalignment.\nThe value labeller was designed as an LLM prompted to classify passages into the\nvariable values based on the specific speaker. Fine-tuning this component led to improved\nranking capabilities, which in turn resulted in better overall recall of the system. In the\ncase of sentencing remarks, addressing data imbalance in human-validated data through\noversampling the minority classes resulted in significant improvements. In court transcripts,\nfine-tuning notably improved the separation of values attributed in the same passages to\nrelevant and irrelevant speakers.\nThe observations from the analysis of the separate components translated into the full\nend-to-end setting. Fine-tuned configurations, especially ones using the LLM-based retriever,\nresulted in better overall recall compared to base model configurations and the current\nELICIT system. Additionally, the passage retriever was identified as the main factor in the\noverall performance of the LLM-enhanced ELICIT, with fine-tuning leading to substantial\nimprovements in recall.\nThe final observation of the project was that the fine-tuned LLM systems resulted in\nincreased efficiency in the human-in-the-loop format. Due to their enhanced automated\nperformance, they required significantly fewer extractions shown to the user to achieve the\nsame recall, thereby reducing potential human validation efforts.\n5.3\nLimitations\nThe main limitations of the project are the following:\n1. The approach of splitting the document and doing extraction on smaller passages\nmeans that the system cannot extract information which spans multiple passages, and\nis limited to information which is contained in a single passage.\n2. The project did not consider the processing time of the LLM-based passage retriever,\nwhich results in longer processing times, as the model is prompted for every passage.\nHowever, this can be done off-line before presenting the results to the human validator.\n",
      "word_count": 382,
      "char_count": 2628,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-ReguItal (11.9pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-ReguItal (12.1pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 65,
      "text": "5.4 Future Directions\n55\n3. The court transcripts used are semi-synthetic; thus it is unclear how the shift to real\ndata would affect the results.\n4. The embedding model used in the vector index was not fine-tuned on human-validated\ndata.\n5. The project did not have access to long-context models, which could have been used\nas a baseline.\n5.4\nFuture Directions\nThis study into the application of LLMs for legal information extraction in the legal domains\nopens up potential avenues for future research. The new task of speaker-centric information\nextraction is of great promise. It was demonstrated that the base model struggles to distinguish\ninformation based on the speaker, with the fine-tuned model performing marginally better.\nThus, the burden of validating if the specified speaker provided the information still falls\non the human validator. Alternative approaches through prompt engineering or fine-tuning\ncould be explored for potential improvement.\nDrawing inspiration from the success of using human-validated data for supervised fine-\ntuning, the application of Reinforcement Learning from Human Feedback (RLHF) (Bai et al.,\n2022; Ouyang et al., 2022) for fine-tuning presents an interesting path for improving passage\nretrieval and value labelling based on human preferences. Particularly in passage retrieval,\nthe imperfect recall indicates that there are passages, which are relevant but the user does\nnot get to validate. If these passages differ significantly from the ones being retrieved and\nvalidated, standard fine-tuning might not solve this. However, RLHF could help address this\nissue through the exploration of the RL agent.\nWhile this project predominantly focused on the broader aspects of the IE process using\nLLMs, namely passage retrieval, value labelling, and fine-tuning, future investigations could\nbe done into improving the prompts used, for example, by exploring the use of LLMs for\ndesigning prompts (Zhou et al., 2023).\nAnother area of future exploration lies in alternative strategies for dealing with long\ndocuments. The use of memory transformers (Bulatov et al., 2023) could be explored to\novercome the limited context length of the LLMs and enable the extraction system to be\nmore context-aware.\nLastly, given the sensitivity of legal data, additional effort should be given into the\nexamination of potential biases inherent in the system, as well as potential mitigation\nstrategies (Nozza et al., 2021, 2022).\n",
      "word_count": 373,
      "char_count": 2457,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 66,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 67,
      "text": "References\nAghajanyan, A., Zettlemoyer, L., and Gupta, S. (2020). Intrinsic Dimensionality Explains\nthe Effectiveness of Language Model Fine-Tuning.\nBahdanau, D., Cho, K., and Bengio, Y. (2016). Neural Machine Translation by Jointly\nLearning to Align and Translate.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S.,\nGanguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk,\nS., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston, S., Kravec, S.,\nLovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah,\nC., Mann, B., and Kaplan, J. (2022). Training a Helpful and Harmless Assistant with\nReinforcement Learning from Human Feedback.\nBansal, N., Sharma, A., and Singh, R. K. (2019). A Review on the Application of Deep\nLearning in Legal Domain. In MacIntyre, J., Maglogiannis, I., Iliadis, L., and Pimenidis, E.,\neditors, Artificial Intelligence Applications and Innovations, IFIP Advances in Information\nand Communication Technology, pages 374–381, Cham. Springer International Publishing.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan,\nT., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler,\nE., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,\nSutskever, I., and Amodei, D. (2020). Language Models are Few-Shot Learners.\nBulatov, A., Kuratov, Y., and Burtsev, M. S. (2023). Scaling Transformer to 1M tokens and\nbeyond with RMT.\nButcher, B., Zilka, M., Cook, D., Hron, J., and Weller, A. (2023). Optimising Human-\nMachine Collaboration for Efficient High-Precision Information Extraction from Text\nDocuments.\nCarnaz, G., Nogueira, V., Antunes, M., and Ferreira, N. (2020). An Automated System for\nCriminal Police Reports Analysis.\nChase, H. (2022). LangChain. https://github.com/hwchase17/langchain.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang,\nY., Gonzalez, J. E., Stoica, I., and Xing, E. P. (2023). Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality.\n",
      "word_count": 335,
      "char_count": 2249,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Medi (24.8pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-ReguItal (11.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 68,
      "text": "58\nReferences\nChoi, E., Hewlett, D., Uszkoreit, J., Polosukhin, I., Lacoste, A., and Berant, J. (2017).\nCoarse-to-Fine Question Answering for Long Documents. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 209–220, Vancouver, Canada. Association for Computational Linguistics.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani,\nM., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A.,\nCastro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao,\nV., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A.,\nZhou, D., Le, Q. V., and Wei, J. (2022). Scaling Instruction-Finetuned Language Models.\nCommon Crawl (2007). Common Crawl. http://commoncrawl.org/.\nDavis, J. and Goadrich, M. (2006). The relationship between Precision-Recall and ROC\ncurves. In Proceedings of the 23rd International Conference on Machine Learning - ICML\n’06, pages 233–240, Pittsburgh, Pennsylvania. ACM Press.\nEccleston, D. (2023). ShareGPT. https://github.com/domeccleston/sharegpt.\nGhoddusi, H., Creamer, G. G., and Rafizadeh, N. (2019). Machine learning in energy\neconomics and finance: A review. Energy Economics, 81:709–727.\nHanley, J. A. and McNeil, B. J. (1982). The meaning and use of the area under a receiver\noperating characteristic (ROC) curve. Radiology, 143(1):29–36.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas,\nD. d. L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K.,\nvan den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae,\nJ. W., Vinyals, O., and Sifre, L. (2022). Training Compute-Optimal Large Language\nModels.\nHonnibal, M., Montani, I., Van Landeghem, S., and Boyd, A. (2020). spaCy: Industrial-\nstrength Natural Language Processing in Python.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A.,\nAttariyan, M., and Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2021).\nLoRA: Low-Rank Adaptation of Large Language Models.\nHu, X., Chen, P.-Y., and Ho, T.-Y. (2023). RADAR: Robust AI-Text Detection via Adversarial\nLearning.\nHuber,\nJ. (2022).\nChroma:\nThe AI-native open-source embedding database.\nhttps://github.com/chroma-core/chroma.\nIzacard, G. and Grave, E. (2021). Leveraging Passage Retrieval with Generative Models for\nOpen Domain Question Answering.\nJavaid, M., Haleem, A., Pratap Singh, R., Suman, R., and Rab, S. (2022). Significance of\nmachine learning in healthcare: Features, pillars and applications. International Journal\nof Intelligent Networks, 3:58–73.\n",
      "word_count": 412,
      "char_count": 2822,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (12.0pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-ReguItal (12.1pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-ReguItal (11.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 69,
      "text": "References\n59\nKamalloo, E., Zhang, X., Ogundepo, O., Thakur, N., Alfonso-Hermelo, D., Rezagholizadeh,\nM., and Lin, J. (2023). Evaluating Embedding APIs for Information Retrieval.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. (2020). ALBERT:\nA Lite BERT for Self-supervised Learning of Language Representations.\nLawrence, J. and Reed, C. (2019). Argument Mining: A Survey. Computational Linguistics,\n45(4):765–818.\nLi, J., Liu, M., Kan, M.-Y., Zheng, Z., Wang, Z., Lei, W., Liu, T., and Qin, B. (2020).\nMolweni: A Challenge Multiparty Dialogues-based Machine Reading Comprehension\nDataset with Discourse Structure.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L.,\nand Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach.\narXiv:1907.11692 [cs].\nMalkov, Y. A. and Yashunin, D. A. (2018). Efficient and robust approximate nearest neighbor\nsearch using Hierarchical Navigable Small World graphs.\nManyika, J. (2023). An overview of Bard: an early experiment with generative AI. Google\nAI.\nNozza, D., Bianchi, F., and Hovy, D. (2021). HONEST: Measuring Hurtful Sentence\nCompletion in Language Models. In Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 2398–2406, Online. Association for Computational Linguistics.\nNozza, D., Bianchi, F., and Hovy, D. (2022). Pipelines for Social Bias Testing of Large\nLanguage Models. In Proceedings of BigScience Episode #5 – Workshop on Challenges &\nPerspectives in Creating Large Language Models, pages 68–74, virtual+Dublin. Associa-\ntion for Computational Linguistics.\nOpenAI (2022). ChatGPT. https://chat.openai.com.\nOpenAI (2023a). GPT-4. https://openai.com/research/gpt-4.\nOpenAI (2023b). GPT-4 Technical Report.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C.,\nAgarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M.,\nAskell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language\nmodels to follow instructions with human feedback.\nPascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training Recurrent\nNeural Networks.\nQayyum, A., Qadir, J., Bilal, M., and Al-Fuqaha, A. (2020). Secure and Robust Machine\nLearning for Healthcare: A Survey.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2020). Improving Language\nUnderstanding by Generative Pre-Training.\n",
      "word_count": 359,
      "char_count": 2530,
      "fonts": [
        "NimbusRomNo9L-ReguItal (11.8pt)",
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-ReguItal (11.9pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-ReguItal (12.1pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 70,
      "text": "60\nReferences\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and\nLiu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text\nTransformer.\nRajpurkar, P., Jia, R., and Liang, P. (2018). Know What You Don’t Know: Unanswerable\nQuestions for SQuAD. arXiv:1806.03822 [cs].\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016). SQuAD: 100,000+ Questions for\nMachine Comprehension of Text.\nReimers, N. and Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese\nBERT-Networks.\nSaito, T. and Rehmsmeier, M. (2015). The precision-recall plot is more informative than\nthe ROC plot when evaluating binary classifiers on imbalanced datasets. PloS One,\n10(3):e0118432.\nSennrich, R., Haddow, B., and Birch, A. (2016). Neural Machine Translation of Rare Words\nwith Subword Units.\nSofaer, H. R., Hoeting, J. A., and Jarnevich, C. S. (2019). The area under the precision-recall\ncurve as a performance metric for rare binary events. Methods in Ecology and Evolution,\n10(4):565–577.\nSun, C., Qiu, X., Xu, Y., and Huang, X. (2020). How to Fine-Tune BERT for Text Classifica-\ntion?\nSun, K., Yu, D., Chen, J., Yu, D., Choi, Y., and Cardie, C. (2019). DREAM: A Challenge\nDataset and Models for Dialogue-Based Reading Comprehension.\nSun, S., Luo, C., and Chen, J. (2017). A review of natural language processing techniques\nfor opinion mining systems. Information Fusion, 36:10–25.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B.,\nGoyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.\n(2023). LLaMA: Open and Efficient Foundation Language Models.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and\nPolosukhin, I. (2017). Attention Is All You Need.\nWikimedia Foundation (2001). Wikipedia. http://wikipedia.org/.\nWu, C.-S., Madotto, A., Liu, W., Fung, P., and Xiong, C. (2022). QAConv: Question\nAnswering on Informative Conversations.\nXu, W., Grishman, R., and Zhao, L. (2011). Passage Retrieval for Information Extraction\nusing Distant Supervision. In Proceedings of 5th International Joint Conference on\nNatural Language Processing, pages 1046–1054, Chiang Mai, Thailand. Asian Federation\nof Natural Language Processing.\nYang, Y., Wu, Z., Yang, Y., Lian, S., Guo, F., and Wang, Z. (2022). A Survey of Information\nExtraction Based on Deep Learning. Applied Sciences, 12:9691.\n",
      "word_count": 369,
      "char_count": 2462,
      "fonts": [
        "NimbusRomNo9L-ReguItal (11.8pt)",
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-ReguItal (11.9pt)",
        "NimbusRomNo9L-Regu (11.9pt)",
        "NimbusRomNo9L-ReguItal (12.1pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 71,
      "text": "References\n61\nYang, Z. and Choi, J. D. (2019). FriendsQA: Open-Domain Question Answering on TV\nShow Transcripts. In Proceedings of the 20th Annual SIGdial Meeting on Discourse and\nDialogue, pages 188–197, Stockholm, Sweden. Association for Computational Linguistics.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., and Le, Q. V. (2020). XLNet:\nGeneralized Autoregressive Pretraining for Language Understanding.\nYin, W., Hay, J., and Roth, D. (2019). Benchmarking Zero-shot Text Classification: Datasets,\nEvaluation and Entailment Approach. arXiv:1909.00161 [cs].\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D.,\nXing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. (2023). Judging LLM-as-a-judge\nwith MT-Bench and Chatbot Arena.\nZhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. (2023). Large\nLanguage Models Are Human-Level Prompt Engineers.\n",
      "word_count": 139,
      "char_count": 936,
      "fonts": [
        "NimbusRomNo9L-Regu (11.8pt)",
        "NimbusRomNo9L-Medi (12.0pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "NimbusRomNo9L-ReguItal (12.0pt)",
        "NimbusRomNo9L-ReguItal (11.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 72,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 73,
      "text": "Appendix A\nFine-tuning Details\nA.1\nSentencing Remarks\nA.1.1\nPassage Retriever\nTable A.1 Optimal hyperparameters of the Vicuna-13B passage retriever fine-tuned on\nuser-validated sentencing remarks extractions.\nType\nNo. of Training Samples\nNo. of Validation Samples\nBatch Size\nLearning Rate\nRegular\n467\n117\n32\n0.0012\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nLoss\nTraining Loss\nValidation Loss\nFig. A.1 Training and validation loss of the Vicuna-13B passage retriever fine-tuned on\nuser-validated sentencing remarks extractions.\n",
      "word_count": 83,
      "char_count": 559,
      "fonts": [
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "ArialMT (9.4pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Medi (24.8pt)",
        "ArialMT (5.2pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 74,
      "text": "64\nFine-tuning Details\nA.1.2\nValue Labellers\nTable A.2 Optimal hyperparameters of the Vicuna-13B value labellers fine-tuned on user-\nvalidated sentencing remarks extractions.\nParameter\nRegular\nMinority Oversampled\nNo. of Training Samples\n467\n926\nNo. of Validation Samples\n117\n117\nBatch Size\n8\n16\nLearning Rate\n0.0006\n0.00008\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nLoss\nRegular\nTraining Loss\nValidation Loss\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nLoss\nMinority Oversampled\nTraining Loss\nValidation Loss\nFig. A.2 Training and validation losses of Vicuna-13B value labellers fine-tuned on regular\nuser-validated sentencing remarks extractions, and with minority classes oversampled.\n",
      "word_count": 116,
      "char_count": 746,
      "fonts": [
        "ArialMT (4.9pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "ArialMT (8.8pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 75,
      "text": "A.2 Court Transcripts\n65\nA.2\nCourt Transcripts\nA.2.1\nPassage Retriever\nTable A.3 Optimal hyperparameters of the Vicuna-13B passage retriever fine-tuned on\nuser-validated court transcript extractions.\nType\nNo. of Training Samples\nNo. of Validation Samples\nBatch Size\nLearning Rate\nRegular\n424\n113\n32\n0.0012\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nLoss\nTraining Loss\nValidation Loss\nFig. A.3 Training and validation loss of the Vicuna-13B passage retriever fine-tuned on\nuser-validated court transcript extractions.\n",
      "word_count": 82,
      "char_count": 544,
      "fonts": [
        "NimbusRomNo9L-Medi (17.2pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "ArialMT (9.4pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "ArialMT (5.2pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    },
    {
      "page_number": 76,
      "text": "66\nFine-tuning Details\nA.2.2\nValue Labellers\nTable A.4 Optimal hyperparameters of the Vicuna-13B value labellers fine-tuned on user-\nvalidated court transcript extractions.\nParameter\nRegular\nMinority Oversampled\nNo. of Training Samples\n520\n878\nNo. of Validation Samples\n138\n138\nBatch Size\n32\n16\nLearning Rate\n0.0006\n0.00008\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLoss\nRegular\nTraining Loss\nValidation Loss\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nLoss\nMinority Oversampled\nTraining Loss\nValidation Loss\nFig. A.4 Training and validation losses of Vicuna-13B value labellers fine-tuned on regular\nuser-validated court transcript extractions, and with minority classes oversampled.\n",
      "word_count": 115,
      "char_count": 739,
      "fonts": [
        "ArialMT (4.9pt)",
        "NimbusRomNo9L-Regu (12.0pt)",
        "NimbusRomNo9L-Regu (12.1pt)",
        "ArialMT (8.8pt)",
        "NimbusRomNo9L-Medi (14.3pt)",
        "NimbusRomNo9L-Medi (12.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        595.2760009765625,
        841.8900146484375
      ]
    }
  ],
  "extraction_time": 0.3800477981567383,
  "file_size_mb": 0.8887062072753906,
  "basic_extraction": {
    "page_count": 76,
    "text_stats": {
      "total_words": 21498,
      "total_chars": 136405
    },
    "fonts_used": [
      "Arial-BoldMT (8.7pt)",
      "Arial-ItalicMT (6.5pt)",
      "ArialMT (4.9pt)",
      "ArialMT (5.2pt)",
      "ArialMT (6.3pt)",
      "ArialMT (6.5pt)",
      "ArialMT (6.6pt)",
      "ArialMT (7.0pt)",
      "ArialMT (8.7pt)",
      "ArialMT (8.8pt)",
      "ArialMT (9.4pt)",
      "CMEX10 (12.0pt)",
      "CMMI10 (12.0pt)",
      "CMR10 (12.0pt)",
      "CMR10 (9.0pt)",
      "CMSY10 (12.0pt)",
      "CMSY10 (9.0pt)",
      "DejaVuSans (10.6pt)",
      "DejaVuSans (11.4pt)",
      "DejaVuSans (4.5pt)",
      "DejaVuSans (5.4pt)",
      "DejaVuSans (5.5pt)",
      "DejaVuSans (5.7pt)",
      "DejaVuSans (5.8pt)",
      "DejaVuSans (5.9pt)",
      "DejaVuSans (6.1pt)",
      "DejaVuSans (6.2pt)",
      "DejaVuSans (6.3pt)",
      "DejaVuSans (6.6pt)",
      "DejaVuSans (6.8pt)",
      "DejaVuSans (6.9pt)",
      "DejaVuSans (7.1pt)",
      "DejaVuSans (7.3pt)",
      "DejaVuSans (7.7pt)",
      "DejaVuSans (9.1pt)",
      "MSBM10 (12.0pt)",
      "NimbusRomNo9L-Medi (11.8pt)",
      "NimbusRomNo9L-Medi (11.9pt)",
      "NimbusRomNo9L-Medi (12.0pt)",
      "NimbusRomNo9L-Medi (12.1pt)",
      "NimbusRomNo9L-Medi (14.3pt)",
      "NimbusRomNo9L-Medi (17.2pt)",
      "NimbusRomNo9L-Medi (24.8pt)",
      "NimbusRomNo9L-Regu (10.0pt)",
      "NimbusRomNo9L-Regu (10.1pt)",
      "NimbusRomNo9L-Regu (11.8pt)",
      "NimbusRomNo9L-Regu (11.9pt)",
      "NimbusRomNo9L-Regu (12.0pt)",
      "NimbusRomNo9L-Regu (12.1pt)",
      "NimbusRomNo9L-Regu (14.3pt)",
      "NimbusRomNo9L-Regu (7.4pt)",
      "NimbusRomNo9L-Regu (9.0pt)",
      "NimbusRomNo9L-ReguItal (11.8pt)",
      "NimbusRomNo9L-ReguItal (11.9pt)",
      "NimbusRomNo9L-ReguItal (12.0pt)",
      "NimbusRomNo9L-ReguItal (12.1pt)",
      "NimbusRomNo9L-ReguItal (14.3pt)",
      "NimbusRomNo9L-ReguItal (9.0pt)",
      "SFTT1200 (12.0pt)",
      "StandardSymL (12.0pt)",
      "StandardSymL (17.2pt)",
      "StandardSymL-Slant_167 (12.0pt)"
    ],
    "images_count": 2,
    "metadata": {
      "format": "PDF 1.5",
      "title": "Large Language Models for Reliable Information Extraction",
      "author": "Lukas Baliunas",
      "subject": "LaTeX",
      "keywords": "LaTeX Master Thesis Engineering University of Cambridge",
      "creator": "LaTeX with hyperref",
      "producer": "pdfTeX-1.40.24",
      "creationDate": "D:20230817110817Z",
      "modDate": "D:20230817110817Z"
    },
    "pages": [
      {
        "page_number": 1,
        "text": "Large Language Models for Reliable\nInformation Extraction\nLukas Baliunas\nDepartment of Engineering\nUniversity of Cambridge\nThis dissertation is submitted for the degree of\nMaster of Philosophy in Machine Learning and Machine Intelligence\nChurchill College\nAugust 2023\n",
        "word_count": 36,
        "char_count": 268,
        "fonts": [
          "NimbusRomNo9L-ReguItal (14.3pt)",
          "NimbusRomNo9L-Regu (14.3pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Medi (24.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 2,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 3,
        "text": "Declaration\nI, Lukas Baliunas, of Churchill College, being a candidate for the MPhil in Machine Learning\nand Machine Intelligence, hereby declare that this report and the work described in it are my\nown work, unaided except as may be specified below, and that the report does not contain\nmaterial that has already been used to any substantial extent for a comparable purpose.\nAll of the software in this project was developed using Python, and can be found in a\npublic GitHub repository1. This project extended the ELICIT library2 to make use of Large\nLanguage Models and perform speaker-centric information extraction. The Large Language\nModels, used in this project, were retrieved through the Huggingface transformers3 library.\nThe model used in this project, Vicuna4 — a fine-tuned LLaMA model — is a publicly\navailable model with its use subject to the model license of LLaMA. The use of LLaMA was\napproved by Meta AI after filling out a request form. Low-Rank Adaption fine-tuning was\ndone by modifying the code in the Alpaca-LoRA repository5, which is available under an\nApache-2.0 license, and uses the PyTorch6 and transformers libraries. The embedding\nmodel used in the project, ’all-mpnet-base-v2’, came from the SentenceTransformers7\nlibrary. Vector index functionality was provided by the LangChain8 and ChromaDB9 Python\nlibraries. OpenAI’s GPT-4 API10 was used for generating synthetic data under a paid license.\nThis dissertation contains 14993 words, excluding declarations, bibliography, pho-\ntographs and diagrams, but including tables, footnotes, figure captions and appendices.\nLukas Baliunas\nAugust 2023\n1https://github.com/lbaliunas/elicit/tree/lb956-project\n2https://github.com/Bradley-Butcher/elicit\n3https://huggingface.co/docs/transformers\n4https://lmsys.org/blog/2023-03-30-vicuna/\n5https://github.com/tloen/alpaca-lora\n6https://pytorch.org\n7https://www.sbert.net/\n8https://python.langchain.com/\n9https://www.trychroma.com\n10https://openai.com/gpt-4\n",
        "word_count": 257,
        "char_count": 1977,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "SFTT1200 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Regu (7.4pt)",
          "NimbusRomNo9L-Regu (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 4,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 5,
        "text": "Acknowledgements\nI wish to express my sincere thanks to my supervisor Dr. Miri Zilka, and Dr. Jiri Hron for\ntheir guidance and invaluable lessons throughout this project.\nI would also like to thank my girlfriend, Ieva, and my family for their support and\ncontinuous encouragement.\n",
        "word_count": 45,
        "char_count": 281,
        "fonts": [
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 6,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 7,
        "text": "Abstract\nThis project addresses the ongoing challenge of achieving reliable Information Extraction\n(IE), particularly in domains which require near-perfect precision. ELICIT (Butcher et al.,\n2023) introduced a novel approach that combines the processing speed of automated IE tools\nwith the precision of manual annotation through a unique approach of weak supervision\nlabeling and human validation. The existing setup achieved impressive results when it came\nto precision, but the recall of the system can still be greatly improved. Recognizing the\npotential of Large Language Models (LLMs) in tasks that require language understanding,\nthis project focuses on extending ELICIT’s capabilities with LLMs and evaluating their\neffectiveness in the context of structured IE. The research is grounded in the legal domain,\nfocusing on evaluations on UK Crown Court sentencing remarks, and court transcripts. The\nlatter usually contains multiple speakers with different narratives, necessitating the extraction\nof reliable information along with speaker attribution. This project builds upon the existing\nsystem, improving it in two signficant ways: firstly, by introducing LLMs as an extraction\ntool, and secondly, by extending the system’s functionality to extract information attributed\nto specific speakers. The integration of LLMs into ELICIT resulted in improved recall\ncompared to the previous version. Moreover, through the utilization of human-validated\nextractions for fine-tuning, the enhanced system showcased improved recall on both already\nextracted documents and unseen ones. The research demonstrates how fine-tuning with\nlimited data leads to enhanced performance, while also requiring fewer extractions to be\nshown to the user, thereby reducing potential manual efforts.\n",
        "word_count": 250,
        "char_count": 1782,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 8,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 9,
        "text": "Table of contents\n1\nIntroduction\n1\n1.1\nContext and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.3\nThesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2\nBackground\n5\n2.1\nInformation Extraction and Question Answering . . . . . . . . . . . . . . .\n5\n2.2\nMulti-Value Extraction and Source Attribution . . . . . . . . . . . . . . . .\n7\n2.3\nLarge Language Models\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3.1\nAuto-regressive Language Models . . . . . . . . . . . . . . . . . .\n8\n2.3.2\nTransformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.3.3\nLLaMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.3.4\nVicuna\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n2.3.5\nFine-tuning using Low-Rank Adaptation\n. . . . . . . . . . . . . .\n12\n2.4\nELICIT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.4.1\nApproach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.4.2\nLabelling Functions\n. . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3\nMethodology\n17\n3.1\nDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.1.1\nSentencing Remarks . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.1.2\nCourt Transcripts . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.2\nInformation Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.2.1\nApproach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.2.2\nPassage Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.2.3\nValue Labelling . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n3.3\nSource Attribution\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.3.1\nSource Attribution as the Final Component . . . . . . . . . . . . .\n25\n",
        "word_count": 726,
        "char_count": 1946,
        "fonts": [
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)",
          "NimbusRomNo9L-Medi (24.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 10,
        "text": "x\nTable of contents\n3.3.2\nSource Attribution in Passage Retrieval . . . . . . . . . . . . . . .\n25\n3.3.3\nSource Attribution in Value Labelling . . . . . . . . . . . . . . . .\n26\n3.3.4\nImplementation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3.4\nFine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.5\nMetrics\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.5.1\nEnd-to-End Metrics . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n3.5.2\nSupervised Dataset Metrics . . . . . . . . . . . . . . . . . . . . . .\n30\n4\nResults and Discussion\n35\n4.1\nSentencing Remarks Dataset . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.1.1\nEnd-to-end Performance . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.1.2\nFine-tuned Performance on Unseen Documents . . . . . . . . . . .\n37\n4.1.3\nFine-tuning for Recalibration . . . . . . . . . . . . . . . . . . . . .\n42\n4.2\nCourt Transcripts Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n4.2.1\nPerformance on Unseen Documents . . . . . . . . . . . . . . . . .\n43\n4.2.2\nFine-tuning for Recalibration . . . . . . . . . . . . . . . . . . . . .\n47\n4.2.3\nRecall and Shown Extractions . . . . . . . . . . . . . . . . . . . .\n49\n5\nConclusions\n53\n5.1\nProject Recap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.2\nKey Findings\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n5.3\nLimitations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n5.4\nFuture Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nReferences\n57\nAppendix A Fine-tuning Details\n63\nA.1\nSentencing Remarks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\nA.1.1\nPassage Retriever . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\nA.1.2\nValue Labellers . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nA.2\nCourt Transcripts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\nA.2.1\nPassage Retriever . . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\nA.2.2\nValue Labellers . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\n",
        "word_count": 776,
        "char_count": 2165,
        "fonts": [
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 11,
        "text": "Chapter 1\nIntroduction\nIn recent years, Large Language Models (LLMs) have demonstrated exceptional performance\nin tasks which require language understanding (Brown et al., 2020). Among the domains\nstanding to benefit from this advancement is Information Extraction (IE), which concerns\nthe extraction of structured information from unstructured textual data. IE finds applications\nin many fields like healthcare (Javaid et al., 2022; Qayyum et al., 2020), finance (Ghoddusi\net al., 2019), and law (Bansal et al., 2019; Carnaz et al., 2020).\nHowever, these domains often involve sensitive data, the use of which has serious\nconsequences, highlighting the importance of IE with near-perfect precision. The legal sector\nfor instance, is especially sensitive to inaccuracies in IE, which could lead to wasted resources,\nunfair legal outcomes, or the loss of public trust. These issues could be addressed by tasking\na human with IE, but such a solution requires significant time and financial resources.\nConversely, automated methods, while demonstrating rapid processing, compromise on\nprecision of the extracted information (Yang et al., 2022).\nAddressing this challenge, the ELICIT framework, introduced by Butcher et al. (2023),\noffers a novel approach by combining automated IE systems with human validation. This\napproach attains precision comparable to manual annotation, surpassing automated tools,\nand most importantly reduces annotation time relative to manual annotation.\nNevertheless, the limiting element of the system is its imperfect recall, primarily caused\nby the automated component. In light of the promising advancements by LLMs in natural\nlanguage processing, this project extends ELICIT by incorporating LLMs as the automated\nIE element. Furthermore, the objective of this research is to evaluate the effectiveness of\nLLMs in IE.\nSince it is not possible to fit the full documents into LLM prompts due to their context\nwindow limits, the decision was made to divide them into smaller passages, extracting the\ninformation from the most relevant ones. While more commonly embedding models are used\n",
        "word_count": 312,
        "char_count": 2113,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (24.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 12,
        "text": "2\nIntroduction\nfor passage retrieval (Kamalloo et al., 2023), this research also explores the use of prompting\nLLMs for relevant passage retrieval. Moreover, the impact of using validated extractions for\nfine-tuning are evaluated, with the objective of enhancing the system’s performance.\nThe research is grounded in the legal domain through evaluations on UK Crown Court\nsentencing remarks, the same dataset utilized by Butcher et al. (2023). Additionally, a novel\ntask of IE involving source attribution is introduced, focusing on extraction of information\nprovided by the speakers present in the text. Synthetic court transcript, generated using\nGPT-4 (OpenAI, 2023b) based on real sentencing remarks, serve as the basis for evaluation.\nIn the baseline model setting, the developed system exhibits superior recall to the current\nversion of ELICIT. The baseline LLM performs worse than the embedding model in passage\nretrieval, showing the limitation of direct use of general LLMs. However, fine-tuning\non a limited amount of human validated extractions results in significant improvement in\nperformance, matching the embedding model’s performance for sentencing remarks, and\neven exceeding it for court transcripts. This improvement also transfers to the complete IE\nsystem, which includes the human validator. Notably, the fine-tuning results in significant\nincreases in recall in both unseen documents, and already processed ones, requiring fewer\nextractions shown to the human validator.\n1.1\nContext and Motivation\nThe motivation for the project comes from the desire to accomplish structured information\nextraction on various types of documents, specifically those in the legal domain - sentencing\nremarks and transcripts of court proceedings. Sentencing remarks, which are the judge’s\nremarks during the announcement of the sentence to the defendant, follow a consistent\nformat - they are given from the perspective of the judge and directed to the defendant.\nThey encapsulate a lot of information about the case, including the description of the crime,\nsubstantial evidence, and discussions on the mitigating and aggravating factors. Thus, they\npresent a valuable summary of information related to the case, whose extraction would\nbenefit future researchers with more comprehensive datasets from the legal domain.\nThe project is also dictated by the types of variables aimed to be extracted from these\ndocuments. The extraction system is designed to have an open nature, accommodating a\nwide variety of categorical variables. The ability to extract information, regardless if it’s\npresented explicitly, or if it is subjective, is desired. For instance, users should be able to\ndefine simple variables, such as the victim’s sex, but also more complex and subjective one’s\nsuch as the presence of emotional abuse or whether remorse was shown by the defendant.\n",
        "word_count": 428,
        "char_count": 2867,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 13,
        "text": "1.2 Contributions\n3\nTranscripts of court proceedings represent another source of information. Criminal court\nproceedings consist of multiple stages, including counsel’s opening and closing remarks,\nwitness evidence (examination), and others. Thus, these are extensive documents, usually\nrunning into hundreds of pages, presented in dialogue format with each speaker identified by\na dialogue tag. Whereas sentencing remarks are from a single person’s perspective, court\ntranscripts consist of the multiple parties involved in the case. This variety adds a layer of\nsubjectivity to the information presented, since the defense and prosecution often present\nopposing narratives and provide different interpretations of the same facts. The reliability of\nwitness testimonies can also vary, since the examinations are led by the prosecutor or defense\nand will usually consist of questions favouring the respective counsel’s narrative. Therefore,\nit’s evident that court transcripts pose a significantly more challenging setting for information\nextraction.\nThe inherent subjectivity and potential presence of contradictory facts in court transcripts\nnecessitate the need for an alternative information extraction process. Unlike with sentencing\nremarks, where the goal is to extract a single value for a variable, the aim now is to extract all\nvalues presented by each speaker for a given variable. This changes the nature of the problem\nfrom variable-centric to speaker-centric.\nThis type of information could potentially reveal what facts or interpretations of those\nfacts were presented by each counsel. Moreover, what evidence was provided by each witness\nand whether there were any contradictions could be identified. Extraction of data through\nthis novel approach, can lead to a more comprehensive understanding of legal cases, as well\nas, potential practical applications for future cases and policy-making.\n1.2\nContributions\nThe main contributions of this project are the following:\n1. Development of an end-to-end system for structured information extraction employing\nLLMs, and its implementation in the ELICIT codebase. The developed system out-\nperforms the original ELICIT system in terms of recall in sentencing remarks using\nVicuna-13B (Chiang et al., 2023), and with a significant margin on human-validated\ndata after fine-tuning the models.\n2. Introduction of a novel task of information extraction with source attribution, which\nfocuses on the extraction of information for distinct speakers within documents. The\nchallenges inherent in this task are outlined, and a methodology utilizing LLM prompt-\n",
        "word_count": 375,
        "char_count": 2613,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 14,
        "text": "4\nIntroduction\ning is proposed and implemented. The evaluation of this methodology is carried out\non a dataset of synthetic legal court transcripts.\n3. A method of generating synthetic text documents, namely legal court transcripts, is\nproposed, which uses an iterative summarization approach to overcome context window\nlimitations of GPT-4.\n4. Analysis of the impact of human-validated data for LLM-finetuning. This fine-tuning\nenhances recall for already extracted and unseen documents, requiring fewer extractions\nto be shown to the human validator.\n5. Comparison of passage retrieval using a vector index storing passage embeddings\nwith an LLM-based approach. Particularly noteworthy is the demonstration of the\nperformance of Vicuna-13B fine-tuned with limited data, which is comparable to an\nembedding model specifically trained for semantic search.\n1.3\nThesis Outline\nIn Chapter 2, the background pertinent to understanding the concepts utilized in the project\nis introduced. This encompasses the concepts of IE and Question Answering (QA), LLMs,\nand the ELICIT framework.\nIn Chapter 3, the methodology employed to achieve the established research goals is\ndescribed. This includes the selection of datasets, including the generation of synthetic court\ntranscripts, as well as the evaluation metrics. Furthermore, the specific design choices made\nto accomplish information extraction using LLMs are outlined.\nIn Chapter 4, the results of evaluating the different components of the LLM-enhanced\nsystem are presented and discussed, both in a supervised dataset and end-to-end setting.\nIn Chapter 5, an overview of the project is provided, key findings are reiterated, and\npotential future research directions are proposed.\n",
        "word_count": 250,
        "char_count": 1728,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 15,
        "text": "Chapter 2\nBackground\nIn this chapter, I present the background relevant to the project in order to better understand\nthe context and motivation behind the research. I begin by introducing the concepts of\nInformation Extraction and Question Answering, which set the stage for our project task.\nNext, I discuss the idea of extracting multiple variable values and assigning them to a specific\nspeaker. The concepts of Large Language Models, the specific model that was chosen, and\nthe selected framework of fine-tuning are covered. Finally, the ELICIT system (Butcher\net al., 2023), which is the practical setting for our information extraction task, is introduced.\n2.1\nInformation Extraction and Question Answering\nInformation Extraction (IE) is a crucial task in natural language processing, which aims to\nextract structured information from unstructured textual data. In this project, the goal of\nthe system is to create tabular datasets from sentencing remarks and court transcripts with\nvariables consisting of categorical values.\nMany IE tasks are typically focused on explicit information, with a common approach\nbeing Named Entity Recognition (NER) methods. These include the identification of persons,\nlocations, dates, and other information. Additionally, Relationship Extraction methods seek\nto identify the semantic connections between different entities in a document. Conversely,\nQuestion Answering (QA) presents a broader spectrum of tasks since it can accommodate a\nmultitude of different queries.\nQA tasks are usually categorized by the type of context they encompass: closed-book\ntasks, which provide no context, and open-book tasks, where context is given. Within IE, QA\nis considered an open-book task, more specifically a reading comprehension one, where the\ngoal is to find evidence in a provided context. The output of QA tasks also varies, ranging\n",
        "word_count": 278,
        "char_count": 1869,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (24.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 16,
        "text": "6\nBackground\nfrom span prediction, where part of the context serves as the answer, multiple choice QA, or\nfree-form responses.\nThe SQuAD dataset (Rajpurkar et al., 2018, 2016) is one of the most common benchmarks\nfor QA tasks, containing question-answer pairs generated from Wikipedia articles, and\nrequiring that answers form part of the provided context. Currently the best performing\nmethods are pre-trained language models fine-tuned on question answering datasets (Lan\net al., 2020; Raffel et al., 2020). More recently, large language models in a few-shot setting\nhave also demonstrated competitive performance in such tasks (Brown et al., 2020).\nA recent topic of interest is the application of IE and QA in conversational settings. The\nFriendsQA (Yang and Choi, 2019) dataset was introduced to tackle questions asked about\nmulti-party dialogue, utilizing transcripts from the TV show \"Friends\". Its tested outputs\ninclude answer utterance selection and answer span prediction. The paper highlights the\nincreased complexity of QA tasks in a multi-party dialogue scenario compared to conventional\nQA tasks.\nAnother dataset, DREAM (Sun et al., 2019), introduced a multiple-choice QA problem\nfor dialogue-based texts. This dataset presents unique challenges since most answers are not\nextractive and necessitate multi-sentence reasoning. The language in a conversational setting\ncan also include interruptions or reasoning, which spans multiple turns. The Molweni dataset\n(Li et al., 2020) consists of multi-party chat conversations on the topic of Ubuntu. The authors\nshowcase that strong QA models display a significant drop in their performance compared\nto the SQuAD 2.0 dataset. However, Molweni only consists of question answering over\nshort spans of text. QAConv (Wu et al., 2022) introduces a dataset for question answering\nwith long and complex conversations as the context. Furthermore it consists of two testing\nscenarios - where only the relevant chunk is provided, or the full text. This is closer to the\nsetting of this project, where information needs to be extracted over long documents.\nThese recent research undertakings provide the context for the task at hand, which in-\nvolves extracting information from sentencing remarks and court transcripts. Sentencing\nremarks align with standard IE tasks but introduce complexity due to the length of the docu-\nments and the complex variables being extracted, which are not explicit factual information.\nCourt transcripts introduce a conversational setting with multiple parties, a scenario which\nrecent research has identified as particularly complex. This helps in understand how the work\nof this project fits into the larger field of IE and QA, and shows the need for more advanced\nmethods to solve these problems.\n",
        "word_count": 420,
        "char_count": 2781,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 17,
        "text": "2.2 Multi-Value Extraction and Source Attribution\n7\n2.2\nMulti-Value Extraction and Source Attribution\nThe unique context of court transcripts introduces additional challenges to standard Informa-\ntion Extraction (IE) and Question Answering (QA) tasks. One such complication arises from\nthe fact that a single variable within a document can attain multiple values, dependent on the\nspeaker.\nTraditional IE methods, such as Named Entity Recognition (NER), are capable of ex-\ntracting a varying number of values. However, as discussed in the previous section, it is not\nsufficient for the task, which demands extraction of complex, non-explicit information.\nOn the other hand, most QA tasks are set with a context that contains a single answer, or\nno answer at all. This structure is notably different from the setting, where multiple variable\nvalues may exist in the text, derived from multiple speakers.\nThe task of this project holds some resemblance to argument mining (Lawrence and\nReed, 2019) or opinion mining (Sun et al., 2017), since the information about a variable\ncould be thought of as an argument or opinion. While these methods focus on explicit\narguments or opinions, the task of this project involves a wider range of textual nuances,\nsuch as implications, answers to questions, and other forms of language. Thus, it necessitates\na broader approach to capture the richness and diversity of information present in the texts.\nIn addition to dealing with multiple possible values for a variable, the project also aims to\nattribute these values to their respective speakers. While quotation-to-speaker attribution in\ndialogue-based texts of known format can often be completed with pattern matching, which\nis a relatively straightforward approach, research which attempts to assign the extracted\ninformation to one of multiple speakers has not been identified.\n2.3\nLarge Language Models\nThe core of this thesis is Large Language Models (LLMs), which have become very popular\ntools in natural language processing tasks, competing with most specialised models (Radford\net al., 2020; Sun et al., 2020; Yang et al., 2020). LLMs are neural networks with many\nparameters (with state-of-the-art models in the trillions), which are trained on vast amounts\nof unlabeled texts from general sources, such as Wikipedia (Wikimedia Foundation, 2001),\nCommon Crawl (Common Crawl, 2007) and others.\nLLMs are used in this project based on the fact that they have been shown to work well\nin different tasks which require language understanding (Brown et al., 2020; Chung et al.,\n2022), which is the main problem of our thesis - extracting information from unstructured\ntexts.\n",
        "word_count": 414,
        "char_count": 2668,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 18,
        "text": "8\nBackground\nIn this section, the concept of auto-regressive language models, is described. This is\nfollowed by the introduction of the Transformer architecture (Vaswani et al., 2017), which is\nthe most popular architecture of modern LLMs. This leads to the presentation of the LLaMA\nmodels (Touvron et al., 2023) and its extension, Vicuna (Chiang et al., 2023), which is the\nmodel used in this project. Finally, the concept of fine-tuning through Low-Rank Adaption\n(Hu et al., 2021) is introduced, and why it is a suitable fine-tuning approach for this project.\n2.3.1\nAuto-regressive Language Models\nAuto-regressive models, such as (Chiang et al., 2023, introduced in Section 2.3.4), are a\nclass of sequential models that generate each output by conditioning it on all the previously\ngenerated outputs.\nThis process can be formalized by the following formula:\np(sequence) = p(y0:N) = p(y0)\nN\n∏\ni=1\np(yi|y0:i−1)\n(2.1)\nThis distribution can then be used to generate model responses p(response|prompt) using\na decoding strategy (e.g. sampling token by token, or choosing the highest probability token\nat each step). In the context of language models, yi represents a token at the i-th position in\nthe sequence of length N.\nA token is an integer that uniquely represents some text. Tokens are produced by\na tokenizer, which is trained independently, before the language model itself. Various\ntokenization strategies exist, broadly categorized into word-level and subword tokenization.\nThe Vicuna model, which is based on the LLaMA model (Touvron et al., 2023), employs a\nByte-Pair Encoding tokenization strategy (Sennrich et al., 2016). This strategy splits words\ninto subword units merging together frequently occurring pairs, resulting in an efficient\ntokenizer capable of handling words not present in the vocabulary.\nThe significant advancements in auto-regressive and other language models in recent\nyears can be attributed to the introduction of the Transformer architecture by Vaswani et al.\n(2017). This architecture revolutionized the training of models with a large number of\nparameters on vasts amounts of data.\n2.3.2\nTransformer\nThe Transformer addressed several limitations of previous state-of-the-art models, such as\ngated Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) models,\nand enabled effective handling of long-range dependencies.\n",
        "word_count": 355,
        "char_count": 2369,
        "fonts": [
          "NimbusRomNo9L-ReguItal (9.0pt)",
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "CMR10 (9.0pt)",
          "CMSY10 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "StandardSymL (17.2pt)",
          "CMSY10 (9.0pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 19,
        "text": "2.3 Large Language Models\n9\nThe key concept behind the Transformer is the attention mechanism (Bahdanau et al.,\n2016), which allows the model to selectively focus or attend to different parts of the input\ncontext. Unlike sequential processing, the Transformer looks at multiple parts of the context\nsimultaneously and focuses on parts it finds more important. This is achieved through scaled\ndot-product attention and multi-head attention mechanisms.\nThe scaled dot-product attention, shown in the left of Figure 2.1, is computed as:\nAttention = softmax\n\u0012QKT\n√dk\n\u0013\nV\n(2.2)\nwhere Q, K, and V represent the query, key, and value, respectively, and dk is the dimension\nof each key. Q = xW Q, K = xW K, V = xWV are linear projections of the layer input x, itself\na matrix with s (sequence length) rows and d (embedding dimension) columns. The weights\nare trained so that QT\ni Kj expresses how much the i-th input depends on the j-th input.\nTo enhance the attention mechanism, the Transformer employs multi-head attention,\nshown in the right of Figure 2.1, which uses multiple attention functions or \"heads\", allowing\nthe model to focus on multiple segments of the context simultaneously. Each head computes\nthe attention independently, and the results are concatenated and combined as follows:\nMultiHead = Concat(head1,...,headh)W O\n(2.3)\nwhere headi = Attention(xW Q\ni ,xW K\ni ,xWV\ni ) represents the attention computed by the i-th\nhead. W Q\ni , W K\ni , and WV\ni are the weight matrices for each head, and W O is the output weight\nmatrix. Each head is applied to a subset of the embedding dimension. For h heads and\nan embedding dimension dmodel, the weight matrices have dimensions dmodel ×dw, where\ndw = dmodel/h (h is chosen such that dw is an integer).\nThe Transformer architecture follows an encoder-decoder structure (Vaswani et al., 2017).\nBoth the encoder and decoder consist of self-attention blocks. In the encoder self-attention\nmechanism, the queries, keys, and values are derived from the outputs of the previous encoder\nlayer. On the other hand, in the decoder self-attention mechanism, the inputs are obtained\nfrom the outputs of the previous decoder layer, but with masked attention, as illustrated in\nthe left of Figure 2.1, so that it only attends to preceeding tokens. Equations 2.3.2 and 2.3.2\nprovide the formulas for self-attention. Additionally, the decoder includes a cross-attention\nblock, however, it is not utilized in the models employed in this project; further details on\ncross-attention can be found in Vaswani et al. (2017).\nThe absence of recurrence in the Transformer architecture makes it highly suitable for\nparallelization on multiple GPUs, leading to significantly reduced training times. Addition-\n",
        "word_count": 447,
        "char_count": 2734,
        "fonts": [
          "NimbusRomNo9L-ReguItal (11.8pt)",
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-ReguItal (9.0pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "CMSY10 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "CMMI10 (12.0pt)",
          "CMEX10 (12.0pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-ReguItal (12.1pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)",
          "NimbusRomNo9L-Medi (12.1pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 20,
        "text": "10\nBackground\nFig. 2.1 Scaled dot-product attention (left) and Multi-head attention (right). Figures taken\nfrom Vaswani et al. (2017).\nally, Transformers do not suffer from vanishing and exploding gradient problems, which is\nan issue with RNNs and LSTMs when processing extremely long sequences (Pascanu et al.,\n2013). This advantage stems from the Transformer’s ability to attend to all input positions\nsimultaneously.\nIn the upcoming sections, the LLaMA model (Touvron et al., 2023) and its fine-tuned\nversion Vicuna (Chiang et al., 2023), will be introduced, which are pre-trained auto-regressive\nlanguage models based on the Transformer architecture. As mentioned before, these models\nonly use the decoder part (without cross-attention), not the full architecture proposed by\nVaswani et al. (2017).\n2.3.3\nLLaMA\nLLaMA (Touvron et al., 2023) is a collection of auto-regressive language models developed\nby Meta AI, which have showcased competitive performance at a smaller parameter size.\nLLaMA was developed based on recent work in Hoffmann et al. (2022), which showed that\nbetter performance can be achieved not by making the models bigger, but by increasing the\namount of training data. It was trained on 1 trillion tokens for the 7B and 13B models (the\n\"B\" indicates the number of parameters in billions), and 1.4 trillion for the 33B and 65B\n",
        "word_count": 209,
        "char_count": 1349,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 922,
            "height": 508,
            "ext": "png",
            "size_bytes": 66848
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 21,
        "text": "2.3 Large Language Models\n11\nmodels. LLaMA-13B has been shown to outperform GPT-3 (Brown et al., 2020), while\nbeing more than ten times smaller (Touvron et al., 2023).\nLLaMA has become a popular option for fine-tuning for downstream tasks, since the\nsmaller models can be run on a single GPU with lower floating point precision, but still result\nin competitive performance.\n2.3.4\nVicuna\nThe open-source nature of LLaMA has resulted in many publicly available models fine-tuned\non more specific datasets. Vicuna (Chiang et al., 2023) is an open-source language model,\nwhich was trained on 140k (for the 1.3 version, which is used in the project; the version in\nthe article was trained on 70k) user-shared chatbot conversations from ShareGPT (Eccleston,\n2023). Using an automated evaluation framework with GPT-4 (Zheng et al., 2023), the\nauthors found that Vicuna-13B performs on-par with OpenAI’s ChatGPT-3.5 (OpenAI, 2022)\nand Google’s Bard (March 2023 version) (Manyika, 2023).\nVicuna was trained as an autoregressive chatbot model, meaning that it generates output\nin an autoregressive fashion and produces sequences in a chatbot conversation style - user\nmessages are followed by assistant (chatbot) messages. The standard way of using Vicuna\nis to provide a prompt consisting of a series of user and assistant messages, with the final\nassistant message being left empty for the model to generate a response. Figure 2.2 showcases\nthe prompting format of Vicuna.\nA chat between a curious user and an artificial\nintelligence assistant. The assistant gives helpful,\ndetailed, and polite answers to the user's questions.\nUSER:\nHow long would it take for a commercial airline to fly\naround the Earth?\nASSISTANT:\nIt would take a commercial airline approximately 40\nhours to fly around the Earth, assuming a direct flight\nand no stops. However, most commercial flights do not\ntake this route, as it would be both impractical and\ninefficient due to the need to refuel and the fact that\nmost airports are not equipped to handle such long\nflights. Instead, commercial flights typically follow more\ndirect routes that take them over oceans and between\nmajor hubs.\nFig. 2.2 An example of the prompt format in Vicuna. The black bold text represents the\nfixed template, which is required in every prompt for consistent performance. The red text\nhighlights the user input, while the blue text is the completion from Vicuna-13B.\n",
        "word_count": 386,
        "char_count": 2416,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "Arial-BoldMT (8.7pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "ArialMT (8.7pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 22,
        "text": "12\nBackground\n2.3.5\nFine-tuning using Low-Rank Adaptation\nLow-Rank Adaptation (LoRA) (Hu et al., 2021) is a technique for parameter efficient fine-\ntuning of large pre-trained language models. Full fine-tuning large pre-trained models with\nbillions of parameters can be computationally expensive. To address this, the authors propose\nthe LoRA approach.\nIn LoRA, during the fine-tuning process, the update of a pre-trained weight matrix\nW0 ∈ Rd×k can be expressed as W0 +∆W. Typically the weight matrices are full-rank, thus\nthe authors propose expressing the update matrix as a low-rank decomposition W0 +∆W =\nW0 +BA, with matrices B ∈ Rd×r and A ∈ Rr×k, where rank r ≪ min(d,k). By freezing W0\nduring fine-tuning, while B and A are updated, LoRA achieves parameter efficiency.\nThe low-rank assumption is based on the observation that the rank of the update matrix is\noften low when adapting a pre-trained model to a specific task. This is because the fine-tuning\ntask is usually narrower in scope compared to the broad pre-training task. The authors refer\nto Aghajanyan et al. (2020), which shows that large pre-trained models have a low \"intrinsic\ndimension\" when adapting to a task, and thus can be reparameterised in more efficient forms.\nLoRA offers several advantages over other existing fine-tuning methods:\n• LoRA is parameter and memory efficient, resulting in reduced training times and\nlower GPU memory requirements. According to the authors, the number of trainable\nparameters can be reduced by up to 10,000 times, while the GPU memory requirements\ncan be reduced by up to 3 times (Hu et al., 2021).\n• A single pre-trained model can be used with multiple LoRA modules. This flexibility\nallows for task-specific modules to be easily swapped, enabling the use of a single\npre-trained model for multiple downstream tasks. For instance, in this project, where\nthe aim is to utilize user-validated data for fine-tuning for different subtasks, LoRA\nenables the use of just a single pre-trained model and fine-tune multiple modules to the\nspecific subtasks, which eliminates the need of using multiple large models.\n• LoRA introduces less additional inference latency compared to methods such as\nadapters (Houlsby et al., 2019). This is due to the linear of nature of the method, where\nthe existing weight matrices are modified by adding the update matrices, avoiding\nthe need for additional modules. This property makes LoRA an efficient choice for\nreal-time applications.\nIn summary, LoRA provides an efficient fine-tuning process that is suitable for this project.\nIts parameter and memory efficiency, along with the ability to use a single pre-trained model\nfor multiple tasks, make it an appealing choice.\n",
        "word_count": 432,
        "char_count": 2716,
        "fonts": [
          "NimbusRomNo9L-ReguItal (9.0pt)",
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "CMSY10 (12.0pt)",
          "MSBM10 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "CMMI10 (12.0pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "StandardSymL (12.0pt)",
          "CMSY10 (9.0pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 23,
        "text": "2.4 ELICIT\n13\n2.4\nELICIT\nELICIT is an information extraction system, introduced in Butcher et al. (2023), designed\nto enhance extraction efficiency and precision through a combination of weak supervision\nand human validation. The purpose of ELICIT was to address the imperfect accuracy of\nautomated systems, and the slowness of manual annotation by combining the two. By incor-\nporating modern language model capabilities and the ability to identify differing opinions\nand attribute them to specific speakers, we aim to further extend the system’s functionality in\nthis project.\n2.4.1\nApproach\nIn ELICIT, users begin by defining a set of variables they wish to extract, with each variable\nconsisting of a range of possible values. This is called a categorical schema, an example of\nwhich is given in Listing 2.4.1. To handle cases where the desired information is not present\nin the document, an \"Abstain\" value is used in the system.\nListing 2.1 An example of a categorical schema. It defines the possible values that each\nvariable can take.\nr e l a t i o n s h i p :\n#\nv a r i a b l e\nname\n#\nv a r i a b l e\nvalues :\n− family\n− f r i e n d\n− p a r t n e r\n− c o l l e a g u e\n− none\nTo automate the extraction process, the users selects a set of labelling functions. These\nfunctions are automated tools that return a predetermined number of candidates, which\nare excerpts from the document labeled with a particular variable value. Each candidate\nis accompanied by a confidence score, indicating the system’s level of certainty in its\nassignment.\nELICIT utilizes a ranking mechanism to prioritize the candidates and presents the Top-k\ncandidates per variable to the user. This ranking enables the user to focus on the most\nrelevant and potentially accurate extractions. The system also combines excerpts from\ndifferent labelling functions with significant overlap.\n",
        "word_count": 330,
        "char_count": 1867,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 24,
        "text": "14\nBackground\nFinally, the user engages in the validation process by reviewing and validating each\ncandidate. This validation step contributes to the completion of the extraction process,\nensuring that the extracted information is accurate and reliable.\nFig. 2.3 A high-level overview of ELICIT for a single variable extraction.\n2.4.2\nLabelling Functions\nThe recall of ELICIT is mainly determined by the quality of the labelling functions. Each\nselected labelling function is applied to the complete set of documents under examination.\nELICIT is comprised of four main labelling functions, each specifically designed to identify\nand categorize a singular variable within a document. These functions are explained in detail\nbelow.\n1. QA Model Followed by Zero-shot Classification: This labeling function requires the\nuser to define a question schema, which contains supporting questions for each variable.\nSubsequently, the document is split up into smaller contexts, and each question-context\npair is supplied to a RoBERTa model that has been fine-tuned for question-answering\ntasks on the Squad2 (Liu et al., 2019; Rajpurkar et al., 2018). The output from the\nmodel is an extract from the context, where the model believes the answer to be.\nAlong with extract, a relevance score is also computed, interpretable as the conditional\nprobability PQA(excerpt|question). This extracted portion is then fed into a RoBERTa\nNatural Language Inference zero-shot classification model (Yin et al., 2019), with the\nvariable values operating as labels. The model generates a score PNLI(label|excerpt)\nfor every value, and the total confidence score is computed as the product of these two\nprobabilities: PNLI(label|excerpt)·PQA(excerpt|question).\n2. QA Model Followed by Cosine Similarity: This labelling function has the same\nmechanism of the previous one, except for the final classification phase. Here, instead\n",
        "word_count": 279,
        "char_count": 1902,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "CMSY10 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Medi (11.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)",
          "NimbusRomNo9L-Medi (12.1pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 911,
            "height": 271,
            "ext": "png",
            "size_bytes": 17453
          }
        ],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 25,
        "text": "2.4 ELICIT\n15\nof zero-shot classification, the function uses cosine similarity computation among\nRoBERTa embeddings for the final classification. Each text excerpt and variable value\nis embedded in the pattern “this is a {word}”, with \"{word}\" referring to the excerpt or\nvariable value, which is being embedded. Subsequently, the cosine similarity between\nall excerpt embeddings and label (variable value) embeddings is calculated. The\naggregate confidence score in this approach is the product of the cosine similarity and\nthe QA probability: Pcos(label,excerpt)·PQA(excerpt|question).\n3. Semantic Search: This method employs a transformer model to embed every sentence\nfrom the document. The dot product between each sentence embedding and the embed-\ndings of questions from the question schema is calculated. The average score for all\nquestions is taken as Pquestions(sentence,questions). Then these sentence embeddings\nare compared to the embeddings of each label, yielding the score Plabel(sentence,label).\nThe total score for a sentence-label pairing is the product of these two scores, specifi-\ncally, Plabel(sentence,label)·Pquestions(sentence,questions).\n4. Keyword Search: This labelling function requires the user to define a keyword schema,\nwhich contains a set of keywords for each variable. The function employs a phrase\nmatcher from the SpaCy library (Honnibal et al., 2020) to detect all instances of these\nkeywords in the document. Due to the absence of a scoring mechanism in the keyword\nmatching, the extractions are attributed a default value of 0.1.\nIn conclusion, the ELICIT system uses each of these labelling functions to produce\ncandidate excerpts from the text along with the identified labels. The recall of the system\nis dependant on at least one labelling function being able to identify a relevant part of the\ndocument, as well as, to assign it to the correct label. However, the current version of ELICIT\nassumes that only a single variable value corresponds to a document, which is not the case\nfor many documents, for example, court transcripts. In this project, the work is focused on\nextending ELICIT’s capabilities by introducing new labelling functions, which make use of\nLLMs, and which have the ability to attribute information to speakers in the text.\n",
        "word_count": 346,
        "char_count": 2293,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "CMSY10 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "CMMI10 (12.0pt)",
          "NimbusRomNo9L-Medi (11.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 26,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 27,
        "text": "Chapter 3\nMethodology\nIn this chapter, I describe the methodology of the project, which is used to achieve the\nresearch goals. The process of creating a suitable dataset for the project is detailed, and\nthe specific approaches of using LLMs for information extraction in a single-speaker and\nmulti-speaker setting. The technical modifications implemented in ELICIT to accommodate\nthe approach are also discussed. Furthermore, the method of utilizing user-validated data\nfor model fine-tuning is explained. Lastly, the metrics employed to assess and compare the\nimpact of the decisions on the results are described.\n3.1\nDatasets\n3.1.1\nSentencing Remarks\nFor a fair comparison against the extraction methods presented in Butcher et al. (2023), the\nsame extracted sentencing remarks as in the paper are used for the single-speaker information\nextraction. The documents used are part of openly accessible cases published by the United\nKingdom Judiciary for cases of public interest.\nThe authors have selected the sentencing remarks of nineteen murder cases. Each of\nthe documents was manually labelled for eighteen categorical variables. To ensure privacy,\neach named person in the text was anonymized, and the names of the defendant and victim\nwere replaced with generic identifiers \"the defendant\" and \"the victim\", respectively. The\ndocuments averaged 3357 tokens.\nOut of the eighteen variables, thirteen, which occurred five or more times in the docu-\nments, were used for evaluation. These variables cover non-trivial information about the\nvictim, the defendant, and the crime or case. The victim-focused variables include their sex,\nemployment status, pregnancy status, and whether they were considered vulnerable. The\n",
        "word_count": 256,
        "char_count": 1721,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Medi (24.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 28,
        "text": "18\nMethodology\nvariables covering the defendant include prior convictions, remorse, and whether their age\nwas considered a mitigating factor. The remaining variables, which do not strictly fall under\nthe previous two categories, include evidence of physical abuse, domestic abuse, instance\nof emotional abuse, the premeditation of the crime, and whether the crime was sexually\nmotivated.\nIn every document, the victim sex is noted, either through pronouns or gender-specific\nwords. Some information might be explicitly stated, inferred through other facts, or not\nmentioned at all. Most variables require complex reasoning through implied statements by\nthe judge or from the descriptions of the crime.\nThe sentencing remarks dataset comprises 19 documents with 13 distinct variables,\nresulting in a total of 189 non-abstain and 58 abstain data points. This dataset provides a\ngood setting to test the ability of LLMs to comprehend non-trivial information.\n3.1.2\nCourt Transcripts\nFor the analysis of information extraction in a multi-speaker setting, transcripts of UK\ncourt proceedings were used as the document of choice. Although transcripts of UK court\nproceedings are not available to the general public and cannot be used for public research,\ntheir format is known. To overcome this limitation, GPT-4 (OpenAI, 2023b) was used\nto produce synthetic court transcripts based on the information present in real sentencing\nremarks, as modern LLMs have shown to be capable of producing human-like text (Hu et al.,\n2023). Although synthetic text has limitations, it is expected that approaches which do not\nwork here will also be ineffective with the more challenging real data.\nI established the following criteria for the generated documents:\n1. Size: The generated transcripts should be at least a few pages long, totaling at least\na few thousand words and thus exceeding the context window (the number of tokens\nin the generated sequence, including the prompt) of most commonly available LLMs,\nthus, facilitating the need for splitting the text, mimicking the process of extracting\nfrom real, lengthy transcripts. For reference, the Vicuna models were trained on a\ncontext length of 2048 tokens (approximately 1500 words).\n2. Multi-turn dialogue: The transcripts should consist of multi-turn dialogue, including\nspeakers addressing someone, raising questions and answering them.\n3. Multiple variable values: The documents should contain instances of multiple differ-\nent values applying to the same variable, including some speakers indicating different\nvalues to the same variable, i.e. giving contradicting statements.\n",
        "word_count": 391,
        "char_count": 2623,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)",
          "NimbusRomNo9L-Medi (12.1pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 29,
        "text": "3.1 Datasets\n19\n4. Stage structure: The transcript should follow a predefined stage structure, starting\nwith opening remarks by the prosecutor and the defense (counsel), followed by a series\nof cross-examinations of different witnesses, and end with closing remarks by the\ncounsel. This structure, with additional stages, is found in real court transcripts, and\nalso facilitates the inclusion of competing narratives.\nThe chat completion model of GPT-4 was used to generate the transcripts, which consists\nof three types of messages: system, user, and assistant. While the user and assistant messages\nwork the same way as described for Vicuna in Section 2.3.4, the system message allows\nus to preconfigure the model so that it adheres to a certain tone, style or task in its future\nresponses (OpenAI, 2023a). In this case, the system message contained a description of the\ntask, and outlined the structure that the transcript should follow. Specific variable values\nwere included by providing a list of these variables as statements in the system message,\nsuch as \"the victim was unemployed\". Additionally, the real sentencing remarks are provided\nin the system message for better context.\nGenerating a transcript with the desired length using a single model response is challeng-\ning due to the model’s tendency to produce compact responses. Instructing the model to\nproduce longer responses did result in slightly longer generations, but not enough to meet the\nneeds of the project. Similarly, instructing the model to generate text of a defined word count\nwould result in generations significantly below the requested word length. To overcome this,\nan iterative approach is employed. The model is prompted to generate the transcript of a\nsingle stage, and the responses are concatenated to produce the full transcript. This iterative\napproach allows better control over the length of the transcript, and over the content of the\ntranscript.\nTo ensure variation in variable values across speakers, a subset of variables that can\nlogically take different values in the text is established, such as premeditation, remorse,\nvulnerability of the victim, emotional abuse, and defendant’s age as a mitigating factor. For\neach witness, two of these variables were randomly sampled, and the model was instructed\nto include the information during the examination by the prosecutor and defense, with each\ncounsel taking opposing values.\nTo ensure a logically consistent document, the model has to take into account its previous\ngenerations. However, the available model has a context length limit of 8000 tokens, which is\nnot sufficient to include previous generated stages. To address this, an iterative summarization\napproach is implemented. The system message and the opening remarks are included in\nevery generation, and after generating a pair of witness examinations, the model is requested\nto summarize them. These summarizations replace the longer generations in the message\n",
        "word_count": 459,
        "char_count": 2974,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)",
          "NimbusRomNo9L-Medi (12.1pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 30,
        "text": "20\nMethodology\nhistory, circumventing the limited context length problem while ensuring that the model sees\nthe essential information.\nUsing this approach, five transcripts were generated, with an average of 5592 tokens. A\nsingle data point is defined as the tuple (document, variable, value, speaker), representing a\nspeaker indicating a distinct variable value in a document. The transcripts were manually\nannotated to identify the variable values each speaker produces, resulting in a total of 171\nnon-abstain data points. The annotation revealed occasional issues with the synthetic data,\nincluding repeating sentences in the opening and closing remarks, and instances of logical\nincoherence. Nevertheless, the synthetic dataset is a good starting point for analyzing the\nchallenges that would be faced in information extraction from real court transcripts.\n3.2\nInformation Extraction\n3.2.1\nApproach\nIn order to extend ELICIT with the capability of using LLMs for information extraction, a\nnew labelling function had to be created, which would take the document text and variable\nname as input, and return a set of candidate extractions as the output. Each extraction should\ncontain the identified variable value and a corresponding confidence score.\n3.2.2\nPassage Retrieval\nThe first challenge faced was the issue of large document length, which exceeded the context\nlength of most modern LLMs. To address this, the approach of splitting the text into smaller\npassages was taken. Subsequently, extraction on the n most relevant ones identified by the\npassage retrieval part is performed. This technique, visualized in Figure 3.1, is a standard\napproach in IE and QA systems over long documents or multiple documents (Choi et al.,\n2017; Izacard and Grave, 2021; Xu et al., 2011).\nD passages\nDocument\nn relevant\npassages\nPassage Retrieval\nk extractions\nValue Labelling\n(passage, value,\nconfidence score)\nFig. 3.1 High-level overview of the extraction process for the LLM-enhanced system. For\neach variable, the document is split into D passages, which are provided to retrieval system.\nThe passage retrieval system returns n most relevant ones, which are fed to the value labelling\npart. It assigns a confidence score to each variable value for the passage, and top k extractions,\nbased on confidence, are shown to the user.\n",
        "word_count": 356,
        "char_count": 2328,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "ArialMT (7.0pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "Arial-ItalicMT (6.5pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "ArialMT (6.5pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 31,
        "text": "3.2 Information Extraction\n21\nIn the project, experimentation was conducted with two passage retrieval methods,\ndescribed in the following subsections. The first approach involved storing embedded\npassages in a vector index and querying them to identify the n most relevant ones. The\nsecond approach used an LLM in a Yes/No question format to identify whether the passage\ncontains the relevant information.\nVector Index\nIn this approach to passage retrieval, semantic search is performed across all passages from\na single document. The all-mpnet-base-v2 pre-trained model from SentenceTransformers,\nwhich has shown strong performance in semantic search (Reimers and Gurevych, 2019), was\nused to generate vector embeddings of the individual passages.\nThe Chroma vector index (Huber, 2022) was utilized for storing these embeddings and\nperforming the similarity search. This eliminates the need for recomputing the embeddings of\na single document. Additionally, a vector index allows relevant passages to be retrieved using\nefficient approximate nearest neighbour search for high-dimensional vector data (Malkov\nand Yashunin, 2018).\nThe LangChain Python package (Chase, 2022) was employed, providing a simple inter-\nface for storing and querying passages, and directly supporting Chroma and other popular\nvector indices. All embedded passages from the documents are stored in a single index, with\nthe document name assigned as metadata. During the search, filtering is applied to only\ninclude passages from the relevant document, removing the overhead of creating multiple\nindices for every new document and creating the possibility of performing extractions across\na set of documents in the future. The queries are statements describing each variable, which\nthe user defines in an information schema, for example, the statement \"the relationship\nbetween the victim and the defendant\" for the \"relationship\" variable.\nThe embedding vectors are stored as 768-dimensional L2-normalized vectors. The\nsquared L2 (Euclidean) distance is used as the distance metric between the query and passage\nembeddings. The following formula was used to convert it into a cosine similarity score:\nsimilarity = 1− distance\n2\n(3.1)\nLLM-Based Retrieval\nIn this method, for each passage, a binary classification approach is adopted. The LLM is\npresented with the passage and asked whether the context includes information about the\n",
        "word_count": 353,
        "char_count": 2408,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "CMSY10 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-ReguItal (12.1pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 32,
        "text": "22\nMethodology\nspecified variable. The prompt template (the user message in Vicuna), as shown in the left of\nFigure 3.2, is used, with a sample prompt visualized in the right of the figure.\nThe context is an excerpt from a legal\ndocument. Does it contain discussions\nabout or indications to {information}? Yes\nor no.\nContext:\n{context}\nThe context is an excerpt from a legal document.\nDoes it contain discussions about or indications\nto the premeditation of the crime? Yes or no.\nContext:\n6. Text messages sent by you and internet\nsearches done on your mobile phone reveal a\ndegree of premeditation. the victim had two\nhospital admissions prior to her death when\nyou claimed that she had suffered fits.\nFig. 3.2 Prompt template (left) and example prompt (right) for the LLM-based passage\nretriever. The {information} tag is replaced with the description of the variable from the\ninformation schema, as described in Section 3.2.2, while the {context} tag is replaced with\nthe passage.\nPreliminary experiments indicated that the majority of answers start with a \"Yes\" or \"No\",\nthus, decoding is unnecessary, and it is sufficient to check the conditional probability of the\ncompletion being \"Yes\" or \"No\", given the prompt.\nThis is accomplished by concatenating the answer to the prompt and completing a single\nforward pass, which yields the logits for each token in the vocabulary at each sequence\nposition. The logits, representing the unnormalized score for each token in the vocabulary,\nare passed through a softmax function. This results in the (log) probability distribution over\nthe entire vocabulary at that sequence position, given the previous tokens. The conditional\nprobability of the answer given the prompt is obtained by summing the log probabilities of\nthe answer tokens and applying an exponential:\nP(answer|prompt) = P(yN+1:N+T+1|y1:N) = exp\n \nN+T+1\n∑\ni=N+1\nlogP(yi|y1:i−1)\n!\n(3.2)\nwhere N is the number of tokens in the prompt, T is the number of tokens in the answer, and\nyi denotes the token at the i-th position in the sequence.\nUsing a baseline model in this manner leads to some probability mass being lost to other\ngenerations, which have the same meaning but do not start with a \"Yes\" or \"No\", for example,\nthe sequence \"The answer is Yes\". The conditional probabilities of the next token being\n\"Yes\" or \"No\" are used and normalized as follows:\nP(Yes) =\nP(label = Yes|prompt)\nP(label = Yes|prompt)+P(label = No|prompt)\n(3.3)\n",
        "word_count": 399,
        "char_count": 2448,
        "fonts": [
          "NimbusRomNo9L-ReguItal (9.0pt)",
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "SFTT1200 (12.0pt)",
          "CMSY10 (12.0pt)",
          "CMR10 (9.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "ArialMT (8.8pt)",
          "StandardSymL (17.2pt)",
          "CMSY10 (9.0pt)",
          "CMEX10 (12.0pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 33,
        "text": "3.2 Information Extraction\n23\nThe normalized conditional probability of \"Yes\" is used as the confidence score of the\npassage retrieval component, and the top n highest scoring passages are retrieved.\n3.2.3\nValue Labelling\nThe retrieved passages are taken as input to the value labelling component with the objective\nof producing a set of extracted variable values with respective confidence scores. This is\na multi-class classification problem, but with the aim of using a single model and with the\nability to have an arbitrary number of possible variable values.\nIn the prompt to the LLM, given in Figure 3.3, the task is first described, which is to\nidentify the most suitable variable value (referred to as ’label’) based on the context (the\ncandidate passage) and questions from the question schema. The latter are provided as\nadditional context to the model, helping it align better with the specific task. This reduces\npotential confusion during classification. For example, in the classification of the ’victim\nsex’ variable, the questions are \"what sex was the victim?\", \"was the victim a male?\". These\nquestions guide the model to identify the sex of the victim specifically, thus reducing the\nchance of misclassification.\nThe context is an excerpt from a legal document. Identify the\nlabel based on the provided context and questions.\nReturn the label 'unclear' if the text does not provide enough\ninformation to identify a label.\nYour response should be in the following format:\nLabel: '<identified label>'\nHere is an example of the task.\nInput:\nLabels: 'premeditated', 'not premeditated'\nQuestions: \n'was \nthe \ncrime \nplanned?', \n'was \nthere \nany\npremeditation?'\nContext:\nIn your favour is your good character save for the three cannabis\nmatters; your record of\ncontinuous \nemployment; \nthe \nlack \nof \nany \nsignificant\npremeditation; and (to a limited\nextent only for the reasons I have already expressed) that you\nmay not have intended to\nkill her. Additionally, there were no previous indications that you\nharboured violent\nintentions towards your sister.\nOutput:\nLabel: 'not premeditated'\nIdentify the label for the following context:\nLabels: {labels}\nQuestions: {questions}\nContext:\n{context}\nThe context is an excerpt from a legal document. Identify the label\nbased on the provided context and questions.\nReturn the label 'unclear' if the text does not provide enough\ninformation to identify a label.\nYour response should be in the following format:\nLabel: '<identified label>'\nHere is an example of the task.\nInput:\nLabels: 'premeditated', 'not premeditated'\nQuestions: 'was the crime planned?', 'was there any premeditation?'\nContext:\nIn your favour is your good character save for the three cannabis\nmatters; your record of\ncontinuous employment; the lack of any significant premeditation;\nand (to a limited\nextent only for the reasons I have already expressed) that you may\nnot have intended to\nkill her. Additionally, there were no previous indications that you\nharboured violent\nintentions towards your sister.\nOutput:\nLabel: 'not premeditated'\nIdentify the label for the following context:\nLabels: 'mitigate', 'not mitigate'\nQuestions: 'was age a mitigating factor?'\nContext:\n20. Your age and relatively good character afford you mitigation.\nHaving seen you give\nevidence and from all that I have heard about you during the trial, I\nconsider that you\nare immature. \nFig. 3.3 Prompt template (left) and example prompt (right) for the value labelling model. A\ndetailed explanation is given in Section 3.2.3.\n",
        "word_count": 545,
        "char_count": 3530,
        "fonts": [
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "ArialMT (6.6pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 34,
        "text": "24\nMethodology\nAdditionally, the model is instructed to return the label \"unclear\", if the context does not\ncontain sufficient information to make a decision. This acts a safeguard noting the fact that\nthe passage retrieval system is not perfect and may return some irrelevant passages. Thus, the\nmodel does not need to make a decision between two incorrect values. The \"unclear\" label is\nalso used to handle situations where the variable in question is mentioned, but a definitive\nvalue is not provided. For example, the sentence \"We have yet to determine if the victim was\npregnant.\" mentions the victim’s pregnancy, but does not give a clear answer, and thus fits\nthe \"unclear\" label.\nThe label \"unclear\" is not returned to the user as an extraction, but is employed to indicate\nuncertainty and remove some probability mass from the other variable values during model\ninference.\nFinally, the prompt also contains the response format and a single example of the task.\nBoth of these are used to focus more probability onto the exact possible labels, while the\nexample also improves the model’s understanding of the task, as prompts including in-context\nexamples have shown to produce better results in LLMs Brown et al. (2020).\nFor the labelling, the conditional probability of each of the variable values, including\n\"unclear\", being the completion to the prompt is computed, as is done in Section 3.2.2.\nThe probabilities are also normalized as some probability mass is distributed to alternative\ncompletions. This is formalized, for a label x, as:\nP(label = x) =\nP(label = x|prompt)\n∑\ny∈labels\nP(label = y|prompt)\n(3.4)\nThis process results in a set of variable values with confidence scores for each retrieved\npassage. The overall confidence score of the end-to-end extraction system is the product\nof the passage retrieval confidence score and the value labelling confidence score. Finally,\nthe Top-k highest-scoring passage-value pairs for each variable are presented to the user for\nvalidation.\n3.3\nSource Attribution\nIn this section, the process of extending ELICIT to assign source information to extracted\nvariables when there are multiple speakers present, is described. While the primary focus of\nthe project lies in dialogue, the methods and design choices described in this section hold\nthe potential for future extension to other types of attributions, such as indirect quotations or\n",
        "word_count": 381,
        "char_count": 2399,
        "fonts": [
          "NimbusRomNo9L-ReguItal (9.0pt)",
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "CMSY10 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "StandardSymL (12.0pt)",
          "CMSY10 (9.0pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 35,
        "text": "3.3 Source Attribution\n25\ndirect quotations in a non-dialogue context. The modified objective of the extraction process\nis to correctly identify the variable values given by a speaker.\nThe primary design choice lies in the placement of the source attribution component\nwithin the system. Three possible scenarios have been identified:\n1. Integration of the source attribution as a final step after value labelling.\n2. Incorporation of the source attribution within the passage retrieval process.\n3. Embedding of the source attribution into the value labelling component.\n3.3.1\nSource Attribution as the Final Component\nWhen source attribution is considered as the final step in the system, the component would\nreceive the extracted passage-value pairs, and the approach taken would involve instructing\nthe LLM to identify the speaker who provided evidence for a specific value.\nThe advantage of this approach is that the source attribution part would remain decoupled\nfrom the passage retrieval and value labelling parts, and thus could be improved independently\nthrough prompt engineering or fine-tuning. It would also be compatible with other labelling\nfunctions since it only requires an assigned variable value and a context.\nHowever, this approach is limited in that not all possible speaker-value combinations\ncan be extracted. If there are multiple speakers giving the same value in a passage, the\nmodel would have to choose between them, considering the task is formed as a multi-class\nclassification one. If the LLM is used in a multi-label classification fashion, where multiple\nspeakers can be identified for a single value, issues arise with getting the output in an\nexpected format, as well, as getting a representative confidence score, which is speaker-\nspecific. Furthermore, if there are multiples speakers giving different values in the same\npassage, the value labelling part will have distribute the probability to each of the values, or\neven give most of the probability mass to the \"unclear\" label. Hence, the score would not be\nreflective of the confidence of a single speaker giving a single value.\n3.3.2\nSource Attribution in Passage Retrieval\nAn alternative approach could be the incorporation of source attribution into the passage\nretrieval segment, allowing only those passages to be retrieved where the speaker gives some\nindication of the variable. Assuming that a list of speakers for the passage is available, it is\nnecessary to determine the relevance of the passage for each speaker. This would involve\nmodifying the LLM-based passage retriever’s prompt to incorporate speaker information, for\n",
        "word_count": 404,
        "char_count": 2628,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 36,
        "text": "26\nMethodology\ninstance, \"Based on the context, are there discussions or indications given by the {speaker}\nabout {variable}?\".\nWith this methodology, only passages where the speaker discusses the specific variable\nare provided to the value labelling segment. However, this approach precludes the use of\nthe designed strategy for value labelling without additional speaker context. For example,\nin a passage containing two speakers indicating differing variable values, the modified\npassage retrieval system would identify the passage as relevant for both speakers. However,\nproviding the passage to the value labelling segment would pose a challenge for the model in\nidentifying the correct value, given the absence of a distinct value for the passage. Therefore,\nthis approach would also necessitate a modification to the value labelling segment, which\nwould lead to two systems facing more complex tasks, and thus, likely a larger decrease in\nperformance.\n3.3.3\nSource Attribution in Value Labelling\nThe final candidate approach is to include source attribution as part of the value labelling\nphase. Given a set of speakers present in the passage, the model would be prompted to\nidentify the variable value, based on each specific speaker or, in other words, according\nto that speaker’s point of view. This approach has several advantages over the previously\noutlined methods.\nFirstly, it enables the capture of all values identified by a single speaker, since it performs\nlabelling per-speaker, and not per-variable first. As a result, the confidence score is also\nreflective of assigning a distinct variable value to a distinct speaker. In contrast to the previous\ntwo methods, the case where multiple speakers give the same or different values in a single\npassage can be handled, and the speaker-value pairs extracted separately.\nThe only disadvantage of this method is that the model will be prompted for speakers\nthat are irrelevant, i.e. who do not give any indication to any of the variable values. This is\ndue to the fact that labelling is performed for every speaker appearing in the passage, and\nwill have no information if that speaker discusses that variable.\n3.3.4\nImplementation\nTaking into account all of the advantages and disadvantages of the proposed approaches, the\ndecision was made to include the source attribution in the value labelling stage, as it best\naligns with the objective of correctly identifying the variable values given by a speaker, and\nis expected to result in the smallest decrease in performance compared to the single-speaker\napproach.\n",
        "word_count": 402,
        "char_count": 2579,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "SFTT1200 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 37,
        "text": "3.3 Source Attribution\n27\nThe implementation of this involves two main modifications to the single-speaker scenario.\nThe first modification is the extension of the passage retrieval component to store the speakers\npresent in each passage.\nThe context is an excerpt from a legal document. Identify the\nlabel based on the provided questions and the information\nprovided by the specified speaker in the context.\nReturn the label 'unclear' if the speaker does not provide\nenough information to identify a label.\nYour response should be in the following format:\nLabel: '<identified label>'\nHere is an example of the task.\nInput:\nLabels: 'premeditated', 'not premeditated'\nQuestions: 'was the crime planned?', 'was there any\npremeditation?'\nContext:\nProsecutor: Text messages sent by the defendant and internet\nsearches done on her mobile phone prove\npremeditation. Would you agree?\nFirst Witness: No, I would not. She could have been searching\nfor anything.\nSpeaker: 'Prosecutor'\nOutput:\nLabel: 'premeditated'\nInput:\nLabels: 'remorse', 'no remorse', 'unclear'\nQuestions: 'was the defendant remorseful?', 'remorse?'\nContext:\nDefense: Would you say that the defendant showed remorse for\nher actions?\nFirst Witness: I would say so, yes.\nDefense: Why is that?\nFirst Witness: She was crying when she was arrested.\nSpeaker: 'First Witness'\nOutput:\nLabel: 'remorse'\nIdentify the label for the following context and specified speaker:\nLabels: {labels}\nQuestions: {questions}\nContext:\n{context}\nSpeaker: '{speaker}'\nThe context is an excerpt from a legal document. Identify the label\nbased on the provided questions and the information provided by the\nspecified speaker in the context.\nReturn the label 'unclear' if the speaker does not provide enough\ninformation to identify a label.\nYour response should be in the following format:\nLabel: '<identified label>'\nHere is an example of the task.\nInput:\nLabels: 'premeditated', 'not premeditated'\nQuestions: 'was the crime planned?', 'was there any premeditation?'\nContext:\nProsecutor: Text messages sent by the defendant and internet searches\ndone on her mobile phone prove\npremeditation. Would you agree?\nFirst Witness: No, I would not. She could have been searching for\nanything.\nSpeaker: 'Prosecutor'\nOutput:\nLabel: 'premeditated'\nInput:\nLabels: 'remorse', 'no remorse', 'unclear'\nQuestions: 'was the defendant remorseful?', 'remorse?'\nContext:\nDefense: Would you say that the defendant showed remorse for her\nactions?\nFirst Witness: I would say so, yes.\nDefense: Why is that?\nFirst Witness: She was crying when she was arrested.\nSpeaker: 'First Witness'\nOutput:\nLabel: 'remorse'\nIdentify the label for the following context and specified speaker:\nLabels: 'male', 'female'\nQuestions: 'what sex was the victim?', 'was the victim a male?', 'was the\nvictim a female?'\nContext:\nDefense: Good morning, First Witness.\nFirst Witness: Good morning.\nDefense: You mentioned earlier that the victim was a close friend of\nyours. Can you tell us more about her personality?\nFirst Witness: She was a very strong, independent woman. She was\nalways doing things her own way, and she didn't let anyone stop her.\nSpeaker: 'First Witness'\nFig. 3.4 Prompt template (left) and example prompt (right) for the value labelling based on\nthe specified speaker. The prompt is a modified version of the prompt in Figure 3.3, but\ninstructs the model to assign the value based on a specific speaker’s point of view.\nWith every passage, the dialogue tag pattern matching is run to identify the speakers in\nit. If no dialogue tags are present, a check is performed to see if a speaker was assigned to\nthe previous chunk. If it was, this suggests that a single speaker’s speech was split into two\npassages, and thus the same speaker is assigned. If the previous chunk did not have a speaker\nassigned to it, then it means that the passage does not contain direct speech, and it is assigned\nthe default speaker (narrator, or first person case).\n",
        "word_count": 612,
        "char_count": 3947,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "ArialMT (6.3pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 38,
        "text": "28\nMethodology\nIn the passage retriever, all relevant passages for each speaker in the document are\ngathered. In both the vector index and the LLM-based approach this is done by only\nsearching the passages which have been assigned the specific speaker. This results in a\nmapping of \"speaker\" to \"relevant passages\". This mapping is then used to executed the\nvalue labelling for each speaker’s relevant passages.\nIn situations where only a single speaker is present in a passage, the prompt described in\nSection 3.2.3 is used. If there are multiple speakers in a passage, a modified prompt template,\ngiven in Figure 3.4, is used. The prompt is designed for the new task of identifying a variable\nvalue based on information from a specific speaker. The format of the prompt is similar to\nthe prompt in Figure 3.3, with the addition of the speaker as part of the input and instructions\nto identify the label based on it.\nThe probabilities of each variable value are computed and normalized as described in\nSection 3.2.3. The Top-k passage-value pairs for each speaker and variable are presented to\nthe user for validation.\n3.4\nFine-tuning\nThe task of obtaining a complete supervised dataset for full fine-tuning and evaluation in\nthis project’s setup is practically infeasible. The documents are very long, containing many\npassages when split up, and each of these passages would need to be manually labelled for\nevery single variable. However, the human validation component in ELICIT yields a smaller\nsupervised dataset, which can be used for fine-tuning to achieve two goals: improving\nperformance on unseen documents, and presenting new extractions in already analyzed\ndocuments.\nA single human validation run gives two types of information for each passage shown\nto the user - whether its relevant for that variable, and if the assigned value is correct for\nthe passage and speaker. This data can be used to further improve the LLM-based passage\nretriever and value labelling components through fine-tuning.\nIn order to evaluate the impact of fine-tuning on unseen documents, four documents are\nheld for testing in the sentencing remarks dataset, and two in the court transcripts dataset.\nThe remaining data samples are split into training and validations sets using an 80-20 ratio.\nStratified sampling was applied to ensure the proportions of variables and variable values were\nconsistent across the training and validation sets. Hyperparameter tuning was accomplished\nusing a grid search to find the configuration resulting in the lowest validation loss.\nAfter a single human validation run, there are some variables which had to be abstained,\nsince the presented passages did not contain relevant or sufficient information. While some\n",
        "word_count": 436,
        "char_count": 2740,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 39,
        "text": "3.5 Metrics\n29\nof these are true abstain data points, the rest represent imperfect recall and the potential\nfor further improvement. Therefore, the models are fine-tuned on the full set of extractions\npresented to the user, keeping 20% of the samples as validation data for early stopping,\nand the extraction is re-run again with the fine-tuned models. The human validator then\nvalidates the previously abstained variables with new extractions presented. This is expected\nto improve recall, and it also creates the opportunity to iteratively improve the model with\nnewly validated data used as training examples.\nThe model fine-tuning is accomplished using Low-Rank Adaptation (LoRA), as intro-\nduced in Section 2.3.5. A result of this method is that only a small fraction of parameters\nare trainable, resulting in a small LoRA adapter containing the weight differences, which\ncan be applied to the base model. The cross-entropy loss function was used with the input\n(prompt) tokens masked, which means that they were ignored when computing the loss and\nthe model was only trained on the output tokens. This was done, since the model is used in\na sequence-to-sequence fashion, where the objective is to maximise the probability of the\noutput sequence given the input sequence:\nθ ∗\nseq2seq = argmax p(yN+1:N+T+1|y0:N;θ)\n(3.5)\nwhere yi is the i-th token, N is the length of the prompt, and T is the length of the completion.\nThis is in contrast to the default auto-regressive objective of maximising the probability of\nthe next token given the previous tokens:\nθ ∗\nAR = argmax p(y0;θ)\nN\n∏\ni=1\np(yi|y0:i−1;θ)\n(3.6)\n3.5\nMetrics\nThis section is dedicated to discussing the metrics used in the project to assess the perfor-\nmance of the different parts of the system. These metrics can be categorized into two distinct\ngroups: end-to-end metrics and supervised dataset metrics.\n3.5.1\nEnd-to-End Metrics\nEnd-to-end metrics are employed to evaluate the performance of the full information extrac-\ntion system, which includes human validation. They are computed using a manually labelled\ngold standard for a set of documents. These metrics are used to compare the LLM-extended\nsystem against the earlier version of ELICIT, and also to compare between different configu-\n",
        "word_count": 362,
        "char_count": 2261,
        "fonts": [
          "NimbusRomNo9L-ReguItal (9.0pt)",
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "CMR10 (9.0pt)",
          "CMSY10 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "StandardSymL (17.2pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "CMSY10 (9.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "CMR10 (12.0pt)",
          "StandardSymL-Slant_167 (12.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 40,
        "text": "30\nMethodology\nrations of the components, such as the different passage retrievers, and the base model versus\nfine-tuned model.\nPrecision\nPrecision, as defined in Equation 3.7, measures how many non-abstain predictions were\ncorrect. The precision is largely determined by the human validator, hence the system is\nexpected to have near-perfect precision. The only instance where losses in precision may\nappear are when the passages provided to the user are misleading, and do not contain the full\ncontext.\nPrecision =\nTrue Positives\nTrue Positives+False Positives = Correct Non-Abstain Predictions\nTotal Non-Abstain Predictions\n(3.7)\nRecall\nRecall, as defined in Equation 3.8, measures how many of the true non-abstain samples\nwere identified by the system. Assuming a near perfect ability by the human validator to\nidentify the correct variable value given a relevant passage, the recall is largely determined\nby the capability of the automated component to retrieve a relevant passage and label it\naccurately. The Top-k parameter, that is, the number of extractions shown to the human\nvalidator, controls the trade-off between recall and time-efficiency. A large Top-k will require\nthe user to scan through many extractions, increasing the likelihood of identifying the correct\nvalue, but consuming more time in the process. The objective of the project is to enhance the\nrecall of system by improving the automated component.\nRecall =\nTrue Positives\nTrue Positives+False Negatives = Correct Non-Abstain Predictions\nTotal Non-Abstain Data Points\n(3.8)\n3.5.2\nSupervised Dataset Metrics\nThe supervised metrics are a set of metrics used to evaluate the individual components of the\nsystem, specifically to compare the fine-tuned models against the base models. As explained\nin Section 3.4, a single extraction run with human validation results in labelled datasets of\npassages and values. Despite being imbalanced, containing significantly more irrelevant\npassages, these datasets represent a true use case of doing extraction with the system and\nimproving it through human validation and fine-tuning.\n",
        "word_count": 313,
        "char_count": 2100,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 41,
        "text": "3.5 Metrics\n31\nReceiver Operating Curve and Area Under the Receiver Operating Curve\nThe Receiver Operating Characteristic (ROC) curve is used to represent the performance of a\nmodel in a binary classification task. It is a plot of the true positive rate (recall) in the y-axis\nagainst the false positive rate in the x-axis at different classification thresholds applied to the\nmodel’s predicted probability for the positive class.\nThe ROC curve is employed in the passage retriever analysis, since it is a binary classifi-\ncation task - a passage is labeled as relevant or not. The ROC curve gives us an insight into\nthe performance of the retriever at various thresholds.\nThe Area Under the ROC curve (ROC-AUC) gives an aggregate measure of the classifier’s\noverall performance. It quantifies the probability that a randomly chosen true positive sample\nwill be ranked higher, i.e. have a higher confidence score, than a randomly chosen true\nnegative sample (Hanley and McNeil, 1982). This aligns with the passage retrieval task,\nwhere the top-n passages based on their confidence scores are retrieved, thus the aim is for\nthe classifier to rank the true positives higher than the true negatives.\nPrecision-Recall Curve and Average Precision\nThe Precision-Recall (PR) curve illustrates the effectiveness of the binary classifier predic-\ntions. It is a plot of the precision (y-axis) against recall (x-axis) at varying classification\nthresholds. In the context of passage retrieval, the precision measures how many of the\nretrieved passages are truly relevant, while the recall measures how many of the true relevant\npassages are retrieved. The PR curve better reflects a classifier’s performance in heavily\nimbalanced datasets than the ROC curve (Davis and Goadrich, 2006; Saito and Rehmsmeier,\n2015), since the precision is not skewed by a large number of negative samples present in the\ndataset, whereas the ROC curve treats both false negative and false positives equally.\nThe Average Precision (AP) is a summary of the PR curve, which measures the average\nprecision across all recall levels, and is commonly used for classification tasks with imbal-\nanced datasets (Sofaer et al., 2019). In theory, it is computed as the integral of the PR curve,\nbut in practice the following formula is used:\nAP =\n1\n∑N\ni=1 rel(i)\nN\n∑\ni=1\n(P(i)×rel(i))\n(3.9)\nThe term P(i) is the precision at cut-off level i (only including the top-i results), while\nthe term rel(i) is the relevance score for a result at position i, with 1 if the result is relevant,\nand 0 otherwise.\n",
        "word_count": 416,
        "char_count": 2557,
        "fonts": [
          "NimbusRomNo9L-ReguItal (9.0pt)",
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "CMR10 (9.0pt)",
          "CMSY10 (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "StandardSymL (17.2pt)",
          "StandardSymL (12.0pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 42,
        "text": "32\nMethodology\nAP is also employed to evaluate the value labelling system. The computation of AP\nis reflective of the use case of ELICIT. In the value labelling part, each passage receives\na confidence score for each non-abstain (or non-\"unclear\") value. The Top-k of these are\nshown to the user for human validation. The objective is to ensure that most of these results\n(passage-value pairs) have the correct value assigned to them. Therefore, AP is used in the\nsame way as in the binary case, by assigning rel(i) to 1 if the result is assigned the correct\nvalue, and in P(i) computing the amount of correct results present in the result set with\ncut-off i.\nNormalized Discounted Cumulative Gain\nThe Normalized Discounted Cumulative Gain (nDCG), commonly used in information\nretrieval tasks, evaluates the quality of retrieved results by taking into account their relative\nscores or ranking. It is used according to the assumption that relevant results should appear\nhigher in the results list. In the project, this metric is utilized to assess the effectiveness\nof the passage retrieval task, where the objective is to assign higher relevance scores to\ntruly relevant passages. Similarly, in the value labelling task, the aim is to place the correct\nvariable value assignments higher according to their confidence score. Both objectives aim\nto improve the recall of the end-to-end system for smaller Top-k settings, showing fewer but\nrelevant extractions to the human validator. The nDCG score is computed using the same\napproach as AP, but with the following formula:\nnDCGp = DCGp\nIDCGp\nDCGp =\np\n∑\ni=1\nrel(i)\nlog2(i+1)\n(3.10)\nwhere the parameter p defines the number of results under consideration, while rel(i)\nis the relevance score as in Equation 3.5.2. The score is normalized by dividing the score\nwith the ideal DCG score (IDCG), which, in this context, represents the scenario where all\ncorrect documents or values appear at the top of the result list. This normalization enables\nthe comparison of different queries, which, in this task, corresponds to different variables.\nThe nDCG score is related to AP as both take into account the order of the presented\nresults. However, the former emphasizes placing the correct results at the top of the list,\nwhereas the latter focuses on maximising the number of correct results in the list. Thus,\nin the passage retrieval system, AP is a better indicator of performance, given the goal of\nproviding the value labelling part with the maximum amount of truly relevant passages.\nConversely, in the value labelling part, the focus is on displaying the correct extraction to the\nhuman validator as high as possible in the results list, making nDCG a better indicator of\n",
        "word_count": 446,
        "char_count": 2720,
        "fonts": [
          "NimbusRomNo9L-ReguItal (9.0pt)",
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "CMR10 (9.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "StandardSymL (17.2pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "CMR10 (12.0pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 43,
        "text": "3.5 Metrics\n33\nits effectiveness. Both metrics are presented for the two tasks to provide a comprehensive\nperformance evaluation.\n",
        "word_count": 19,
        "char_count": 130,
        "fonts": [
          "NimbusRomNo9L-Medi (12.0pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 44,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 45,
        "text": "Chapter 4\nResults and Discussion\nIn this chapter, I present and discuss the outcomes of several experiments conducted using our\nLLM-enhanced information extraction system on the sentencing remarks and court transcripts\ndatasets. Both sections follow a structured analysis of individual components - the passage\nretriever and value labeller. This is followed by the evaluation of end-to-end performance and\nrecalibration. In Section 4.2, I also address limitations related to speaker-specific information\nextraction, and how fine-tuning impacts overall performance.\n4.1\nSentencing Remarks Dataset\n4.1.1\nEnd-to-end Performance\nIn this section, the results from using ELICIT with all previous labelling functions, referred\nto as ELICIT-1.0, are compared to the results from LLM-enhanced ELICIT with the vector\nindex passage retriever (ELICIT-VI), and the LLM-based passage retriever (ELICIT-LLM).\nIn the human validation step, the validator is presented with the Top-k extractions to\nvalidate if the assigned label is correct based on the passage presented. If none of the\nextractions are valid, then the variable is assigned the \"abstain\" value.\nIt is important to consider that the end-to-end results are validator specific, since the\nhuman validator determines if the extraction contains sufficient information to be assigned a\nspecific label. Nevertheless, they are sufficient to highlight the differences in performance\nbetween the different configurations. In this project, I validated the extractions, however, in\nthe future, a more comprehensive analysis should be accomplished with multiple different\nvalidators.\nAs detailed in Section 3.5.1, precision is primarily determined by the human validator,\nwith losses potentially occurring if the presented passage is misleading. The difference in\n",
        "word_count": 254,
        "char_count": 1799,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Medi (24.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 46,
        "text": "36\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nELICIT-1.0\nELICIT-VI\nELICIT-LLM\nFig. 4.1 Top-3 weighted precision on the complete sentencing remarks dataset.\nobserved precision between different systems is not substantial, as seen in Figure 4.1, nor\ndoes it provide a significant edge for any system. Typically, precision losses result from the\nvalidator indicating a negative value, for example \"no physical abuse\", when the true value\nwas \"abstain\".\nThe recalls of the three configurations are compared in Figure 4.2 for Top-1 and Top-3.\nThe recall is weighted by per-class support to avoid skewing results due to imbalanced\nclass distributions. In Top-3, ELICIT-VI performs better or equally than ELICIT-1.0 in six\nvariables, while ELICIT-LLM outperforms ELICIT-1.0 in seven out of thirteen variables.\nWhen we look at the recall averaged across all variables, both LLM-enhanced systems\noutperform ELICIT-1.0, by 0.06 and 0.11, respectively.\nUsing Top-1 reduces the recall for the LLM configurations by approximately 0.2, as\nseen in Table 4.1, suggesting a weaker automated performance. A performance drop is also\nobserved with ELICIT-1.0, but it is less significant, only by 0.13. The difference in Top-1\nbetween the LLM-enhanced systems and ELICIT-1.0 is less noticeable, by only 0.02.\nTable 4.1 Top-1 and Top-3 mean recall across all variables for different configurations on the\ncomplete sentencing remarks dataset.\nConfiguration\nRecall\nTop-1\nTop-3\nELICIT-1.0\n0.3426\n0.4769\nELICIT-VI\n0.3685\n0.5397\nELICIT-LLM\n0.3687\n0.5892\n",
        "word_count": 255,
        "char_count": 1760,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "DejaVuSans (7.1pt)",
          "DejaVuSans (6.3pt)",
          "NimbusRomNo9L-Medi (12.0pt)",
          "DejaVuSans (5.4pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 47,
        "text": "4.1 Sentencing Remarks Dataset\n37\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nTop-1\nELICIT-1.0\nELICIT-VI\nELICIT-LLM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nTop-3\nELICIT-1.0\nELICIT-VI\nELICIT-LLM\nFig. 4.2 Top-1 and Top-3 weighted recall on the complete sentencing remarks dataset.\n4.1.2\nFine-tuned Performance on Unseen Documents\nIn this part, the performance of the passage retriever and value labeller with fine-tuning is\nanalyzed. Both components are fine-tuned using a supervised dataset, which was created\nby validating the extractions of 15 out of the 19 sentencing remarks with a Top-3 setting,\nresulting in 585 extractions. The components are evaluated on the remaining 4 sentencing\nremarks (156 extractions). The fine-tuning details for this task are presented in Appendix A.1.\nPassage Retriever\nThe different retrievers were tested in a binary classification manner to determine if the\npassage is relevant or not, as labeled by the human validator (the user of ELICIT). For the\nVector Index retriever, the relevance scores between the query and passages were calculated\nas is described in Section 3.2.2. The ROC and PR curves, given in Figure 4.3, computed\nacross all variables together, showcase the general classification capacity of our classifier.\nThe ROC curve and ROC-AUC score demonstrate that the base model retriever struggles\n",
        "word_count": 255,
        "char_count": 1760,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "DejaVuSans (5.9pt)",
          "DejaVuSans (5.5pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "DejaVuSans (6.6pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 48,
        "text": "38\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nReceiver Operating Characteristic (ROC) Curves\nBase (AUC = 0.46)\nFine-tuned (AUC = 0.78)\nVector Index (AUC = 0.67)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall (Sensitivity)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrecision\nPrecision-Recall Curves\nBase (AUC-PR = 0.29)\nFine-tuned (AUC-PR = 0.67)\nVector Index (AUC-PR = 0.41)\nFig. 4.3 ROC and Precision-Recall curves for different passage retrievers on supervised\nsentencing remarks data.\nwith this task, performing worse than random guessing1. Despite this, the fine-tuned model\nexcels, even surpassing the embedding model, in terms of ROC-AUC.\nThe PR curve displays a similar performance trend, with the fine-tuned model performing\nthe best, resulting in higher precision over all recall levels. Both metrics indicate the\nimprovement of the retrievers’ ability to classify relevant from irrelevant passages when the\nmodel is fine-tuned on validated data, even when most of the samples are negative.\nThe nDCG score and Average Precision (AP) are utilized to evaluate how well the\nretrievers rank the passages by relevance for each variable. While ROC and PR are computed\n1The classification decision could be reversed in this case. This was tested and did not lead to better\nperformance in other metrics due to the imbalanced dataset.\n",
        "word_count": 221,
        "char_count": 1380,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Regu (7.4pt)",
          "DejaVuSans (6.8pt)",
          "DejaVuSans (4.5pt)",
          "NimbusRomNo9L-Regu (10.1pt)",
          "DejaVuSans (7.7pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 49,
        "text": "4.1 Sentencing Remarks Dataset\n39\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnDCG\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nVulnerable Victim\nPhysical Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nVector Index\nFig. 4.4 Per-variable nDCG score for different passage retrievers on supervised sentencing\nremarks data.\nfor all predictions in the supervised dataset, these are computed per-variable by considering\nthe predictions of each variable separately2. In terms of these ranking scores, as illustrated in\nFigures 4.4 and 4.5, fine-tuning enhances or equals the performance in all variables except\nfor \"Victim Domestic Abuse\".\nThe fine-tuned model has the highest score in 7 out of the 11 variables3, indicating strong\noverall performance. When the scores are averaged across variables, the fine-tuned retriever\nhas lower nDCG and AP than the vector index retriever. This outcome is a result of the\nfine-tuned model marginally improving in most variables and surpassing the vector index,\nbut it performs significantly worse in a few variables, most notably in \"Victim Pregnancy\".\nNo notable correlation was found between the number of positive training samples for a\nvariable and its corresponding scores. Overall, these results indicate that the passage retriever\nresponds positively to fine-tuning from a relatively small amount of human validated data,\nand it is anticipated to perform even better with more validation. Increasing the Top-k setting\nduring the validation of the first few documents could be a viable solution.\nValue Labeller\nIn the value labelling task, the nDCG score increases in six variables, and slightly decreases\nin five for the fine-tuned model compared to the base model, as given in Figure 4.6. Overall,\nthe nDCG score is higher by 0.07 on average across all variables4. Due to the model being\nmore likely to label variables as “unclear”, and the prevalence of it in our dataset, another\nmodel was trained with the non-\"unclear\" data points oversampled to match the number of\n2The PR-AUC in Figure 4.3 is equivalent to AP computed for all predictions.\n3Variables which did not have positive samples were removed when computing nDCG and AP.\n4Uniform averaging is used when computing the average across all variables, i.e. each variable has equal\nweighting.\n",
        "word_count": 363,
        "char_count": 2363,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "DejaVuSans (5.8pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Regu (7.4pt)",
          "DejaVuSans (6.2pt)",
          "DejaVuSans (7.1pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 50,
        "text": "40\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Precision\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nVulnerable Victim\nPhysical Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nVector Index\nFig. 4.5 Per-variable Average Precision for different passage retrievers on supervised test\nsentencing remarks data.\n\"unclear\" samples for each variable. This led to a better overall performance in terms of\nnDCG, outperforming the base model in six variables, and the regularly fine-tuned model\nin seven. Averaged across all variables, the nDCG is higher by 0.124 than the base model.\nThis shows that when dealing with an imbalanced dataset, the model benefits from seeing the\npositive samples more frequently, and thus results in better ranking capabilities.\nThe nDCG score evaluates the ability of the model to place the correct predictions higher.\nAs the fine-tuned models result in higher nDCG, this is expected to transfer to stronger\nperformance in smaller Top-k settings, as the quality of extractions returned will be better.\nIn terms of AP, visualized in Figure 4.7, both fine-tuned models displayed improvements\nin seven variables. On average, the standard fine-tuned model achieved an AP score higher by\napproximately 0.13, while the model trained on oversampled minority data showed a larger\nimprovement of 0.18. AP summarizes the precision of the models at various recall levels,\nthus, this together with the nDCG scores signifies the strength of the fine-tuned automated\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnDCG\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nVulnerable Victim\nPhysical Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nMinority Oversampled\nFig. 4.6 Per-variable nDCG score for different value labellers on supervised test sentencing\nremarks data.\n",
        "word_count": 286,
        "char_count": 1945,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "DejaVuSans (5.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "DejaVuSans (6.2pt)",
          "DejaVuSans (7.1pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 51,
        "text": "4.1 Sentencing Remarks Dataset\n41\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Precision\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nVulnerable Victim\nPhysical Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nMinority Oversampled\nFig. 4.7 Per-variable Average Precision for different value labellers on supervised test\nsentencing remarks data.\ncomponent of ELICIT, and suggests that using human validated data is expected to result in\nbetter value labelling on unseen documents.\nEnd-to-end Performance\nSince the supervised dataset was created from a single human-validation run, it means that\nthe components were evaluated only on a fraction of the passages from the documents.\nBy running a complete extraction process using fine-tuned models, we can see how the\nsystem operates when presented with full unseen documents. As given in Table 4.2, for\nTop-3, ELICIT-1.0 performs worse than ELICIT-VI, but slightly better than ELICIT-LLM.\nHowever, fine-tuning resulted in a significant improvement for the LLM-enhanced systems,\nwith FT-ELICIT-VI improving by 0.09, while FT-ELICIT-LLM improved by 0.084, resulting\nin a higher recall than ELICIT-1.0. Overall, for Top-3, FT-ELICIT-VI has the highest recall,\nconsistent with the stronger per-variable nDCG and AP scores of the vector index retriever.\nAn important observation is the enhanced performance in Top-1, which is the most\nrestrictive setting, requiring the best performance from the automated components in a\nnarrow recall window. FT-ELICIT-VI only improves by 0.04 from its base configuration,\nwhile FT-ELICIT-LLM improves by close to 0.15, even showing better performance than\nFT-ELICIT-VI. The difference in these two configurations indicates the differing performance\nof the passage retrievers for small Top-k settings. FT-ELICIT-LLM performs better in Top-1,\nwhich suggests that the LLM-based retriever is better at positioning the most relevant passage\nat the top, however, in Top-3, FT-ELICIT-VI achieves the highest recall, meaning that it\nplaces better passages at the second and third position. As more and better-quality data are\nused for training, it is expected that FT-ELICIT-LLM would surpass FT-ELICIT-VI across a\nwider Top-k range.\n",
        "word_count": 327,
        "char_count": 2281,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "DejaVuSans (5.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "DejaVuSans (6.2pt)",
          "DejaVuSans (7.1pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 52,
        "text": "42\nResults and Discussion\nTable 4.2 Top-1 and Top-3 recall on unseen sentencing remarks for different configurations.\n\"FT\" refers to fine-tuned configurations.\nConfiguration\nRecall\nTop-1\nTop-3\nELICIT-1.0\n0.3141\n0.4231\nELICIT-VI\n0.3397\n0.4744\nELICIT-LLM\n0.2500\n0.4038\nFT-ELICIT-VI\n0.3782\n0.5641\nFT-ELICIT-LLM\n0.3974\n0.4872\nIn conclusion, these results demonstrate the strength of using human validated data from\nextractions on other documents to fine-tune both the passage retriever and the value labeller.\nThey both learn to position the most relevant passages and values at the top of the result list\nand can extract information better from unseen documents.\n4.1.3\nFine-tuning for Recalibration\nAfter the model is fine-tuned using labeled data from a single human validation run, the\nextraction process could be run again to identify new potential extractions from previously\nabstained variables. This potential improvement in the LLM-enhanced system primarily\noriginates from two sources: the fine-tuned LLM passage retriever returning more relevant\npassages; and the fine-tuned value labeller assigning values with a greater accuracy and\nconfidence. As illustrated in Figure 4.8, the performance of the fine-tuned ELICIT-LLM\nsystem showcases discernible improvements. Specifically, out of 13 variables, enhancements\nwere observed in 10, increasing the average recall by 0.097 to 0.687. Conversely, ELICIT-VI\nexhibited a slightly smaller increase by 0.06, attributable mainly to the enhancement in the\nvalue labeller alone.\nOverall, these results showcase the significance of human-validated data as an invaluable\ninformation source for the models. Using this data not only improves the performance on\nalready extracted documents but also enhances the extraction of previously unseen ones\nthrough fine-tuning the models.\n",
        "word_count": 256,
        "char_count": 1823,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 53,
        "text": "4.2 Court Transcripts Dataset\n43\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nSexually Motivated\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nELICIT-LLM\nFT-ELICIT-LLM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nELICIT-VI\nFT-ELICIT-VI\nFig. 4.8 Improvement in recall for ELICIT-LLM and ELICIT-VI from recalibration using\nfine-tuning on extracted sentencing remarks.\n4.2\nCourt Transcripts Dataset\n4.2.1\nPerformance on Unseen Documents\nIn this part, the passage retriever and value labeller were fine-tuned on validated Top-3\nextractions of 3 out of 5 court transcripts, resulting in 663 extractions. The remaining\ntranscripts form the test set containing 485 extractions. The fine-tuning details for this task\nare presented in Appendix A.2.\nPassage Retriever\nTranscripts of court proceedings have a known dialogue structure containing speaker tags,\nand featuring questions followed by answers. This inherent structure provides contextual\ncues that aid passage retrievers in assessing the passage relevance. In contrast, sentencing\nremarks lack these cues and contain more nuanced and indirect language.\nThese differences manifest in the performance of passage retrievers on unseen human\nvalidated court transcript passages. The ROC curve, presented in Figure 4.9, demonstrates\nbetter performance across all passage retrievers compared to the sentencing remarks dataset\nin Figure 4.3. The fine-tuned retriever emerges as the best with an ROC-AUC of 0.91. In\ncomparison, the base retriever and vector index achieve scores of 0.65 and 0.70, respectively.\nWhile ROC evaluates the general performance of the binary classifier, the Precision-\nRecall suggest how the model performs considering class imbalance. The retriever, fine-tuned\non user-validated extractions from court transcripts, shows strong performance with an AUC-\nPR of 0.70, while the lower scores for the base and vector index retriever suggest them\nstruggling with the minority class. The AUC-PR achieved by the retrievers on the court\ntranscripts is similar as on the sentencing remarks, hence, the difference in AUC-ROC might\n",
        "word_count": 317,
        "char_count": 2214,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "DejaVuSans (5.7pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "DejaVuSans (6.1pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "DejaVuSans (6.9pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 54,
        "text": "44\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nReceiver Operating Characteristic (ROC) Curves\nBase (AUC = 0.65)\nFine-tuned (AUC = 0.91)\nVector Index (AUC = 0.70)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall (Sensitivity)\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nPrecision-Recall Curves\nBase (AUC-PR = 0.35)\nFine-tuned (AUC-PR = 0.70)\nVector Index (AUC-PR = 0.37)\nFig. 4.9 ROC and Precision-Recall curves for different passage retrievers on supervised court\ntranscript data.\nbe due to its sensitivity to class imbalanced. The court transcripts dataset contained fewer\npositive samples, specifically 16%, compared to 25% in the sentencing remarks.\nEvaluation using nDCG and AP reveals similar trends. In terms of nDCG, which focuses\non how well the retriever positions relevant passages at the top of the results list, the fine-\ntuned retriever achieves the highest mean score of 0.85, improving from the base model’s\nscore of 0.66, and ahead of the vector index score of 0.79. In terms of AP, which measures\nprecision across varying recall levels, the fine-tuned retriever leads again with a mean score\nof 0.74, while the vector index and base model score 0.63 and 0.46, respectively.\nThe improvements in nDCG and AP across all variables, as seen in Figures 4.10 and\n4.11, highlight the fine-tuned retriever’s effectiveness on court transcripts. Furthermore, it\n",
        "word_count": 226,
        "char_count": 1402,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "DejaVuSans (6.8pt)",
          "DejaVuSans (4.5pt)",
          "DejaVuSans (7.7pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 55,
        "text": "4.2 Court Transcripts Dataset\n45\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnDCG\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nVector Index\nFig. 4.10 Per-variable nDCG score for different passage retrievers on supervised test court\ntranscript data.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Precision\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nVector Index\nFig. 4.11 Per-variable Average Precision for different passage retrievers on supervised test\ncourt transcript data.\nshowcases better performance across most variables compared to the vector index. These\nresults indicate the model’s successful learning from human-validated data, resulting in\nsuperior ranking and precision capabilities on unseen court transcript documents.\nValue Labeller\nThe value labelling component, used for dialogue texts, is responsible for assigning variable\nvalues to passages based on specific speakers. If a speaker does not provide a value, it is\ninstructed to assign the \"unclear\" value. The evaluation of nDCG and AP follows the same\napproach as in the sentencing remarks scenario, by comparing every non-zero probability\nnon-”unclear” prediction with the correct label. As in Section 4.1.2, the model is fine-tuned\nwith two approaches: using the complete imbalanced training set, and with oversampling the\nminority values for each variable.\nIn terms of nDCG, the labeller improves significantly from fine-tuning. On average,\nthe regular fine-tuned model shows an improvement of 0.21, while the oversampled model\n",
        "word_count": 255,
        "char_count": 1814,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "DejaVuSans (5.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "DejaVuSans (6.2pt)",
          "DejaVuSans (7.1pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 56,
        "text": "46\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnDCG\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nMinority Oversampled\nFig. 4.12 Per-variable nDCG score for different value labellers on supervised test court\ntranscript data.\nimproves by 0.13. Furthermore, as illustrated in Figure 4.12, the regular fine-tuned model\nimproves over the base model in 10 out of 12 variables, while the oversampled model\nimproves in 11. Notably, the increases in the regular fine-tuned model are more significant,\nsurpassing the oversampled model’s nDCG scores in all 10 of the improved variables.\nWith AP, as given in Figure 4.13, the same trends are apparent, with the regularly\nfine-tuned model resulting in higher mean AP, and improving in 9 variables over the base\nmodel.\nThe presented results outline the significant benefits of fine-tuning when dealing with\nthe complexity of speaker-based labelling. This fine-tuning results in an improved value\nlabeller, which more effectively places correct values at the top of the result list, and with\ngreater confidence. Interestingly, it was also observed that contrary to the performance on\nsentencing remarks, oversampling the minority classes does not lead to better performance\nthan using the full imbalanced dataset.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAverage Precision\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nBase\nFine-tuned\nMinority Oversampled\nFig. 4.13 Per-variable Average Precision for different value labellers on supervised test court\ntranscript data.\n",
        "word_count": 262,
        "char_count": 1815,
        "fonts": [
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "DejaVuSans (5.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "DejaVuSans (6.2pt)",
          "DejaVuSans (7.1pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 57,
        "text": "4.2 Court Transcripts Dataset\n47\nTable 4.3 Top-1 and Top-3 recall results on unseen court transcripts for different configu-\nrations. \"Per-variable\" is obtained by taking the mean of the weighted recalls across all\nvariables. \"Total\" is obtained by computing the recall considering all data points equally.\nConfiguration\nPer-variable\nTotal\nTop-1\nTop-3\nTop-1\nTop-3\nELICIT-VI\n0.4815\n0.5858\n0.5172\n0.6724\nELICIT-LLM\n0.3321\n0.4595\n0.4310\n0.6207\nFT-ELICIT-VI\n0.5545\n0.6494\n0.6207\n0.7759\nFT-ELICIT-LLM\n0.6390\n0.7386\n0.7586\n0.8621\nEnd-to-end Performance\nThe evaluation of the passage retriever and value labeller on the human validated supervised\ndataset serves as a proxy for the true end-to-end performance. This subsection focuses on the\nevaluation of the different configurations with base and fine-tuned models to determine if the\nperformance improvements in the supervised dataset transfer to improvements on complete\nunseen transcripts with the human validation element.\nAs discussed in Section 4.1.2, precision is predominantly determined by the human\nvalidator rather than the automated component. Consequently, no significant differences in\nprecision between various configurations were observed.\nThe mean recall results, given in Table 4.3, align with the trends identified in the evalu-\nations on the supervised dataset. Among the base model configurations, the system using\nthe vector index passage retriever (ELICIT-VI) performs better due to the better retrieving\ncapabilities using the embedding model compared to the LLM (ELICIT-LLM).\nWith fine-tuning, the model with both LLM components (FT-ELICIT-LLM) achieves the\nhighest recall. Notably, it improves from its base model by a more significant margin (0.31\nin Top-1 and 0.28 in Top-3, per-variable) than FT-ELICIT-VI (0.07 in Top-1 and 0.06 in\nTop-3, per-variable), indicating that the majority of the recall gain comes from the fine-tuned\npassage retriever.\n4.2.2\nFine-tuning for Recalibration\nFine-tuning the models with a complete human validation dataset (consisting of all five\ntranscripts) and running extraction again to find new potential extractions results in similar\ntrends as in previous tasks. As expected, the full LLM-enhanced system benefits significantly\nfrom fine-tuning due to the improvement of the value labeller and passage retriever, resulting\nin a mean improvement in recall of 0.12 per-variable and in total, as given in Table 4.4.\n",
        "word_count": 348,
        "char_count": 2419,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 58,
        "text": "48\nResults and Discussion\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nTop-3\nELICIT-VI\nELICIT-LLM\nFT-ELICIT-VI\nFT-ELICIT-LLM\nFig. 4.14 Per-variable recall for different configurations on the unseen court transcripts with\nTop-3. FT-ELICIT-LLM outperforms its base configuration in 11 out of 12 variables, and\nachieves the highest overall recall in 10 variables.\nTable 4.4 Improvement in mean total and per-variable recall on unseen court transcripts from\nrecalibration through fine-tuning.\nConfiguration\nPer-variable\nTotal\nOriginal\nImproved\nOriginal\nImproved\nELICIT-VI\n0.5245\n0.5828\n0.5945\n0.6667\nELICIT-LLM\n0.5088\n0.6290\n0.5882\n0.7059\nConversely, the configuration using the vector index passage retriever improves by 0.06\nper-variable and 0.07 across all data points. The recalibrated ELICIT-LLM results in the\nhighest overall recall, and it also improves in more variables (eight versus ELICIT-VI’s five),\nas visualized in Figure 4.15.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nVictim Employment Status\nVictim Pregnancy\nVictim Sex\nVictim Domestic Abuse\nPrior Convictions\nPremeditated\nRemorse\nVulnerable Victim\nPhysical Abuse\nEmotional Abuse\nAge Mitigating\nRelationship\nVariable\nELICIT-LLM\nFT-ELICIT-LLM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nELICIT-VI\nFT-ELICIT-VI\nFig. 4.15 Improvement in recall for ELICIT-LLM and ELICIT-VI from recalibration using\nfine-tuning on extracted court transcripts.\n",
        "word_count": 211,
        "char_count": 1564,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "DejaVuSans (5.7pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "DejaVuSans (6.1pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "DejaVuSans (5.8pt)",
          "DejaVuSans (6.9pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "DejaVuSans (6.2pt)",
          "DejaVuSans (7.1pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 59,
        "text": "4.2 Court Transcripts Dataset\n49\n4.2.3\nRecall and Shown Extractions\nWith the objective of finding every variable value indicated by a speaker, the number of\nextractions shown to the user can quickly increase and become cumbersome. Specifically,\nfor a single document, the number of extractions shown the user is the product of the number\nof speakers, the number of variables, and k (in Top-k). While the user defines the number of\nvariables and k, the speaker count is inherent to the document. A limitation of this is that any\nspeaker which appears in the transcript will have extractions associated with it, even though\nin reality, they might not have given a relevant value. To avoid showing the user unnecessary\nextractions, a good automated component is essential.\nThis part details how for the same recall, fine-tuned configurations require fewer extrac-\ntions be shown to the user, and how the fine-tuned labeller better separates relevant from\nirrelevant speakers for given passages.\nFigure 4.16 illustrates the mean recall against the number of extractions shown to the\nuser from a Top-3 validation run on the unseen court transcripts. The maximum number of\nextractions shown to the user is 448, when all top 3 extractions are shown for each speaker\nand variable combination. As fewer examples are shown to the user, based on their confidence\nscores, a logarithmic decrease in recall is observed across all configurations. This logarithmic\ntrend signifies that, for equivalent recall levels, more efficient systems need a significantly\nsmaller number of displayed extractions.\n0\n100\n200\n300\n400\nShown Extractions\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nRecall vs Shown Extractions\nELICIT-VI\nELICIT-LLM\nFT-ELICIT-VI\nFT-ELICIT-LLM\nFig. 4.16 Recall against number of shown extractions in a Top-3 run on unseen court\ntranscripts. FT-ELICIT-LLM reaches the top recall level of ELICIT-LLM (achieved at 404\nextractions) with just 120 extractions; of ELICIT-VI (achieved at 396) with 142; and of\nFT-ELICIT-VI (achieved at 259) with 191 extractions.\n",
        "word_count": 321,
        "char_count": 2041,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "DejaVuSans (7.3pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "DejaVuSans (5.8pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 60,
        "text": "50\nResults and Discussion\nThis outlined setup can be conceptualized as a global Top-k setting, which is applied\nacross the entire extraction set rather than per variable. Thus, users can save manual extraction\ntime while only marginally compromising on recall. This also indicates the benefits of fine-\ntuning on human-validated data, and as the models are trained on more data, they will require\nfewer extractions to be shown to the user.\nIn the analysis of unseen court transcripts, certain passages are repeatedly presented\nas part of extractions attributed to different speakers, however, not all of the speakers are\nrelevant for the specific variable. An important part of our system is for the automated\ncomponent to assign a lower confidence or rank for extractions associated with irrelevant\nspeakers compared to those with relevant speakers.\nTo quantify this disparity in ranking, a comparative approach is employed by measuring\nthe difference in rankings5 between relevant and irrelevant speaker extractions for identical\npassage-variable pairs, which were presented to the user for validation. This evaluation\nfocuses on the configuration using the vector index passage retriever, enabling the isolation\nof the value labeller’s performance.\nIt was found that FT-ELICIT-VI ranked the extractions of the relevant speaker on average\n31.23 positions higher than the irrelevant one’s, while ELICIT-VI only separated them by\n9.18 positions. This suggests that fine-tuning improves the model’s ability in differentiating\nrelevant from irrelevant speakers for an identical passage and variable.\nA specific example of this is the “Justice” speaker, which in the unseen transcripts only\nhas procedural speech, not giving any valuable information. Thus, any extraction associated\nwith it should be deemed irrelevant. However, the \"Justice\" dialogue tag appears in essential\npassages, such as the counsel’s opening or closing remarks, which are full of important\ninformation. The value labeller is required to distinguish that the information in these\npassages is not associated with the \"Justice\" speaker, resulting in low confidence extractions.\nThe distribution of per-variable rankings for each \"Justice\" extraction — where each is ranked\ncompared to other extractions of the same variable — is depicted in Figure 4.17 for ELICIT-\nVI and FT-ELICIT-VI. Notably, both configurations effectively rank these extractions with\nlow priority. However, the fine-tuned system displays a more favorable outcome, with a\nhigher mean rank and reduced variance compared to the base configuration.\n5The difference in rankings is used, since the confidence scores among various configurations are not\ncalibrated, making them unreliable for comparison.\n",
        "word_count": 404,
        "char_count": 2738,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (10.0pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Regu (7.4pt)",
          "NimbusRomNo9L-Regu (10.1pt)",
          "NimbusRomNo9L-Regu (9.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 61,
        "text": "4.2 Court Transcripts Dataset\n51\n5\n10\n15\n20\n25\n30\n35\nRank\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nFrequency\nDistribution of Per-Variable Rankings\nELICIT-VI\nFT-ELICIT-VI\nFig. 4.17 Per-variable ranking distribution of extractions attributed to the \"Justice\" speaker.\n",
        "word_count": 40,
        "char_count": 265,
        "fonts": [
          "DejaVuSans (11.4pt)",
          "DejaVuSans (10.6pt)",
          "DejaVuSans (9.1pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 62,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 63,
        "text": "Chapter 5\nConclusions\nIn this project I delved into the analysis and evaluation of utilizing LLMs for the extraction\nof structured information from long unstructured documents, specifically in the legal domain,\nfocusing on sentencing remarks and court transcripts. This chapter presents a recap of the\nproject, highlights the key findings, and proposes promising future research directions.\n5.1\nProject Recap\nThe primary objective of this project was to extend the capabilities of ELICIT by integrating\nthe use of LLMs. The study focused on two particular tasks with distinct datasets: the\nextraction of single corresponding variable values from sentencing remarks, and the novel\ntask of extracting speaker-specific variable values from court transcripts.\nTo overcome the challenges associated with the length of the documents, the approach of\nsplitting the documents into smaller passages was taken. This resulted in the development\nand analysis of two separate components: (a) the passage retriever, which identifies relevant\npassages, and (b) the value labeller, which assigns variable values to these passages. The\nsubsequent evaluation of these components led to a comprehensive investigation of how\nLLMs perform in the task of IE, the impact of the individual components to the overall\nperformance, and how human validated data gathered through the use of ELICIT could be\nutilized for fine-tuning.\n5.2\nKey Findings\nThe evaluation on sentencing remarks and court trancripts, in supervised data and end-to-end\nsettings, led to several key findings.\n",
        "word_count": 231,
        "char_count": 1553,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (24.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 64,
        "text": "54\nConclusions\nA comparative analysis of two passage retrievers — a vector index using the ’all-mpnet-\nbase-v2’ embedding model, and the Vicuna-13B LLM, which was prompted to identify if the\npassage is relevant or not — revealed that the LLM-based retriever initially underperformed\nbut significantly improved from fine-tuning with human-validated data. In the case of court\ntranscripts, the LLM-based retriever even outperformed the embedding model in both the\nsupervised and end-to-end setting. A possible explanation for this is that the fine-tuned model\naligns with the particular task and user, whereas the embedding model relies on semantic\ninformation and might result in misalignment.\nThe value labeller was designed as an LLM prompted to classify passages into the\nvariable values based on the specific speaker. Fine-tuning this component led to improved\nranking capabilities, which in turn resulted in better overall recall of the system. In the\ncase of sentencing remarks, addressing data imbalance in human-validated data through\noversampling the minority classes resulted in significant improvements. In court transcripts,\nfine-tuning notably improved the separation of values attributed in the same passages to\nrelevant and irrelevant speakers.\nThe observations from the analysis of the separate components translated into the full\nend-to-end setting. Fine-tuned configurations, especially ones using the LLM-based retriever,\nresulted in better overall recall compared to base model configurations and the current\nELICIT system. Additionally, the passage retriever was identified as the main factor in the\noverall performance of the LLM-enhanced ELICIT, with fine-tuning leading to substantial\nimprovements in recall.\nThe final observation of the project was that the fine-tuned LLM systems resulted in\nincreased efficiency in the human-in-the-loop format. Due to their enhanced automated\nperformance, they required significantly fewer extractions shown to the user to achieve the\nsame recall, thereby reducing potential human validation efforts.\n5.3\nLimitations\nThe main limitations of the project are the following:\n1. The approach of splitting the document and doing extraction on smaller passages\nmeans that the system cannot extract information which spans multiple passages, and\nis limited to information which is contained in a single passage.\n2. The project did not consider the processing time of the LLM-based passage retriever,\nwhich results in longer processing times, as the model is prompted for every passage.\nHowever, this can be done off-line before presenting the results to the human validator.\n",
        "word_count": 382,
        "char_count": 2628,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-ReguItal (11.9pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-ReguItal (12.1pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 65,
        "text": "5.4 Future Directions\n55\n3. The court transcripts used are semi-synthetic; thus it is unclear how the shift to real\ndata would affect the results.\n4. The embedding model used in the vector index was not fine-tuned on human-validated\ndata.\n5. The project did not have access to long-context models, which could have been used\nas a baseline.\n5.4\nFuture Directions\nThis study into the application of LLMs for legal information extraction in the legal domains\nopens up potential avenues for future research. The new task of speaker-centric information\nextraction is of great promise. It was demonstrated that the base model struggles to distinguish\ninformation based on the speaker, with the fine-tuned model performing marginally better.\nThus, the burden of validating if the specified speaker provided the information still falls\non the human validator. Alternative approaches through prompt engineering or fine-tuning\ncould be explored for potential improvement.\nDrawing inspiration from the success of using human-validated data for supervised fine-\ntuning, the application of Reinforcement Learning from Human Feedback (RLHF) (Bai et al.,\n2022; Ouyang et al., 2022) for fine-tuning presents an interesting path for improving passage\nretrieval and value labelling based on human preferences. Particularly in passage retrieval,\nthe imperfect recall indicates that there are passages, which are relevant but the user does\nnot get to validate. If these passages differ significantly from the ones being retrieved and\nvalidated, standard fine-tuning might not solve this. However, RLHF could help address this\nissue through the exploration of the RL agent.\nWhile this project predominantly focused on the broader aspects of the IE process using\nLLMs, namely passage retrieval, value labelling, and fine-tuning, future investigations could\nbe done into improving the prompts used, for example, by exploring the use of LLMs for\ndesigning prompts (Zhou et al., 2023).\nAnother area of future exploration lies in alternative strategies for dealing with long\ndocuments. The use of memory transformers (Bulatov et al., 2023) could be explored to\novercome the limited context length of the LLMs and enable the extraction system to be\nmore context-aware.\nLastly, given the sensitivity of legal data, additional effort should be given into the\nexamination of potential biases inherent in the system, as well as potential mitigation\nstrategies (Nozza et al., 2021, 2022).\n",
        "word_count": 373,
        "char_count": 2457,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 66,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 67,
        "text": "References\nAghajanyan, A., Zettlemoyer, L., and Gupta, S. (2020). Intrinsic Dimensionality Explains\nthe Effectiveness of Language Model Fine-Tuning.\nBahdanau, D., Cho, K., and Bengio, Y. (2016). Neural Machine Translation by Jointly\nLearning to Align and Translate.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S.,\nGanguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk,\nS., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston, S., Kravec, S.,\nLovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah,\nC., Mann, B., and Kaplan, J. (2022). Training a Helpful and Harmless Assistant with\nReinforcement Learning from Human Feedback.\nBansal, N., Sharma, A., and Singh, R. K. (2019). A Review on the Application of Deep\nLearning in Legal Domain. In MacIntyre, J., Maglogiannis, I., Iliadis, L., and Pimenidis, E.,\neditors, Artificial Intelligence Applications and Innovations, IFIP Advances in Information\nand Communication Technology, pages 374–381, Cham. Springer International Publishing.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan,\nT., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler,\nE., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,\nSutskever, I., and Amodei, D. (2020). Language Models are Few-Shot Learners.\nBulatov, A., Kuratov, Y., and Burtsev, M. S. (2023). Scaling Transformer to 1M tokens and\nbeyond with RMT.\nButcher, B., Zilka, M., Cook, D., Hron, J., and Weller, A. (2023). Optimising Human-\nMachine Collaboration for Efficient High-Precision Information Extraction from Text\nDocuments.\nCarnaz, G., Nogueira, V., Antunes, M., and Ferreira, N. (2020). An Automated System for\nCriminal Police Reports Analysis.\nChase, H. (2022). LangChain. https://github.com/hwchase17/langchain.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang,\nY., Gonzalez, J. E., Stoica, I., and Xing, E. P. (2023). Vicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality.\n",
        "word_count": 335,
        "char_count": 2249,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Medi (24.8pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-ReguItal (11.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 68,
        "text": "58\nReferences\nChoi, E., Hewlett, D., Uszkoreit, J., Polosukhin, I., Lacoste, A., and Berant, J. (2017).\nCoarse-to-Fine Question Answering for Long Documents. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 209–220, Vancouver, Canada. Association for Computational Linguistics.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani,\nM., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A.,\nCastro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao,\nV., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A.,\nZhou, D., Le, Q. V., and Wei, J. (2022). Scaling Instruction-Finetuned Language Models.\nCommon Crawl (2007). Common Crawl. http://commoncrawl.org/.\nDavis, J. and Goadrich, M. (2006). The relationship between Precision-Recall and ROC\ncurves. In Proceedings of the 23rd International Conference on Machine Learning - ICML\n’06, pages 233–240, Pittsburgh, Pennsylvania. ACM Press.\nEccleston, D. (2023). ShareGPT. https://github.com/domeccleston/sharegpt.\nGhoddusi, H., Creamer, G. G., and Rafizadeh, N. (2019). Machine learning in energy\neconomics and finance: A review. Energy Economics, 81:709–727.\nHanley, J. A. and McNeil, B. J. (1982). The meaning and use of the area under a receiver\noperating characteristic (ROC) curve. Radiology, 143(1):29–36.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas,\nD. d. L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K.,\nvan den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae,\nJ. W., Vinyals, O., and Sifre, L. (2022). Training Compute-Optimal Large Language\nModels.\nHonnibal, M., Montani, I., Van Landeghem, S., and Boyd, A. (2020). spaCy: Industrial-\nstrength Natural Language Processing in Python.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A.,\nAttariyan, M., and Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2021).\nLoRA: Low-Rank Adaptation of Large Language Models.\nHu, X., Chen, P.-Y., and Ho, T.-Y. (2023). RADAR: Robust AI-Text Detection via Adversarial\nLearning.\nHuber,\nJ. (2022).\nChroma:\nThe AI-native open-source embedding database.\nhttps://github.com/chroma-core/chroma.\nIzacard, G. and Grave, E. (2021). Leveraging Passage Retrieval with Generative Models for\nOpen Domain Question Answering.\nJavaid, M., Haleem, A., Pratap Singh, R., Suman, R., and Rab, S. (2022). Significance of\nmachine learning in healthcare: Features, pillars and applications. International Journal\nof Intelligent Networks, 3:58–73.\n",
        "word_count": 412,
        "char_count": 2822,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (12.0pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-ReguItal (12.1pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-ReguItal (11.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 69,
        "text": "References\n59\nKamalloo, E., Zhang, X., Ogundepo, O., Thakur, N., Alfonso-Hermelo, D., Rezagholizadeh,\nM., and Lin, J. (2023). Evaluating Embedding APIs for Information Retrieval.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. (2020). ALBERT:\nA Lite BERT for Self-supervised Learning of Language Representations.\nLawrence, J. and Reed, C. (2019). Argument Mining: A Survey. Computational Linguistics,\n45(4):765–818.\nLi, J., Liu, M., Kan, M.-Y., Zheng, Z., Wang, Z., Lei, W., Liu, T., and Qin, B. (2020).\nMolweni: A Challenge Multiparty Dialogues-based Machine Reading Comprehension\nDataset with Discourse Structure.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L.,\nand Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach.\narXiv:1907.11692 [cs].\nMalkov, Y. A. and Yashunin, D. A. (2018). Efficient and robust approximate nearest neighbor\nsearch using Hierarchical Navigable Small World graphs.\nManyika, J. (2023). An overview of Bard: an early experiment with generative AI. Google\nAI.\nNozza, D., Bianchi, F., and Hovy, D. (2021). HONEST: Measuring Hurtful Sentence\nCompletion in Language Models. In Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 2398–2406, Online. Association for Computational Linguistics.\nNozza, D., Bianchi, F., and Hovy, D. (2022). Pipelines for Social Bias Testing of Large\nLanguage Models. In Proceedings of BigScience Episode #5 – Workshop on Challenges &\nPerspectives in Creating Large Language Models, pages 68–74, virtual+Dublin. Associa-\ntion for Computational Linguistics.\nOpenAI (2022). ChatGPT. https://chat.openai.com.\nOpenAI (2023a). GPT-4. https://openai.com/research/gpt-4.\nOpenAI (2023b). GPT-4 Technical Report.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C.,\nAgarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M.,\nAskell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training language\nmodels to follow instructions with human feedback.\nPascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training Recurrent\nNeural Networks.\nQayyum, A., Qadir, J., Bilal, M., and Al-Fuqaha, A. (2020). Secure and Robust Machine\nLearning for Healthcare: A Survey.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2020). Improving Language\nUnderstanding by Generative Pre-Training.\n",
        "word_count": 359,
        "char_count": 2530,
        "fonts": [
          "NimbusRomNo9L-ReguItal (11.8pt)",
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-ReguItal (11.9pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-ReguItal (12.1pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 70,
        "text": "60\nReferences\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and\nLiu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text\nTransformer.\nRajpurkar, P., Jia, R., and Liang, P. (2018). Know What You Don’t Know: Unanswerable\nQuestions for SQuAD. arXiv:1806.03822 [cs].\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016). SQuAD: 100,000+ Questions for\nMachine Comprehension of Text.\nReimers, N. and Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese\nBERT-Networks.\nSaito, T. and Rehmsmeier, M. (2015). The precision-recall plot is more informative than\nthe ROC plot when evaluating binary classifiers on imbalanced datasets. PloS One,\n10(3):e0118432.\nSennrich, R., Haddow, B., and Birch, A. (2016). Neural Machine Translation of Rare Words\nwith Subword Units.\nSofaer, H. R., Hoeting, J. A., and Jarnevich, C. S. (2019). The area under the precision-recall\ncurve as a performance metric for rare binary events. Methods in Ecology and Evolution,\n10(4):565–577.\nSun, C., Qiu, X., Xu, Y., and Huang, X. (2020). How to Fine-Tune BERT for Text Classifica-\ntion?\nSun, K., Yu, D., Chen, J., Yu, D., Choi, Y., and Cardie, C. (2019). DREAM: A Challenge\nDataset and Models for Dialogue-Based Reading Comprehension.\nSun, S., Luo, C., and Chen, J. (2017). A review of natural language processing techniques\nfor opinion mining systems. Information Fusion, 36:10–25.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B.,\nGoyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.\n(2023). LLaMA: Open and Efficient Foundation Language Models.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and\nPolosukhin, I. (2017). Attention Is All You Need.\nWikimedia Foundation (2001). Wikipedia. http://wikipedia.org/.\nWu, C.-S., Madotto, A., Liu, W., Fung, P., and Xiong, C. (2022). QAConv: Question\nAnswering on Informative Conversations.\nXu, W., Grishman, R., and Zhao, L. (2011). Passage Retrieval for Information Extraction\nusing Distant Supervision. In Proceedings of 5th International Joint Conference on\nNatural Language Processing, pages 1046–1054, Chiang Mai, Thailand. Asian Federation\nof Natural Language Processing.\nYang, Y., Wu, Z., Yang, Y., Lian, S., Guo, F., and Wang, Z. (2022). A Survey of Information\nExtraction Based on Deep Learning. Applied Sciences, 12:9691.\n",
        "word_count": 369,
        "char_count": 2462,
        "fonts": [
          "NimbusRomNo9L-ReguItal (11.8pt)",
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-ReguItal (11.9pt)",
          "NimbusRomNo9L-Regu (11.9pt)",
          "NimbusRomNo9L-ReguItal (12.1pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 71,
        "text": "References\n61\nYang, Z. and Choi, J. D. (2019). FriendsQA: Open-Domain Question Answering on TV\nShow Transcripts. In Proceedings of the 20th Annual SIGdial Meeting on Discourse and\nDialogue, pages 188–197, Stockholm, Sweden. Association for Computational Linguistics.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., and Le, Q. V. (2020). XLNet:\nGeneralized Autoregressive Pretraining for Language Understanding.\nYin, W., Hay, J., and Roth, D. (2019). Benchmarking Zero-shot Text Classification: Datasets,\nEvaluation and Entailment Approach. arXiv:1909.00161 [cs].\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D.,\nXing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. (2023). Judging LLM-as-a-judge\nwith MT-Bench and Chatbot Arena.\nZhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. (2023). Large\nLanguage Models Are Human-Level Prompt Engineers.\n",
        "word_count": 139,
        "char_count": 936,
        "fonts": [
          "NimbusRomNo9L-Regu (11.8pt)",
          "NimbusRomNo9L-Medi (12.0pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "NimbusRomNo9L-ReguItal (12.0pt)",
          "NimbusRomNo9L-ReguItal (11.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 72,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 73,
        "text": "Appendix A\nFine-tuning Details\nA.1\nSentencing Remarks\nA.1.1\nPassage Retriever\nTable A.1 Optimal hyperparameters of the Vicuna-13B passage retriever fine-tuned on\nuser-validated sentencing remarks extractions.\nType\nNo. of Training Samples\nNo. of Validation Samples\nBatch Size\nLearning Rate\nRegular\n467\n117\n32\n0.0012\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nLoss\nTraining Loss\nValidation Loss\nFig. A.1 Training and validation loss of the Vicuna-13B passage retriever fine-tuned on\nuser-validated sentencing remarks extractions.\n",
        "word_count": 83,
        "char_count": 559,
        "fonts": [
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "ArialMT (9.4pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Medi (24.8pt)",
          "ArialMT (5.2pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 74,
        "text": "64\nFine-tuning Details\nA.1.2\nValue Labellers\nTable A.2 Optimal hyperparameters of the Vicuna-13B value labellers fine-tuned on user-\nvalidated sentencing remarks extractions.\nParameter\nRegular\nMinority Oversampled\nNo. of Training Samples\n467\n926\nNo. of Validation Samples\n117\n117\nBatch Size\n8\n16\nLearning Rate\n0.0006\n0.00008\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nLoss\nRegular\nTraining Loss\nValidation Loss\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nLoss\nMinority Oversampled\nTraining Loss\nValidation Loss\nFig. A.2 Training and validation losses of Vicuna-13B value labellers fine-tuned on regular\nuser-validated sentencing remarks extractions, and with minority classes oversampled.\n",
        "word_count": 116,
        "char_count": 746,
        "fonts": [
          "ArialMT (4.9pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "ArialMT (8.8pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 75,
        "text": "A.2 Court Transcripts\n65\nA.2\nCourt Transcripts\nA.2.1\nPassage Retriever\nTable A.3 Optimal hyperparameters of the Vicuna-13B passage retriever fine-tuned on\nuser-validated court transcript extractions.\nType\nNo. of Training Samples\nNo. of Validation Samples\nBatch Size\nLearning Rate\nRegular\n424\n113\n32\n0.0012\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nLoss\nTraining Loss\nValidation Loss\nFig. A.3 Training and validation loss of the Vicuna-13B passage retriever fine-tuned on\nuser-validated court transcript extractions.\n",
        "word_count": 82,
        "char_count": 544,
        "fonts": [
          "NimbusRomNo9L-Medi (17.2pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "ArialMT (9.4pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "ArialMT (5.2pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      },
      {
        "page_number": 76,
        "text": "66\nFine-tuning Details\nA.2.2\nValue Labellers\nTable A.4 Optimal hyperparameters of the Vicuna-13B value labellers fine-tuned on user-\nvalidated court transcript extractions.\nParameter\nRegular\nMinority Oversampled\nNo. of Training Samples\n520\n878\nNo. of Validation Samples\n138\n138\nBatch Size\n32\n16\nLearning Rate\n0.0006\n0.00008\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLoss\nRegular\nTraining Loss\nValidation Loss\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nLoss\nMinority Oversampled\nTraining Loss\nValidation Loss\nFig. A.4 Training and validation losses of Vicuna-13B value labellers fine-tuned on regular\nuser-validated court transcript extractions, and with minority classes oversampled.\n",
        "word_count": 115,
        "char_count": 739,
        "fonts": [
          "ArialMT (4.9pt)",
          "NimbusRomNo9L-Regu (12.0pt)",
          "NimbusRomNo9L-Regu (12.1pt)",
          "ArialMT (8.8pt)",
          "NimbusRomNo9L-Medi (14.3pt)",
          "NimbusRomNo9L-Medi (12.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          595.2760009765625,
          841.8900146484375
        ]
      }
    ]
  }
}