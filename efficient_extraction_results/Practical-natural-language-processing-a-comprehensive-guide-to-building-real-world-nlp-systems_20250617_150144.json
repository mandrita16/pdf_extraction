{
  "file_path": "../test_pdfs/Practical-natural-language-processing-a-comprehensive-guide-to-building-real-world-nlp-systems.pdf",
  "file_hash": "e95d950b81b7cda404070077eef3bb62",
  "timestamp": "2025-06-17T15:01:44.315122",
  "page_count": 455,
  "total_words": 135768,
  "total_chars": 874799,
  "fonts_used": [
    "ArialUnicodeMS (10.5pt)",
    "ArialUnicodeMS (9.0pt)",
    "Gilroy-Light (15.5pt)",
    "Gilroy-Light (17.0pt)",
    "Gilroy-Medium (22.0pt)",
    "Gilroy-SemiBold (28.0pt)",
    "Gilroy-SemiBold (52.0pt)",
    "GuardianSans-Light (4.5pt)",
    "GuardianSansNarrow-Regul (24.0pt)",
    "MinionPro-Bold (10.0pt)",
    "MinionPro-Bold (10.5pt)",
    "MinionPro-It (10.0pt)",
    "MinionPro-It (10.5pt)",
    "MinionPro-It (6.3pt)",
    "MinionPro-It (8.0pt)",
    "MinionPro-It (8.5pt)",
    "MinionPro-It (9.0pt)",
    "MinionPro-It (9.3pt)",
    "MinionPro-It (9.6pt)",
    "MinionPro-Regular (10.0pt)",
    "MinionPro-Regular (10.5pt)",
    "MinionPro-Regular (14.1pt)",
    "MinionPro-Regular (6.3pt)",
    "MinionPro-Regular (8.0pt)",
    "MinionPro-Regular (8.5pt)",
    "MinionPro-Regular (9.0pt)",
    "MinionPro-Regular (9.3pt)",
    "MinionPro-Regular (9.6pt)",
    "MinionPro-SemiboldIt (16.0pt)",
    "MinionPro-SemiboldIt (18.0pt)",
    "MyriadPro-Cond (12.3pt)",
    "MyriadPro-Cond (6.3pt)",
    "MyriadPro-Cond (9.0pt)",
    "MyriadPro-SemiboldCond (10.0pt)",
    "MyriadPro-SemiboldCond (11.5pt)",
    "MyriadPro-SemiboldCond (11.6pt)",
    "MyriadPro-SemiboldCond (12.0pt)",
    "MyriadPro-SemiboldCond (14.0pt)",
    "MyriadPro-SemiboldCond (15.8pt)",
    "MyriadPro-SemiboldCond (16.8pt)",
    "MyriadPro-SemiboldCond (18.9pt)",
    "MyriadPro-SemiboldCond (20.0pt)",
    "MyriadPro-SemiboldCond (25.2pt)",
    "MyriadPro-SemiboldCond (28.4pt)",
    "MyriadPro-SemiboldCond (31.5pt)",
    "MyriadPro-SemiboldCond (9.0pt)",
    "MyriadPro-SemiboldCondIt (20.0pt)",
    "Symbola (10.5pt)",
    "Symbola (8.5pt)",
    "UbuntuMono-Bold (10.0pt)",
    "UbuntuMono-Bold (10.5pt)",
    "UbuntuMono-Bold (8.5pt)",
    "UbuntuMono-Italic (10.0pt)",
    "UbuntuMono-Italic (8.5pt)",
    "UbuntuMono-Regular (10.0pt)",
    "UbuntuMono-Regular (8.5pt)",
    "UbuntuMono-Regular (8.9pt)"
  ],
  "images_count": 243,
  "metadata": {
    "format": "PDF 1.6",
    "title": "Practical Natural Language Processing",
    "author": "Sowmya  Vajjala;Bodhisattwa  Majumder;Anuj  Gupta;Harshit  Surana;",
    "creator": "AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)",
    "producer": "Antenna House PDF Output Library 6.2.609 (Linux64)",
    "creationDate": "D:20200617144731Z",
    "modDate": "D:20200618124937+05'30'"
  },
  "pages": [
    {
      "page_number": 1,
      "text": "Sowmya Vajjala, \nBodhisattwa Majumder, \nAnuj Gupta & Harshit Surana\nPractical \nNatural Language \nProcessing\nA Comprehensive Guide to Building \nReal-World NLP Systems\n",
      "word_count": 21,
      "char_count": 166,
      "fonts": [
        "Gilroy-SemiBold (52.0pt)",
        "Gilroy-Medium (22.0pt)",
        "GuardianSansNarrow-Regul (24.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1967,
          "height": 1651,
          "ext": "jpeg",
          "size_bytes": 2426876
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.4639892578125
      ]
    },
    {
      "page_number": 2,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [
        {
          "index": 0,
          "width": 2100,
          "height": 2756,
          "ext": "jpeg",
          "size_bytes": 1112530
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.4400024414062
      ]
    },
    {
      "page_number": 3,
      "text": "Praise for Practical Natural Language Processing\nPractical NLP focuses squarely on an overlooked demographic: the practitioners and\nbusiness leaders in industry! While many great books focus on ML’s algorithmic\nfundamentals, this book exposes the anatomy of real-world systems: from e-commerce\napplications to virtual assistants. Painting a realistic picture of modern production\nsystems, the book teaches not only deep learning, but also the heuristics and patchwork\npipelines that define the (actual) state of the art for deployed NLP systems. The authors\nzoom out, teaching problem formulation, and aren’t afraid to zoom in on the grimy\ndetails, including handling messy data and sustaining live systems. This book will prove\ninvaluable to industry professionals keen to build and deploy NLP in the wild.\n—Zachary Lipton, Assistant Professor, Carnegie Mellon\nUniversity, Scientist at Amazon AI, Author of Dive into Deep Learning\nThis book does a great job bridging the gap between natural language processing (NLP)\nresearch and practical applications. From healthcare to e-commerce and finance, it\ncovers many of the most sought-after domains where NLP is being put to use and\nwalks through core tasks in a clear and understandable manner. Overall, the book\nis a great manual on how to get the most out of current NLP in your industry.\n—Sebastian Ruder, Research Scientist, Google DeepMind\nThere are two kinds of computer science books on the market: academic textbooks\nthat give you a deep understanding of a domain but can be difficult to access for a\nnon-academic, and “cookbooks” that outline solutions to very specific problems\nwithout providing the technical foundations that would allow the reader to generalize\nthe recipes. This book offers the best of both worlds: it is thorough yet accessible. It\nprovides the reader with a solid foundation in natural-language processing. . . . If\nyou would like to go from zero to one in NLP, this book is for you!\n—Marc Najork, Research Engineering Director, Google AI,\nACM & IEEE Fellow\n",
      "word_count": 322,
      "char_count": 2038,
      "fonts": [
        "MinionPro-Regular (10.0pt)",
        "MyriadPro-SemiboldCondIt (20.0pt)",
        "MinionPro-It (10.0pt)",
        "MyriadPro-SemiboldCond (20.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 4,
      "text": "There are text books or research papers or books on programming tips, but not a book\nthat tells us how to build an end-to-end NLP system from scratch. I am happy to see\nthis book on practical NLP, which fills this much needed gap. The authors have\nmeticulously, thoughtfully and lucidly covered each and every aspect of NLP\nthat one has to be aware of while building large scale practical systems; at the same\ntime, this book has also managed to cover a large number of examples and varied\napplication areas and verticals. This book is a must for all aspiring NLP engineers,\nentrepreneurs who want to build companies around language technologies, and also\nacademic researchers who would like to see their inventions reach the real users.\n—Monojit, Principal Researcher, Microsoft Research India,\nAdjunct Faculty at IIIT Hyderabad, Ashoka University, IIT Kharagpur\nThis book bridges the gap between theory and practice by explaining the underlying\nconcepts while keeping in mind varied real-world deployments across different\nbusiness verticals. There is much hard-fought practical advice from the trenches\nwhether it is about tweaking parameters of open source libraries, setting up\ndata pipelines for building models, or optimizing for fast inference.\nA must-read for engineers building NLP applications.\n—Vinayak Hegde, CTO-in-Residence, Microsoft For Startups\nThis book shows how to put NLP to practice. It bridges the gap between NLP theory and\npractical engineering. The authors achieved a rare feat by simplifying the esoteric art\nof design and architecture of production quality machine learning systems.\nI wish I had access to this book early on in my professional career and evaded\nthe mistakes I made along the way. . . . I am deeply convinced that this\nbook is an essential read for anybody aiming to develop involved\nin developing a robust, high-performing NLP system.\n—Siddharth Sharma, ML Engineer, Facebook\nI feel this is not only an essential book for NLP practitioners, it is also a valuable reference\nfor the research community to understand the problem spaces in real-world\napplications. I very much appreciate this book and wish this could be a\nlong-term project with up-to-date NLP application trending!\n—Mengting Wan, Data Scientist (ML&NLP) at Airbnb,\nMicrosoft Research Fellow\n",
      "word_count": 365,
      "char_count": 2301,
      "fonts": [
        "MinionPro-Regular (10.0pt)",
        "MinionPro-It (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 5,
      "text": "Sowmya Vajjala, Bodhisattwa Majumder,\nAnuj Gupta, and Harshit Surana\nPractical Natural Language\nProcessing\nA Comprehensive Guide to Building\nReal-World NLP Systems\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n",
      "word_count": 31,
      "char_count": 244,
      "fonts": [
        "MinionPro-SemiboldIt (16.0pt)",
        "MyriadPro-Cond (12.3pt)",
        "MinionPro-SemiboldIt (18.0pt)",
        "MyriadPro-SemiboldCond (31.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 6,
      "text": "978-1-492-05405-4\n[LSI]\nPractical Natural Language Processing\nby Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, and Harshit Surana\nCopyright © 2020 Anuj Gupta, Bodhisattwa Prasad Majumder, Sowmya Vajjala, and Harshit Surana. All\nrights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com.\nAcquistions Editor: Jonathan Hassell\nDevelopmental Editor: Melissa Potter\nProduction Editor: Beth Kelly\nCopyeditor: Holly Forsyth\nProofreader: Charles Roumeliotis\nIndexer: nSight Inc.\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nJune 2020:\n First Edition\nRevision History for the First Edition\n2020-06-17: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492054054 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Practical Natural Language Processing,\nthe cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\n",
      "word_count": 287,
      "char_count": 2074,
      "fonts": [
        "MinionPro-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (10.0pt)",
        "MinionPro-It (8.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 7,
      "text": "This book is dedicated to our respective advisors: Detmar Meurers, Julian McAuley,\nKannan Srinathan, and Luis von Ahn.\n",
      "word_count": 18,
      "char_count": 119,
      "fonts": [
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 8,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 9,
      "text": "Table of Contents\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xv\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xvii\nPart I. \nFoundations\n1. NLP: A Primer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\nNLP in the Real World                                                                                                    5\nNLP Tasks                                                                                                                      6\nWhat Is Language?                                                                                                           8\nBuilding Blocks of Language                                                                                      9\nWhy Is NLP Challenging?                                                                                         12\nMachine Learning, Deep Learning, and NLP: An Overview                                  14\nApproaches to NLP                                                                                                       16\nHeuristics-Based NLP                                                                                                16\nMachine Learning for NLP                                                                                       19\nDeep Learning for NLP                                                                                             22\nWhy Deep Learning Is Not Yet the Silver Bullet for NLP                                    28\nAn NLP Walkthrough: Conversational Agents                                                         31\nWrapping Up                                                                                                                  33\n2. NLP Pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\nData Acquisition                                                                                                            39\nText Extraction and Cleanup                                                                                        42\nHTML Parsing and Cleanup                                                                                    44\nUnicode Normalization                                                                                             45\nSpelling Correction                                                                                                    46\nvii\n",
      "word_count": 368,
      "char_count": 2720,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (12.0pt)",
        "MyriadPro-SemiboldCond (25.2pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 10,
      "text": "System-Specific Error Correction                                                                            47\nPre-Processing                                                                                                                49\nPreliminaries                                                                                                               50\nFrequent Steps                                                                                                            52\nOther Pre-Processing Steps                                                                                      55\nAdvanced Processing                                                                                                 57\nFeature Engineering                                                                                                      60\nClassical NLP/ML Pipeline                                                                                       62\nDL Pipeline                                                                                                                  62\nModeling                                                                                                                         62\nStart with Simple Heuristics                                                                                     63\nBuilding Your Model                                                                                                 64\nBuilding THE Model                                                                                                 65\nEvaluation                                                                                                                       68\nIntrinsic Evaluation                                                                                                    68\nExtrinsic Evaluation                                                                                                   71\nPost-Modeling Phases                                                                                                   72\nDeployment                                                                                                                 72\nMonitoring                                                                                                                  72\nModel Updating                                                                                                          73\nWorking with Other Languages                                                                                   73\nCase Study                                                                                                                       74\nWrapping Up                                                                                                                  76\n3. Text Representation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81\nVector Space Models                                                                                                     84\nBasic Vectorization Approaches                                                                                  85\nOne-Hot Encoding                                                                                                     85\nBag of Words                                                                                                               87\nBag of N-Grams                                                                                                          89\nTF-IDF                                                                                                                         90\nDistributed Representations                                                                                         92\nWord Embeddings                                                                                                     94\nGoing Beyond Words                                                                                              103\nDistributed Representations Beyond Words and Characters                               105\nUniversal Text Representations                                                                                 107\nVisualizing Embeddings                                                                                             108\nHandcrafted Feature Representations                                                                      112\nWrapping Up                                                                                                                113\nviii \n| \nTable of Contents\n",
      "word_count": 187,
      "char_count": 4641,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (12.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 11,
      "text": "Part II. \nEssentials\n4. Text Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  119\nApplications                                                                                                                  121\nA Pipeline for Building Text Classification Systems                                               123\nA Simple Classifier Without the Text Classification Pipeline                            125\nUsing Existing Text Classification APIs                                                                126\nOne Pipeline, Many Classifiers                                                                                  126\nNaive Bayes Classifier                                                                                              127\nLogistic Regression                                                                                                  131\nSupport Vector Machine                                                                                         132\nUsing Neural Embeddings in Text Classification                                                    134\nWord Embeddings                                                                                                   134\nSubword Embeddings and fastText                                                                       136\nDocument Embeddings                                                                                          138\nDeep Learning for Text Classification                                                                      140\nCNNs for Text Classification                                                                                  143\nLSTMs for Text Classification                                                                                144\nText Classification with Large, Pre-Trained Language Models                         145\nInterpreting Text Classification Models                                                                   147\nExplaining Classifier Predictions with Lime                                                        148\nLearning with No or Less Data and Adapting to New Domains                          149\nNo Training Data                                                                                                     149\nLess Training Data: Active Learning and Domain Adaptation                         150\nCase Study: Corporate Ticketing                                                                               152\nPractical Advice                                                                                                           155\nWrapping Up                                                                                                                157\n5. Information Extraction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  161\nIE Applications                                                                                                             162\nIE Tasks                                                                                                                         164\nThe General Pipeline for IE                                                                                        165\nKeyphrase Extraction                                                                                                  166\nImplementing KPE                                                                                                  167\nPractical Advice                                                                                                        168\nNamed Entity Recognition                                                                                         169\nBuilding an NER System                                                                                         171\nNER Using an Existing Library                                                                              175\nNER Using Active Learning                                                                                    176\nPractical Advice                                                                                                        177\nNamed Entity Disambiguation and Linking                                                            178\nNEL Using Azure API                                                                                             179\nTable of Contents \n| \nix\n",
      "word_count": 307,
      "char_count": 4505,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (12.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 12,
      "text": "Relationship Extraction                                                                                              181\nApproaches to RE                                                                                                     182\nRE with the Watson API                                                                                         184\nOther Advanced IE Tasks                                                                                           185\nTemporal Information Extraction                                                                         186\nEvent Extraction                                                                                                       187\nTemplate Filling                                                                                                        189\nCase Study                                                                                                                     190\nWrapping Up                                                                                                                193\n6. Chatbots. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  197\nApplications                                                                                                                  198\nA Simple FAQ Bot                                                                                                    199\nA Taxonomy of Chatbots                                                                                            200\nGoal-Oriented Dialog                                                                                              202\nChitchats                                                                                                                    202\nA Pipeline for Building Dialog Systems                                                                   203\nDialog Systems in Detail                                                                                             204\nPizzaStop Chatbot                                                                                                    206\nDeep Dive into Components of a Dialog System                                                    216\nDialog Act Classification                                                                                         217\nIdentifying Slots                                                                                                        217\nResponse Generation                                                                                               218\nDialog Examples with Code Walkthrough                                                           219\nOther Dialog Pipelines                                                                                                224\nEnd-to-End Approach                                                                                             225\nDeep Reinforcement Learning for Dialogue Generation                                   225\nHuman-in-the-Loop                                                                                                226\nRasa NLU                                                                                                                      227\nA Case Study: Recipe Recommendations                                                                230\nUtilizing Existing Frameworks                                                                               231\nOpen-Ended Generative Chatbots                                                                        233\nWrapping Up                                                                                                                234\n7. Topics in Brief. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  239\nSearch and Information Retrieval                                                                             241\nComponents of a Search Engine                                                                            243\nA Typical Enterprise Search Pipeline                                                                    246\nSetting Up a Search Engine: An Example                                                             247\nA Case Study: Book Store Search                                                                          249\nTopic Modeling                                                                                                            250\nTraining a Topic Model: An Example                                                                   254\nx \n| \nTable of Contents\n",
      "word_count": 303,
      "char_count": 4721,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (12.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 13,
      "text": "What’s Next?                                                                                                             255\nText Summarization                                                                                                    256\nSummarization Use Cases                                                                                       256\nSetting Up a Summarizer: An Example                                                                257\nPractical Advice                                                                                                        258\nRecommender Systems for Textual Data                                                                 260\nCreating a Book Recommender System: An Example                                       261\nPractical Advice                                                                                                        262\nMachine Translation                                                                                                   263\nUsing a Machine Translation API: An Example                                                  264\nPractical Advice                                                                                                        265\nQuestion-Answering Systems                                                                                    266\nDeveloping a Custom Question-Answering System                                          268\nLooking for Deeper Answers                                                                                  268\nWrapping Up                                                                                                                269\nPart III. \nApplied\n8. Social Media. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  275\nApplications                                                                                                                  277\nUnique Challenges                                                                                                       278\nNLP for Social Data                                                                                                     284\nWord Cloud                                                                                                              284\nTokenizer for SMTD                                                                                                286\nTrending Topics                                                                                                        286\nUnderstanding Twitter Sentiment                                                                         288\nPre-Processing SMTD                                                                                             290\nText Representation for SMTD                                                                              294\nCustomer Support on Social Channels                                                                 297\nMemes and Fake News                                                                                                299\nIdentifying Memes                                                                                                   299\nFake News                                                                                                                  300\nWrapping Up                                                                                                                302\n9. E-Commerce and Retail. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  307\nE-Commerce Catalog                                                                                                  308\nReview Analysis                                                                                                        308\nProduct Search                                                                                                         309\nProduct Recommendations                                                                                    309\nSearch in E-Commerce                                                                                               309\nBuilding an E-Commerce Catalog                                                                            312\nTable of Contents \n| \nxi\n",
      "word_count": 268,
      "char_count": 4390,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (12.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 14,
      "text": "Attribute Extraction                                                                                                 312\nProduct Categorization and Taxonomy                                                                317\nProduct Enrichment                                                                                                321\nProduct Deduplication and Matching                                                                  323\nReview Analysis                                                                                                           324\nSentiment Analysis                                                                                                   325\nAspect-Level Sentiment Analysis                                                                           327\nConnecting Overall Ratings to Aspects                                                                329\nUnderstanding Aspects                                                                                           330\nRecommendations for E-Commerce                                                                        332\nA Case Study: Substitutes and Complements                                                      333\nWrapping Up                                                                                                                336\n10. Healthcare, Finance, and Law. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  339\nHealthcare                                                                                                                     339\nHealth and Medical Records                                                                                  341\nPatient Prioritization and Billing                                                                           342\nPharmacovigilance                                                                                                   342\nClinical Decision Support Systems                                                                        342\nHealth Assistants                                                                                                      342\nElectronic Health Records                                                                                      344\nMental Healthcare Monitoring                                                                              353\nMedical Information Extraction and Analysis                                                    355\nFinance and Law                                                                                                          358\nNLP Applications in Finance                                                                                 360\nNLP and the Legal Landscape                                                                                363\nWrapping Up                                                                                                                366\nPart IV. \nBringing It All Together\n11. The End-to-End NLP Process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  371\nRevisiting the NLP Pipeline: Deploying NLP Software                                         372\nAn Example Scenario                                                                                              374\nBuilding and Maintaining a Mature System                                                            376\nFinding Better Features                                                                                           377\nIterating Existing Models                                                                                        378\nCode and Model Reproducibility                                                                          379\nTroubleshooting and Interpretability                                                                    379\nMonitoring                                                                                                                382\nMinimizing Technical Debt                                                                                    383\nAutomating Machine Learning                                                                              384\nxii \n| \nTable of Contents\n",
      "word_count": 263,
      "char_count": 4298,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (12.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 15,
      "text": "The Data Science Process                                                                                           388\nThe KDD Process                                                                                                     388\nMicrosoft Team Data Science Process                                                                   390\nMaking AI Succeed at Your Organization                                                                392\nTeam                                                                                                                           392\nRight Problem and Right Expectations                                                                 393\nData and Timing                                                                                                      394\nA Good Process                                                                                                        395\nOther Aspects                                                                                                           396\nPeeking over the Horizon                                                                                           398\nFinal Words                                                                                                                  401\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  407\nTable of Contents \n| \nxiii\n",
      "word_count": 126,
      "char_count": 1481,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (12.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 16,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 17,
      "text": "Foreword\nThe field of natural language processing (NLP) has undergone a dramatic shift in\nrecent years, both in terms of methodology and in terms of the applications sup‐\nported. Methodological advances have ranged from new ways of representing docu‐\nments to new techniques for language synthesis. With these have come new\napplications ranging from open-ended conversational systems to techniques that use\nnatural language for model interpretability. Finally, these advances have seen NLP\ngain a foothold in related areas, such as computer vision and recommender systems,\nsome of which my lab is working on with support from Amazon, Samsung, and the\nNational Science Foundation.\nAs NLP is expanding into these exciting new areas, so too has the audience of practi‐\ntioners wanting to make use of NLP techniques. In the Data Science course (CSE 258)\nthat I take at the University of California–San Diego, which is often the most attended\nin the computer science department, I see that more and more students are doing\ntheir projects on NLP-based topics. NLP is rapidly becoming a necessary skill\nrequired by engineers, product managers, scientists, students, and enthusiasts wish‐\ning to build applications on top of natural language data. On one hand, new tools and\nlibraries for NLP and machine learning have made natural language modeling more\naccessible than ever. But on the other hand, resources for learning NLP must target\nthis ever-growing and diverse audience. This is especially true for organizations that\nhave recently adopted NLP or for students working with natural language data for the\nfirst time.\nIt has been my pleasure over the last few years to collaborate with Bodhisattwa\nMajumder on exciting new applications in NLP and dialog, so I was thrilled to hear\nabout his efforts (along with Sowmya Vajjala, Anuj Gupta, and Harshit Surana) to\nwrite a book on NLP. They have a wide experience in scaling NLP including at early-\nstage startups, the MIT Media Lab, Microsoft Research, and Google AI.\nI am excited by the end-to-end approach taken in their book, which will make it use‐\nful for a range of scenarios and will help readers to work with the labyrinth of\nxv\n",
      "word_count": 357,
      "char_count": 2182,
      "fonts": [
        "MyriadPro-SemiboldCond (25.2pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 18,
      "text": "possible options while building NLP applications. I am especially thrilled about the\nemphasis on modern NLP applications such as chatbots, as well as the focus on inter‐\ndisciplinary topics such as ecommerce and retail. These topics will be especially use‐\nful for industry leaders and researchers, and are critical subjects that have been given\nonly limited coverage in existing textbooks. This book is ideal both as a first resource\nto discover the field of natural language processing and a guide for seasoned practi‐\ntioners looking to discover the latest developments in this exciting area.\n— Julian McAuley\nProfessor of Computer Science and Engineering\nUniversity of California, San Diego\nxvi \n| \nForeword\n",
      "word_count": 111,
      "char_count": 712,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 19,
      "text": "Preface\nNatural language processing (NLP) is a field at the intersection of computer science,\nartificial intelligence, and linguistics. It concerns building systems that can process\nand understand human language. Since its inception in the 1950s and until very\nrecently, NLP has primarily been the domain of academia and research labs, requir‐\ning long formal education and training. The past decade’s breakthroughs have resul‐\nted in NLP being increasingly used in a range of diverse domains such as retail,\nhealthcare, finance, law, marketing, human resources, and many more. There are a\nrange of driving forces for these developments:\n• Widely available and easy-to-use NLP tools, techniques, and APIs are now all-\npervading in the industry. There has never been a better time to build quick NLP\nsolutions.\n• Development of more interpretable and generalized approaches has improved\nthe baseline performance for even complex NLP tasks, such as open-domain\nconversational tasks and question answering, which were not practically feasible\nbefore.\n• More and more organizations, including Google, Microsoft, and Amazon, are\ninvesting heavily in more interactive consumer products, where language is used\nas the primary medium of communication.\n• Increased availability of useful open source datasets, along with standard bench‐\nmarks on them, has acted as a catalyst in this revolution, as opposed to being\nimpeded by proprietary datasets only available to limited organizations and\nindividuals.\n• The viability of NLP has moved beyond English or other major languages. Data‐\nsets and language-specific models are being created for the less-frequently digi‐\ntized languages too. A fruitful product that came out this effort was a near-\nperfect automatic machine translation tool available to all individuals with a\nsmartphone.\nxvii\n",
      "word_count": 276,
      "char_count": 1832,
      "fonts": [
        "MyriadPro-SemiboldCond (25.2pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 20,
      "text": "With this rapidly expanding usage, a growing proportion of the workforce that builds\nthese NLP systems is grappling with limited experience and theoretical knowledge\nabout the topic. This book addresses this need from an applied perspective. Our book\naims to guide the readers to build, iterate, and scale NLP systems in a business set‐\nting, and to tailor them for various industry verticals.\nWhy We Wrote This Book\nThere are many popular books on NLP available. While some of these serve as text‐\nbooks, focusing on theoretical aspects, some others aim to introduce NLP concepts\nthrough a lot of code examples. There are a few others that focus on specific NLP or\nmachine learning libraries and provide “how-to” guides on solving different NLP\nproblems using the libraries. So, why do we need another book on NLP?\nWe have been building and scaling NLP solutions for over a decade at leading univer‐\nsities and technology companies. While mentoring colleagues and other engineers,\nwe noticed a gap between NLP practice in the industry and the NLP skill sets of new\nengineers and those who are just starting with NLP in particular. We started under‐\nstanding these gaps even better during NLP workshops we were conducting for\nindustry professionals, where we noticed that business and engineering leaders also\nhave these gaps.\nMost online courses and books tackle NLP problems using toy use cases and popular\n(often large, clean, and well-defined) datasets. While this imparts the general meth‐\nods of NLP, we believe it does not provide enough of a foundation to tackle new\nproblems and develop specific solutions in the real world. Commonly encountered\nproblems while building real-world applications, such as data collection, working\nwith noisy data and signals, incremental development of solutions, and issues\ninvolved in deploying the solutions as a part of a larger application, are not dealt with\nby existing resources, to the best of our knowledge. We also saw that best practices to\ndevelop NLP systems were missing in most scenarios. We felt a book was needed to\nbridge this gap, and that is how this book was born!\nThe Philosophy\nWe want to provide a holistic and practical perspective that enables the reader to suc‐\ncessfully build real-world NLP solutions embedded in larger product setups. Thus,\nmost chapters are accompanied by code walkthroughs in the associated Git reposi‐\ntory. The book is also supplemented with extensive references for readers who want\nto delve deeper. Throughout the book, we start with a simple solution and incremen‐\ntally build more complex solutions by taking a minimum viable product (MVP)\napproach, as commonly found in industry practice. We also give tips based on our\nexperience and learnings. Where possible, each chapter is accompanied by a\nxviii \n| \nPreface\n",
      "word_count": 457,
      "char_count": 2810,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 21,
      "text": "discussion on the state-of-the-art in that topic. Most chapters conclude with a case\nstudy of real-world use cases.\nConsider the task of building a chatbot or text classification system at your organiza‐\ntion. In the beginning there may be little or no data to work with. At this point, a\nbasic solution using rule-based systems or traditional machine learning will be apt.\nAs you accumulate more data, more sophisticated NLP techniques (which are often\ndata intensive) can be used, including deep learning. At each step of this journey\nthere are dozens of alternative approaches one can take. This book will help you navi‐\ngate this maze of options.\nScope\nThis book provides a comprehensive view on building real-world NLP applications.\nWe will cover the complete lifecycle of a typical NLP project—from data collection to\ndeploying and monitoring the model. Some of these steps are applicable to any ML\npipeline, while some are very specific to NLP. We also introduce task-specific case\nstudies and domain-specific guides to build an NLP system from scratch. We specifi‐\ncally cover a gamut of tasks ranging from text classification to question answering to\ninformation extraction and dialog systems. Similarly, we provide recipes to apply\nthese tasks in domains ranging from e-commerce to healthcare, social media, and\nfinance. Owing to the depth and breadth of the topics and scenarios we cover, we will\nnot go step by step explaining the code and all the concepts. For details of the imple‐\nmentation, we have provided detailed source code notebooks. The code snippets in\nthis book cover the core logic and often skip introductory steps like setting up a\nlibrary or importing a package as they are covered in the associated notebooks. To\ncover the wide range of concepts we have given more than 450 extensive references to\ndelve deeper into these topics. This book will be a day-to-day cookbook giving you a\npragmatic view while building any NLP system, as well as be a stepping stone to\nbroaden the application of NLP into your domain.\nWho Should Read This Book\nThis book is for anyone involved in building NLP applications for real-world use\ncases. This includes software developers and testers, machine learning engineers, data\nengineers, MLOps engineers, NLP engineers, data scientists, product managers, peo‐\nple managers, VPs, CXOs, and startup founders. This also includes those involved in\ndata creation and annotation processes—in short anyone and everyone who is\ninvolved in any way in building NLP systems in industry. While not all chapters are\nuseful for people with all roles, we tried to give lucid explanations using less technical\njargon and more intuitive understanding wherever possible. We believe there is\nsomething in every chapter for all potential readers interested in getting a holistic\nperspective about building NLP applications.\nPreface \n| \nxix\n",
      "word_count": 463,
      "char_count": 2879,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 22,
      "text": "Some chapters or sections can be understood without much coding experience and\ncode bits can be skipped as needed. For example, the first two sections in Chapter 1\nand Chapter 9, or the sections “The Data Science Process” and “Making AI succeed in\nyour organization” in Chapter 11 can be understood without any coding experience\nby all groups of readers. As you progress through the book, you will find more such\nsections in all chapters. However, to extract maximum benefit from this book, its\nnotebooks, and references, we expect the reader to have the following background:\n• Intermediate proficiency in Python programming. For example, understanding\nPython features such as list comprehension, writing functions and classes, and\nusing existing libraries.\n• Familiarity with various aspects of the software development life cycle (SDLC)\nsuch as design, development, testing, DevOps, etc.\n• Basics of machine learning, including familiarity with commonly used machine\nlearning algorithms such as logistic regression and decision trees and the ability\nto use them in Python with existing libraries such as scikit-learn.\n• Basic knowledge of NLP is useful but not mandatory. Having an idea of tasks\nsuch as text classification and named entity recognition is also helpful.\nWhat You Will Learn\nOur primary audience is comprised of engineers and scientists involved in building\nreal-world NLP systems for different verticals. Some of the common job titles are:\nSoftware Engineer, NLP Engineer, ML Engineer, and Data Scientist. The book may\nalso be helpful for product managers and engineering leaders. However, it may not be\nas helpful for those pursuing cutting-edge research in NLP because we do not cover\nin-depth theoretical and technical details related to NLP concepts. With this book,\nyou will:\n• Understand the wide spectrum of problem statements, tasks, and solution\napproaches within NLP.\n• Gain experience in implementing and evaluating different NLP applications and\napplying machine learning and deep learning methods for this process.\n• Fine-tune an NLP solution based on the business problem and industry vertical.\n• Evaluate various algorithms and approaches for the given task, dataset, and stage\nof the NLP product.\n• Plan the lifecycle of the NLP product and produce software solutions following\nbest practices around release, deployment, and DevOps for NLP systems.\nxx \n| \nPreface\n",
      "word_count": 372,
      "char_count": 2399,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 23,
      "text": "• Understand best practices, opportunities, and the roadmap for NLP from a busi‐\nness and product leader’s perspective.\nYou will also learn to adapt your solutions for different industry verticals like health‐\ncare, finance, and retail. Moreover, you will learn about specific caveats you will\nencounter in each.\nStructure of the Book\nThe book is divided into four sections. Figure P-1 illustrates the chapter organization.\nIsolated chapters that are not directly connected to other chapters are easiest to skip\nwhile moving forward.\nFigure P-1. How the book’s sections are structured\nPart I, Foundations acts as a bedrock for the rest of the book, by giving an overview of\nNLP (Chapter 1), discussing typical data processing and modeling pipelines used in\nbuilding NLP systems (Chapter 2), and introducing different ways of representing\ntextual data in NLP (Chapter 3).\nPreface \n| \nxxi\n",
      "word_count": 140,
      "char_count": 887,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1213,
          "height": 1197,
          "ext": "png",
          "size_bytes": 71384
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 24,
      "text": "Part II, Essentials, focuses on the most common NLP applications, with an emphasis\non real-world use cases. Where possible, we show multiple solutions to the problem\nat hand to demonstrate how to choose among different options. Some applications\ninclude text classification (Chapter 4), information extraction (Chapter 5), and the\nbuilding of chat bots (Chapter 6). We also introduce other applications, such as\nsearch, topic modeling, text summarization, and machine translation, along with a\ndiscussion about practical use cases (Chapter 7).\nPart III, Applied (Chapters 8–10) specifically focuses on three industry verticals where\nNLP is heavily used, with a detailed discussion on those domains’ specific problems\nand how NLP is useful in addressing them.\nFinally, Part IV (Chapter 11) brings all the learning together by dealing with the\nissues involved in end-to-end deployment of NLP systems in practice.\nHow to Read This Book\nHow one reads the book depends on their role and objective. For a data scientist or\nan engineer delving into NLP, we recommend reading Chapters 1–6 and then focus‐\ning on the particular domain or subproblem of interest. For someone in a leadership\nrole, we recommend focusing on Chapters 1, 2, and 11. They might want to give extra\nfocus to case studies for Chapters 3–7, which provide more ideas on the process of\nbuilding NLP applications from scratch. A product leader might want to delve deep\ninto the references provided for relevant chapters, as well as Chapter 11.\nNLP applications for various domains can be different from the general problems\ncovered in Chapters 3–7. That is why we have focused more on certain domains such\nas e-commerce, social media, healthcare, finance, and law. If your interest or work\ntakes you to these areas, you can dig deeper into those chapters and corresponding\nreferences.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program ele‐\nments such as variable or function names, databases, data types, environment\nvariables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nxxii \n| \nPreface\n",
      "word_count": 371,
      "char_count": 2342,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "UbuntuMono-Bold (10.0pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 25,
      "text": "Constant width italic\nShows text that should be replaced with user-supplied values or by values deter‐\nmined by context.\nThis element signifies a tip or suggestion.\nThis element signifies a general note.\nThis element indicates a warning or caution.\nUsing Code Examples\nSupplemental material (code examples, exercises, etc.) is available for download at\nhttps://oreil.ly/PracticalNLP.\nIf you have a technical question or a problem using the code examples, please send\nemail to bookquestions@oreilly.com.\nThis book is here to help you get your job done. In general, if example code is offered\nwith this book, you may use it in your programs and documentation. You do not\nneed to contact us for permission unless you’re reproducing a significant portion of\nthe code. For example, writing a program that uses several chunks of code from this\nbook does not require permission. Selling or distributing examples from O’Reilly\nbooks does require permission. Answering a question by citing this book and quoting\nexample code does not require permission. Incorporating a significant amount of\nexample code from this book into your product’s documentation does require\npermission.\nWe appreciate, but generally do not require, attribution. An attribution usually\nincludes the title, author, publisher, and ISBN. For example: “Practical Natural Lan‐\nguage Processing by Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, and Har‐\nshit Surana (O’Reilly). Copyright 2020 Anuj Gupta, Bodhisattwa Prasad Majumder,\nSowmya Vajjala, and Harshit Surana, 978-1-492-05405-4.”\nPreface \n| \nxxiii\n",
      "word_count": 233,
      "char_count": 1570,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (9.6pt)",
        "UbuntuMono-Italic (10.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        },
        {
          "index": 1,
          "width": 394,
          "height": 514,
          "ext": "png",
          "size_bytes": 7986
        },
        {
          "index": 2,
          "width": 503,
          "height": 479,
          "ext": "png",
          "size_bytes": 10854
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 26,
      "text": "If you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com.\nO’Reilly Online Learning\nFor more than 40 years, O’Reilly Media has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nOur unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at https://oreil.ly/PNLP.\nEmail bookquestions@oreilly.com to comment or ask technical questions about this\nbook.\nFor news and information about our books and courses, visit http://oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://youtube.com/oreillymedia\nFurther Information\nIn the GitHub repo you will find notebooks explaining various concepts covered in\nthe book. The notebooks have been organized by chapter. We also provide additional\nnotebooks, not necessarily covered in the book.\nxxiv \n| \nPreface\n",
      "word_count": 242,
      "char_count": 1716,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1456,
          "height": 488,
          "ext": "png",
          "size_bytes": 16533
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 27,
      "text": "Book website: http://www.practicalnlp.ai\nThe world of NLP is always evolving. To stay updated on how concept mentioned in\nthe book fit into the broader context one, two, and five years from now, follow our\nblog. We keep it updated with relevant writeups and articles, and tag every post with\nthe book’s corresponding chapter title.\nContact the authors:\nEmail: authors@practicalnlp.ai\nLinkedin: https://linkedin.com/company/practical-nlp\nTwitter: https://twitter.com/PracticalNLProc\nFacebook: https://oreil.ly/facebookPNLP\nAcknowledgments\nA book like this is a compendium of knowledge; hence, it cannot exist in isolation.\nWhile writing this book, we drew a lot of inspiration and information from several\nbooks, research papers, software projects and numerous other resources on the inter‐\nnet. We thank the NLP and Machine Learning community for all their efforts. We\nhave merely stood on the shoulders on these giants. We also thank various people\nwho attended some of the authors’ talks and workshops and participated in discus‐\nsions that lead to the idea of writing this book and shaping its premise. This book is\nthe result of a long collaborative effort and several people supported us in different\nways in our endeavor.\nWe thank the O’Reilly reviewers Will Scott, Darren Cook, Ramya Balasubramaniam,\nPriyanka Raghavan, and Siddharth Narayanan for their meticulous, invaluable, and\ndetailed comments which helped us improve earlier drafts. Detailed feedback from\nSiddharth Sharma, Sumod Mohan, Vinayak Hegde, Aasish Pappu, Taranjeet Singh,\nKartikay Bagla, and Varun Purushotham were instrumental in improving the quality\nof the content.\nWe are also very thankful to Rui Shu, Shreyans Dhankhar, Jitin Kapila, Kumarjit\nPathak, Ernest Kirubakaran Selvaraj, Robin Singh, Ayush Datta, Vishal Gupta, and\nNachiketh for helping us prepare the early versions of code notebooks. We would\nespecially like to thank Varun Purushotham, who spent several weeks reading and\nrereading our drafts and preparing and cross-checking the code notebooks. This\nbook would not be the same without his contribution.\nWe would like to thank the O’Reilly Media team, without whom this would not have\nbeen possible: Jonathan Hassell, for giving us this opportunity; Melissa Potter, for\nregularly following up with us throughout this journey and patiently answering all\nPreface \n| \nxxv\n",
      "word_count": 350,
      "char_count": 2362,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 28,
      "text": "our questions! Beth Kelly and Holly Forsyth, for all the help and support in shaping it\ninto a book from the chapter drafts.\nFinally, the following are personal thank you notes by each author:\nSowmya: My first and biggest thank you goes to my daughter, Sahasra Malathi, whose\nbirth and first year of life coincided with the writing of this book. It is not easy to\nwrite a book, and not easy at all to write it with a newborn. And yet, here we are.\nThank you, Sahasra! My mom, Geethamani, and my husband, Sriram, supported my\nwriting by taking up baby care and household duties at different phases of writing.\nMy friends, Purnima and Visala, were always available to listen to my excited updates\nas well as rants about the book. My boss, Cyril Goutte, encouraged me and checked\non my writing progress throughout. Discussions with my former colleagues, Chris\nCardinal and Eric Le Fort, taught me a lot about developing NLP solutions for indus‐\ntry problems, without which I perhaps would never have thought of being a part of\nthis kind of book. I thank all of them for their support.\nBodhisattwa: I would like to take this opportunity to thank my parents, their unques‐\ntionable sacrifice, and the constant encouragement that made me the person I am\ntoday. Their efforts have instilled in me the love and dedication to learning in my life.\nI am eternally grateful to my advisors, Prof. Animesh Mukherjee and Pawan Goyal,\nwho introduced me to this world of NLP; and Prof. Julian McAuley, who is nothing\nless than fundamental to my technical, academic, and personal development in my\nPhD career. The courses taken by my other professors—Taylor Berg-Kirkpatrick,\nLawrence Saul, David Kriegman, Debasis Sengupta, Sudeshna Sarkar, and Sourav Sen\nGupta—have significantly shaped my learning on the subject. In the early days of the\nbook, my colleagues from Walmart Labs—especially Subhasish Misra, Arunita Das,\nSmaranya Dey, Sumanth Prabhu, and Rajesh Bhat—gave me the motivation for this\ncrazy idea. To my mentors at Google AI, Microsoft Research, Amazon Alexa, and my\nlabmates from the UCSD NLP Group, thank you for being supportive and helpful in\nthis entire journey. Also, my friends Sanchaita Hazra, Sujoy Paul, and Digbalay Bose,\nwho stood by me through thick and thin in this mammoth project, deserve a special\nmention. At last, none of this would have been possible without my coauthors, who\nbelieved in this project and stayed together till the last bit of it!\nAnuj: First and foremost, I would like to express my sincere gratitude to my wife,\nAnu, and my son, Nirvaan. Without their unwavering support, I would not have been\nable to devote the last three years to this endeavor. Thank you so much! I would also\nlike to thank my parents and family for their encouragement. A big shout out goes to\nSaurabh Arora, for introducing me to the world of NLP. Many thanks to my friends,\nthe late Vivek Jain and Mayur Hemani; they have always encouraged me to keep\ngoing, especially in hard times. I would also like to thank all of the amazing people\ninvolved in machine learning communities in Bangalore; especially: Sumod Mohan,\nVijay Gabale, Nishant Sinha, Ashwin Kumar, Mukundhan Srinivasan, Zainab Bawa,\nxxvi \n| \nPreface\n",
      "word_count": 546,
      "char_count": 3218,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 29,
      "text": "and Naresh Jain for all the wonderful and thought-provoking discussions. I would\nlike to thank my colleagues—former and present—at CSTAR, Airwoot, FreshWorks,\nHuawei Research, Intuit, and Vahan, Inc., for everything they taught me. To my pro‐\nfessors, Kannan Srinathan, P.R.K Rao, and B. Yegnanarayana, whose teachings have\nhad a profound impact on me.\nHarshit: I want to thank my parents, who have supported and encouraged me to pur‐\nsue every crazy idea I have had. I cannot thank my dear friends Preeti Shrimal and\nDev Chandan enough. They have been with me throughout the book’s entire journey.\nTo my cofounders, Abhimanyu Vyas and Aviral Mathur, thank you for adjusting our\nstartup endeavor to help me complete the book. I want to thank all my former collea‐\ngues at Quipio and Notify.io who helped crystalize my thinking, especially Zubin\nWadia, Amit Kumar, and Naveen Koorakula. None of this would have been possible\nwithout my professors and everything they taught me—thank you, Prof. Luis von\nAhn, Anil Kumar Singh, Alan W Black, William Cohen, Lori Levin, and Carlos\nGuestrin. I also want to acknowledge Kaustuv DeBiswas, Siddharth Narayanan, Sid‐\ndharth Sharma, Alok Parlikar, Nathan Schneider, Aasish Pappu, Manish Jawa, Sumit\nPandey, and Mohit Ranka, who have supported me at various junctures of this\njourney.\nPreface \n| \nxxvii\n",
      "word_count": 215,
      "char_count": 1342,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 30,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 31,
      "text": "PART I\nFoundations\n",
      "word_count": 3,
      "char_count": 19,
      "fonts": [
        "MyriadPro-SemiboldCond (28.4pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 32,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 33,
      "text": "CHAPTER 1\nNLP: A Primer\nA language is not just words. It’s a culture, a tradition,\na unification of a community,\na whole history that creates what a community is.\nIt’s all embodied in a language.\n—Noam Chomsky\nImagine a hypothetical person, John Doe. He’s the CTO of a fast-growing technology\nstartup. On a busy day, John wakes up and has this conversation with his digital\nassistant:\nJohn: “How is the weather today?”\nDigital assistant: “It is 37 degrees centigrade outside with no rain today.”\nJohn: “What does my schedule look like?”\nDigital assistant: “You have a strategy meeting at 4 p.m. and an all-hands at 5:30 p.m.\nBased on today’s traffic situation, it is recommended you leave for the office by\n8:15 a.m.”\nWhile he’s getting dressed, John probes the assistant on his fashion choices:\nJohn: “What should I wear today?”\nDigital assistant: “White seems like a good choice.”\nYou might have used smart assistants such as Amazon Alexa, Google Home, or Apple\nSiri to do similar things. We talk to these assistants not in a programming language,\nbut in our natural language—the language we all communicate in. This natural lan‐\nguage has been the primary medium of communication between humans since time\nimmemorial. But computers can only process data in binary, i.e., 0s and 1s. While we\ncan represent language data in binary, how do we make machines understand the\n3\n",
      "word_count": 233,
      "char_count": 1374,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (9.3pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 34,
      "text": "language? This is where natural language processing (NLP) comes in. It is an area of\ncomputer science that deals with methods to analyze, model, and understand human\nlanguage. Every intelligent application involving human language has some NLP\nbehind it. In this book, we’ll explain what NLP is as well as how to use NLP to build\nand scale intelligent applications. Due to the open-ended nature of NLP problems,\nthere are dozens of alternative approaches one can take to solve a given problem. This\nbook will help you navigate this maze of options and suggests how to choose the best\noption based on your problem.\nThis chapter aims to give a quick primer of what NLP is before we start delving\ndeeper into how to implement NLP-based solutions for different application scenar‐\nios. We’ll start with an overview of numerous applications of NLP in real-world sce‐\nnarios, then cover the various tasks that form the basis of building different NLP\napplications. This will be followed by an understanding of language from an NLP\nperspective and of why NLP is difficult. After that, we’ll give an overview of heuris‐\ntics, machine learning, and deep learning, then introduce a few commonly used algo‐\nrithms in NLP. This will be followed by a walkthrough of an NLP application. Finally,\nwe’ll conclude the chapter with an overview of the rest of the topics in the book.\nFigure 1-1 shows a preview of the organization of the chapters in terms of various\nNLP tasks and applications.\nFigure 1-1. NLP tasks and applications\nLet’s start by taking a look at some popular applications you use in everyday life that\nhave some form of NLP as a major component.\n4 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 288,
      "char_count": 1678,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1431,
          "height": 730,
          "ext": "png",
          "size_bytes": 109911
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 35,
      "text": "NLP in the Real World\nNLP is an important component in a wide range of software applications that we use\nin our daily lives. In this section, we’ll introduce some key applications and also take a\nlook at some common tasks that you’ll see across different NLP applications. This\nsection reinforces the applications we showed you in Figure 1-1, which you’ll see in\nmore detail throughout the book.\nCore applications:\n• Email platforms, such as Gmail, Outlook, etc., use NLP extensively to provide a\nrange of product features, such as spam classification, priority inbox, calendar\nevent extraction, auto-complete, etc. We’ll discuss some of these in detail in\nChapters 4 and 5.\n• Voice-based assistants, such as Apple Siri, Google Assistant, Microsoft Cortana,\nand Amazon Alexa rely on a range of NLP techniques to interact with the user,\nunderstand user commands, and respond accordingly. We’ll cover key aspects of\nsuch systems in Chapter 6, where we discuss chatbots.\n• Modern search engines, such as Google and Bing, which are the cornerstone of\ntoday’s internet, use NLP heavily for various subtasks, such as query understand‐\ning, query expansion, question answering, information retrieval, and ranking\nand grouping of the results, to name a few. We’ll discuss some of these subtasks\nin Chapter 7.\n• Machine translation services, such as Google Translate, Bing Microsoft Transla‐\ntor, and Amazon Translate are increasingly used in today’s world to solve a wide\nrange of scenarios and business use cases. These services are direct applications\nof NLP. We’ll touch on machine translation in Chapter 7.\nOther applications:\n• Organizations across verticals analyze their social media feeds to build a better\nand deeper understanding of the voice of their customers. We’ll cover this in\nChapter 8.\n• NLP is widely used to solve diverse sets of use cases on e-commerce platforms\nlike Amazon. These vary from extracting relevant information from product\ndescriptions to understanding user reviews. Chapter 9 covers these in detail.\n• Advances in NLP are being applied to solve use cases in domains such as health‐\ncare, finance, and law. Chapter 10 addresses these.\n• Companies such as Arria [1] are working to use NLP techniques to automatically\ngenerate reports for various domains, from weather forecasting to financial\nservices.\nNLP in the Real World \n| \n5\n",
      "word_count": 379,
      "char_count": 2357,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 36,
      "text": "• NLP forms the backbone of spelling- and grammar-correction tools, such as \nGrammarly and spell check in Microsoft Word and Google Docs.\n• Jeopardy! is a popular quiz show on TV. In the show, contestants are presented\nwith clues in the form of answers, and the contestants must phrase their respon‐\nses in the form of questions. IBM built the Watson AI to compete with the show’s\ntop players. Watson won the first prize with a million dollars, more than the\nworld champions. Watson AI was built using NLP techniques and is one of the\nexamples of NLP bots winning a world competition.\n• NLP is used in a range of learning and assessment tools and technologies, such as\nautomated scoring in exams like the Graduate Record Examination (GRE), plagi‐\narism detection (e.g., Turnitin), intelligent tutoring systems, and language learn‐\ning apps like Duolingo.\n• NLP is used to build large knowledge bases, such as the Google Knowledge\nGraph, which are useful in a range of applications like search and question\nanswering.\nThis list is by no means exhaustive. NLP is increasingly being used across several\nother applications, and newer applications of NLP are coming up as we speak. Our\nmain focus is to introduce you to the ideas behind building these applications. We do\nso by discussing different kinds of NLP problems and how to solve them. To get a\nperspective on what you are about to learn in this book, and to appreciate the nuan‐\nces that go into building these NLP applications, let’s take a look at some key NLP\ntasks that form the bedrock of many NLP applications and industry use cases.\nNLP Tasks\nThere is a collection of fundamental tasks that appear frequently across various NLP\nprojects. Owing to their repetitive and fundamental nature, these tasks have been\nstudied extensively. Having a good grip on them will make you ready to build various\nNLP applications across verticals. (We also saw some of these tasks earlier in\nFigure 1-1.) Let’s briefly introduce them:\nLanguage modeling\nThis is the task of predicting what the next word in a sentence will be based on\nthe history of previous words. The goal of this task is to learn the probability of a\nsequence of words appearing in a given language. Language modeling is useful\nfor building solutions for a wide variety of problems, such as speech recognition,\noptical character recognition, handwriting recognition, machine translation, and\nspelling correction.\n6 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 415,
      "char_count": 2456,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 37,
      "text": "Text classification\nThis is the task of bucketing the text into a known set of categories based on its\ncontent. Text classification is by far the most popular task in NLP and is used in a\nvariety of tools, from email spam identification to sentiment analysis.\nInformation extraction\nAs the name indicates, this is the task of extracting relevant information from\ntext, such as calendar events from emails or the names of people mentioned in a\nsocial media post.\nInformation retrieval\nThis is the task of finding documents relevant to a user query from a large collec‐\ntion. Applications like Google Search are well-known use cases of information\nretrieval.\nConversational agent\nThis is the task of building dialogue systems that can converse in human lan‐\nguages. Alexa, Siri, etc., are some common applications of this task.\nText summarization\nThis task aims to create short summaries of longer documents while retaining the\ncore content and preserving the overall meaning of the text.\nQuestion answering\nThis is the task of building a system that can automatically answer questions\nposed in natural language.\nMachine translation\nThis is the task of converting a piece of text from one language to another. Tools\nlike Google Translate are common applications of this task.\nTopic modeling\nThis is the task of uncovering the topical structure of a large collection of docu‐\nments. Topic modeling is a common text-mining tool and is used in a wide range\nof domains, from literature to bioinformatics.\nFigure 1-2 shows a depiction of these tasks based on their relative difficulty in terms\nof developing comprehensive solutions.\nNLP in the Real World \n| \n7\n",
      "word_count": 271,
      "char_count": 1654,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 38,
      "text": "Figure 1-2. NLP tasks organized according to their relative difficulty\nIn the rest of the chapters in this book, we’ll see these tasks’ challenges and learn how\nto develop solutions that work for certain use cases (even the hard tasks shown in the\nfigure). To get there, it is useful to have an understanding of the nature of human lan‐\nguage and the challenges in automating language processing. The next two sections\nprovide a basic overview.\nWhat Is Language?\nLanguage is a structured system of communication that involves complex combina‐\ntions of its constituent components, such as characters, words, sentences, etc. Lin‐\nguistics is the systematic study of language. In order to study NLP, it is important to\nunderstand some concepts from linguistics about how language is structured. In this\nsection, we’ll introduce them and cover how they relate to some of the NLP tasks we\nlisted earlier.\nWe can think of human language as composed of four major building blocks: pho‐\nnemes, morphemes and lexemes, syntax, and context. NLP applications need knowl‐\nedge of different levels of these building blocks, starting from the basic sounds\nof language (phonemes) to texts with some meaningful expressions (context).\n8 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 203,
      "char_count": 1248,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1346,
          "height": 1078,
          "ext": "png",
          "size_bytes": 50275
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 39,
      "text": "Figure 1-3 shows these building blocks of language, what they encompass, and a few\nNLP applications we introduced earlier that require this knowledge. Some of the\nterms listed here that were not introduced earlier in this chapter (e.g., parsing, word\nembeddings, etc.) will be introduced later in these first three chapters.\nFigure 1-3. Building blocks of language and their applications\nBuilding Blocks of Language\nLet’s first introduce what these blocks of language are to give context for the chal‐\nlenges involved in NLP.\nPhonemes\nPhonemes are the smallest units of sound in a language. They may not have any\nmeaning by themselves but can induce meanings when uttered in combination with\nother phonemes. For example, standard English has 44 phonemes, which are either\nsingle letters or a combination of letters [2]. Figure 1-4 shows these phonemes along\nwith sample words. Phonemes are particularly important in applications involving\nspeech understanding, such as speech recognition, speech-to-text transcription, and\ntext-to-speech conversion.\nWhat Is Language? \n| \n9\n",
      "word_count": 163,
      "char_count": 1074,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 968,
          "height": 726,
          "ext": "png",
          "size_bytes": 50386
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 40,
      "text": "Figure 1-4. Phonemes and examples\nMorphemes and lexemes\nA morpheme is the smallest unit of language that has a meaning. It is formed by a\ncombination of phonemes. Not all morphemes are words, but all prefixes and suffixes\nare morphemes. For example, in the word “multimedia,” “multi-” is not a word but a\nprefix that changes the meaning when put together with “media.” “Multi-” is a mor‐\npheme. Figure 1-5 illustrates some words and their morphemes. For words like “cats”\nand “unbreakable,” their morphemes are just constituents of the full word, whereas\nfor words like “tumbling” and “unreliability,” there is some variation when breaking\nthe words down into their morphemes.\nFigure 1-5. Morpheme examples\nLexemes are the structural variations of morphemes related to one another by mean‐\ning. For example, “run” and “running” belong to the same lexeme form. Morphologi‐\ncal analysis, which analyzes the structure of words by studying its morphemes and\nlexemes, is a foundational block for many NLP tasks, such as tokenization, stemming,\n10 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 173,
      "char_count": 1071,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1302,
          "height": 843,
          "ext": "png",
          "size_bytes": 62957
        },
        {
          "index": 1,
          "width": 598,
          "height": 264,
          "ext": "png",
          "size_bytes": 9160
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 41,
      "text": "learning word embeddings, and part-of-speech tagging, which we’ll introduce in the\nnext chapter.\nSyntax\nSyntax is a set of rules to construct grammatically correct sentences out of words and\nphrases in a language. Syntactic structure in linguistics is represented in many differ‐\nent ways. A common approach to representing sentences is a parse tree. Figure 1-6\nshows an example parse tree for two English sentences.\nFigure 1-6. Syntactic structure of two syntactically similar sentences\nThis has a hierarchical structure of language, with words at the lowest level, followed\nby part-of-speech tags, followed by phrases, and ending with a sentence at the highest\nlevel. In Figure 1-6, both sentences have a similar structure and hence a similar syn‐\ntactic parse tree. In this representation, N stands for noun, V for verb, and P for prep‐\nosition. Noun phrase is denoted by NP and verb phrase by VP. The two noun phrases\nare “The girl” and “The boat,” while the two verb phrases are “laughed at the monkey”\nand “sailed up the river.” The syntactic structure is guided by a set of grammar rules\nfor the language (e.g., the sentence comprises an NP and a VP), and this in turn\nguides some of the fundamental tasks of language processing, such as parsing. Pars‐\ning is the NLP task of constructing such trees automatically. Entity extraction and\nrelation extraction are some of the NLP tasks that build on this knowledge of parsing,\nwhich we’ll discuss in more detail in Chapter 5. Note that the parse structure\ndescribed above is specific to English. The syntax of one language can be very differ‐\nent from that of another language, and the language-processing approaches needed\nfor that language will change accordingly.\nWhat Is Language? \n| \n11\n",
      "word_count": 290,
      "char_count": 1746,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 739,
          "height": 514,
          "ext": "png",
          "size_bytes": 22899
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 42,
      "text": "Context\nContext is how various parts in a language come together to convey a particular\nmeaning. Context includes long-term references, world knowledge, and common\nsense along with the literal meaning of words and phrases. The meaning of a sentence\ncan change based on the context, as words and phrases can sometimes have multiple\nmeanings. Generally, context is composed from semantics and pragmatics. Semantics\nis the direct meaning of the words and sentences without external context. Pragmat‐\nics adds world knowledge and external context of the conversation to enable us to\ninfer implied meaning. Complex NLP tasks such as sarcasm detection, summariza‐\ntion, and topic modeling are some of tasks that use context heavily.\nLinguistics is the study of language and hence is a vast area in itself, and we only\nintroduced some basic ideas to illustrate the role of linguistic knowledge in NLP. Dif‐\nferent tasks in NLP require varying degrees of knowledge about these building blocks\nof language. An interested reader can refer to the books written by Emily Bender [3,\n4] on the linguistic fundamentals for NLP for further study. Now that we have some\nidea of what the building blocks of language are, let’s see why language can be hard\nfor computers to understand and what makes NLP challenging.\nWhy Is NLP Challenging?\nWhat makes NLP a challenging problem domain? The ambiguity and creativity of\nhuman language are just two of the characteristics that make NLP a demanding area\nto work in. This section explores each characteristic in more detail, starting with\nambiguity of language.\nAmbiguity\nAmbiguity means uncertainty of meaning. Most human languages are inherently\nambiguous. Consider the following sentence: “I made her duck.” This sentence has\nmultiple meanings. The first one is: I cooked a duck for her. The second meaning is: I\nmade her bend down to avoid an object. (There are other possible meanings, too;\nwe’ll leave them for the reader to think of.) Here, the ambiguity comes from the use\nof the word “made.” Which of the two meanings applies depends on the context in\nwhich the sentence appears. If the sentence appears in a story about a mother and a\nchild, then the first meaning will probably apply. But if the sentence appears in a book\nabout sports, then the second meaning will likely apply. The example we saw is a\ndirect sentence.\nWhen it comes to figurative language—i.e., idioms—the ambiguity only increases.\nFor example, “He is as good as John Doe.” Try to answer, “How good is he?” The\nanswer depends on how good John Doe is. Figure 1-7 shows some examples illustrat‐\ning ambiguity in language.\n12 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 444,
      "char_count": 2658,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 43,
      "text": "Figure 1-7. Examples of ambiguity in language from the Winograd Schema Challenge\nThe examples come from the Winograd Schema Challenge [5], named after Professor\nTerry Winograd of Stanford University. This schema has pairs of sentences that differ\nby only a few words, but the meaning of the sentences is often flipped because of this\nminor change. These examples are easily disambiguated by a human but are not solv‐\nable using most NLP techniques. Consider the pairs of sentences in the figure and the\nquestions associated with them. With some thought, how the answer changes should\nbe apparent based on a single word variation. As another experiment, consider taking\nan off-the-shelf NLP system like Google Translate and try various examples to see\nhow such ambiguities affect (or don’t affect) the output of the system.\nCommon knowledge\nA key aspect of any human language is “common knowledge.” It is the set of all facts\nthat most humans are aware of. In any conversation, it is assumed that these facts are\nknown, hence they’re not explicitly mentioned, but they do have a bearing on the\nmeaning of the sentence. For example, consider two sentences: “man bit dog” and\n“dog bit man.” We all know that the first sentence is unlikely to happen, while the sec‐\nond one is very possible. Why do we say so? Because we all “know” that it is very\nunlikely that a human will bite a dog. Further, dogs are known to bite humans. This\nknowledge is required for us to say that the first sentence is unlikely to happen while\nthe second one is possible. Note that this common knowledge was not mentioned in\nWhat Is Language? \n| \n13\n",
      "word_count": 281,
      "char_count": 1622,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 960,
          "height": 982,
          "ext": "png",
          "size_bytes": 60029
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 44,
      "text": "either sentence. Humans use common knowledge all the time to understand and\nprocess any language. In the above example, the two sentences are syntactically very\nsimilar, but a computer would find it very difficult to differentiate between the two, as\nit lacks the common knowledge humans have. One of the key challenges in NLP is\nhow to encode all the things that are common knowledge to humans in a computa‐\ntional model.\nCreativity\nLanguage is not just rule driven; there is also a creative aspect to it. Various styles,\ndialects, genres, and variations are used in any language. Poems are a great example\nof creativity in language. Making machines understand creativity is a hard problem\nnot just in NLP, but in AI in general.\nDiversity across languages\nFor most languages in the world, there is no direct mapping between the vocabularies\nof any two languages. This makes porting an NLP solution from one language to\nanother hard. A solution that works for one language might not work at all for\nanother language. This means that one either builds a solution that is language agnos‐\ntic or that one needs to build separate solutions for each language. While the first one\nis conceptually very hard, the other is laborious and time intensive.\nAll these issues make NLP a challenging—yet rewarding—domain to work in. Before\nlooking into how some of these challenges are tackled in NLP, we should know the\ncommon approaches to solving NLP problems. Let’s start with an overview of how\nmachine learning and deep learning are connected to NLP before delving deeper into\ndifferent approaches to NLP.\nMachine Learning, Deep Learning, and NLP: An Overview\nLoosely speaking, artificial intelligence (AI) is a branch of computer science that aims\nto build systems that can perform tasks that require human intelligence. This is some‐\ntimes also called “machine intelligence.” The foundations of AI were laid in the 1950s\nat a workshop organized at Dartmouth College [6]. Initial AI was largely built out of\nlogic-, heuristics-, and rule-based systems. Machine learning (ML) is a branch of AI\nthat deals with the development of algorithms that can learn to perform tasks auto‐\nmatically based on a large number of examples, without requiring handcrafted rules.\nDeep learning (DL) refers to the branch of machine learning that is based on artificial\nneural network architectures. ML, DL, and NLP are all subfields within AI, and the\nrelationship between them is depicted in Figure 1-8.\nWhile there is some overlap between NLP, ML, and DL, they are also quite different\nareas of study, as the figure illustrates. Like other early work in AI, early NLP applica‐\ntions were also based on rules and heuristics. In the past few decades, though, NLP\n14 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 461,
      "char_count": 2767,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 45,
      "text": "application development has been heavily influenced by methods from ML. More\nrecently, DL has also been frequently used to build NLP applications. Considering\nthis, let’s do a short overview of ML and DL in this section.\nFigure 1-8. How NLP, ML, and DL are related\nThe goal of ML is to “learn” to perform tasks based on examples (called “training\ndata”) without explicit instruction. This is typically done by creating a numeric repre‐\nsentation (called “features”) of the training data and using this representation to learn\nthe patterns in those examples. Machine learning algorithms can be grouped into\nthree primary paradigms: supervised learning, unsupervised learning, and reinforce‐\nment learning. In supervised learning, the goal is to learn the mapping function from\ninput to output given a large number of examples in the form of input-output pairs.\nThe input-output pairs are known as training data, and the outputs are specifically\nknown as labels or ground truth. An example of a supervised learning problem related\nto language is learning to classify email messages as spam or non-spam given thou‐\nsands of examples in both categories. This is a common scenario in NLP, and we’ll see\nexamples of supervised learning throughout the book, especially in Chapter 4.\nUnsupervised learning refers to a set of machine learning methods that aim to find\nhidden patterns in given input data without any reference output. That is, in contrast\nto supervised learning, unsupervised learning works with large collections of unla‐\nbeled data. In NLP, an example of such a task is to identify latent topics in a large col‐\nlection of textual data without any knowledge of these topics. This is known as topic\nmodeling, and we’ll discuss it in Chapter 7.\nMachine Learning, Deep Learning, and NLP: An Overview \n| \n15\n",
      "word_count": 295,
      "char_count": 1813,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1388,
          "height": 864,
          "ext": "png",
          "size_bytes": 83542
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 46,
      "text": "Common in real-world NLP projects is a case of semi-supervised learning, where we\nhave a small labeled dataset and a large unlabeled dataset. Semi-supervised techni‐\nques involve using both datasets to learn the task at hand. Last but not least, rein‐\nforcement learning deals with methods to learn tasks via trial and error and is\ncharacterized by the absence of either labeled or unlabeled data in large quantities.\nThe learning is done in a self-contained environment and improves via feedback\n(reward or punishment) facilitated by the environment. This form of learning is not\ncommon in applied NLP (yet). It is more common in applications such as machine-\nplaying games like go or chess, in the design of autonomous vehicles, and in robotics.\nDeep learning refers to the branch of machine learning that is based on artificial neu‐\nral network architectures. The ideas behind neural networks are inspired by neurons\nin the human brain and how they interact with one another. In the past decade, deep\nlearning–based neural architectures have been used to successfully improve the per‐\nformance of various intelligent applications, such as image and speech recognition\nand machine translation. This has resulted in a proliferation of deep learning–based\nsolutions in industry, including in NLP applications.\nThroughout this book, we’ll discuss how all these approaches are used for developing\nvarious NLP applications. Let’s now discuss the different approaches to solve any\ngiven NLP problem.\nApproaches to NLP\nThe different approaches used to solve NLP problems commonly fall into three cate‐\ngories: heuristics, machine learning, and deep learning. This section is simply an\nintroduction to each approach—don’t worry if you can’t quite grasp the concepts yet,\nas they’ll be discussed in detail throughout the rest of the book. Let’s jump in by dis‐\ncussing heuristics-based NLP.\nHeuristics-Based NLP\nSimilar to other early AI systems, early attempts at designing NLP systems were based\non building rules for the task at hand. This required that the developers had some\nexpertise in the domain to formulate rules that could be incorporated into a program.\nSuch systems also required resources like dictionaries and thesauruses, typically com‐\npiled and digitized over a period of time. An example of designing rules to solve an\nNLP problem using such resources is lexicon-based sentiment analysis. It uses counts\nof positive and negative words in the text to deduce the sentiment of the text. We’ll\ncover this briefly in Chapter 4.\nBesides dictionaries and thesauruses, more elaborate knowledge bases have been built\nto aid NLP in general and rule-based NLP in particular. One example is Wordnet [7],\nwhich is a database of words and the semantic relationships between them. Some\n16 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 447,
      "char_count": 2816,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 47,
      "text": "examples of such relationships are synonyms, hyponyms, and meronyms. Synonyms\nrefer to words with similar meanings. Hyponyms capture is-type-of relationships. For\nexample, baseball, sumo wrestling, and tennis are all hyponyms of sports. Meronyms\ncapture is-part-of relationships. For example, hands and legs are meronyms of the\nbody. All this information becomes useful when building rule-based systems around\nlanguage. Figure 1-9 shows an example depiction of such relationships between\nwords using Wordnet.\nFigure 1-9. Wordnet graph for the word “sport” [8]\nMore recently, common sense world knowledge has also been incorporated into\nknowledge bases like Open Mind Common Sense [9], which also aids such rule-based\nsystems. While what we’ve seen so far are largely lexical resources based on word-\nlevel information, rule-based systems go beyond words and can incorporate other\nforms of information, too. Some of them are introduced below.\nRegular expressions (regex) are a great tool for text analysis and building rule-based\nsystems. A regex is a set of characters or a pattern that is used to match and find\nApproaches to NLP \n| \n17\n",
      "word_count": 174,
      "char_count": 1138,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1417,
          "height": 1267,
          "ext": "png",
          "size_bytes": 113646
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 48,
      "text": "substrings in text. For example, a regex like ‘^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]\n+)\\.([a-zA-Z]{2,5})$’ is used to find all email IDs in a piece of text. Regexes are a great\nway to incorporate domain knowledge in your NLP system. For example, given a cus‐\ntomer complaint that comes via chat or email, we want to build a system to automati‐\ncally identify the product the complaint is about. There is a range of product codes\nthat map to certain brand names. We can use regexes to match these easily.\nRegexes are a very popular paradigm for building rule-based systems. NLP software\nlike StanfordCoreNLP includes TokensRegex [10], which is a framework for defining\nregular expressions. It is used to identify patterns in text and use matched text to cre‐\nate rules. Regexes are used for deterministic matches—meaning it’s either a match or\nit’s not. Probabilistic regexes is a sub-branch that addresses this limitation by includ‐\ning a probability of a match. Interested readers can look at software libraries such as\npregex [11]. Last accessed June 15, 2020.\nContext-free grammar (CFG) is a type of formal grammar that is used to model natu‐\nral languages. CFG was invented by Professor Noam Chomsky, a renowned linguist\nand scientist. CFGs can be used to capture more complex and hierarchical informa‐\ntion that a regex might not. The Earley parser [12] allows parsing of all kinds of\nCFGs. To model more complex rules, grammar languages like JAPE (Java Annotation\nPatterns Engine) can be used [13]. JAPE has features from both regexes as well as\nCFGs and can be used for rule-based NLP systems like GATE (General Architecture\nfor Text Engineering) [14]. GATE is used for building text extraction for closed and\nwell-defined domains where accuracy and completeness of coverage is more impor‐\ntant. As an example, JAPE and GATE were used to extract information on pacemaker\nimplantation procedures from clinical reports [15]. Figure 1-10 shows the GATE\ninterface along with several types of information highlighted in the text as an example\nof a rule-based system.\nRules and heuristics play a role across the entire life cycle of NLP projects even now.\nAt one end, they’re a great way to build first versions of NLP systems. Put simply, \nrules and heuristics help you quickly build the first version of the model and get a\nbetter understanding of the problem at hand. We’ll discuss this point in depth in\nChapters 4 and 11. Rules and heuristics can also be useful as features for machine\nlearning–based NLP systems. At the other end of the spectrum of the project life\ncycle, rules and heuristics are used to plug the gaps in the system. Any NLP system\nbuilt using statistical, machine learning, or deep learning techniques will make mis‐\ntakes. Some mistakes can be too expensive—for example, a healthcare system that\nlooks into all the medical records of a patient and wrongly decides to not advise a\ncritical test. This mistake could even cost a life. Rules and heuristics are a great way to\nplug such gaps in production systems. Now let’s turn our attention to machine learn‐\ning techniques used for NLP.\n18 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 526,
      "char_count": 3146,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 49,
      "text": "Figure 1-10. GATE tool\nMachine Learning for NLP\nMachine learning techniques are applied to textual data just as they’re used on other\nforms of data, such as images, speech, and structured data. Supervised machine learn‐\ning techniques such as classification and regression methods are heavily used for vari‐\nous NLP tasks. As an example, an NLP classification task would be to classify news\narticles into a set of news topics like sports or politics. On the other hand, regression\ntechniques, which give a numeric prediction, can be used to estimate the price of a\nstock based on processing the social media discussion about that stock. Similarly,\nunsupervised clustering algorithms can be used to club together text documents.\nAny machine learning approach for NLP, supervised or unsupervised, can be\ndescribed as consisting of three common steps: extracting features from text, using\nthe feature representation to learn a model, and evaluating and improving the model.\nWe’ll learn more about feature representations for text specifically in Chapter 3 and\nevaluation in Chapter 2. We’ll now briefly outline some of the commonly used super‐\nvised ML methods in NLP for the second step (using the feature representation to\nlearn a model). Having a basic idea of these methods will help you understand the\nconcepts discussed in later chapters.\nApproaches to NLP \n| \n19\n",
      "word_count": 219,
      "char_count": 1367,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 775,
          "height": 541,
          "ext": "png",
          "size_bytes": 26704
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 50,
      "text": "Naive Bayes\nNaive Bayes is a classic algorithm for classification tasks [16] that mainly relies on\nBayes’ theorem (as is evident from the name). Using Bayes’ theorem, it calculates the\nprobability of observing a class label given the set of features for the input data. A\ncharacteristic of this algorithm is that it assumes each feature is independent of all\nother features. For the news classification example mentioned earlier in this chapter,\none way to represent the text numerically is by using the count of domain-specific\nwords, such as sport-specific or politics-specific words, present in the text. We\nassume that these word counts are not correlated to one another. If the assumption\nholds, we can use Naive Bayes to classify news articles. While this is a strong assump‐\ntion to make in many cases, Naive Bayes is commonly used as a starting algorithm for\ntext classification. This is primarily because it is simple to understand and very fast to\ntrain and run.\nSupport vector machine\nThe support vector machine (SVM) is another popular classification [17] algorithm.\nThe goal in any classification approach is to learn a decision boundary that acts as a\nseparation between different categories of text (e.g., politics versus sports in our news\nclassification example). This decision boundary can be linear or nonlinear (e.g., a cir‐\ncle). An SVM can learn both a linear and nonlinear decision boundary to separate\ndata points belonging to different classes. A linear decision boundary learns to repre‐\nsent the data in a way that the class differences become apparent. For two-\ndimensional feature representations, an illustrative example is given in Figure 1-11,\nwhere the black and white points belong to different classes (e.g., sports and politics\nnews groups). An SVM learns an optimal decision boundary so that the distance\nbetween points across classes is at its maximum. The biggest strength of SVMs are\ntheir robustness to variation and noise in the data. A major weakness is the time\ntaken to train and the inability to scale when there are large amounts of training data.\n20 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 349,
      "char_count": 2127,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 51,
      "text": "Figure 1-11. A two-dimensional feature representation of an SVM\nHidden Markov Model\nThe hidden Markov model (HMM) is a statistical model [18] that assumes there is an\nunderlying, unobservable process with hidden states that generates the data—i.e., we\ncan only observe the data once it is generated. An HMM then tries to model the hid‐\nden states from this data. For example, consider the NLP task of part-of-speech (POS)\ntagging, which deals with assigning part-of-speech tags to sentences. HMMs are used\nfor POS tagging of text data. Here, we assume that the text is generated according to\nan underlying grammar, which is hidden underneath the text. The hidden states are\nparts of speech that inherently define the structure of the sentence following the lan‐\nguage grammar, but we only observe the words that are governed by these latent\nstates. Along with this, HMMs also make the Markov assumption, which means that\neach hidden state is dependent on the previous state(s). Human language is sequential\nin nature, and the current word in a sentence depends on what occurred before it.\nHence, HMMs with these two assumptions are a powerful tool for modeling textual\nApproaches to NLP \n| \n21\n",
      "word_count": 197,
      "char_count": 1194,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1289,
          "height": 1286,
          "ext": "png",
          "size_bytes": 98602
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 52,
      "text": "data. In Figure 1-12, we can see an example of an HMM that learns parts of speech\nfrom a given sentence. Parts of speech like JJ (adjective) and NN (noun) are hidden\nstates, while the sentence “natural language processing ( nlp )…” is directly observed.\nFigure 1-12. A graphical representation of a hidden Markov model\nFor a detailed discussion on HMMs for NLP, refer to Chapter 8 in the book Speech\nand Language Processing by Professor Jurafsky [19].\nConditional random fields\nThe conditional random field (CRF) is another algorithm that is used for sequential\ndata. Conceptually, a CRF essentially performs a classification task on each element in\nthe sequence [20]. Imagine the same example of POS tagging, where a CRF can tag\nword by word by classifying them to one of the parts of speech from the pool of all\nPOS tags. Since it takes the sequential input and the context of tags into considera‐\ntion, it becomes more expressive than the usual classification methods and generally\nperforms better. CRFs outperform HMMs for tasks such as POS tagging, which rely\non the sequential nature of language. We discuss CRFs and their variants along with\napplications in Chapters 5, 6, and 9.\nThese are some of the popular ML algorithms that are used heavily across NLP tasks.\nHaving some understanding of these ML methods helps to understand various solu‐\ntions discussed in the book. Apart from that, it is also important to understand when\nto use which algorithm, which we’ll discuss in the upcoming chapters. To learn more\nabout other steps and further theoretical details of the machine learning process, we\nrecommend the textbook Pattern Recognition and Machine Learning by Christopher\nBishop [21]. For a more applied machine learning perspective, Aurélien Géron’s book\n[22] is a great resource to start with. Let’s now take a look at deep learning approaches\nto NLP.\nDeep Learning for NLP\nWe briefly touched on a couple of popular machine learning methods that are used\nheavily in various NLP tasks. In the last few years, we have seen a huge surge in using\nneural networks to deal with complex, unstructured data. Language is inherently\ncomplex and unstructured. Therefore, we need models with better representation and\n22 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 376,
      "char_count": 2254,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1387,
          "height": 297,
          "ext": "png",
          "size_bytes": 41046
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 53,
      "text": "learning capability to understand and solve language tasks. Here are a few popular \ndeep neural network architectures that have become the status quo in NLP.\nRecurrent neural networks\nAs we mentioned earlier, language is inherently sequential. A sentence in any lan‐\nguage flows from one direction to another (e.g., English reads from left to right).\nThus, a model that can progressively read an input text from one end to another can\nbe very useful for language understanding. Recurrent neural networks (RNNs) are\nspecially designed to keep such sequential processing and learning in mind. RNNs\nhave neural units that are capable of remembering what they have processed so far.\nThis memory is temporal, and the information is stored and updated with every time\nstep as the RNN reads the next word in the input. Figure 1-13 shows an unrolled\nRNN and how it keeps track of the input at different time steps.\nFigure 1-13. An unrolled recurrent neural network [23]\nRNNs are powerful and work very well for solving a variety of NLP tasks, such as text\nclassification, named entity recognition, machine translation, etc. One can also use\nRNNs to generate text where the goal is to read the preceding text and predict the\nnext word or the next character. Refer to “The Unreasonable Effectiveness of Recur‐\nrent Neural Networks” [24] for a detailed discussion on the versatility of RNNs and\nthe range of applications within and outside NLP for which they are useful.\nLong short-term memory\nDespite their capability and versatility, RNNs suffer from the problem of forgetful\nmemory—they cannot remember longer contexts and therefore do not perform well\nwhen the input text is long, which is typically the case with text inputs. Long short-\nterm memory networks (LSTMs), a type of RNN, were invented to mitigate this\nshortcoming of the RNNs. LSTMs circumvent this problem by letting go of the irrele‐\nvant context and only remembering the part of the context that is needed to solve the\ntask at hand. This relieves the load of remembering very long context in one vector\nrepresentation. LSTMs have replaced RNNs in most applications because of this\nworkaround. Gated recurrent units (GRUs) are another variant of RNNs that are used\nmostly in language generation. (The article written by Christopher Olah [23] covers\nApproaches to NLP \n| \n23\n",
      "word_count": 384,
      "char_count": 2332,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1132,
          "height": 369,
          "ext": "png",
          "size_bytes": 40794
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 54,
      "text": "the family of RNN models in great detail.) Figure 1-14 illustrates the architecture of a\nsingle LSTM cell. We’ll discuss specific uses of LSTMs in various NLP applications in\nChapters 4, 5, 6, and 9.\nFigure 1-14. Architecture of an LSTM cell [23]\nConvolutional neural networks\nConvolutional neural networks (CNNs) are very popular and used heavily in com‐\nputer vision tasks like image classification, video recognition, etc. CNNs have also\nseen success in NLP, especially in text-classification tasks. One can replace each word\nin a sentence with its corresponding word vector, and all vectors are of the same size\n(d) (refer to “Word Embeddings” in Chapter 3). Thus, they can be stacked one over\nanother to form a matrix or 2D array of dimension n ✕ d, where n is the number of\nwords in the sentence and d is the size of the word vectors. This matrix can now be\ntreated similar to an image and can be modeled by a CNN. The main advantage\nCNNs have is their ability to look at a group of words together using a context win‐\ndow. For example, we are doing sentiment classification, and we get a sentence like, “I\nlike this movie very much!” In order to make sense of this sentence, it is better to look\nat words and different sets of contiguous words. CNNs can do exactly this by defini‐\ntion of their architecture. We’ll touch on this in more detail in later chapters.\nFigure 1-15 shows a CNN in action on a piece of text to extract useful phrases to ulti‐\nmately arrive at a binary number indicating the sentiment of the sentence from a\ngiven piece of text.\nAs shown in the figure, CNN uses a collection of convolution and pooling layers to\nachieve this condensed representation of the text, which is then fed as input to a fully\nconnected layer to learn some NLP tasks like text classification. More details on the\nusage CNNs for NLP can be found in [25] and [26]. We also cover them in Chapter 4.\n24 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 351,
      "char_count": 1933,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "Symbola (10.5pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1388,
          "height": 548,
          "ext": "png",
          "size_bytes": 60504
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 55,
      "text": "Figure 1-15. CNN model in action [27]\nTransformers\nTransformers [28] are the latest entry in the league of deep learning models for NLP.\nTransformer models have achieved state of the art in almost all major NLP tasks in\nthe past two years. They model the textual context but not in a sequential manner.\nGiven a word in the input, it prefers to look at all the words around it (known as self-\nattention) and represent each word with respect to its context. For example, the word\n“bank” can have different meanings depending on the context in which it appears. If\nthe context talks about finance, then “bank” probably denotes a financial institution.\nOn the other hand, if the context mentions a river, then it probably indicates a bank\nof the river. Transformers can model such context and hence have been used heavily\nApproaches to NLP \n| \n25\n",
      "word_count": 147,
      "char_count": 843,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1410,
          "height": 1460,
          "ext": "png",
          "size_bytes": 111943
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 56,
      "text": "in NLP tasks due to this higher representation capacity as compared to other deep\nnetworks.\nRecently, large transformers have been used for transfer learning with smaller down‐\nstream tasks. Transfer learning is a technique in AI where the knowledge gained\nwhile solving one problem is applied to a different but related problem. With trans‐\nformers, the idea is to train a very large transformer mode in an unsupervised man‐\nner (known as pre-training) to predict a part of a sentence given the rest of the\ncontent so that it can encode the high-level nuances of the language in it. These mod‐\nels are trained on more than 40 GB of textual data, scraped from the whole internet.\nAn example of a large transformer is BERT (Bidirectional Encoder Representations\nfrom Transformers) [29], shown in Figure 1-16, which is pre-trained on massive data\nand open sourced by Google.\nFigure 1-16. BERT architecture: pre-trained model and fine-tuned, task-specific models\nThe pre-trained model is shown on the left side of Figure 1-16. This model is then\nfine-tuned on downstream NLP tasks, such as text classification, entity extraction,\nquestion answering, etc., as shown on the right of Figure 1-16. Due to the sheer\namount of pre-trained knowledge, BERT works efficiently in transferring the knowl‐\nedge for downstream tasks and achieves state of the art for many of these tasks.\nThroughout the book, we have covered various examples of using BERT for various\ntasks. Figure 1-17 illustrates the workings of a self-attention mechanism, which is a\nkey component of a transformer. Interested readers can look at [30] for more details\non self-attention mechanisms and transformer architecture. We cover BERT and its\napplications in Chapters 4, 6, and 10.\n26 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 289,
      "char_count": 1775,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1428,
          "height": 597,
          "ext": "png",
          "size_bytes": 100494
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 57,
      "text": "Figure 1-17. Self-attention mechanism in a transformer [30]\nAutoencoders\nAn autoencoder is a different kind of network that is used mainly for learning com‐\npressed vector representation of the input. For example, if we want to represent a text\nby a vector, what is a good way to do it? We can learn a mapping function from input\ntext to the vector. To make this mapping function useful, we “reconstruct” the input\nback from the vector representation. This is a form of unsupervised learning since\nyou don’t need human-annotated labels for it. After the training, we collect the vector\nrepresentation, which serves as an encoding of the input text as a dense vector.\nAutoencoders are typically used to create feature representations needed for any\ndownstream tasks. Figure 1-18 depicts the architecture of an autoencoder.\nApproaches to NLP \n| \n27\n",
      "word_count": 139,
      "char_count": 847,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1166,
          "height": 1156,
          "ext": "png",
          "size_bytes": 82294
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 58,
      "text": "Figure 1-18. Architecture of an autoencoder\nIn this scheme, the hidden layer gives a compressed representation of input data, cap‐\nturing the essence, and the output layer (decoder) reconstructs the input representa‐\ntion from the compressed representation. While the architecture of the autoencoder\nshown in Figure 1-18 cannot handle specific properties of sequential data like text,\nvariations of autoencoders, such as LSTM autoencoders, address these well. More\ninformation about autoencoders can be found in [31].\nWe briefly introduced some of the popular DL architectures for NLP here. For a more\ndetailed study of deep learning architectures in general, refer to [31], and specifically\nfor NLP, refer to [25]. We hope this introduction gives you enough background to\nunderstand the use of DL in the rest of this book.\nGoing by all the recent achievements of DL models, one might think that DL should\nbe the go-to way to build NLP systems. However, that is far from the truth for most\nindustry use cases. Let’s look at why this is the case.\nWhy Deep Learning Is Not Yet the Silver Bullet for NLP\nOver the last few years, DL has made amazing advances in NLP. For example, in text\nclassification, LSTM- and CNN-based models have surpassed the performance of\nstandard machine learning techniques such as Naive Bayes and SVM for many classi‐\nfication tasks. Similarly, LSTMs have performed better in sequence-labeling tasks like\nentity extraction as compared to CRF models. Recently, powerful transformer models\nhave become state of the art in most of these NLP tasks, ranging from classification to\nsequence labeling. A huge trend right now is to leverage large (in terms of number of\nparameters) transformer models, train them on huge datasets for generic NLP tasks\nlike language models, then adapt them to smaller downstream tasks. This approach\n28 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 306,
      "char_count": 1882,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1356,
          "height": 671,
          "ext": "png",
          "size_bytes": 75616
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 59,
      "text": "(known as transfer learning) has also been successful in other domains, such as com‐\nputer vision and speech.\nDespite such tremendous success, DL is still not the silver bullet for all NLP tasks\nwhen it comes to industrial applications. Some of the key reasons for this are as\nfollows:\nOverfitting on small datasets\nDL models tend to have more parameters than traditional ML models, which\nmeans they possess more expressivity. This also comes with a curse. Occam’s\nrazor [32] suggests that a simpler solution is always preferable given that all other\nconditions are equal. Many times, in the development phase, sufficient training\ndata is not available to train a complex network. In such cases, a simpler model\nshould be preferred over a DL model. DL models overfit on small datasets and\nsubsequently lead to poor generalization capability, which in turn leads to poor\nperformance in production.\nFew-shot learning and synthetic data generation\nIn disciplines like computer vision, DL has made significant strides in few-shot\nlearning (i.e., learning from very few training examples) [33] and in models that\ncan generate superior-quality images [34]. Both of these advances have made\ntraining DL-based vision models on small amounts of data feasible. Therefore,\nDL has achieved much wider adoption for solving problems in industrial set‐\ntings. We have not yet seen similar DL techniques be successfully developed for\nNLP.\nDomain adaptation\nIf we utilize a large DL model that is trained on datasets originating from some\ncommon domains (e.g., news articles) and apply the trained model to a newer\ndomain that is different from the common domains (e.g., social media posts), it\nmay yield poor performance. This loss in generalization performance indicates\nthat DL models are not always useful. For example, models trained on internet\ntexts and product reviews will not work well when applied to domains such as\nlaw, social media, or healthcare, where both the syntactic and semantic structure\nof the language is specific to the domain. We need specialized models to encode\nthe domain knowledge, which could be as simple as domain-specific, rule-based\nmodels.\nInterpretable models\nApart from efficient domain adaptation, controllability and interpretability is\nhard for DL models because, most of the time, they work like a black box. Busi‐\nnesses often demand more interpretable results that can be explained to the cus‐\ntomer or end user. In those cases, traditional techniques might be more useful.\nFor example, a Naive Bayes model for sentiment classification may explain the\neffect of strong positive and negative words on the final prediction of sentiment.\nApproaches to NLP \n| \n29\n",
      "word_count": 426,
      "char_count": 2687,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 60,
      "text": "As of today, obtaining such insights from an LSTM-based classification model is\ndifficult. This is in contrast to computer vision, where DL models are not black\nboxes. There are plenty of techniques [35] in computer vision that are used to\ngain insight into why a model is making a particular prediction. Such approaches\nfor NLP are not as common.\nCommon sense and world knowledge\nEven though we have achieved good performance on benchmark NLP tasks\nusing ML and DL models, language remains a bigger enigma to scientists.\nBeyond syntax and semantics, language encompasses knowledge of the world\naround us. Language for communication relies on logical reasoning and com‐\nmon sense regarding events from the world. For example, “I like pizza” implies “I\nfeel happy when I eat pizza.” A more complex reasoning example would be, “If\nJohn walks out of the bedroom and goes to the garden, then John is not in the\nbedroom anymore, and his current location is the garden.” This might seem triv‐\nial to us humans, but it requires multistep reasoning for a machine to identify\nevents and understand their consequences. Since this world knowledge and com‐\nmon sense are inherent in language, understanding them is crucial for any DL\nmodel to perform well on various language tasks. Current DL models may per‐\nform well on standard benchmarks but are still not capable of common sense\nunderstanding and logical reasoning. There are some efforts to collect common\nsense events and logical rules (such as if-them reasoning), but they are not well\nintegrated yet with ML or DL models.\nCost\nBuilding DL-based solutions for NLP tasks can be pretty expensive. The cost, in\nterms of both money and time, stems from multiple sources. DL models are\nknown to be data guzzlers. Collecting a large dataset and getting it labeled can be\nvery expensive. Owing to the size of DL models, training them to achieve desired\nperformance can not only increase your development cycles but also result in a\nheavy bill for the specialized hardware (GPUs). Further, deploying and maintain‐\ning DL models can be expensive in terms of both hardware requirements and\neffort. Last but not least, because they’re bulky, these models may cause latency\nissues during inference time and may not be useful in cases where low latency is a\nmust. To this list, one can also add technical debt arising from building and\nmaintaining a heavy model. Loosely speaking, technical debt is the cost of rework\nthat arises from prioritizing speedy delivery over good design and implementa‐\ntion choices.\nOn-device deployment\nFor many use cases, the NLP solution needs to be deployed on an embedded\ndevice rather than in the cloud—for example, a machine-translation system that\nhelps tourists speak the translated text even without the internet. In such cases,\nowing to limitations of the device, the solution must work with limited memory\n30 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 484,
      "char_count": 2912,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 61,
      "text": "and power. Most DL solutions do not fit such constraints. There are some efforts\nin this direction [36, 37, 38] where one can deploy DL models on edge devices,\nbut we’re still quite far from generic solutions.\nIn most industry projects, one or more of the points mentioned above plays out. This\nleads to longer project cycles and higher costs (hardware, manpower), and yet the\nperformance is either comparable or sometimes even lower than ML models. This\nresults in a poor return on investment and often causes the NLP project to fail.\nBased on this discussion, it may be apparent that DL is not always the go-to solution\nfor all industrial NLP applications. So, this book starts with fundamental aspects of\nvarious NLP tasks and how we can solve them using techniques ranging from rule-\nbased systems to DL models. We emphasize the data requirements and model-\nbuilding pipeline, not just the technical details of individual models. Given the rapid\nadvances in this area, we anticipate that newer DL models will come in the future to\nadvance the state of the art but that the fundamentals of NLP tasks will not change\nsubstantially. This is why we’ll discuss the basics of NLP and build on them to develop\nmodels of increasing complexity wherever possible, rather than directly jumping to\nthe cutting edge.\nEchoing Professor Zachary Lipton from Carnegie Mellon University and Professor\nJacob Steinhardt from UC Berkeley [39], we also want to provide a word of caution\nabout consuming a lot of scientific articles, research papers, and blogs on ML and\nNLP without context and proper training. Following a large volume of cutting-edge\nwork may cause confusion and not-so-precise understanding. Many recent DL mod‐\nels are not interpretable enough to indicate the sources of empirical gains. Lipton and\nSteinhardt also recognize the possible conflation of technical terms and misuse of\nlanguage in ML-related scientific articles, which often fail to provide any clear path to\nsolving the problem at hand. Therefore, in this book, we carefully describe various\ntechnical concepts in the application of ML in NLP tasks via examples, code, and tips\nthroughout the chapters.\nSo far, we’ve covered some foundational concepts related to language, NLP, ML, and\nDL. Before we wrap up Chapter 1, let’s look at a case study to help get a better under‐\nstanding of the various components of an NLP application.\nAn NLP Walkthrough: Conversational Agents\nVoice-based conversational agents like Amazon Alexa and Apple Siri are some of the\nmost ubiquitous applications of NLP, and they’re the ones most people are already\nfamiliar with. Figure 1-19 shows the typical interaction model of a conversational\nagent.\nAn NLP Walkthrough: Conversational Agents \n| \n31\n",
      "word_count": 447,
      "char_count": 2744,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 62,
      "text": "Figure 1-19. Flow of conversation agents\nHere, we’ll walk through all the major NLP components used in this flow:\n1. Speech recognition and synthesis: These are the main components of any voice-\nbased conversational agent. Speech recognition involves converting speech sig‐\nnals to their phonemes, which are then transcribed as words. Speech synthesis\nachieves the reverse process by transforming textual results into spoken language\nto the user. Both of these techniques have advanced considerably in the last dec‐\nade, and we recommend using cloud APIs for most standard cases.\n2. Natural language understanding: This is the next component in the conversational\nagent pipeline, where the user response received (transcribed as text) is analyzed\nusing a natural language understanding system. This can be broken into many\nsmall NLP subtasks, such as:\n• Sentiment analysis: Here, we analyze the sentiment of the user response. This\nwill be covered in Chapter 4.\n• Named entity recognition: Here, we identify all the important entities the user\nmentioned in their response. This will be covered in Chapter 5.\n• Coreference resolution: Here, we find out the references of the extracted entities\nfrom the conversation history. For example, a user may say “Avengers Endgame\nwas awesome” and later refer back to the movie, saying “The movie’s special\neffects were great.” In this case, we would want to link that “movie” is referring\nto Avengers Endgame. This is covered briefly in Chapter 5.\n3. Dialog management: Once we’ve extracted the useful information from the user’s\nresponse, we may want to understand the user’s intent—i.e., if they’re asking a\nfactual question like “What is the weather today?” or giving a command like\n“Play Mozart songs.” We can use a text-classification system to classify the user\nresponse as one of the pre-defined intents. This helps the conversational agent\nknow what’s being asked. Intent classification will be covered in Chapters 4 and 6.\nDuring this process, the system may ask a few clarifying questions to elicit fur‐\n32 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 334,
      "char_count": 2086,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1349,
          "height": 519,
          "ext": "png",
          "size_bytes": 47838
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 63,
      "text": "ther information from the user. Once we’ve figured out the user’s intent, we want\nto figure out which suitable action the conversational agent should take to fulfill\nthe user’s request. This is done based on the information and intent extracted\nfrom the user’s response. Examples of suitable actions could be generating an\nanswer from the internet, playing music, dimming lights, or asking a clarifying\nquestion. We’ll cover this in Chapter 6.\n4. Response generation: Finally, the conversational agent generates a suitable action\nto perform based on a semantic interpretation of the user’s intent and additional\ninputs from the dialogue with the user. As mentioned earlier, the agent can\nretrieve information from the knowledge base and generate responses using a\npre-defined template. For example, it might respond by saying, “Now playing\nSymphony No. 25” or “The lights have been dimmed.” In certain scenarios, it can\nalso generate a completely new response.\nWe hope this brief case study provided an overview of how different NLP compo‐\nnents we’ll be discussing throughout this book will come together to build one appli‐\ncation: a conversational agent. We’ll see more details about these components as we\nprogress through the book, and we’ll discuss conversational agents specifically in\nChapter 6.\nWrapping Up\nFrom the broader contours of what a language is to a concrete case study of a real-\nworld NLP application, we’ve covered a range of NLP topics in this chapter. We also\ndiscussed how NLP is applied in the real world, some of its challenges and different\ntasks, and the role of ML and DL in NLP. This chapter was meant to give you a base‐\nline of knowledge that we’ll build on throughout the book. The next two chapters\n(Chapters 2 and 3) will introduce you to some of the foundational steps necessary for\nbuilding NLP applications. Chapters 4–7 focus on core NLP tasks along with indus‐\ntrial use cases that can be solved with them. In Chapters 8–10, we discuss how NLP is\nused across different industry verticals such as e-commerce, healthcare, finance, etc.\nChapter 11 brings everything together and discusses what it takes to build end-to-end\nNLP applications in terms of design, development, testing, and deployment. With this\nbroad overview in place, let’s start delving deeper into the world of NLP.\nReferences\n[1] Arria.com. “NLG for Your Industry”. Last accessed June 15, 2020.\n[2] UCL. Phonetic symbols for English. Last accessed June 15, 2020.\nWrapping Up \n| \n33\n",
      "word_count": 407,
      "char_count": 2488,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 64,
      "text": "[3] Bender, Emily M. “Linguistic Fundamentals for Natural Language Processing: 100\nEssentials From Morphology and Syntax.” Synthesis Lectures on Human Language\nTechnologies 6.3 (2013): 1–184.\n[4] Bender, Emily M. and Alex Lascarides. “Linguistic Fundamentals for Natural Lan‐\nguage Processing II: 100 Essentials from Semantics and Pragmatics.” Synthesis Lec‐\ntures on Human Language Technologies 12.3 (2019): 1–268.\n[5] Levesque, Hector, Ernest Davis, and Leora Morgenstern. “The Winograd Schema\nChallenge.” The Thirteenth International Conference on the Principles of Knowledge\nRepresentation and Reasoning (2012).\n[6] Wikipedia. “Dartmouth workshop”. Last modified March 30, 2020.\n[7] Miller, George A. “WordNet: A Lexical Database for English.” Communications of\nthe ACM 38.11 (1995): 39–41.\n[8] Visual Thesaurus of English Collocations. “Visual Wordnet with D3.js”. Last\naccessed June 15, 2020.\n[9] Singh, Push, Thomas Lin, Erik T. Mueller, Grace Lim, Travell Perkins, and Wan Li\nZhu. “Open Mind Common Sense: Knowledge Acquisition from the General Public,”\nMeersman R. and Tari Z. (eds), On the Move to Meaningful Internet Systems 2002:\nCoopIS, DOA, and ODBASE. OTM 2002. Lecture Notes in Computer Science, vol.\n2519. Berlin, Heidelberg: Springer.\n[10] The Stanford Natural Language Processing Group. Stanford TokensRegex, (soft‐\nware). Last accessed June 15, 2020.\n[11] Hewitt, Luke. Probabilistic regular expressions, (GitHub repo).\n[12] Earley, Jay. “An Efficient Context-Free Parsing Algorithm.” Communications of\nthe ACM 13.2 (1970): 94–102.\n[13] “Java Annotation Patterns Engine: Regular Expressions over Annotations”.\nDeveloping Language Processing Components with GATE Version 9 (a User Guide),\nChapter 8. Last accessed June 15, 2020.\n[14] General Architecture for Text Engineering (GATE). Last accessed June 15, 2020.\n[15] Rosier, Arnaud, Anita Burgun, and Philippe Mabo. “Using Regular Expressions\nto Extract Information on Pacemaker Implantation Procedures from Clinical\nReports.” AMIA Annual Symposium Proceedings v.2008 (2008): 81–85.\n[16] Zhang, Haiyi and Di Li. “Naïve Bayes Text Classifier.” 2007 IEEE International\nConference on Granular Computing (GRC 2007): 708.\n[17] Joachims, Thorsten. Learning to Classify Text Using Support Vector Machines, Vol.\n668. New York: Springer Science & Business Media, 2002. ISBN: 978-1-4615-0907-3\n34 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 334,
      "char_count": 2386,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 65,
      "text": "[18] Baum, Leonard E. and Ted Petrie. “Statistical Inference for Probabilistic Func‐\ntions of Finite State Markov Chains.” The Annals of Mathematical Statistics 37.6\n(1966): 1554–1563.\n[19] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft), 2018.\n[20] Settles, Burr. “Biomedical Named Entity Recognition Using Conditional Ran‐\ndom Fields and Rich Feature Sets.” Proceedings of the International Joint Workshop on\nNatural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)\n(2004): 107–110.\n[21] Bishop, Christopher M. Pattern Recognition and Machine Learning. New York:\nSpringer, 2006. ISBN: 978-0-3873-1073-2\n[22] Géron, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and\nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Boston:\nO’Reilly, 2019. ISBN: 978-1-492-03264-9\n[23] Olah, Christopher. “Understanding LSTM Networks”. August 27, 2015.\n[24] Karpathy, Andrej. “The Unreasonable Effectiveness of Recurrent Neural Net‐\nworks”. May 21, 2015.\n[25] Goldberg, Yoav. “Neural Network Methods for Natural Language Processing.”\nSynthesis Lectures on Human Language Technologies 10.1 (2017): 1–309.\n[26] Britz, Denny. “Understanding Convolutional Neural Networks for NLP”.\nNovember 7, 2015.\n[27] Le, Hoa T., Christophe Cerisara, and Alexandre Denis. “Do Convolutional Net‐\nworks need to be Deep for Text Classification?” Workshops at the Thirty-Second AAAI\nConference on Artificial Intelligence, 2018.\n[28] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.”\nAdvances in Neural Information Processing Systems, 2017: 5998–6008.\n[29] Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding”. Octo‐\nber 11, 2018.\n[30] Alammar, Jay. “The Illustrated Transformer”. June 27, 2018.\n[31] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. Cam‐\nbridge: MIT Press, 2016. ISBN: 978-0-262-03561-3\n[32] Varma, Nakul. COMS 4771: Introduction to Machine Learning, Lecture 6, Slide\n7. Last accessed June 15, 2020.\nWrapping Up \n| \n35\n",
      "word_count": 302,
      "char_count": 2225,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 66,
      "text": "[33] Wang, Yaqing, Quanming Yao, James Kwok, and Lionel M. Ni. “Generalizing\nfrom a Few Examples: A Survey on Few-Shot Learning”, (2019).\n[34] Wang, Zhengwei, Qi She, and Tomas E. Ward. “Generative Adversarial Networks\nin Computer Vision: A Survey and Taxonomy”, (2019).\n[35] Olah, Chris, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert,\nKatherine Ye, and Alexander Mordvintsev. “The Building Blocks of Interpretability.”\nDistill 3.3 (March 2018): e10.\n[36] Nan, Kaiming, Sicong Liu, Junzhao Du, and Hui Liu. “Deep Model Compression\nfor Mobile Platforms: A Survey.” Tsinghua Science and Technology 24.6 (2019): 677–\n693.\n[37] TensorFlow. “Get started with TensorFlow Lite”. Last modified March 21, 2020.\n[38] Ganesh, Prakhar, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Deming\nChen, Marianne Winslett, Hassan Sajjad, and Preslav Nakov. “Compressing Large-\nScale Transformer-Based Models: A Case Study on BERT”, (2020).\n[39] Lipton, Zachary C. and Jacob Steinhardt. “Troubling Trends in Machine Learn‐\ning Scholarship”, (2018).\n36 \n| \nChapter 1: NLP: A Primer\n",
      "word_count": 160,
      "char_count": 1078,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 67,
      "text": "CHAPTER 2\nNLP Pipeline\nThe whole is more than the sum of its parts. It is more correct to say that the whole is\nsomething else than the sum of its parts, because summing up is a meaningless procedure,\nwhereas the whole-part relationship is meaningful.\n—Kurt Koffka\nIn the previous chapter, we saw examples of some common NLP applications that we\nmight encounter in everyday life. If we were asked to build such an application, think\nabout how we would approach doing so at our organization. We would normally\nwalk through the requirements and break the problem down into several sub-\nproblems, then try to develop a step-by-step procedure to solve them. Since language\nprocessing is involved, we would also list all the forms of text processing needed at\neach step. This step-by-step processing of text is known as a pipeline. It is the series of\nsteps involved in building any NLP model. These steps are common in every NLP\nproject, so it makes sense to study them in this chapter. Understanding some com‐\nmon procedures in any NLP pipeline will enable us to get started on any NLP prob‐\nlem encountered in the workplace. Laying out and developing a text-processing\npipeline is seen as a starting point for any NLP application development process. In\nthis chapter, we will learn about the various steps involved and how they play impor‐\ntant roles in solving the NLP problem and we’ll see a few guidelines about when and\nhow to use which step. In later chapters, we’ll discuss specific pipelines for various\nNLP tasks (e.g., Chapters 4–7).\nFigure 2-1 shows the main components of a generic pipeline for modern-day, data-\ndriven NLP system development. The key stages in the pipeline are as follows:\n1. Data acquisition\n2. Text cleaning\n3. Pre-processing\n37\n",
      "word_count": 300,
      "char_count": 1758,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (9.3pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 68,
      "text": "4. Feature engineering\n5. Modeling\n6. Evaluation\n7. Deployment\n8. Monitoring and model updating\nFigure 2-1. Generic NLP pipeline\nThe first step in the process of developing any NLP system is to collect data relevant\nto the given task. Even if we’re building a rule-based system, we still need some data\nto design and test our rules. The data we get is seldom clean, and this is where text\ncleaning comes into play. After cleaning, text data often has a lot of variations and\nneeds to be converted into a canonical form. This is done in the pre-processing step.\nThis is followed by feature engineering, where we carve out indicators that are most\nsuitable for the task at hand. These indicators are converted into a format that is\nunderstandable by modeling algorithms. Then comes the modeling and evaluation\nphase, where we build one or more models and compare and contrast them using a\nrelevant evaluation metric(s). Once the best model among the ones evaluated is\nchosen, we move toward deploying this model in production. Finally, we regularly\nmonitor the performance of the model and, if need be, update it to keep up its\nperformance.\nNote that, in the real world, the process may not always be linear as it’s shown in the\npipeline in Figure 2-1; it often involves going back and forth between individual steps\n(e.g., between feature extraction and modeling, modeling and evaluation, and so on).\nAlso, there are loops in between, most commonly going from evaluation to pre-\nprocessing, feature engineering, modeling, and back to evaluation. There is also an\noverall loop that goes from monitoring to data acquisition, but this loop happens at\nthe project level.\nNote that exact step-by-step procedures may depend on the specific task at hand. For\nexample, a text-classification system may require a different feature extraction step\ncompared to a text-summarization system. We will focus on application-specific\n38 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 321,
      "char_count": 1947,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1288,
          "height": 409,
          "ext": "png",
          "size_bytes": 27657
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 69,
      "text": "pipeline stages in subsequent chapters in the book. Also, depending on the phase of\nthe project, different steps can take different amounts of time. In the initial phases,\nmost of the time is used in modeling and evaluation, whereas once the system\nmatures, feature engineering can take far more time.\nFor the rest of this chapter, we’ll look at the individual stages of the pipeline in detail\nalong with examples. We’ll describe some of the most common procedures at each\nstage and discuss some use cases to illustrate them. Let’s start with the first step: data\nacquisition.\nData Acquisition\nData is the heart of any ML system. In most industrial projects, it is often the data\nthat becomes the bottleneck. In this section, we’ll discuss various strategies for gather‐\ning relevant data for an NLP project.\nLet’s say we’re asked to develop an NLP system to identify whether an incoming cus‐\ntomer query (for example, using a chat interface) is a sales inquiry or a customer care\ninquiry. Depending on the type of query, it should be automatically routed to the\nright team. How can one go about building such a system? Well, the answer depends\non the type and amount of data we have to work with.\nIn an ideal setting, we’ll have the required datasets with thousands—maybe even mil‐\nlions—of data points. In such cases, we don’t have to worry about data acquisition.\nFor example, in the scenario we just described, we have historic queries fro m previ‐\nous years, which sales and support teams responded to. Further, the teams tagged\nthese queries as sales, support, or other. So, not only do we have the data, but we also\nhave the labels. However, in many AI projects, one is not so lucky. Let’s look at what\nwe can do in a less-than-ideal scenario.\nIf we have little or no data, we can start by looking at patterns in the data that indicate\nif the incoming message is a sales or support query. We can then use regular expres‐\nsions and other heuristics to match these patterns to separate sales queries from sup‐\nport queries. We evaluate this solution by collecting a set of queries from both\ncategories and calculating what percentage of the messages were correctly identified\nby our system. Say we get OK-ish numbers. We would like to improve the system\nperformance.\nNow we can start thinking about using NLP techniques. For this, we need labeled\ndata, a collection of queries where each one is labeled with sales or support. How can\nwe get such data?\nData Acquisition \n| \n39\n",
      "word_count": 433,
      "char_count": 2481,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 70,
      "text": "Use a public dataset\nWe could see if there are any public datasets available that we can leverage. Take a\nlook at the compilation by Nicolas Iderhoff [1] or search Google’s specialized\nsearch engine for datasets [2]. If you find a suitable dataset that’s similar to the\ntask at hand, great! Build a model and evaluate. If not, then what?\nScrape data\nWe could find a source of relevant data on the internet—for example, a consumer\nor discussion forum where people have posted queries (sales or support). Scrape\nthe data from there and get it labeled by human annotators.\nFor many industrial settings, gathering data from external sources does not suf‐\nfice because the data doesn’t contain nuances like product names or product-\nspecific user behavior and thus might be very different from the data seen in\nproduction environments. This is when we’ll have to start looking for data inside\nthe organization.\nProduct intervention\nIn most industrial settings, AI models seldom exist by themselves. They’re devel‐\noped mostly to serve users via a feature or product. In all such cases, the AI team\nshould work with the product team to collect more and richer data by developing\nbetter instrumentation in the product. In the tech world, this is called product\nintervention.\nProduct intervention is often the best way to collect data for building intelligent\napplications in industrial settings. Tech giants like Google, Facebook, Microsoft,\nNetflix, etc., have known this for a long time and have tried to collect as much\ndata as possible from as many users as possible.\nData augmentation\nWhile instrumenting products is a great way to collect data, it takes time. Even if\nyou instrument the product today, it can take anywhere between three to six\nmonths to collect a decent-sized, comprehensive dataset. So, can we do some‐\nthing in the meantime?\nNLP has a bunch of techniques through which we can take a small dataset and use\nsome tricks to create more data. These tricks are also called data augmentation, and\nthey try to exploit language properties to create text that is syntactically similar to\nsource text data. They may appear as hacks, but they work very well in practice. Let’s\nlook at some of them:\nSynonym replacement\nRandomly choose “k” words in a sentence that are not stop words. Replace these\nwords with their synonyms. For synonyms, we can use Synsets in Wordnet [3, 4].\n40 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 406,
      "char_count": 2414,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 71,
      "text": "Back translation\nSay we have a sentence, S1, in English. We use a machine-translation library like\nGoogle Translate to translate it into some other language—say, German. Let the\ncorresponding sentence in German be S2. Now, we’ll use the machine-translation\nlibrary again to translate back to English. Let the output sentence be S3.\nWe’ll find that S1 and S3 are very similar in meaning but are slight variations of\neach other. Now we can add S3 to our dataset. This trick works beautifully for\ntext classification. Figure 2-2 [5] shows an example of back translation in action.\nFigure 2-2. Back translation\nTF-IDF–based word replacement\nBack translation can lose certain words that are crucial to the sentence. In [5], the\nauthors use TF-IDF, a concept we’ll introduce in Chapter 3, to handle this.\nBigram flipping\nDivide the sentence into bigrams. Take one bigram at random and flip it. For\nexample: “I am going to the supermarket.” Here, we take the bigram “going to”\nand replace it with the flipped one: “to going.”\nReplacing entities\nReplace entities like person name, location, organization, etc., with other entities\nin the same category. That is, replace person name with another person name,\ncity with another city, etc. For example, in “I live in California,” replace “Califor‐\nnia” with “London.”\nAdding noise to data\nIn many NLP applications, the incoming data contains spelling mistakes. This is\nprimarily due to characteristics of the platform where the data is being generated\n(for example, Twitter). In such cases, we can add a bit of noise to data to train\nrobust models. For example, randomly choose a word in a sentence and replace it\nwith another word that’s closer in spelling to the first word. Another source of\nData Acquisition \n| \n41\n",
      "word_count": 291,
      "char_count": 1758,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1377,
          "height": 563,
          "ext": "png",
          "size_bytes": 47774
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 72,
      "text": "noise is the “fat finger” problem [6] on mobile keyboards. Simulate a QWERTY\nkeyboard error by replacing a few characters with their neighboring characters\non the QWERTY keyboard.\nAdvanced techniques\nThere are other advanced techniques and systems that can augment text data.\nSome of the notable ones are:\nSnorkel [7, 8, 52]\nThis is a system for building training data automatically, without manual\nlabeling. Using Snorkel, a large training dataset can be “created”—without\nmanual labeling—using heuristics and creating synthetic data by transform‐\ning existing data and creating new data samples. This approach was shown to\nwork well at Google in the recent past [9].\nEasy Data Augmentation (EDA) [10, 11] and NLPAug [12]\nThese two libraries are used to create synthetic samples for NLP. They pro‐\nvide implementation of various data augmentation techniques, including\nsome techniques that we discussed previously.\nActive learning [13]\nThis is a specialized paradigm of ML where the learning algorithm can inter‐\nactively query a data point and get its label. It is used in scenarios where\nthere is an abundance of unlabeled data but manually labeling is expensive.\nIn such cases, the question becomes: for which data points should we ask for\nlabels to maximize learning while keeping the labeling cost low?\nIn order for most of the techniques we discussed in this section to work well, one key\nrequirement is a clean dataset to start with, even if it’s not very big. In our experience,\ndata augmentation techniques can work really well. Further, in day-to-day ML prac‐\ntice, datasets come from heterogeneous sources. A combination of public datasets,\nlabeled datasets, and augmented datasets are used for building early-stage production\nmodels, as we often may not have large datasets for our custom scenarios to start\nwith. Once we have the data we want for a given task, we proceed to the next step:\ntext cleaning.\nText Extraction and Cleanup\nText extraction and cleanup refers to the process of extracting raw text from the input\ndata by removing all the other non-textual information, such as markup, metadata,\netc., and converting the text to the required encoding format. Typically, this depends\non the format of available data in the organization (e.g., static data from PDF, HTML\nor text, some form of continuous data stream, etc.), as shown in Figure 2-3.\n42 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 389,
      "char_count": 2398,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 73,
      "text": "Text extraction is a standard data-wrangling step, and we don’t usually employ any\nNLP-specific techniques during this process. However, in our experience, it is an\nimportant step that has implications for all other aspects of the NLP pipeline. Further,\nit can also be the most time-consuming part of a project. While the design of text-\nextraction tools is beyond the scope of this book, we’ll look at a few examples to illus‐\ntrate different issues involved in this step in this section. We’ll also touch on some of\nthe important aspects of text extraction from various sources as well as cleanup to\nmake them consumable in downstream pipelines.\nFigure 2-3. (a) PDF invoice, [14] (b) HTML texts, and (c) text embedded in an image\n[15]\nText Extraction and Cleanup \n| \n43\n",
      "word_count": 131,
      "char_count": 772,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 1614,
          "ext": "png",
          "size_bytes": 393335
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 74,
      "text": "HTML Parsing and Cleanup\nSay we’re working on a project where we’re building a forum search engine for pro‐\ngramming questions. We’ve identified Stack Overflow as a source and decided to\nextract question and best-answer pairs from the website. How can we go through the\ntext-extraction step in this case? If we observe the HTML markup of a typical Stack\nOverflow question page, we notice that questions and answers have special tags asso‐\nciated with them. We can utilize this information while extracting text from the\nHTML page. While it may seem like writing our own HTML parser is the way to go,\nfor most cases we encounter, it’s more feasible to utilize existing libraries such as \nBeautiful Soup [16] and Scrapy [17], which provide a range of utilities to parse web\npages. The following code snippet shows how to use Beautiful Soup to address the\nproblem described here, extracting a question and its best-answer pair from a Stack\nOverflow web page:\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlopen\nmyurl = \"https://stackoverflow.com/questions/415511/ \\\n  how-to-get-the-current-time-in-python\"\nhtml = urlopen(myurl).read()\nsoupified = BeautifulSoup(html, \"html.parser\")\nquestion = soupified.find(\"div\", {\"class\": \"question\"})\nquestiontext = question.find(\"div\", {\"class\": \"post-text\"})\nprint(\"Question: \\n\", questiontext.get_text().strip())\nanswer = soupified.find(\"div\", {\"class\": \"answer\"})\nanswertext = answer.find(\"div\", {\"class\": \"post-text\"})\nprint(\"Best answer: \\n\", answertext.get_text().strip())\nHere, we’re relying on our knowledge of the structure of an HTML document to\nextract what we want from it. This code shows the output as follows:\nQuestion:\nWhat is the module/method used to get the current time?\nBest answer:\n Use:\n>>> import datetime\n>>> datetime.datetime.now()\ndatetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n>>> print(datetime.datetime.now())\n2009-01-06 15:08:24.789150\nAnd just the time:\n>>> datetime.datetime.now().time()\ndatetime.time(15, 8, 24, 78915)\n>>> print(datetime.datetime.now().time())\n15:08:24.789150\nSee the documentation for more information.\n44 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 291,
      "char_count": 2136,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 75,
      "text": "To save typing, you can import the datetime object from the datetime module:\n>>> from datetime import datetime\nThen remove the leading datetime. from all of the above.\nIn this example, we had a specific need: extracting a question and its answer. In some\nscenarios—for example, extracting postal addresses from web pages—we would get\nall the text (instead of only parts of it) from the web page first, before doing anything\nelse. Typically, all HTML libraries have some function that can strip off all HTML\ntags and return only the content between the tags. But this often results in noisy out‐\nput, and you may end up seeing a lot of JavaScript in the extracted content as well. In\nsuch cases, we should look to extract content between only those tags that typically\ncontain text in web pages.\nUnicode Normalization\nAs we develop code for cleaning up HTML tags, we may also encounter various Uni‐\ncode characters, including symbols, emojis, and other graphic characters. A handful\nof Unicode characters are shown in Figure 2-4.\nFigure 2-4. Unicode characters [18]\nTo parse such non-textual symbols and special characters, we use Unicode normaliza‐\ntion. This means that the text we see should be converted into some form of binary\nrepresentation to store in a computer. This process is known as text encoding. Ignor‐\ning encoding issues can result in processing errors further in the pipeline.\nThere are several encoding schemes, and the default encoding can be different for dif‐\nferent operating systems. Sometimes (more commonly than you think), especially\nwhen dealing with text in multiple languages, social media data, etc., we may have to\nconvert between these encoding schemes during the text-extraction process. Refer to\n[19] for an introduction to how language is represented on computers and what dif‐\nference an encoding scheme makes. Here is an example of Unicode handling:\nText Extraction and Cleanup \n| \n45\n",
      "word_count": 315,
      "char_count": 1923,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1442,
          "height": 591,
          "ext": "png",
          "size_bytes": 141455
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 76,
      "text": "text = ’I love \n!  Shall we book a \n to gizza?’\nText = text.encode(\"utf-8\")\nprint(Text)\nwhich outputs:\nb'I love Pizza \\xf0\\x9f\\x8d\\x95!  Shall we book a cab \\xf0\\x9f\\x9a\\x95 \n  to get pizza?'\nThis processed text is machine readable and can be used in downstream pipelines.\nWe address issues regarding handling Unicode characters with this same example in\nmore detail in Chapter 8.\nSpelling Correction\nIn the world of fast typing and fat-finger typing [6], incoming text data often has\nspelling errors. This can be prevalent in search engines, text-based chatbots deployed\non mobile devices, social media, and many other sources. While we remove HTML\ntags and handle Unicode characters, this remains a unique problem that may hurt the\nlinguistic understanding of the data, and shorthand text messages in social micro‐\nblogs often hinder language processing and context understanding. Two such exam‐\nples follow:\nShorthand typing: Hllo world! I am back!\nFat finger problem [20]: I pronise that I will not bresk the silence again!\nWhile shorthand typing is prevalent in chat interfaces, fat-finger problems are com‐\nmon in search engines and are mostly unintentional. Despite our understanding of\nthe problem, we don’t have a robust method to fix this, but we still can make good\nattempts to mitigate the issue. Microsoft released a REST API [21] that can be used in\nPython for potential spell checking:\nimport requests\nimport json\napi_key = \"<ENTER-KEY-HERE>\"\nexample_text = \"Hollo, wrld\" # the text to be spell-checked\ndata = {'text': example_text}\nparams = {\n    'mkt':'en-us',\n    'mode':'proof'\n    }\nheaders = {\n    'Content-Type': 'application/x-www-form-urlencoded',\n    'Ocp-Apim-Subscription-Key': api_key,\n    }\nresponse = requests.post(endpoint, headers=headers, params=params, data=data)\njson_response = response.json()\nprint(json.dumps(json_response, indent=4))\n46 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 276,
      "char_count": 1904,
      "fonts": [
        "UbuntuMono-Bold (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 129,
          "height": 154,
          "ext": "png",
          "size_bytes": 2322
        },
        {
          "index": 1,
          "width": 154,
          "height": 100,
          "ext": "png",
          "size_bytes": 1660
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 77,
      "text": "Output (partially shown here):\n\"suggestions\": [\n            {\n               \"suggestion\": \"Hello\",\n               \"score\": 0.9115257530801\n            },\n            {\n               \"suggestion\": \"Hollow\",\n               \"score\": 0.858039839213461\n            },\n            {\n               \"suggestion\": \"Hallo\",\n               \"score\": 0.597385084464481\n            }\nYou can see the full tutorial in [21].\nGoing beyond APIs, we can build our own spell checker using a huge dictionary of\nwords from a specific language. A naive solution would be to look for all words that\ncan be composed with minimal alteration (addition, deletion, substitution) to its con‐\nstituent letters. For example, if ‘“Hello” is a valid word that is already present in the\ndictionary, then the addition of “o” (minimal) to “Hllo” would make the correction.\nSystem-Specific Error Correction\nHTML or raw text scraped from the web are just a couple of sources for textual data.\nConsider another scenario where our dataset is in the form of a collection of PDF\ndocuments. The pipeline in this case starts with extraction of plain text from PDF\ndocuments. However, different PDF documents are encoded differently, and some‐\ntimes, we may not be able to extract the full text, or the structure of the text may get\nmessed up. If we need full text or our text has to be grammatical or in full sentences\n(e.g., when we want to extract relations between various people in the news based on\nnewspaper text), this can impact our application. While there are several libraries,\nsuch as PyPDF [22], PDFMiner [23], etc., to extract text from PDF documents, they\nare far from perfect, and it’s not uncommon to encounter PDF documents that can’t\nbe processed by such libraries. We leave their exploration as an exercise for the\nreader. [24] discusses some of the issues involved in PDF-to-text extraction in detail.\nAnother common source of textual data is scanned documents. Text extraction from\nscanned documents is typically done through optical character recognition (OCR),\nusing libraries such as Tesseract [25, 26]. Consider the example image—a snippet\nfrom a 1950 article in a journal [27]—shown in Figure 2-5.\nText Extraction and Cleanup \n| \n47\n",
      "word_count": 333,
      "char_count": 2218,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 78,
      "text": "Figure 2-5. An example of scanned text\nThe code snippet below shows how the Python library pytesseract can be used to\nextract text from this image:\nfrom PIL import Image\nfrom pytesseract import image_to_string\nfilename = \"somefile.png\"\ntext = image_to_string(Image.open(filename))\nprint(text)\nThis code will print the output as follows, where “\\n” indicates a newline character:\n’in the nineteenth century the only Kind of linguistics considered\\nseriously\nwas this comparative and historical study of words in languages\\nknown or\nbelieved to Fe cognate—say the Semitic languages, or the Indo-\\nEuropean\nlanguages. It is significant that the Germans who really made\\nthe subject what\nit was, used the term Indo-germanisch. Those who know\\nthe popular works of \nOtto Jespersen will remember how fitmly he\\ndeclares that linguistic \nscience is historical. And those who have noticed’\nWe notice that there are two errors in the output of the OCR system in this case.\nDepending on the quality of the original scan, OCR output can potentially have\nlarger amounts of errors. How do we clean up this text before feeding it into the next\nstage of the pipeline? One approach is to run the text through a spell checker such as\npyenchant [28], which will identify misspellings and suggest some alternatives. More\nrecent approaches use neural network architectures to train word/character-based\nlanguage models, which are in turn used for correcting OCR text output based on the\ncontext [29].\nRecall that we saw an example of a voice-based assistant in Chapter 1. In such cases,\nthe source of text extraction is the output of an automatic speech recognition (ASR)\nsystem. Like OCR, it’s common to see some errors in ASR, owing to various factors,\nsuch as dialectical variations, slang, non-native English, new or domain-specific\nvocabulary, etc. The above-mentioned approach of spell checkers or neural language\nmodels can be followed here as well to clean up the extracted text.\nWhat we’ve seen so far are just some examples of potential issues that may come up\nduring the text-extraction and cleaning process. Though NLP plays a very small role\nin this process, we hope these examples illustrate how text extraction and cleanup\n48 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 355,
      "char_count": 2249,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 732,
          "height": 164,
          "ext": "png",
          "size_bytes": 54338
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 79,
      "text": "could pose challenges in a typical NLP pipeline. We’ll also touch on these aspects in\nupcoming chapters for different NLP applications, where relevant. Let’s move on to\nthe next step in our pipeline: pre-processing.\nPre-Processing\nLet’s start with a simple question: we already did some cleanup in the previous step;\nwhy do we still have to pre-process text? Consider a scenario where we’re processing\ntext from Wikipedia pages about individuals to extract biographical information\nabout them. Our data acquisition starts with crawling such pages. However, our\ncrawled data is all in HTML, with a lot of boilerplate from Wikipedia (e.g., all the\nlinks in the left panel), possibly the presence of links to multiple languages (in their\nscript), etc. All such information is irrelevant for extracting features from text (in\nmost cases). Our text-extraction step removed all this and gave us the plain text of the\narticle we need. However, all NLP software typically works at the sentence level and\nexpects a separation of words at the minimum. So, we need some way to split a text\ninto words and sentences before proceeding further in a processing pipeline. Some‐\ntimes, we need to remove special characters and digits, and sometimes, we don’t care\nwhether a word is in upper or lowercase and want everything in lowercase. Many\nmore decisions like this are made while processing text. Such decisions are addressed\nduring the pre-processing step of the NLP pipeline. Here are some common pre-\nprocessing steps used in NLP software:\nPreliminaries\nSentence segmentation and word tokenization.\nFrequent steps\nStop word removal, stemming and lemmatization, removing digits/punctuation,\nlowercasing, etc.\nOther steps\nNormalization, language detection, code mixing, transliteration, etc.\nAdvanced processing\nPOS tagging, parsing, coreference resolution, etc.\nWhile not all steps will be followed in all the NLP pipelines we encounter, the first\ntwo are more or less seen everywhere. Let’s take a look at what each of these steps\nmean.\nPre-Processing \n| \n49\n",
      "word_count": 321,
      "char_count": 2048,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 80,
      "text": "Preliminaries\nAs mentioned earlier, NLP software typically analyzes text by breaking it up into\nwords (tokens) and sentences. Hence, any NLP pipeline has to start with a reliable\nsystem to split the text into sentences (sentence segmentation) and further split a sen‐\ntence into words (word tokenization). On the surface, these seem like simple tasks,\nand you may wonder why they need special treatment. We will see why in the coming\ntwo subsections.\nSentence segmentation\nAs a simple rule, we can do sentence segmentation by breaking up text into sentences\nat the appearance of full stops and question marks. However, there may be abbrevia‐\ntions, forms of addresses (Dr., Mr., etc.), or ellipses (...) that may break the simple\nrule.\nThankfully, we don’t have to worry about how to solve these issues, as most NLP\nlibraries come with some form of sentence and word splitting implemented. A com‐\nmonly used library is Natural Language Tool Kit (NLTK) [30]. The code example\nbelow shows how to use a sentence and word splitter from NLTK and uses the first\nparagraph of this chapter as input:\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nmytext = \"In the previous chapter, we saw examples of some common NLP \napplications that we might encounter in everyday life. If we were asked to \nbuild such an application, think about how we would approach doing so at our \norganization. We would normally walk through the requirements and break the \nproblem down into several sub-problems, then try to develop a step-by-step \nprocedure to solve them. Since language processing is involved, we would also\nlist all the forms of text processing needed at each step. This step-by-step \nprocessing of text is known as pipeline. It is the series of steps involved in\nbuilding any NLP model. These steps are common in every NLP project, so it \nmakes sense to study them in this chapter. Understanding some common procedures\nin any NLP pipeline will enable us to get started on any NLP problem encountered \nin the workplace. Laying out and developing a text-processing pipeline is seen \nas a starting point for any NLP application development process. In this\nchapter, we will learn about the various steps involved and how they play  \nimportant roles in solving the NLP problem and we’ll see a few guidelines\nabout when and how to use which step. In later chapters, we’ll discuss  \nspecific pipelines for various NLP tasks (e.g., Chapters 4–7).\"\nmy_sentences = sent_tokenize(mytext)\n50 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 412,
      "char_count": 2506,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 81,
      "text": "Word tokenization\nSimilar to sentence tokenization, to tokenize a sentence into words, we can start with\na simple rule to split text into words based on the presence of punctuation marks. The \nNLTK library allows us to do that. If we take the previous example:\nfor sentence in my_sentences:\n   print(sentence)\n   print(word_tokenize(sentence))\nFor the first sentence, the output is printed as follows:\nIn the previous chapter, we saw a quick overview of what is NLP, what are some\nof the common applications and challenges in NLP, and an introduction to \ndifferent tasks in NLP.\n['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'a', 'quick', \n'overview', 'of', 'what', 'is', 'NLP', ',', 'what', 'are', 'some', 'of', 'the', \n'common', 'applications', 'and', 'challenges', 'in', 'NLP', ',', 'and', 'an', \n'introduction', 'to', 'different', 'tasks', 'in', 'NLP', '.']\nWhile readily available solutions work for most of our needs and most NLP libraries\nwill have a tokenizer and sentence splitter bundled with them, it’s important to\nremember that they’re far from perfect. For example, consider this sentence: “Mr. Jack\nO’Neil works at Melitas Marg, located at 245 Yonge Avenue, Austin, 70272.” If we run\nthis through the NLTK tokenizer, O, ‘, and Neil are identified as three separate tokens.\nSimilarly, if we run the sentence: “There are $10,000 and €1000 which are there just\nfor testing a tokenizer” through this tokenizer, while $ and 10,000 are identified as\nseparate tokens, €1000 is identified as a single token. In another scenario, if we want\nto tokenize tweets, this tokenizer will separate a hashtag into two tokens: a “#” sign\nand the string that follows it. In such cases, we may need to use a custom tokenizer\nbuilt for our purpose. To complete our example, we’ll perform word tokenization\nafter we perform sentence tokenization.\nA point to note in this context is that NLTK also has a tweet tokenizer; we’ll see how\nit’s useful in Chapters 4 and 8. To summarize, although word- and sentence-\ntokenization approaches appear to be elementary and easy to implement, they may\nnot always meet our specific tokenization needs, as we saw in the above examples.\nNote that we refer to NLTK’s example, but these observations hold true for any other\nlibrary as well. We leave that exploration as an exercise for the reader.\nAs tokenization may differ from one domain to the other, tokenization is also heavily\ndependent on language. Each language can have various linguistic rules and excep‐\ntions. Figure 2-6 shows an example where “N.Y.!” has a total of three punctuations.\nBut in English, N.Y. stands for New York, hence “N.Y.” should be treated as a single\nword and not be tokenized further. Such language-specific exceptions can be speci‐\nfied in the tokenizer provided by spaCy [31]. It’s also possible in spaCy to develop\ncustom rules to handle such exceptions for languages that have high inflections (pre‐\nfixes or suffixes) and complex morphology.\nPre-Processing \n| \n51\n",
      "word_count": 482,
      "char_count": 2988,
      "fonts": [
        "UbuntuMono-Regular (8.5pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 82,
      "text": "Another important fact to keep in mind is that any sentence segmenter and tokenizer\nwill be sensitive to the input they receive. Let’s say we’re writing software to extract\nsome information, such as company, position, and salary, from job offer letters. They\nfollow a certain format, with a To and a From address, a signed note at the end, and\nso on. How will we decide what a sentence is in such a case? Should the entire address\nbe considered a single “sentence”? Or should each line be split separately? Answers to\nsuch questions depend on what you want to extract and how sensitive the rest of the\npipeline is about such decisions. For identifying specific patterns (e.g., dates or\nmoney expressions), well-formed regular expressions are the first step. In many prac‐\ntical scenarios, we may end up using a custom tokenizer or sentence segmenter that\nsuits our text structure instead of or on top of an existing one available in a standard\nNLP library [32].\nFigure 2-6. Language-specific (English here) exceptions during tokenization [31]\nFrequent Steps\nLet’s look at some other frequently performed pre-processing operations in an NLP\npipeline. Say we’re designing software that identifies the category of a news article as\none of politics, sports, business, and other. Assume we have a good sentence seg‐\nmenter and word tokenizer in place. At that point, we would have to start thinking\nabout what kind of information is useful for developing a categorization tool. Some of\nthe frequently used words in English, such as a, an, the, of, in, etc., are not particu‐\nlarly useful for this task, as they don’t carry any content on their own to separate\nbetween the four categories. Such words are called stop words and are typically\n(though not always) removed from further analysis in such problem scenarios. There\nis no standard list of stop words for English, though. There are some popular lists\n(NLTK has one, for example), although what a stop word is can vary depending on\n52 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 340,
      "char_count": 2013,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 948,
          "height": 709,
          "ext": "png",
          "size_bytes": 40660
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 83,
      "text": "what we’re working on. For example, the word “news” is perhaps a stop word for this\nproblem scenario, but it may not be a stop word for the offer letter data in the exam‐\nple mentioned in the previous step.\nSimilarly, in some cases, upper or lowercase may not make a difference for the prob‐\nlem. So, all text is lowercased (or uppercased, although lowercasing is more com‐\nmon). Removing punctuation and/or numbers is also a common step for many NLP\nproblems, such as text classification (Chapter 4), information retrieval (Chapter 7),\nand social media analytics (Chapter 8). We’ll see examples of how and if these steps\nare useful in upcoming chapters.\nThe code example below shows how to remove stop words, digits, and punctuation\nand lowercase a given collection of texts:\nfrom nltk.corpus import stopwords\nFrom string import punctuation\ndef preprocess_corpus(texts):\n    mystopwords = set(stopwords.words(\"english\"))\n    def remove_stops_digits(tokens):\n       return [token.lower() for token in tokens if token not in mystopwords                      \n               not token.isdigit() and token not in punctuation]\n    return [remove_stops_digits(word_tokenize(text)) for text in texts]\nIt’s important to note that these four processes are neither mandatory nor sequential\nin nature. The above function is just an illustration of how to add those processing\nsteps into our project. The pre-processing we saw here, while specific to textual data,\nhas nothing particularly linguistic about it—we’re not looking at any aspect of lan‐\nguage other than frequency (stop words are very frequent words), and we’re remov‐\ning non-alphabetic data (punctuation, digits). Two commonly used pre-processing\nsteps that take the word-level properties into account are stemming and\nlemmatization.\nStemming and lemmatization\nStemming refers to the process of removing suffixes and reducing a word to some\nbase form such that all different variants of that word can be represented by the same\nform (e.g., “car” and “cars” are both reduced to “car”). This is accomplished by apply‐\ning a fixed set of rules (e.g., if the word ends in “-es,” remove “-es”). More such exam‐\nples are shown in Figure 2-7. Although such rules may not always end up in a\nlinguistically correct base form, stemming is commonly used in search engines to\nmatch user queries to relevant documents and in text classification to reduce the fea‐\nture space to train machine learning models.\nThe following code snippet shows how to use a popular stemming algorithm called\nPorter Stemmer [33] using NLTK:\nPre-Processing \n| \n53\n",
      "word_count": 396,
      "char_count": 2584,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 84,
      "text": "from nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\nword1, word2 = “cars”, “revolution” \nprint(stemmer.stem(word1), stemmer.stem(word2))\nThis gives “car” as the stemmed version for “cars,” but “revolut” as the stemmed form\nof “revolution,” even though the latter is not linguistically correct. While this may not\naffect the performance of a search engine, derivation of correct linguistic form\nbecomes useful in some other scenarios. This is accomplished by another process,\ncloser to stemming, called lemmatization.\nLemmatization is the process of mapping all the different forms of a word to its base\nword, or lemma. While this seems close to the definition of stemming, they are, in\nfact, different. For example, the adjective “better,” when stemmed, remains the same.\nHowever, upon lemmatization, this should become “good,” as shown in Figure 2-7.\nLemmatization requires more linguistic knowledge, and modeling and developing\nefficient lemmatizers remains an open problem in NLP research even now.\nFigure 2-7. Difference between stemming and lemmatization [34]\nThe following code snippet shows the usage of a lemmatizer based on WordNet from \nNLTK:\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordnetLemmatizer()\nprint(lemmatizer.lemmatize(\"better\", pos=\"a\")) #a is for adjective\nAnd this code snippet shows a lemmatizer using spaCy:\nimport spacy\nsp = spacy.load('en_core_web_sm')\ntoken = sp(u'better')\nfor word in token:\n   print(word.text,  word.lemma_)\nNLTK prints the output as “good,” whereas spaCy prints “well”—both are correct.\nSince lemmatization involves some amount of linguistic analysis of the word and its\ncontext, it is expected that it will take longer to run than stemming, and it’s also typi‐\ncally used only if absolutely necessary. We’ll see how stemming and lemmatization are\nuseful in the next chapters. The choice of lemmatizer is optional; we can choose\nNLTK or spaCy given what framework we’re using for other pre-processing steps in\norder to use a single framework in the complete pipeline.\n54 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 304,
      "char_count": 2078,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 608,
          "height": 223,
          "ext": "png",
          "size_bytes": 14487
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 85,
      "text": "Remember that not all of these steps are always necessary, and not all of them are per‐\nformed in the order in which they’re discussed here. For example, if we were to\nremove digits and punctuation, what is removed first may not matter much. How‐\never, we typically lowercase the text before stemming. We also don’t remove tokens or\nlowercase the text before doing lemmatization because we have to know the part of\nspeech of the word to get its lemma, and that requires all tokens in the sentence to be\nintact. A good practice to follow is to prepare a sequential list of pre-processing tasks\nto be done after having a clear understanding of how to process our data.\nFigure 2-8 lists the different pre-processing steps we’ve seen in this subsection so far,\nas a quick summary.\nNote that these are the more common pre-processing steps, but they’re by no means\nexhaustive. Depending on the nature of the data, some additional pre-processing\nsteps may be important. Let’s take a look at a few of those steps.\nFigure 2-8. Common pre-processing steps on a blob of text\nOther Pre-Processing Steps\nSo far, we’ve seen a few common pre-processing steps in an NLP pipeline. While we\nhaven’t explicitly stated the nature of the texts, we have assumed that we’re dealing\nwith regular English text. What’s different if that’s not the case? Let’s introduce a few\nmore pre-processing steps to deal with such scenarios, using a few examples.\nPre-Processing \n| \n55\n",
      "word_count": 249,
      "char_count": 1448,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1142,
          "height": 884,
          "ext": "png",
          "size_bytes": 40145
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 86,
      "text": "Text normalization\nConsider a scenario where we’re working with a collection of social media posts to\ndetect news events. Social media text is very different from the language we’d see in,\nsay, newspapers. A word can be spelled in different ways, including in shortened\nforms, a phone number can be written in different formats (e.g., with and without\nhyphens), names are sometimes in lowercase, and so on. When we’re working on\ndeveloping NLP tools to work with such data, it’s useful to reach a canonical repre‐\nsentation of text that captures all these variations into one representation. This is\nknown as text normalization. Some common steps for text normalization are to con‐\nvert all text to lowercase or uppercase, convert digits to text (e.g., 9 to nine), expand\nabbreviations, and so on. A simple way to incorporate text normalization can be\nfound in Spacy’s source code [35], which is a dictionary showing different spellings of\na preset collection of words mapped to a single spelling. We’ll see more examples of\ntext normalization in Chapter 8.\nLanguage detection\nA lot of web content is in non-English languages. For example, say we’re asked to col‐\nlect all reviews about our product on the web. As we navigate different e-commerce\nwebsites and start crawling pages related to our product, we notice several non-\nEnglish reviews showing up. Since a majority of the pipeline is built with language-\nspecific tools, what will happen to our NLP pipeline, which is expecting English text?\nIn such cases, language detection is performed as the first step in an NLP pipeline.\nWe can use libraries like Polyglot [36] for language detection. Once this step is done,\nthe next steps could follow a language-specific pipeline.\nCode mixing and transliteration\nThe discussion above was about a scenario where the content is in non-English lan‐\nguages. However, there’s another scenario where a single piece of content is in more\nthan one language. Many people across the world speak more than one language in\ntheir day-to-day lives. Thus, it’s not uncommon to see them using multiple languages\nin their social media posts, and a single post may contain many languages. As an\nexample of code mixing, we can look at a Singlish (Singapore slang + English) phrase\nfrom LDC [37] in Figure 2-9.\nFigure 2-9. Code mixing in a single Singlish phrase\n56 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 396,
      "char_count": 2374,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1404,
          "height": 238,
          "ext": "png",
          "size_bytes": 30479
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 87,
      "text": "A single popular phrase has words from Tamil, English, Malay, and three Chinese\nlanguage variants. Code mixing refers to this phenomenon of switching between lan‐\nguages. When people use multiple languages in their write-ups, they often type words\nfrom these languages in Roman script, with English spelling. So, the words of another\nlanguage are written along with English text. This is known as transliteration. Both of\nthese phenomena are common in multilingual communities and need to be handled\nduring the pre-processing of text. We’ll discuss more about these in Chapter 8, where\nwe’ll see examples of these phenomena in social media text.\nThis concludes our discussion of common pre-processing steps. While this list is by\nno means exhaustive, we hope it gives you some idea of the different forms of pre-\nprocessing that may be required, depending on the nature of the dataset. Now, let’s\ntake a look at a few more pre-processing steps in the NLP pipeline—ones that need\nadvanced language processing beyond what we’ve seen so far.\nAdvanced Processing\nImagine we’re asked to develop a system to identify person and organization names\nin our company’s collection of one million documents. The common pre-processing\nsteps we discussed earlier may not be relevant in this context. Identifying names\nrequires us to be able to do POS tagging, as identifying proper nouns can be useful in\nidentifying person and organization names. How do we do POS tagging during the\npre-processing stage of the project? We’re not going into the details of how POS tag‐\ngers are developed (see Chapter 8 in [38] for details) in this book. Pre-trained and\nreadily usable POS taggers are implemented in NLP libraries such as NLTK, spaCy\n[39], and Parsey McParseface Tagger [40], and we generally don’t have to develop our\nown POS-tagging solutions. The following code snippet illustrates how to use many\nof the pre-built pre-processing functions we’ve discussed so far using the NLP library\nspaCy:\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(u'Charles Spencer Chaplin was born on 16 April 1889 toHannah Chaplin \n         (born Hannah Harriet\nPedlingham Hill) and Charles Chaplin Sr')\nfor token in doc:\n    print(token.text, token.lemma_, token.pos_,\n          token.shape_, token.is_alpha, token.is_stop)\nIn this simple snippet, we can see tokenization, lemmatization, POS tagging, and sev‐\neral other steps in action! Note that if needed we can add additional processing steps\nwith the same code snippet; we’ll leave that as an exercise for the reader. A point to\nnote is that there may be differences in the output among different NLP libraries for\nthe same pre-processing step. This is due in part to implementation differences and\nalgorithmic variations among different libraries. Which library (or libraries) you’ll\nPre-Processing \n| \n57\n",
      "word_count": 444,
      "char_count": 2840,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 88,
      "text": "eventually want to use in your project is a subjective decision based on the amount of\nlanguage processing you want.\nLet’s now consider a slightly different problem: along with identifying person and\norganization names in our company’s collection of one million documents, we’re also\nasked to identify if a given person and organization are related to each other in some\nway (e.g., Satya Nadella is related to Microsoft through the relation CEO). This is\nknown as the problem of relation extraction, which we’ll discuss in greater detail in\nChapter 5. But for now, think about what kind of pre-processing we need for this\ncase. We need POS tagging, which we already know how to add to our pipeline. We\nneed a way of identifying person and organization names, which is a separate infor‐\nmation extraction task known as named entity recognition (NER), which we’ll discuss\nin Chapter 5. Apart from these two, we need a way to identify patterns indicating\n“relation” between two entities in a sentence. This requires us to have some form of\nsyntactic representation of the sentence, such as parsing, which we saw in Chapter 1.\nFurther, we also want a way to identify and link multiple mentions of an entity (e.g.,\nSatya Nadella, Mr. Nadella, he, etc.). We accomplish this with the pre-processing step\nknown as coreference resolution. We saw an example of this in “An NLP Walkthrough:\nConversational Agents” on page 31. Figure 2-10 shows the output from Stanford Cor‐\neNLP [41], which illustrates a parser output and coreference resolution output for an\nexample sentence, along with other pre-processing steps we discussed previously.\nWhat we’ve seen so far in this section are some of the most common pre-processing\nsteps in a pipeline. They’re all available as pre-trained, usable models in different NLP\nlibraries. Apart from these, additional, customized pre-processing may be necessary,\ndepending on the application. For example, consider a case where we’re asked to\nmine the social media sentiment on our product. We start by collecting data from,\nsay, Twitter, and quickly realize there are tweets that are not in English. In such cases,\nwe may also need a language-detection step before doing anything else.\nAdditionally, what steps we need also depends on a specific application. If we’re creat‐\ning a system to identify whether the reviewer is expressing a positive or negative sen‐\ntiment about a movie from a review they wrote, we might not worry much about\nparsing or coreference resolution, but we would want to consider stop word removal,\nlowercasing, and removing digits. However, if we’re interested instead in extracting\ncalendar events from emails, we’ll probably be better off not removing stop words or\ndoing stemming, but rather including, say, parsing. In the case where we want to\nextract relationships between different entities in the text and events mentioned in it,\nwe would need coreference resolution, as we discussed previously. We’ll see examples\nof cases requiring such steps in Chapter 5.\n58 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 496,
      "char_count": 3049,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 89,
      "text": "Figure 2-10. Output from different stages of NLP pipeline processing\nFinally, we have to consider the step-by-step procedures of pre-processing in each\ncase, as summarized in Figure 2-11.\nPre-Processing \n| \n59\n",
      "word_count": 31,
      "char_count": 210,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1376,
          "height": 1714,
          "ext": "png",
          "size_bytes": 142057
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 90,
      "text": "Figure 2-11. Advanced pre-processing steps on a blob of text\nFor example, POS tagging cannot be preceded by stop word removal, lowercasing,\netc., as such processing affects POS tagger output by changing the grammatical struc‐\nture of the sentence. How a particular pre-processing step is helping a given NLP\nproblem is another question that is specific to the application, and it can only be\nanswered with a lot of experimentation. We’ll discuss more specific pre-processing\nrequired for different NLP applications in upcoming chapters. For now, let’s move on\nto the next step: feature engineering.\nFeature Engineering\nSo far, we’ve seen different pre-processing steps and where they can be useful. When\nwe use ML methods to perform our modeling step later, we’ll still need a way to feed\nthis pre-processed text into an ML algorithm. Feature engineering refers to the set of\nmethods that will accomplish this task. It’s also referred to as feature extraction. The\ngoal of feature engineering is to capture the characteristics of the text into a numeric\nvector that can be understood by the ML algorithms. We refer to this step as “text\nrepresentation” in this book, and it’s the topic of Chapter 3. We also detail feature\nextraction in the context of developing a complete NLP pipeline and iterating to\nimprove performance in Chapter 11. Here, we’ll briefly touch on two different\napproaches taken in practice for feature engineering in (1) a classical NLP and tradi‐\ntional ML pipeline and (2) a DL pipeline. Figure 2-12 (adapted from [42]) distin‐\nguishes the two approaches.\n60 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 264,
      "char_count": 1610,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1206,
          "height": 884,
          "ext": "png",
          "size_bytes": 57944
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 91,
      "text": "Figure 2-12. Feature engineering for classical NLP versus DL-based NLP\nFeature Engineering \n| \n61\n",
      "word_count": 14,
      "char_count": 98,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1397,
          "height": 2046,
          "ext": "png",
          "size_bytes": 190846
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 92,
      "text": "Classical NLP/ML Pipeline\nFeature engineering is an integral step in any ML pipeline. Feature engineering steps\nconvert the raw data into a format that can be consumed by a machine. These trans‐\nformation functions are usually handcrafted in the classical ML pipeline, aligning to\nthe task at hand. For example, imagine a task of sentiment classification on product\nreviews in e-commerce. One way to convert the reviews into meaningful “numbers”\nthat helps predict the reviews’ sentiments (positive or negative) would be to count the\nnumber of positive and negative words in each review. There are statistical measures\nfor understanding if a feature is useful for a task or not; we’ll discuss this in Chap‐\nter 11. The main takeaway for building classical ML models is that the features are\nheavily inspired by the task at hand as well as domain knowledge (for example, using\nsentiment words in the review example). One of the advantages of handcrafted fea‐\ntures is that the model remains interpretable—it’s possible to quantify exactly how\nmuch each feature is influencing the model prediction.\nDL Pipeline\nThe main drawback of classical ML models is the feature engineering. Handcrafted\nfeature engineering becomes a bottleneck for both model performance and the model\ndevelopment cycle. A noisy or unrelated feature can potentially harm the model’s per‐\nformance by adding more randomness to the data. Recently, with the advent of DL\nmodels, this approach has changed. In the DL pipeline, the raw data (after pre-\nprocessing) is directly fed to a model. The model is capable of “learning” features\nfrom the data. Hence, these features are more in line with the task at hand, so they\ngenerally give improved performance. But, since all these features are learned via\nmodel parameters, the model loses interpretability. It’s very hard to explain a DL\nmodel’s prediction, which is a disadvantage in a business-driven use case. For exam‐\nple, when identifying an email as ham or spam, it might be worth knowing which\nword or phrases played the significant role in making the email ham or spam. While\nthis is easy to do with handcrafted features, it’s not easy in the case of DL models.\nAs we’ve already mentioned, feature engineering is heavily task specific, so we discuss\nit throughout the book in the context of textual data and a range of tasks. With a\nhigh-level understanding of feature engineering, now let’s take a look at the next step\nin the pipeline, which we call modeling.\nModeling\nWe now have some amount of data related to our NLP project and a clear idea of\nwhat sort of cleaning up and pre-processing needs to be done and what features are\nto be extracted. The next step is about how to build a useful solution out of this. At\nthe start, when we have limited data, we can use simpler methods and rules. Over\n62 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 483,
      "char_count": 2856,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 93,
      "text": "time, with more data and a better understanding of the problem, we can add more\ncomplexity and improve performance. We’ll cover this process in this section.\nStart with Simple Heuristics\nAt the very start of building a model, ML may not play a major role by itself. Part of\nthat could be due to a lack of data, but human-built heuristics can also provide a great\nstart in some ways. Heuristics may already be part of your system, either implicitly or\nexplicitly. For instance, in email spam-classification tasks, we may have a blacklist of\ndomains that are used exclusively to send spam. This information can be used to filter\nemails from those domains. Similarly, a blacklist of words in an email that denote a\nhigh chance of spam could also be used for this classification.\nSuch heuristics can be found in a range of tasks, especially at the start of applying ML.\nIn an e-commerce setting, we may use a heuristic based on the number of purchases\nfor ordering search results and show products belonging to the same category as rec‐\nommendations while we collect data that could be used to build a larger, collabora‐\ntive, filtering-based system that can recommend products using a range of other\ncharacteristics based on what customers with similar buying profiles purchased.\nAnother popular approach to incorporating heuristics in your system is using regular\nexpressions. Let’s say we’re developing a system to extract different forms of informa‐\ntion from text documents, such as dates and phone numbers, names of people who\nwork in a given organization, etc. While some information, such as email IDs, dates,\nand telephone numbers can be extracted using normal (albeit complex) regular\nexpressions, Stanford NLP’s TokensRegex [43] and spaCy’s rule-based matching [20]\nare two tools that are useful for defining advanced regular expressions to capture\nother information, such as people who work in a specific organization. Figure 2-13\nshows an example of spaCy’s rule-based matcher in action.\nThis shows a pattern that looks for text containing the lemma “match,” appearing as a\nnoun, optionally preceded by an adjective, and followed by any word form of lemma\n“be.” Such patterns are an advanced form of regular expressions, which require some\nof the NLP pre-processing steps we saw earlier in this chapter. In the absence of large\namounts of training data, and when we have some domain knowledge, we can start\nbuilding systems by encoding this knowledge in the form of rules/heuristics. Even\nwhen we’re building ML-based models, we can use such heuristics to handle special\ncases—for example, cases where the model has failed to learn well. Thus, simple heu‐\nristics can give us a good starting point and be useful in ML models. Now, assuming\nwe built such a heuristics-based system, where do we go from there?\nModeling \n| \n63\n",
      "word_count": 468,
      "char_count": 2833,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 94,
      "text": "Figure 2-13. spaCy’s rule-based matcher\nBuilding Your Model\nWhile a set of simple heuristics is a good start, as our system matures, adding newer\nand newer heuristics may result in a complex, rule-based system. Such a system is\nhard to manage, and it can be even harder to diagnose the cause of errors. We need a\nsystem that’s easier to maintain as it matures. Further, as we collect more data, our\nML model starts beating pure heuristics. At that point, a common practice is to com‐\nbine heuristics directly or indirectly with the ML model. There are two broad ways of\ndoing that:\nCreate a feature from the heuristic for your ML model\nWhen there are many heuristics where the behavior of a single heuristic is deter‐\nministic but their combined behavior is fuzzy in terms of how they predict, it’s\nbest to use these heuristics as features to train your ML model. For instance, in\nthe email spam-classification example, we can add features, such as the number\nof words from the blacklist in a given email or the email bounce rate, to the ML\nmodel.\n64 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 192,
      "char_count": 1079,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1109,
          "height": 842,
          "ext": "png",
          "size_bytes": 96169
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 95,
      "text": "i. This is different from the vertical stacking done in neural networks like LSTM.\nPre-process your input to the ML model\nIf the heuristic has a really high prediction for a particular kind of class, then it’s\nbest to use it before feeding the data in your ML model. For instance, if for cer‐\ntain words in an email, there’s a 99% chance that it’s spam, then it’s best to classify\nthat email as spam instead of sending it to an ML model.\nAdditionally, we have NLP service providers, such as Google Cloud Natural Language\n[44], Amazon Comprehend [45], Microsoft Azure Cognitive Services [46], and IBM\nWatson Natural Language Understanding [47], which provide off-the-shelf APIs to\nsolve various NLP tasks. If your project has an NLP problem that’s addressed by these\nAPIs, you can start by using them to get an estimate of the feasibility of the task and\nhow good your existing dataset is. Once you’re comfortable that the task is feasible\nand conclude that the off-the-shelf models give reasonable results, you can move\ntoward building custom ML models and improving them.\nBuilding THE Model\nWe’ve seen examples of getting started building an NLP system by using heuristics or\nexisting APIs, or by building our own ML models. We start with a baseline approach\nand work toward improving it. We may have to do many iterations of the model-\nbuilding process to “build THE model” that gives good performance and is also\nproduction-ready. We cover some of the approaches to address this issue here:\nEnsemble and stacking\nIn our experience, a common practice is not to have a single model, but to use a\ncollection of ML models, often dealing with different aspects of the prediction\nproblem. There are two ways of doing this: we can feed one model’s output as\ninput for another model, thus sequentially going from one model to another and\nobtaining a final output. This is called model stacking.i Alternatively, we can also\npool predictions from multiple models and make a final prediction. This is called\nmodel ensembling. Figure 2-14 demonstrates both of these procedures.\nIn this figure, training data is used to build Models 1, 2, and 3. Outputs of these\nmodels are then combined to be used in a meta-model (a model that uses other\nmodels) to predict the final outcome. For example, in the email spam-\nclassification case, we can assume that we run three different models: a heuristic-\nbased score, Naive Bayes, and LSTM. The output of these three models is then\nfed into the meta-model based on logistic regression, which then gives the chan‐\nces of the email being spam or not. As the product grows in terms of its features,\nthe model will also grow in complexity. So, we may eventually end up using a\nModeling \n| \n65\n",
      "word_count": 469,
      "char_count": 2718,
      "fonts": [
        "MinionPro-Regular (8.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Regular (6.3pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 96,
      "text": "combination of all of these—i.e., heuristics, machine learning, and stacked and\nensemble models—as part of a large product.\nFigure 2-14. Model ensemble and stacking\nBetter feature engineering\nFor both API-based and custom-built models, feature engineering is an impor‐\ntant step, and it evolves throughout the process. A better feature engineering step\nmay lead to better performance. For instance, if there are a lot of features, then\nwe use feature selection to find a better model. We detail strategies for iterating\nfeature engineering to achieve an optimal setting in Chapter 11.\nTransfer learning\nApart from model stacking or ensemble, there is a newer trend that’s becoming\npopular in the NLP community—transfer learning, which we introduced in\nChapter 1. Often, the model needs external knowledge beyond the dataset for the\ntask to understand the language and the problem well. Transfer learning tries to\ntransfer preexisting knowledge from a big, well-trained model to a newer model\nat its initial phase. Afterward, the new model slowly adapts to the task at hand.\nThis is analogous to a teacher transferring wisdom and knowledge to a student.\nTransfer learning provides a better initialization, which helps in the downstream\ntasks, especially when the dataset for the downstream task is smaller. In these\ncases, transfer learning yields better results than just initializing a downstream\nmodel from scratch with random initialization. As an example, for email spam\nclassification, we can use BERT to fine-tune the email dataset. We cover BERT in\ngreater detail in Chapters 4 through 6.\n66 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 255,
      "char_count": 1627,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1409,
          "height": 817,
          "ext": "png",
          "size_bytes": 65947
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 97,
      "text": "Reapplying heuristics\nNo ML model is perfect. Hence, ML models still make mistakes. It’s possible to\nrevisit these cases again at the end of the modeling pipeline to find any common\npattern in errors and use heuristics to correct them. We can also apply domain-\nspecific knowledge that is not automatically captured in the data to refine the\nmodel predictions. An analogy for reapplying heuristics would be our model as a\ntrapeze artist performing great feats and these rules as the safety net so the artist\ndoesn’t fall off.\nBetween the stage of having no data, when we fully rely on heuristics, to a lot of data,\nwhere we can try a range of modeling techniques, we encounter a situation where we\nhave a small amount of data, which is often not sufficient to build good ML models.\nIn such scenarios, one approach to follow is active learning, where we can use user\nfeedback or other such sources to continuously collect new data to build better mod‐\nels. We’ll discuss this in detail in Chapter 4. As we’ve just seen, modeling strategies\ndepend heavily on the data at hand. Table 2-1 provides a range of decision paths\ngiven our data volume and quality, based on our experience.\nTable 2-1. Data attributes and associated decision paths\nData attribute\nDecision path\nExamples\nLarge data volume\nCan use techniques that require more data, like\nDL. Can use a richer set of features as well.\nIf the data is sufficiently large but unlabeled,\nwe can also apply unsupervised techniques.\nIf we have a lot of reviews and metadata\nassociated with them, we can build a sentiment-\nanalysis tool from scratch.\nSmall data volume\nNeed to start with rule-based or traditional ML\nsolutions that are less data hungry. Can also\nadapt cloud APIs and generate more data with\nweak supervision.\nWe can also use transfer learning if there’s a\nsimilar task that has large data.\nThis often happens at the start of a completely\nnew project.\nData quality is poor\nand the data is\nheterogeneous in\nnature\nMore data cleaning and pre-processing might\nbe required.\nThis entails issues like code mixing (different\nlanguages being mixed in the same sentence),\nunconventional language, transliteration, or noise\n(like social media text).\nData quality is good\nCan directly apply off-the-shelf algorithms or\ncloud APIs more easily.\nLegal text or newspapers.\nData consists of full-\nlength documents\nChoose the right strategy for breaking the\ndocument into lower levels, like paragraphs,\nsentences, or phrases, depending on the\nproblem.\nDocument classification, review analysis, etc.\nSo far, we’ve seen an overview of different forms of modeling that can be useful in an\nNLP pipeline and what modeling path to choose based on the data we have. Super‐\nvised learning, especially classification, is the most common modeling process you’ll\nModeling \n| \n67\n",
      "word_count": 468,
      "char_count": 2812,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 98,
      "text": "encounter in the NLP projects you’ll be building in an industry scenario. We’ll discuss\nclassification models in Chapter 4 and models used for different application scenarios\nin NLP in Chapters 5 through 7. Now, let’s take a look at the next step in the pipeline:\nevaluation.\nEvaluation\nA key step in the NLP pipeline is to measure how good the model we’ve built is.\n“Goodness” of a model can have multiple meanings, but the most common interpre‐\ntation is the measure of the model’s performance on unseen data. Success in this\nphase depends on two factors: (1) using the right metric for evaluation, and (2) fol‐\nlowing the right evaluation process. Let’s first focus on 1. Depending on the NLP task\nor problem, the evaluation metrics can vary. They can also vary depending on the\nphase: the model building, deployment, and production phases. Whereas in the first\ntwo phases, we typically use ML metrics, in the final phase, we also include business\nmetrics to measure business impact.\nAlso, evaluations are of two types: intrinsic and extrinsic. Intrinsic focuses on inter‐\nmediary objectives, while extrinsic focuses on evaluating performance on the final\nobjective. For example, consider a spam-classification system. The ML metric will be\nprecision and recall, while the business metric will be “the amount of time users spent\non a spam email.” Intrinsic evaluation will focus on measuring the system perfor‐\nmance using precision and recall. Extrinsic evaluation will focus on measuring the\ntime a user wasted because a spam email went to their inbox or a genuine email went\nto their spam folder.\nIntrinsic Evaluation\nIn this section, we’ll look at some intrinsic evaluation metrics that are commonly used\nto measure NLP systems. For most metrics in this category, we assume a test set\nwhere we have the ground truth or labels (human annotated, correct answers). Labels\ncould be binary (e.g., 0/1 for text classification), one-to-two words (e.g., names for\nnamed entity recognition), or large text itself (e.g., text translated by machine transla‐\ntion). The output of the NLP model on a data point is compared against the corre‐\nsponding label for that data point, and metrics are calculated based on the match (or\nmismatch) between the output and label. For most NLP tasks, the comparison can be\nautomated, hence intrinsic evaluation can be automated. For some cases, like\nmachine translation or summarization, it’s not always possible to automate evaluation\nsince comparison is not subjective.\nTable 2-2 lists various metrics used for intrinsic evaluation across various NLP tasks.\nFor a more detailed discussion of the metrics, refer to the corresponding reference.\n68 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 438,
      "char_count": 2707,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 99,
      "text": "Table 2-2. Popular metrics and NLP applications where they’re used\nMetric\nDescription\nApplications\nAccuracy [48]\nUsed when the output variable is categorical or discrete. It\ndenotes the fraction of times the model makes correct\npredictions as compared to the total predictions it makes.\nMainly used in classification tasks, such as\nsentiment classification (multiclass), natural\nlanguage inference (binary), paraphrase\ndetection (binary), etc.\nPrecision [48]\nShows how precise or exact the model’s predictions are,\ni.e., given all the positive (the class we care about) cases,\nhow many can the model classify correctly?\nUsed in various classification tasks, especially\nin cases where mistakes in a positive class\nare more costly than mistakes in a negative\nclass, e.g., disease predictions in healthcare.\nRecall [48]\nRecall is complementary to precision. It captures how well\nthe model can recall positive class, i.e., given all the\npositive predictions it makes, how many of them are\nindeed positive?\nUsed in classification tasks, especially where\nretrieving positive results is more important,\ne.g., e-commerce search and other\ninformation-retrieval tasks.\nF1 score [49]\nCombines precision and recall to give a single metric,\nwhich also captures the trade-off between precision and\nrecall, i.e., completeness and exactness.\nF1 is defined as (2 × Precision × Recall) / (Precision +\nRecall).\nUsed simultaneously with accuracy in most\nof the classification tasks. It is also used in\nsequence-labeling tasks, such as entity\nextraction, retrieval-based questions\nanswering, etc.\nAUC [48]\nCaptures the count of positive predictions that are correct\nversus the count of positive predictions that are incorrect\nas we vary the threshold for prediction.\nUsed to measure the quality of a model\nindependent of the prediction threshold. It is\nused to find the optimal prediction threshold\nfor a classification task.\nMRR (mean\nreciprocal rank)\n[50]\nUsed to evaluate the responses retrieved given their\nprobability of correctness. It is the mean of the reciprocal\nof the ranks of the retrieved results.\nUsed heavily in all information-retrieval\ntasks, including article search, e-commerce\nsearch, etc.\nMAP (mean\naverage\nprecision) [51]\nUsed in ranked retrieval results, like MRR. It calculates the\nmean precision across each retrieved result.\nUsed in information-retrieval tasks.\nRMSE (root\nmean squared\nerror) [48]\nCaptures a model’s performance in a real-value prediction\ntask. Calculates the square root of the mean of the squared\nerrors for each data point.\nUsed in conjunction with MAPE in the case of\nregression problems, from temperature\nprediction to stock market price prediction.\nMAPE (mean\nabsolute\npercentage\nerror) [52]\nUsed when the output variable is a continuous variable. It\nis the average of absolute percentage error for each data\npoint.\nUsed to test the performance of a regression\nmodel.\nIt is often used in conjunction with RMSE.\nBLEU (bilingual\nevaluation\nunderstudy) [53]\nCaptures the amount of n-gram overlap between the\noutput sentence and the reference ground truth sentence.\nIt has many variants.\nMainly used in machine-translation tasks.\nRecently adapted to other text-generation\ntasks, such as paraphrase generation and\ntext summarization.\nMETEOR [54]\nA precision-based metric to measure the quality of text\ngenerated. It fixes some of the drawbacks of BLEU, such as\nexact word matching while calculating precision. METEOR\nallows synonyms and stemmed words to be matched with\nthe reference word.\nMainly used in machine translation.\nEvaluation \n| \n69\n",
      "word_count": 537,
      "char_count": 3567,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MinionPro-It (10.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 100,
      "text": "Metric\nDescription\nApplications\nROUGE [55]\nAnother metric to compare quality of generated text with\nrespect to a reference text. As opposed to BLEU, it\nmeasures recall.\nSince it measures recall, it’s mainly used for\nsummarization tasks where it’s important to\nevaluate how many words a model can\nrecall.\nPerplexity [56]\nA probabilistic measure that captures how confused an\nNLP model is. It’s derived from the cross-entropy in a next\nword prediction task. The exact definition can be found at\n[56].\nUsed to evaluate language models. It can\nalso be used in language-generation tasks,\nsuch as dialog generation.\nApart from the list of metrics shown in Table 2-2, there are few more metrics and vis‐\nualizations that are often used for solving NLP problems. While we’ve covered these\ntopics briefly here, we encourage you to follow the references and learn more about\nthese metrics.\nIn the case of classification tasks, a commonly used visual evaluation method is a con‐\nfusion matrix. It allows us to inspect the actual and predicted output for different\nclasses in the dataset. The name stems from the fact that it helps to understand how\n“confused” the classification model is in terms of identifying different classes. A con‐\nfusion matrix is in turn used to compute metrics such as precision, recall, F1 score,\nand accuracy. We’ll see how to use confusion matrices in Chapter 4.\nRanking tasks like information search and retrieval mostly uses ranking-based met‐\nrics, such as MRR and MAP, but usual classification metrics can be used, too. In the\ncase of retrieval, we care mainly about recall, so recall at various ranks is calculated.\nFor example, for information retrieval, a common metric is “Recall at rank K”; it\nlooks for the presence of ground truth in top K retrieved results. If present, it’s a\nsuccess.\nWhen it comes to text-generation tasks, there are a number of metrics that are used,\ndepending on the task. Even though BLEU and METEOR are good metrics for\nmachine translation, they may not be good metrics when applied to other generation\ntasks. For example, in the case of dialog generation, the ground truth is one of the\ncorrect answers, but there could be many variations in responses that are not listed.\nIn cases like this, precision-based metrics such as BLEU and METEOR will com‐\npletely fail to capture the task performance faithfully. For these reasons, perplexity is\none metric that’s used extensively to understand a model’s text-generation ability.\nHowever, any evaluation scheme for text generation is not perfect. This is because\nthere could be multiple sentences that have the same meaning, and it’s not possible to\nhave all the variations listed as ground truth. Therefore, the text generated and the\nground truth can have the same meaning but be different sentences. This makes auto‐\nmated evaluation a difficult process. For example, say we build a machine-translation\nmodel that converts sentences from French to English. Consider the following sen‐\ntence in French: “J’ai mangé trois filberts.” In English, this means, “I ate three filberts.”\n70 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 510,
      "char_count": 3112,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 101,
      "text": "So, we put this sentence as the label. Say our model generates the following English\ntranslation: “I ate three hazelnuts.” Since the output does not match the label, auto‐\nmated evaluation will say the output is incorrect. But this evaluation is incorrect\nbecause English speakers are known to refer to filberts as hazelnuts. Even if we add\nthis sentence as a possible label, our model could still generate “I have eaten three\nhazelnuts” as output. Yet again, the automated evaluation will say the model got it\nwrong since the output does not match either of the two labels. This is where human\nevaluation comes into play. But human evaluation can be expensive both in terms of\ntime and money.\nExtrinsic Evaluation\nLike we said earlier, extrinsic evaluation focuses on evaluating the model perfor‐\nmance on the final objective. In industrial projects, any AI model is built with the aim\nof solving a business problem. For example, a regression model is built with the aim\nof ranking the emails of the users and bringing the most important emails to the top\nof the inbox, thereby helping the users of an email service save time. Consider a sce‐\nnario where the regression model does well on the ML metrics but doesn’t really save\na lot of time for the email service users, or where a question-answering model does\nvery well on intrinsic metrics but fails to address a large number of questions in the\nproduction environment. Would we call such models successful? No, because they\nfailed to achieve their business objectives. While this is not an issue for researchers in\nacademia, for practitioners in industry, it’s very important.\nThe way to carry out extrinsic evaluation is to set up the business metrics and the\nprocess to measure them correctly at the start of the project. We’ll see examples of the\nright business metrics in later chapters.\nWe might ask: if extrinsic evaluation is what matters, why do intrinsic evaluation at\nall? The reason we must do intrinsic evaluation before extrinsic evaluation is that\nextrinsic evaluation often includes project stakeholders outside the AI team—some‐\ntimes even end users. Intrinsic evaluation can be done mostly by the AI team itself.\nThis makes extrinsic evaluation a much more expensive process as compared to\nintrinsic evaluation. Therefore, intrinsic evaluation is used as a proxy for extrinsic\nevaluation. Only when we get consistently good results in intrinsic evaluation should\nwe go for extrinsic evaluation.\nAnother thing to remember is that bad results in intrinsic evaluation often imply bad\nresults in extrinsic evaluation. However, the converse may not be true. That is, we can\nhave a model that does very well in intrinsic evaluation but does badly in extrinsic\nevaluation, but it’s unlikely that a model that does well in extrinsic evaluation did\npoorly during intrinsic evaluation. The reasons for poor performance in extrinsic\nevaluation could be many, from setting up the wrong metrics to not having suitable\nEvaluation \n| \n71\n",
      "word_count": 493,
      "char_count": 2995,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 102,
      "text": "data or having wrong expectations. We touched on some of these in Chapter 1 and\nwill discuss them in more detail in Chapter 11.\nSo far, we’ve seen some metrics commonly used for intrinsic evaluation and also dis‐\ncussed the importance of extrinsic evaluation to measure the performance of NLP\nmodels. There are some more metrics that are task specific, which are not seen across\ndifferent NLP application scenarios. We’ll discuss such evaluation measures in detail\nas we cover these specific applications in upcoming chapters. With this, now let’s look\nat the next components of the pipeline: model deployment, monitoring, and\nupdating.\nPost-Modeling Phases\nOnce our model has been tried and tested, we move on to the post-modeling phase:\ndeploying, monitoring, and updating the model. We’ll cover these briefly in this\nsection.\nDeployment\nIn most practical application scenarios, the NLP module we’re implementing is a part\nof a larger system (e.g., a spam-classification system in a larger email application).\nThus, working through the processing, modeling, and evaluation pipeline is only a\npart of the story. Eventually, once we’re happy with one final solution, it needs to be\ndeployed in a production environment as a part of a larger system. Deployment\nentails plugging the NLP module into the broader system. It may also involve making\nsure input and output data pipelines are in order, as well as making sure our NLP\nmodule is scalable under heavy load.\nAn NLP module is typically deployed as a web service. Let’s say we designed a web\nservice that takes a text as input and returns the email’s category (spam or non-spam)\nas output. Now, each time someone gets a new email, it goes to the microservice,\nwhich classifies the email text. This, in turn, can be used to make a decision about\nwhat to do with the email (either show it or send it to the spam folder). In certain\ncircumstances, like batch processing, the NLP module is deployed in the larger task\nqueue. As an example, take a look at task queues in Google Cloud [57] or AWS [58].\nWe’ll cover deployment in more detail in Chapter 11.\nMonitoring\nLike with any software engineering project, extensive software testing has to be done\nbefore final deployment, and the model performance is monitored constantly after\ndeploying. Monitoring for NLP projects and models has to be handled differently\nthan a regular engineering project, as we need to ensure that the outputs produced by\nour models daily make sense. If we’re automatically training the model frequently, we\n72 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 426,
      "char_count": 2564,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 103,
      "text": "have to make sure that the models behave in a reasonable manner. Part of this is done\nthrough a performance dashboard showing the model parameters and key perfor‐\nmance indicators. We’ll discuss this more in Chapter 11.\nModel Updating\nOnce the model is deployed and we start gathering new data, we’ll iterate the model\nbased on this new data to stay current with predictions. We cover this model update\nfor each task throughout the book, especially in Chapters 4 through 7 and Chap‐\nter 11. As a start, Table 2-3 gives some guidance on how to approach the model\nupdating process for different post-deployment scenarios.\nTable 2-3. Project attribute and associated decision paths\nProject attribute\nDecision paths\nExamples\nMore training data is generated\npost-deployment.\nOnce deployed, extracted signals can be used to\nautomatically improve the model. Can also try online\nlearning to train the model automatically on a daily\nbasis.\nAbuse-detection systems where\nusers flag data.\nTraining data is not generated\npost-deployment.\nManual labeling could be done to improve evaluation\nand the models.\nIdeally, each new model has to be manually built and\nevaluated.\nA subset of a larger NLP pipeline\nwith no direct feedback.\nLow model latency is required,\nor model has to be online with\nnear-real-time response.\nNeed to use models that can be inferred quickly.\nAnother option is to create memoization strategies like\ncaching or have substantially bigger computing power.\nSystems that need to respond\nright away, like any chatbot or\nan emergency tracking system.\nLow model latency is not\nrequired, or model can be run in\nan offline fashion.\nCan use more advanced and slower models. This can\nalso help in optimizing costs where feasible.\nSystems that can be run on a\nbatch process, like retail product\ncatalog analysis.\nWorking with Other Languages\nSo far, our discussion has assumed that we’re dealing mostly with English text.\nDepending on the task at hand, we may need to build models and solutions for other\nlanguages as well. How we approach this will change based on what language we’re\ndealing with. The pipeline for some languages may be very similar to English,\nwhereas some languages and scenarios may require us to rethink how we approach\nthe problem. We’ve compiled some action points for dealing with different languages\nin Table 2-4, based on our experiences working on projects that involve non-English\nlanguage processing.\nWorking with Other Languages \n| \n73\n",
      "word_count": 399,
      "char_count": 2465,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 104,
      "text": "Table 2-4. Language attribute and action plan\nLanguage\nattribute\nExample and languages\nAction\nHigh-resource\nlanguages\nLanguages that have both ample data as well\nas pre-built models.\nExamples include English, French, and Spanish.\nPossible to use pre-trained DL models. Easier to use.\nLow-resource\nlanguages\nLanguages that have limited data and recent\ndigital adoption. May not have pre-built\nmodels.\nExamples include Swahili, Burmese, and\nUzbek.\nDepending on the task, may need to label more data\nas well as explore individual components.\nMorphologically\nrich\nLinguistic and grammatical information like\nsubject, object, predicate, tense, and mode are\nnot separate words, but are joined together.\nExamples include Latin, Turkish, Finnish, and\nMalayalam.\nIf the language is not resource rich, we’ll need to\nexplore morphological analyzers that exist for the\nlanguage. In the worst case, manual rules to handle\ncertain cases might be needed.\nVocabulary\nvariation heavy\nNonstandard spellings and high word\nvariation.\nFor Arabic and Hindi, the spellings are\nnonstandard.\nIf the language is not resource rich, then we may need\nto first normalize the words/spellings before training\nany model.\nThis may not be needed for languages with large\ndatasets, as they can still learn of vocabulary variation.\nCJK languages\nThese languages are derived from ancient\nChinese characters. They’re not alphabet based\nand have several thousand characters for basic\nliteracy and over 40,000 characters for larger\ncoverage. Thus, they have to be handled\ndifferently.\nThey include Chinese, Japanese, and Korean,\nhence the name CJK.\nUse specific tokenization schemes in these languages.\nGiven that an ample amount of CJK data is available,\nit’s possible to build NLP models for various tasks from\nscratch.\nThere are also pre-trained models for them.\nTransfer learning from models trained in other\nlanguages beyond CJK may not be useful in this case.\nNext, we’ll turn our attention to a case study that will put all these steps together.\nCase Study\nSo far, we’ve seen different stages of an NLP pipeline. At each stage, we discussed\nwhat it’s about, why it’s useful, and how it fits into the general framework of an NLP\npipeline. However, we tackled these individual stages separately, away from the over‐\nall context. How do all these stages work together in a real-world NLP system pipe‐\nline? Let’s see a case study, using Uber’s tool to improve customer care: Customer\nObsession Ticketing Assistant (COTA).\nUber operates in 400+ cities worldwide, and cbased on the number of people who use\nUber every day, we can expect that their customer support teams receive several hun‐\ndreds of thousands of tickets on different issues each day. There are a couple of solu‐\ntions to choose from for a given ticket. The goal of COTA is to rank these solutions\n74 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 454,
      "char_count": 2857,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 105,
      "text": "and pick the best possible one. Uber developed COTA using ML and NLP techniques\nto enable better customer support and quick and efficient resolution of such tickets.\nFigure 2-15 shows the pipeline in Uber’s COTA and the various NLP components in\nit.\nFigure 2-15. NLP pipeline for ranking tickets in a ticketing system by Uber [59]\nThe information needed to identify the ticket issue and select a solution in this sys‐\ntem comes from three sources, as shown in the figure. Ticket text is, as the name indi‐\ncates, textual content, which is where NLP comes into the picture. After cleaning up\nthe text by removing HTML tags (not shown in the figure), the pre-processing steps\nconsist of tokenization, lowercasing, stop word removal, and lemmatization. We saw\nhow to do all of these earlier in this chapter. After pre-processing, the ticket text is\nrepresented as a collection of words (known as a bag of words and discussed in detail\nin Chapter 3).\nThe next step in this pipeline is feature engineering. The bag of words we obtained\nearlier is fed to two NLP modules—TF-IDF (term frequency and inverse document\nfrequency) and LSI (latent semantic indexing)—which are used to understand the\nmeaning of a text using this bag of words representation. This process comes under\nthe NLP task called topic modeling, which we’ll discuss in Chapter 7. Exactly how\nUber uses these NLP tasks in this context is an interesting idea: Uber collects the his‐\ntorical tickets for each solution from their database, forms a bag-of-words vector rep‐\nresentation for each solution, and creates a topic model based on these\nrepresentations. An incoming ticket is then mapped to this topic space of solutions,\ncreating a vector representation for the ticket. Cosine similarity is a common measure\nof similarity between any two vectors. It is used to create a vector where each element\nindicates the ticket text’s similarity to one solution. Thus, at the end of this feature\nengineering step, we end up with a representation indicating the ticket text’s similar‐\nity to all possible solutions.\nCase Study \n| \n75\n",
      "word_count": 348,
      "char_count": 2088,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1428,
          "height": 648,
          "ext": "png",
          "size_bytes": 57628
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 106,
      "text": "In the next stage, modeling, this representation is combined with ticket information\nand trip data to build a ranking system that shows the three best solutions for the\nticket. Under the hood, this ranking model consists of a binary classification system,\nwhich classifies each ticket-solution combination as a match or mismatch. The\nmatches are then ranked based on a scoring function. [59] describes more details on\nthe implementation of this system pipeline.\nThe next step in our pipeline is evaluation. How does evaluation work in this context?\nWhile the evaluation of model performance itself can be done in terms of an intrinsic\nevaluation measure such as MRR, the overall effectiveness of this approach is evalu‐\nated extrinsically. It’s estimated that COTA’s quick ticket resolution saves Uber tens of\nmillions of dollars every year.\nAs we learned earlier, a model is not built just once. COTA, too, was continually\nexperimented with and improved upon. After exploring a range of DL architectures,\nthe best solution that was ultimately chosen resulted in a 10% greater accuracy com‐\npared to the previous version with the binary classification-based ranking system.\nThe process does not end here, though. As we can see from the COTA team’s article\n[59], it’s a continuous process of model deployment, monitoring, and updating.\nWrapping Up\nIn this chapter, we saw the different steps involved in developing an NLP pipeline for\na given project description and saw a detailed case study of a real-world application.\nWe also saw how a traditional NLP pipeline and a DL-based NLP pipeline differ from\neach other and learned what to do when working with non-English languages. Aside\nfrom the case study, we looked at these steps in a more general manner in this chap‐\nter. Specific details for each step will depend on the task at hand and the purpose of\nour implementation. We’ll look at a few task-specific pipelines from Chapter 4\nonward, describing in detail what’s unique as well as common across different tasks\nwhile designing such pipelines. In the next chapter, we’ll tackle the question of text\nrepresentation that we mentioned briefly earlier in this chapter.\nReferences\n[1] Iderhoff, Nicolas. nlp-datasets: Alphabetical list of free/public domain datasets\nwith text data for use in Natural Language Processing (NLP), (GitHub repo). Last\naccessed June 15, 2020.\n[2] Google. “Dataset Search”. Last accessed June 15, 2020.\n[3] Miller, George A. “WordNet: A Lexical Database for English.” Communications of\nthe ACM 38.11 (1995): 39–41.\n[4] NTLTK documentation. “WordNet Interface”. Last accessed June 15, 2020.\n76 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 423,
      "char_count": 2652,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 107,
      "text": "[5] Xie, Qizhe, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le.\n“Unsupervised Data Augmentation for Consistency Training”. (2019).\n[6] Wikipedia. “Fat-finger error”. Last modified January 26, 2020.\n[7] Snorkel. “Programmatically Building and Managing Training Data”. Last accessed\nJune 15, 2020.\n[8] Ratner, Alexander, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and\nChristopher Ré. “Snorkel: Rapid Training Data Creation with Weak Supervision.” The\nVLDB Journal 29 (2019): 1–22.\n[9] Bach, Stephen H., Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cas‐\nsandra Xia, Souvik Sen et al. “Snorkel DryBell: A Case Study in Deploying Weak\nSupervision at Industrial Scale”. (2018).\n[10] Wei, Jason W., and Kai Zou. “Eda: Easy Data Augmentation Techniques for\nBoosting Performance on Text Classification Tasks”, (2019).\n[11] GitHub repository for [10]. Last accessed June 15, 2020.\n[12] Ma, Edward. nplaug: Data augmentation for NLP, (GitHub repo). Last accessed\nJune 15, 2020.\n[13] Shioulin and Nisha. “A Guide to Learning with Limited Labeled Data”. April 2,\n2019.\n[14] eForms. “Blank Invoice Templates”. Last accessed June 15, 2020.\n[15] Amazon.com. “Amazon Elements Vitamin B12 Methylcobalamin 5000 mcg -\nNormal Energy Production and Metabolism, Immune System Support - 2 Month\nSupply (65 Berry Flavored Lozenges)”. Last accessed June 15, 2020.\n[16] Beautiful Soup. Last accessed June 15, 2020.\n[17] Scrapy.org. Scrapy. Last accessed June 15, 2020.\n[18] Unicode. Last accessed June 15, 2020.\n[19] Dickinson, Markus, Chris Brew, and Detmar Meurers. Language and Computers.\nNew Jersey: John Wiley & Sons, 2012. ISBN: 978-1-405-18305-5\n[20] Explosion.ai. \"Rule-based matching\". Last accessed June 15, 2020.\n[21] Microsoft documentation. “Quickstart: Check spelling with the Bing Spell Check\nREST API and Python”. Last accessed June 15, 2020.\n[22] Stamy, Matthew. PyPDF2: A utility to read and write PDFs with Python, (GitHub\nrepo). Last accessed June 15, 2020.\n[23] pdfminer. pdfminer.six: Community maintained fork of pdfminer, (GitHub\nrepo). Last accessed June 15, 2020.\nWrapping Up \n| \n77\n",
      "word_count": 315,
      "char_count": 2115,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 108,
      "text": "[24] FilingDB. “What’s so hard about PDF text extraction?” Last accessed June 15,\n2020.\n[25] Tesseract-OCR. “Tesseract Open Source OCR Engine (main repository)”, (Git‐\nHub repo). Last accessed June 15, 2020.\n[26] Python-tesseract documentationPython-tesseract. Last accessed June 15, 2020.\n[27] Firth, John Rupert. “Personality and Language in Society.” The Sociological\nReview 42.1 (1950): 37–52.\n[28] pyenchant. Spellchecking library for python, (GitHub repo). Last accessed June\n15, 2020.\n[29] KBNL Research. ochre: Toolbox for OCR post-correction, (GitHub repo). Last\naccessed June 15, 2020.\n[30] “Natural Language ToolKit”. Last accessed June 15, 2020.\n[31] Explosion.ai. “spaCy 101: Everything you need to know”. Last accessed June 15,\n2020.\n[32] Evang, Kilian, Valerio Basile, Grzegorz Chrupała, and Johan Bos. “Elephant:\nSequence Labeling for Word and Sentence Segmentation.” Proceedings of the 2013\nConference on Empirical Methods in Natural Language Processing (2013): 1422–1426.\n[33] Porter, Martin F. “An Algorithm For Suffix Stripping.” Program: electronic library\nand information systems 14.3 (1980): 130–137.\n[34] Padmanabhan, Arvind. “Lemmatization”. October 11, 2019.\n[35] Explosion.ai. “spaCy”. Last accessed June 15, 2020.\n[36] Polyglot documentation.Polyglot Python library. Last accessed June 15, 2020.\n[37] Mair, Victor. “Singlish: alive and well”. May 14, 2016.\n[38] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft), 2018.\n[39] Explosion.ai. “spaCy: Industrial-Strength Natural Language Processing in\nPython”. Last accessed June 15, 2020.\n[40] DeepAI. “Parsey Mcparseface API”. Last accessed June 15, 2020.\n[41] Stanford CoreNLP.Stanford CoreNLP – Natural language software. Last accessed\nJune 15, 2020.\n[42] Ghaffari, Parsa. “Leveraging Deep Learning for Multilingual Sentiment Analy‐\nsis”. July 14, 2016.\n78 \n| \nChapter 2: NLP Pipeline\n",
      "word_count": 263,
      "char_count": 1903,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 109,
      "text": "[43] The Stanford Natural Language Processing Group. “Stanford TokensRegex”. Last\naccessed June 15, 2020.\n[44] Google. “Cloud Natural Language”. Last accessed June 15, 2020.\n[45] Amazon. “AWS Comprehend”. Last accessed June 15, 2020.\n[46] Microsoft. “Azure Cognitive Services documentation”. Last accessed June 15,\n2020.\n[47] IBM. “Watson Natural Language Understanding”. Last accessed June 15, 2020.\n[48] Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The Elements of Statisti‐\ncal Learning, Second Edition. New York: Springer, 2001. ISBN: 978-0-387-84857-0\n[49] Wikipedia. “F1 score”. Last modified April 18, 2020.\n[50] Wikipedia. “Mean reciprocal rank”. Last modified December 6, 2018.\n[51] Wikipedia. “Evaluation measures (information retrieval)”. Last modified Febru‐\nary 12, 2020.\n[52] Wikipedia. “Mean absolute percentage error”. Last modified February 6, 2020.\n[53] Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. “BLEU: A\nMethod for Automatic Evaluation of Machine Translation.” Proceedings of the 40th\nAnnual Meeting on Association for Computational Linguistics (2002): 311–318.\n[54] Banerjee, Satanjeev and Alon Lavie. “METEOR: An Automatic Metric for MT\nEvaluation with Improved Correlation with Human Judgments.” Proceedings of the\nACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation\nand/or Summarization (2005): 65–72.\n[55] Lin, Chin-Yew. “ROUGE: A Package for Automatic Evaluation of Summaries.”\nText Summarization Branches Out (2004): 74–81.\n[56] Wikipedia. “Perplexity”. Last modified February 13, 2020.\n[57] Google Cloud. “Quickstart for Cloud Tasks queues”. Last accessed June 15, 2020.\n[58] Amazon. “Amazon Simple Queue Service”. Last accessed June 15, 2020.\n[59] Zheng, Huaixiu., Yi-Chia Wang, and Piero Molino. “COTA: Improving Uber\nCustomer Care with NLP & Machine Learning”. January 3, 2018.\nWrapping Up \n| \n79\n",
      "word_count": 262,
      "char_count": 1890,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 110,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 111,
      "text": "CHAPTER 3\nText Representation\nIn language processing,\nthe vectors x are derived from textual data,\nin order to reflect various linguistic properties of the text.\n—Yoav Goldberg\nFeature extraction is an important step for any machine learning problem. No matter\nhow good a modeling algorithm you use, if you feed in poor features, you will get\npoor results. In computer science, this is often called “garbage in, garbage out.” In the\nprevious two chapters, we saw an overview of NLP, the different tasks and challenges\ninvolved, and what a typical NLP pipeline looks like. In this chapter, we’ll address the\nquestion: how do we go about doing feature engineering for text data? In other\nwords, how do we transform a given text into numerical form so that it can be fed\ninto NLP and ML algorithms? In NLP parlance, this conversion of raw text to a suit‐\nable numerical form is called text representation. In this chapter, we’ll take a look at\nthe different methods for text representation, or representing text as a numeric vec‐\ntor. With respect to the larger picture for any NLP problem, the scope of this chapter\nis depicted by the dotted box in Figure 3-1.\nFigure 3-1. Scope of this chapter within the NLP pipeline\n81\n",
      "word_count": 211,
      "char_count": 1220,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (9.3pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1075,
          "height": 425,
          "ext": "png",
          "size_bytes": 23034
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 112,
      "text": "Feature representation is a common step in any ML project, whether the data is text,\nimages, videos, or speech. However, feature representation for text is often much\nmore involved as compared to other formats of data. To understand this, let’s look at a\nfew examples of how other data formats can be represented numerically. First, con‐\nsider the case of images. Say we want to build a classifier that can distinguish images\nof cats from images of dogs. Now, in order to train an ML model to accomplish this\ntask, we need to feed it (labeled) images. How do we feed images to an ML model?\nThe way an image is stored in a computer is in the form of a matrix of pixels where\neach cell[i,j] in the matrix represents pixel i,j of the image. The real value stored at\ncell[i,j] represents the intensity of the corresponding pixel in the image, as shown\nin Figure 3-2. This matrix representation accurately represents the complete image.\nVideo is similar: a video is just a collection of frames where each frame is an image.\nHence, any video can be represented as a sequential collection of matrices, one per\nframe, in the same order.\nFigure 3-2. How we see an image versus how computers see it [1]\nNow consider speech—it’s transmitted as a wave. To represent it mathematically, we\nsample the wave and record its amplitude (height), as shown in Figure 3-3.\nFigure 3-3. Sampling a speech wave\n82 \n| \nChapter 3: Text Representation\n",
      "word_count": 251,
      "char_count": 1424,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 601,
          "ext": "png",
          "size_bytes": 672183
        },
        {
          "index": 1,
          "width": 1390,
          "height": 408,
          "ext": "png",
          "size_bytes": 53779
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 113,
      "text": "This gives us a numerical array representing the amplitude of a sound wave at fixed\ntime intervals, as shown in Figure 3-4.\nFigure 3-4. Speech signal represented by a numerical vector\nFrom this discussion, it’s clear that mathematically representing images, video, and\nspeech is straightforward. What about text? It turns out that representing text is not\nstraightforward, hence a whole chapter focusing on various schemes to address this\nquestion. We’re given a piece of text, and we’re asked to find a scheme to represent it\nmathematically. In literature, this is called text representation. Text representation has\nbeen an active area of research in the past decades, especially the last one. In this\nchapter, we’ll start with simple approaches and go all the way to state-of-the-art tech‐\nniques for representing text. These approaches are classified into four categories:\n• Basic vectorization approaches\n• Distributed representations\n• Universal language representation\n• Handcrafted features\nThe rest of this chapter describes these categories one by one, covering various algo‐\nrithms in each. Before we delve deeper into various schemes, consider the following\nscenario: we’re given a labeled text corpus and asked to build a sentiment analysis\nmodel. To correctly predict the sentiment of a sentence, the model needs to under‐\nstand the meaning of the sentence. In order to correctly extract the meaning of the\nsentence, the most crucial data points are:\n1. Break the sentence into lexical units such as lexemes, words, and phrases\n2. Derive the meaning for each of the lexical units\n3. Understand the syntactic (grammatical) structure of the sentence\n4. Understand the context in which the sentence appears\nThe semantics (meaning) of the sentence arises from the combination of the above\npoints. Thus, any good text representation scheme must facilitate the extraction of\nthose data points in the best possible way to reflect the linguistic properties of the\ntext. Without this, a text representation scheme isn’t of much use.\nText Representation \n| \n83\n",
      "word_count": 324,
      "char_count": 2065,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1403,
          "height": 311,
          "ext": "png",
          "size_bytes": 35688
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 114,
      "text": "i. It is sometimes also referred to as the term vector model, but we’ll stick to the notation VSM.\nOften in NLP, feeding a good text representation to an ordinary\nalgorithm will get you much farther compared to applying a top-\nnotch algorithm to an ordinary text representation.\nLet’s take a look at a key concept that carries throughout this entire chapter: the vec‐\ntor space model.\nVector Space Models\nIt should be clear from the introduction that, in order for ML algorithms to work\nwith text data, the text data must be converted into some mathematical form.\nThroughout this chapter, we’ll represent text units (characters, phonemes, words,\nphrases, sentences, paragraphs, and documents) with vectors of numbers. This is\nknown as the vector space model (VSM).i It’s a simple algebraic model used extensively\nfor representing any text blob. VSM is fundamental to many information-retrieval\noperations, from scoring documents on a query to document classification and docu‐\nment clustering [2]. It’s a mathematical model that represents text units as vectors. In\nthe simplest form, these are vectors of identifiers, such as index numbers in a corpus\nvocabulary. In this setting, the most common way to calculate similarity between two\ntext blobs is using cosine similarity: the cosine of the angle between their\ncorresponding vectors. The cosine of 0° is 1 and the cosine of 180° is –1, with the\ncosine monotonically decreasing from 0° to 180°. Given two vectors, A and B, each\nwith n components, the similarity between them is computed as follows:\nsimilarity = cos θ =\nA · B\nA 2 B 2\n=\n∑\ni = 1\nn\nAiBi\n∑\ni = 1\nn\nAi\n2 ∑\ni = 1\nn\nBi\n2\nwhere Ai and Bi are the ith components of vectors A and B, respectively. Sometimes,\npeople also use Euclidean distance between vectors to capture similarity.\nAll the text representation schemes we’ll study in this chapter fall within the scope of\nvector space models. What differentiates one scheme from another is how well the\nresulting vector captures the linguistic properties of the text it represents. With this,\nwe’re ready to discuss various text representation schemes.\n84 \n| \nChapter 3: Text Representation\n",
      "word_count": 366,
      "char_count": 2150,
      "fonts": [
        "MinionPro-Regular (8.0pt)",
        "MinionPro-Bold (10.0pt)",
        "MinionPro-Regular (14.1pt)",
        "MinionPro-It (6.3pt)",
        "MinionPro-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (8.0pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Regular (6.3pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 115,
      "text": "ii. This mapping is arbitrary. Any other mapping works just as well.\nBasic Vectorization Approaches\nLet’s start with a basic idea of text representation: map each word in the vocabulary\n(V) of the text corpus to a unique ID (integer value), then represent each sentence or\ndocument in the corpus as a V-dimensional vector. How do we operationalize this\nidea? To understand this better, let’s take a toy corpus (shown in Table 3-1) with only\nfour documents—D1, D2, D3, D4 —as an example.\nTable 3-1. Our toy corpus\nD1 Dog bites man.\nD2 Man bites dog.\nD3 Dog eats meat.\nD4 Man eats food.\nLowercasing text and ignoring punctuation, the vocabulary of this corpus is com‐\nprised of six words: [dog, bites, man, eats, meat, food]. We can organize the vocabu‐\nlary in any order. In this example, we simply take the order in which the words\nappear in the corpus. Every document in this corpus can now be represented with a\nvector of size six. We’ll discuss multiple ways in which we can do this. We’ll assume\nthat the text is already pre-processed (lowercased, punctuation removed, etc.) and\ntokenized (text string split into tokens), following the pre-processing step in the NLP\npipeline described in Chapter 2. We’ll start with one-hot encoding.\nOne-Hot Encoding\nIn one-hot encoding, each word w in the corpus vocabulary is given a unique integer\nID wid that is between 1 and |V|, where V is the set of the corpus vocabulary. Each\nword is then represented by a V-dimensional binary vector of 0s and 1s. This is done\nvia a |V| dimension vector filled with all 0s barring the index, where index = wid. At\nthis index, we simply put a 1. The representation for individual words is then com‐\nbined to form a sentence representation.\nLet’s understand this via our toy corpus. We first map each of the six words to unique\nIDs: dog = 1, bites = 2, man = 3, meat = 4 , food = 5, eats = 6.ii Let’s consider the\ndocument D1: “dog bites man”. As per the scheme, each word is a six-dimensional\nvector. Dog is represented as [1 0 0 0 0 0], as the word “dog” is mapped to ID 1. Bites\nis represented as [0 1 0 0 0 0], and so on and so forth. Thus, D1 is represented as [ [1 0\n0 0 0 0] [0 1 0 0 0 0] [0 0 1 0 0 0]]. D4 is represented as [ [ 0 0 1 0 0] [0 0 0 0 1 0] [0 0 0\n0 0 1]]. Other documents in the corpus can be represented similarly.\nBasic Vectorization Approaches \n| \n85\n",
      "word_count": 452,
      "char_count": 2356,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MinionPro-Regular (8.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Regular (6.3pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 116,
      "text": "Let’s look at a simple way to implement this in Python from first principles. The note‐\nbook Ch3/OneHotEncoding.ipynb demonstrates an example of this. The code that fol‐\nlows is borrowed from the notebook and implements one-hot encoding. In real-\nworld projects, we mostly use scikit-learn’s implementation of one-hot encoding,\nwhich is much more optimized. We’ve provided the same in the notebook.\nSince we assume that the text is tokenized, we can just split the text on white space in\nthis example:\ndef get_onehot_vector(somestring):\n  onehot_encoded = []\n  for word in somestring.split():\n             temp = [0]*len(vocab)\n             if word in vocab:\n                        temp[vocab[word]-1] = 1\n             onehot_encoded.append(temp)\n  return onehot_encoded\nget_onehot_vector(processed_docs[1])\nOutput: [[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]\nNow that we understand the scheme, let’s discuss some of its pros and cons. On the\npositive side, one-hot encoding is intuitive to understand and straightforward to\nimplement. However, it suffers from a few shortcomings:\n• The size of a one-hot vector is directly proportional to size of the vocabulary, and\nmost real-world corpora have large vocabularies. This results in a sparse repre‐\nsentation where most of the entries in the vectors are zeroes, making it computa‐\ntionally inefficient to store, compute with, and learn from (sparsity leads to\noverfitting).\n• This representation does not give a fixed-length representation for text, i.e., if a\ntext has 10 words, you get a longer representation for it as compared to a text\nwith 5 words. For most learning algorithms, we need the feature vectors to be of\nthe same length.\n• It treats words as atomic units and has no notion of (dis)similarity between\nwords. For example, consider three words: run, ran, and apple. Run and ran have\nsimilar meanings as opposed to run and apple. But if we take their respective vec‐\ntors and compute Euclidean distance between them, they’re all equally apart ( 2).\nThus, semantically, they’re very poor at capturing the meaning of the word in\nrelation to other words.\n• Say we train a model using our toy corpus. At runtime, we get a sentence: “man\neats fruits.” The training data didn’t include “fruit” and there’s no way to represent\nit in our model. This is known as the out of vocabulary (OOV) problem. A\n86 \n| \nChapter 3: Text Representation\n",
      "word_count": 386,
      "char_count": 2413,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 117,
      "text": "one-hot encoding scheme cannot handle this. The only way is to retrain the\nmodel: start by expanding the vocabulary, give an ID to the new word, etc.\nThese days, one-hot encoding scheme is seldom used.\nSome of these shortcomings can be addressed by the bag-of-words approach\ndescribed next.\nBag of Words\nBag of words (BoW) is a classical text representation technique that has been used\ncommonly in NLP, especially in text classification problems (see Chapter 4). The key\nidea behind it is as follows: represent the text under consideration as a bag (collec‐\ntion) of words while ignoring the order and context. The basic intuition behind it is\nthat it assumes that the text belonging to a given class in the dataset is characterized\nby a unique set of words. If two text pieces have nearly the same words, then they\nbelong to the same bag (class). Thus, by analyzing the words present in a piece of text,\none can identify the class (bag) it belongs to.\nSimilar to one-hot encoding, BoW maps words to unique integer IDs between 1 and\n|V|. Each document in the corpus is then converted into a vector of |V| dimensions\nwhere in the ith component of the vector, i = wid, is simply the number of times the\nword w occurs in the document, i.e., we simply score each word in V by their occur‐\nrence count in the document.\nThus, for our toy corpus (Table 3-1), where the word IDs are dog = 1, bites = 2, man\n= 3, meat = 4 , food = 5, eats = 6, D1 becomes [1 1 1 0 0 0]. This is because the first\nthree words in the vocabulary appeared exactly once in D1, and the last three did not\nappear at all. D4 becomes [0 0 1 0 1 1]. The notebook Ch3/Bag_of_Words.ipynb dem‐\nonstrates how we can implement BoW text representation. The following code shows\nthe key parts:\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\n#Build a BOW representation for the corpus\nbow_rep = count_vect.fit_transform(processed_docs)\n#Look at the vocabulary mapping\nprint(\"Our vocabulary: \", count_vect.vocabulary_)\n#See the BOW rep for first 2 documents\nBasic Vectorization Approaches \n| \n87\n",
      "word_count": 365,
      "char_count": 2096,
      "fonts": [
        "MinionPro-It (6.3pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MinionPro-Regular (6.3pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 118,
      "text": "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\nprint(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n#Get the representation using this vocabulary, for a new text\ntemp = count_vect.transform([\"dog and dog are friends\"])\nprint(\"Bow representation for 'dog and dog are friends':\", \ntemp.toarray())\nIf we run this code, we’ll notice that the BoW representation for a sentence like “dog\nand dog are friends” has a value of 2 for the dimension of the word “dog,” indicating\nits frequency in the text. Sometimes, we don’t care about the frequency of occurrence\nof words in text and we only want to represent whether a word exists in the text or\nnot. Researchers have shown that such a representation without considering fre‐\nquency is useful for sentiment analysis (see Chapter 4 in [3]). In such cases, we just\ninitialize CountVectorizer with the binary=True option, as shown in the following\ncode:\ncount_vect = CountVectorizer(binary=True)\nbow_rep_bin = count_vect.fit_transform(processed_docs)\ntemp = count_vect.transform([\"dog and dog are friends\"])\nprint(\"Bow representation for 'dog and dog are friends':\", temp.toarray())\nThis results in a different representation for the same sentence. CountVectorizer\nsupports both word as well as character n-grams.\nLet’s look at some of the advantages of this encoding:\n• Like one-hot encoding, BoW is fairly simple to understand and implement.\n• With this representation, documents having the same words will have their vec‐\ntor representations closer to each other in Euclidean space as compared to docu‐\nments with completely different words. The distance between D1 and D2 is 0 as\ncompared to the distance between D1 and D4, which is 2. Thus, the vector space\nresulting from the BoW scheme captures the semantic similarity of documents.\nSo if two documents have similar vocabulary, they’ll be closer to each other in the\nvector space and vice versa.\n• We have a fixed-length encoding for any sentence of arbitrary length.\nHowever, it has its share of disadvantages, too:\n• The size of the vector increases with the size of the vocabulary. Thus, sparsity\ncontinues to be a problem. One way to control it is by limiting the vocabulary to\nn number of the most frequent words.\n• It does not capture the similarity between different words that mean the same\nthing. Say we have three documents: “I run”, “I ran”, and “I ate”. BoW vectors of\nall three documents will be equally apart.\n88 \n| \nChapter 3: Text Representation\n",
      "word_count": 395,
      "char_count": 2500,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MinionPro-Regular (6.3pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 119,
      "text": "• This representation does not have any way to handle out of vocabulary words\n(i.e., new words that were not seen in the corpus that was used to build the vec‐\ntorizer).\n• As the name indicates, it is a “bag” of words—word order information is lost in\nthis representation. Both D1 and D2 will have the same representation in this\nscheme.\nHowever, despite these shortcomings, due to its simplicity and ease of implementa‐\ntion, BoW is a commonly used text representation scheme, especially for text classifi‐\ncation among other NLP problems.\nBag of N-Grams\nAll the representation schemes we’ve seen so far treat words as independent units.\nThere is no notion of phrases or word ordering. The bag-of-n-grams (BoN) approach\ntries to remedy this. It does so by breaking text into chunks of n contiguous words (or\ntokens). This can help us capture some context, which earlier approaches could not\ndo. Each chunk is called an n-gram. The corpus vocabulary, V, is then nothing but a\ncollection of all unique n-grams across the text corpus. Then, each document in the\ncorpus is represented by a vector of length |V|. This vector simply contains the fre‐\nquency counts of n-grams present in the document and zero for the n-grams that are\nnot present.\nTo elaborate, let’s consider our example corpus. Let’s construct a 2-gram (a.k.a.\nbigram) model for it. The set of all bigrams in the corpus is as follows: {dog bites,\nbites man, man bites, bites dog, dog eats, eats meat, man eats, eats food}. Then, BoN\nrepresentation consists of an eight-dimensional vector for each document. The\nbigram representation for the first two documents is as follows: D1 : [1,1,0,0,0,0,0,0],\nD2 : [0,0,1,1,0,0,0,0]. The other two documents follow similarly. Note that the BoW\nscheme is a special case of the BoN scheme, with n=1. n=2 is called a “bigram model,”\nand n=3 is called a “trigram model.” Further, note that, by increasing the value of n,\nwe can incorporate larger context; however, this further increases the sparsity. In NLP\nparlance, the BoN scheme is also called “n-gram feature selection.”\nThe following code (Ch3/Bag_of_N_Grams.ipynb) shows an example of a BoN repre‐\nsentation considering 1–3 n-gram word features to represent the corpus that we’ve\nused so far. Here, we use unigram, bigram, and trigram vectors by setting\nngram_range = (1,3):\n#n-gram vectorization example with count vectorizer and uni, bi, trigrams\ncount_vect = CountVectorizer(ngram_range=(1,3))\n#Build a BOW representation for the corpus\nbow_rep = count_vect.fit_transform(processed_docs)\nBasic Vectorization Approaches \n| \n89\n",
      "word_count": 417,
      "char_count": 2586,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MinionPro-Regular (6.3pt)",
        "UbuntuMono-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 120,
      "text": "#Look at the vocabulary mapping\nprint(\"Our vocabulary: \", count_vect.vocabulary_)\n#Get the representation using this vocabulary, for a new text\ntemp = count_vect.transform([\"dog and dog are friends\"])\nprint(\"Bow representation for 'dog and dog are friends':\", temp.toarray())\nHere are the main pros and cons of BoN:\n• It captures some context and word-order information in the form of n-grams.\n• Thus, resulting vector space is able to capture some semantic similarity. Docu‐\nments having the same n-grams will have their vectors closer to each other in\nEuclidean space as compared to documents with completely different n-grams.\n• As n increases, dimensionality (and therefore sparsity) only increases rapidly.\n• It still provides no way to address the OOV problem.\nTF-IDF\nIn all the three approaches we’ve seen so far, all the words in the text are treated as\nequally important—there’s no notion of some words in the document being more\nimportant than others. TF-IDF, or term frequency–inverse document frequency,\naddresses this issue. It aims to quantify the importance of a given word relative to\nother words in the document and in the corpus. It’s a commonly used representation\nscheme for information-retrieval systems, for extracting relevant documents from a\ncorpus for a given text query.\nThe intuition behind TF-IDF is as follows: if a word w appears many times in a docu‐\nment di but does not occur much in the rest of the documents dj in the corpus, then\nthe word w must be of great importance to the document di. The importance of w\nshould increase in proportion to its frequency in di, but at the same time, its impor‐\ntance should decrease in proportion to the word’s frequency in other documents dj in\nthe corpus. Mathematically, this is captured using two quantities: TF and IDF. The\ntwo are then combined to arrive at the TF-IDF score.\nTF (term frequency) measures how often a term or word occurs in a given document.\nSince different documents in the corpus may be of different lengths, a term may\noccur more often in a longer document as compared to a shorter document. To nor‐\nmalize these counts, we divide the number of occurrences by the length of the docu‐\nment. TF of a term t in a document d is defined as:\nTF t, d = Number of occurrences of term t in document d\nTotal number of terms in the document d\n90 \n| \nChapter 3: Text Representation\n",
      "word_count": 404,
      "char_count": 2367,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MinionPro-Regular (6.3pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 121,
      "text": "IDF (inverse document frequency) measures the importance of the term across a cor‐\npus. In computing TF, all terms are given equal importance (weightage). However, it’s\na well-known fact that stop words like is, are, am, etc., are not important, even though\nthey occur frequently. To account for such cases, IDF weighs down the terms that are\nvery common across a corpus and weighs up the rare terms. IDF of a term t is calcu‐\nlated as follows:\nIDF t = loge\nTotal number of documents in the corpus\nNumber of documents with term t in them \nThe TF-IDF score is a product of these two terms. Thus, TF-IDF score = TF * IDF.\nLet’s compute TF-IDF scores for our toy corpus. Some terms appear in only one\ndocument, some appear in two, while others appear in three documents. The size of\nour corpus is N=4. Hence, corresponding TF-IDF values for each term are shown in\nTable 3-2.\nTable 3-2. TF-IDF values for our toy corpus\nWord TF score\nIDF score\nTF-IDF score\ndog\n⅓ = 0.33\nlog2(4/3) = 0.4114 0.4114 * 0.33 = 0.136\nbites\n⅙ = 0.17\nlog2(4/2) = 1\n1* 0.17 = 0.17\nman\n0.33\nlog2(4/3) =0.4114\n0.4114 * 0.33 = 0.136\neats\n0.17\nlog2(4/2) =1\n1* 0.17 = 0.17\nmeat\n1/12 = 0.083 log2(4/1) =2\n2* 0.083 = 0.17\nfood\n0.083\nlog2(4/1) =2\n2* 0.083 = 0.17\nThe TF-IDF vector representation for a document is then simply the TF-IDF score\nfor each term in that document. So, for D1 we get\nDog\nbites\nman\neats\nmeat\nfood\n0.136 0.17\n0.136 0\n0\n0\nThe following code (Ch3/TF_IDF.ipynb) shows how to use TF-IDF to represent text:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\nbow_rep_tfidf = tfidf.fit_transform(processed_docs)\nprint(tfidf.idf_) #IDF for all words in the vocabulary\nprint(tfidf.get_feature_names()) #All words in the vocabulary.\ntemp = tfidf.transform([\"dog and man are friends\"])\nprint(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())\nBasic Vectorization Approaches \n| \n91\n",
      "word_count": 324,
      "char_count": 1913,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "MyriadPro-Cond (6.3pt)",
        "MinionPro-It (8.0pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MinionPro-Regular (6.3pt)",
        "ArialUnicodeMS (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 122,
      "text": "There are several variations of the basic TF-IDF formula that are used in practice.\nNotice that the TF-IDF scores that we calculated for our corpus in Table 3-2 might\nnot match the TF-IDF scores given by scikit-learn. This is because scikit-learn uses a\nslightly modified version of the IDF formula. This stems from provisions to account\nfor possible zero divisions and to not entirely ignore terms that appear in all docu‐\nments. An interested reader can look into the TF-IDF vectorizer documentation [4]\nfor the exact formula.\nSimilar to BoW, we can use the TF-IDF vectors to calculate similarity between two\ntexts using a similarity measure like Euclidean distance or cosine similarity. TF-IDF is\na commonly used representation in application scenarios such as information\nretrieval and text classification. However, despite the fact that TF-IDF is better than\nthe vectorization methods we saw earlier in terms of capturing similarities between\nwords, it still suffers from the curse of high dimensionality.\nEven today, TF-IDF continues to be a popular representation\nscheme for many NLP tasks, especially the initial versions of the\nsolution.\nIf we look back at all the representation schemes we’ve discussed so far, we notice\nthree fundamental drawbacks:\n• They’re discrete representations—i.e., they treat language units (words, n-grams,\netc.) as atomic units. This discreteness hampers their ability to capture relation‐\nships between words.\n• The feature vectors are sparse and high-dimensional representations. The dimen‐\nsionality increases with the size of the vocabulary, with most values being zero\nfor any vector. This hampers learning capability. Further, high-dimensionality\nrepresentation makes them computationally inefficient.\n• They cannot handle OOV words.\nWith this, we come to the end of basic vectorization approaches. Now, let’s start look‐\ning at distributed representations.\nDistributed Representations\nIn the previous section, we saw some key drawbacks that are common to all basic vec‐\ntorization approaches. To overcome these limitations, methods to learn low-\ndimensional representations were devised. These methods, covered in this section,\ngained momentum in the past six to seven years. They use neural network\n92 \n| \nChapter 3: Text Representation\n",
      "word_count": 343,
      "char_count": 2283,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 123,
      "text": "architectures to create dense, low-dimensional representations of words and texts.\nBut before we look into these methods, we need to understand some key terms:\nDistributional similarity\nThis is the idea that the meaning of a word can be understood from the context\nin which the word appears. This is also known as connotation: meaning is defined\nby context. This is opposed to denotation: the literal meaning of any word. For\nexample: “NLP rocks.” The literal meaning of the word “rocks” is “stones,” but\nfrom the context, it’s used to refer to something good and fashionable.\nDistributional hypothesis [5]\nIn linguistics, this hypothesizes that words that occur in similar contexts have\nsimilar meanings. For example, the English words “dog” and “cat” occur in simi‐\nlar contexts. Thus, according to the distributional hypothesis, there must be a\nstrong similarity between the meanings of these two words. Now, following from\nVSM, the meaning of a word is represented by the vector. Thus, if two words\noften occur in similar context, then their corresponding representation vectors\nmust also be close to each other.\nDistributional representation [6]\nThis refers to representation schemes that are obtained based on distribution of\nwords from the context in which the words appear. These schemes are based on\ndistributional hypotheses. The distributional property is induced from context\n(textual vicinity). Mathematically, distributional representation schemes use\nhigh-dimensional vectors to represent words. These vectors are obtained from a\nco-occurrence matrix that captures co-occurrence of word and context. The\ndimension of this matrix is equal to the size of the vocabulary of the corpus. The\nfour schemes that we’ve seen so far—one-hot, bag of words, bag of n-grams, and\nTF-IDF—all fall under the umbrella of distributional representation.\nDistributed representation [6]\nThis is a related concept. It, too, is based on the distributional hypothesis. As dis‐\ncussed in the previous paragraph, the vectors in distributional representation are\nvery high dimensional and sparse. This makes them computationally inefficient\nand hampers learning. To alleviate this, distributed representation schemes sig‐\nnificantly compress the dimensionality. This results in vectors that are compact\n(i.e., low dimensional) and dense (i.e., hardly any zeros). The resulting vector\nspace is known as distributed representation. All the subsequent schemes we’ll dis‐\ncuss in this chapter are examples of distributed representation.\nEmbedding\nFor the set of words in a corpus, embedding is a mapping between vector space\ncoming from distributional representation to vector space coming from dis‐\ntributed representation.\nDistributed Representations \n| \n93\n",
      "word_count": 408,
      "char_count": 2744,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 124,
      "text": "Vector semantics\nThis refers to the set of NLP methods that aim to learn the word representations\nbased on distributional properties of words in a large corpus.\nNow that you have a basic understanding of these terms, we can move on to our first\nmethod: word embeddings.\nWord Embeddings\nWhat does it mean when we say a text representation should capture “distributional\nsimilarities between words”? Let’s consider some examples. If we’re given the word\n“USA,” distributionally similar words could be other countries (e.g., Canada, Ger‐\nmany, India, etc.) or cities in the USA. If we’re given the word “beautiful,” words that\nshare some relationship with this word (e.g., synonyms, antonyms) could be consid‐\nered distributionally similar words. These are words that are likely to occur in similar\ncontexts. In 2013, a seminal work by Mikolov et al. [7] showed that their neural net‐\nwork–based word representation model known as “Word2vec,” based on “distribu‐\ntional similarity,” can capture word analogy relationships such as:\nKing – Man + Woman ≈ Queen\nTheir model was able to correctly answer many more analogies like this. Figure 3-5\nshows a snapshot of a system based on Word2vec answering analogies. The\nWord2vec model is in many ways the dawn of modern-day NLP.\nWhile learning such semantically rich relationships, Word2vec ensures that the\nlearned word representations are low dimensional (vectors of dimensions 50–500,\ninstead of several thousands, as with previously studied representations in this chap‐\nter) and dense (that is, most values in these vectors are non-zero). Such representa‐\ntions make ML tasks more tractable and efficient. Word2vec led to a lot of work (both\npure and applied) in the direction of learning text representations using neural\nnetworks. These representations are also called “embeddings.” Let’s build an intuition\nof how they work and how to use them to represent text.\nGiven a text corpus, the aim is to learn embeddings for every word in the corpus such\nthat the word vector in the embedding space best captures the meaning of the word.\nTo “derive” the meaning of the word, Word2vec uses distributional similarity and dis‐\ntributional hypothesis. That is, it derives the meaning of a word from its context:\nwords that appear in its neighborhood in the text. So, if two different words (often)\noccur in similar context, then it’s highly likely that their meanings are also similar.\nWord2vec operationalizes this by projecting the meaning of the words in a vector\nspace where words with similar meanings will tend to cluster together, and words\nwith very different meanings are far from one another.\n94 \n| \nChapter 3: Text Representation\n",
      "word_count": 432,
      "char_count": 2679,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 125,
      "text": "Figure 3-5. Word2vec-based analogy-answering system\nConceptually, Word2vec takes a large corpus of text as input and “learns” to represent\nthe words in a common vector space based on the contexts in which they appear in\nthe corpus. Given a word w and the words appearing in its context C, how do we find\nthe vector that best represents the meaning of the word? For every word w in corpus,\nwe start with a vector vw initialized with random values. The Word2vec model refines\nthe values in vw by predicting vw, given the vectors for words in the context C. It does\nthis using a two-layer neural network. We’ll dive deeper into this by discussing pre-\ntrained embeddings before moving on to train our own.\nPre-trained word embeddings\nTraining your own word embeddings is a pretty expensive process (in terms of both\ntime and computing). Thankfully, for many scenarios, it’s not necessary to train your\nown embeddings, and using pre-trained word embeddings often suffices. What are\npre-trained word embeddings? Someone has done the hard work of training word\nembeddings on a large corpus, such as Wikipedia, news articles, or even the entire\nweb, and has put words and their corresponding vectors on the web. These embed‐\ndings can be downloaded and used to get the vectors for the words you want. Such\nembeddings can be thought of as a large collection of key-value pairs, where keys are\nthe words in the vocabulary and values are their corresponding word vectors. Some\nof the most popular pre-trained embeddings are Word2vec by Google [8], GloVe by\nStanford [9], and fasttext embeddings by Facebook [10], to name a few. Further,\nthey’re available for various dimensions like d = 25, 50, 100, 200, 300, 600.\nDistributed Representations \n| \n95\n",
      "word_count": 293,
      "char_count": 1740,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Regular (6.3pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 629,
          "height": 347,
          "ext": "png",
          "size_bytes": 19310
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 126,
      "text": "Ch3/Pre_Trained_Word_Embeddings.ipynb, the notebook associated with the rest of\nthis section, shows an example of how to load pre-trained Word2vec embeddings and\nlook for the most similar words (ranked by cosine similarity) to a given word. The\ncode that follows covers the key steps. Here, we find the words that are semantically\nmost similar to the word “beautiful”; the last line returns the embedding vector of the\nword “beautiful”:\nfrom gensim.models import Word2Vec, KeyedVectors\npretrainedpath = \"NLPBookTut/GoogleNews-vectors-negative300.bin\"\nw2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True)\nprint('done loading Word2Vec')\nprint(len(w2v_model.vocab)) #Number of words in the vocabulary.\nprint(w2v_model.most_similar['beautiful'])\nW2v_model['beautiful']\nmost_similar('beautiful') returns the most similar words to the word “beautiful.”\nThe output is shown below. Each word is accompanied by a similarity score. The\nhigher the score, the more similar the word is to the query word:\n[('gorgeous', 0.8353004455566406),\n ('lovely', 0.810693621635437),\n ('stunningly_beautiful', 0.7329413890838623),\n ('breathtakingly_beautiful', 0.7231341004371643),\n ('wonderful', 0.6854087114334106),\n ('fabulous', 0.6700063943862915),\n ('loveliest', 0.6612576246261597),\n ('prettiest', 0.6595001816749573),\n ('beatiful', 0.6593326330184937),\n ('magnificent', 0.6591402292251587)]\nw2v_model returns the vector for the query word. For the word “beautiful,” we get the\nvector as shown in Figure 3-6.\nNote that if we search for a word that is not present in the Word2vec model (e.g.,\n“practicalnlp”), we’ll see a “key not found” error. Hence, as a good coding practice, it’s\nalways advised to first check if the word is present in the model’s vocabulary before\nattempting to retrieve its vector. The Python library we used in this code snippet,\ngensim, also supports training and loading GloVe pre-trained models.\nIf you’re new to embeddings, always start by using pre-trained\nword embeddings in your project. Understand their pros and cons,\nthen start thinking of building your own embeddings. Using pre-\ntrained embeddings will quickly give you a strong baseline for the\ntask at hand.\n96 \n| \nChapter 3: Text Representation\n",
      "word_count": 290,
      "char_count": 2236,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "MinionPro-Regular (9.6pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 127,
      "text": "Figure 3-6. Vector representing the word “beautiful” in pre-trained Word2vec\nNow let’s look at training our own word embeddings.\nDistributed Representations \n| \n97\n",
      "word_count": 23,
      "char_count": 164,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 667,
          "height": 851,
          "ext": "png",
          "size_bytes": 94171
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 128,
      "text": "Training our own embeddings\nNow we’ll focus on training our own word embeddings. For this, we’ll look at two\narchitectural variants that were proposed in the original Word2vec approach. The\ntwo variants are:\n• Continuous bag of words (CBOW)\n• SkipGram\nBoth of these have a lot of similarities in many respects. We’ll begin by understanding\nthe CBOW model, then we’ll look at SkipGram. Throughout this section, we’ll use the\nsentence “The quick brown fox jumps over the lazy dog” as our toy corpus.\nCBOW.    In CBOW, the primary task is to build a language model that correctly pre‐\ndicts the center word given the context words in which the center word appears.\nWhat is a language model? It is a (statistical) model that tries to give a probability dis‐\ntribution over sequences of words. Given a sentence of, say, m words, it assigns a\nprobability Pr(w1, w2, ….., wn) to the whole sentence. The objective of a language\nmodel is to assign probabilities in such a way that it gives high probability to “good”\nsentences and low probabilities to “bad” sentences. By good, we mean sentences that\nare semantically and syntactically correct. By bad, we mean sentences that are incor‐\nrect—semantically or syntactically or both. So, for a sentence like “The cat jumped\nover the dog,” it will try to assign a probability close to 1.0, whereas for a sentence like\n“jumped over the the cat dog,” it tries to assign a probability close to 0.0.\nCBOW tries to learn a language model that tries to predict the “center” word from the\nwords in its context. Let’s understand this using our toy corpus. If we take the word\n“jumps” as the center word, then its context is formed by words in its vicinity. If we\ntake the context size of 2, then for our example, the context is given by brown, fox,\nover, the. CBOW uses the context words to predict the target word—jumps—as\nshown in Figure 3-7. CBOW tries to do this for every word in the corpus; i.e., it takes\nevery word in the corpus as the target word and tries to predict the target word from\nits corresponding context words.\nFigure 3-7. CBOW: given the context words, predict the center word\n98 \n| \nChapter 3: Text Representation\n",
      "word_count": 381,
      "char_count": 2165,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (11.5pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Regular (6.3pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1439,
          "height": 242,
          "ext": "png",
          "size_bytes": 37230
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 129,
      "text": "The idea discussed in the previous paragraph is then extended to the entire corpus to\nbuild the training set. Details are as follows: we run a sliding window of size 2k+1\nover the text corpus. For our example, we took k as 2. Each position of the window\nmarks the set of 2k+1 words that are under consideration. The center word in the\nwindow is the target, and k words on either side of the center word form the context.\nThis gives us one data point. If the point is represented as (X,Y), then the context is\nthe X and the target word is the Y. A single data point consists of a pair of numbers:\n(2k indices of words in context, index of word in target). To get the next data point,\nwe simply shift the window to the right on the corpus by one word and repeat the\nprocess. This way, we slide the window across the entire corpus to create the training\nset. This is shown in Figure 3-8. Here, the target word is shown in blue and k=2.\nFigure 3-8. Preparing a dataset for CBOW\nNow that we have the training data ready, let’s focus on the model. For this, we con‐\nstruct a shallow net (it’s shallow since it has a single hidden layer), as shown in\nFigure 3-9. We assume we want to learn D-dim word embeddings. Further, let V be\nthe vocabulary of the text corpus.\nDistributed Representations \n| \n99\n",
      "word_count": 247,
      "char_count": 1294,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1418,
          "height": 495,
          "ext": "png",
          "size_bytes": 42237
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 130,
      "text": "iii. Technically speaking, both E and E’ are two different learned embeddings. You can use either of them or\neven combine the two by simply averaging them.\nFigure 3-9. CBOW model [7]\nThe objective is to learn an embedding matrix E|V| x d.To begin with, we initialize the\nmatrix randomly. Here, |V| is the size of corpus vocabulary and d is the dimension of\nthe embedding. Let’s break down the shallow net in Figure 3-9 layer by layer. In the\ninput layer, indices of the words in context are used to fetch the corresponding rows\nfrom the embedding matrix E|V| x d. The vectors fetched are then added to get a single\nD-dim vector, and this is passed to the next layer. The next layer simply takes this d\nvector and multiplies it with another matrix E’d x |V|.. This gives a 1 x |V| vector, which\nis fed to a softmax function to get probability distribution over the vocabulary space.\nThis distribution is compared with the label and uses back propagation to update\nboth the matrices E and E’ accordingly. At the end of the training, E is the embedding\nmatrixiii we wanted to learn.\n100 \n| \nChapter 3: Text Representation\n",
      "word_count": 202,
      "char_count": 1119,
      "fonts": [
        "MinionPro-Regular (8.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Regular (6.3pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1395,
          "height": 1018,
          "ext": "png",
          "size_bytes": 82137
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 131,
      "text": "SkipGram.    SkipGram is very similar to CBOW, with some minor changes. In Skip‐\nGram, the task is to predict the context words from the center word. For our toy cor‐\npus with context size 2, using the center word “jumps,” we try to predict every word\nin context—“brown,” “fox,” “over,” “the”—as shown in Figure 3-10. This constitutes\none step. SkipGram repeats this one step for every word in the corpus as the center\nword.\nFigure 3-10. SkipGram: given the center word, predict every word in context\nThe dataset to train a SkipGram is prepared as follows: we run a sliding window of\nsize 2k+1 over the text corpus to get the set of 2k+1 words that are under considera‐\ntion. The center word in the window is the X, and k words on either side of the center\nword are Y. Unlike CBOW, this gives us 2k data points. A single data point consists of\na pair: (index of the center word, index of a target word). We then shift the window to\nthe right on the corpus by one word and repeat the process. This way, we slide the\nwindow across the entire corpus to create the training set. This is shown in\nFigure 3-11.\nFigure 3-11. Preparing a dataset for SkipGram\nDistributed Representations \n| \n101\n",
      "word_count": 217,
      "char_count": 1187,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (11.5pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 241,
          "ext": "png",
          "size_bytes": 35164
        },
        {
          "index": 1,
          "width": 1214,
          "height": 822,
          "ext": "png",
          "size_bytes": 52289
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 132,
      "text": "The shallow network used to train the SkipGram model, shown in Figure 3-12, is\nvery similar to the network used for CBOW, with some minor changes. In the input\nlayer, the index of the word in the target is used to fetch the corresponding row from\nthe embedding matrix E|V| x d. The vectors fetched are then passed to the next layer.\nThe next layer simply takes this d vector and multiplies it with another matrix E’d x |V|.\nThis gives a 1 x |V| vector, which is fed to a softmax function to get probability distri‐\nbution over the vocabulary space. This distribution is compared with the label and\nuses back propagation to update both the matrices E and E’ accordingly. At the end of\nthe training, E is the embedding matrix we wanted to learn.\nFigure 3-12. SkipGram architecture [7]\nThere are a lot of other minute details that go into both CBOW and the SkipGram\nmodel. An interested reader can look at the three-part blog post by Sebastian Ruder\n[11]. You can also refer to Rong (2016) [12] for a step-by-step derivation of Word2vec\nparameter learning. Another key aspect to keep in mind is hyperparameters of the\nmodel. There are several hyperparameters: window size, dimensionality of the vectors\nto be learned, learning rate, number of epochs, etc. It’s a well-established fact that\nhyperparameters play a crucial role in the quality of the final model [13, 14].\n102 \n| \nChapter 3: Text Representation\n",
      "word_count": 245,
      "char_count": 1406,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (6.3pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1336,
          "height": 1120,
          "ext": "png",
          "size_bytes": 80199
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 133,
      "text": "To use both the CBOW and SkipGram algorithms in practice, there are several avail‐\nable implementations that abstract the mathematical details for us. One of the most\ncommonly used implementations is gensim [15].\nDespite the availability of several off-the-shelf implementations, we still have to make\ndecisions on several hyperparameters (i.e., the variables that need to be set before\nstarting the training process). Let’s look at two examples.\nDimensionality of the word vectors\nAs the name indicates, this decides the space of the learned embeddings. While\nthere is no ideal number, it’s common to construct word vectors with dimensions\nin the range of 50–500 and evaluate them on the task we’re using them for to\nchoose the best option.\nContext window\nHow long or short the context we look for to learn the vector representation is.\nThere are also other choices we make, such as whether to use CBOW or SkipGram to\nlearn the embeddings. These choices are more of an art than science at this point, and\nthere’s a lot of ongoing research on methods for choosing the right hyperparameters.\nUsing packages like gensim, it’s pretty straightforward from a code point of view to\nimplement Word2vec. The following code shows how to train our own Word2vec\nmodel using a toy corpus called common_texts that’s available in gensim. Assuming\nyou have the corpus for your domain, following this code snippet will quickly give\nyou your own embeddings:\n#Import a test data set provided in gensim to train a model\nfrom gensim.test.utils import common_texts\n#Build the model, by selecting the parameters.\nour_model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4)\n#Save the model\nour_model.save(\"tempmodel.w2v\")\n#Inspect the model by looking for the most similar words for a test word.\nprint(our_model.wv.most_similar('computer', topn=5))\n#Let us see what the 10-dimensional vector for 'computer' looks like.\nprint(our_model['computer'])\nNow, we can get the vector representation for any word in our corpus, provided it’s in\nthe model’s vocabulary—we just look up the word in the model. But what if we have a\nphrase (e.g., “word embeddings”) for which we need a vector?\nGoing Beyond Words\nSo far, we’ve seen examples of how to use pre-trained word embeddings and train our\nown word embeddings. This gives us a compact and dense representation for words\nin our vocabulary. However, in most NLP applications, we seldom deal with atomic\nunits like words—we deal with sentences, paragraphs, or even full texts. So, we need a\nDistributed Representations \n| \n103\n",
      "word_count": 404,
      "char_count": 2562,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 134,
      "text": "way to represent larger units of text. Is there a way we can use word embeddings to\nget feature representations for larger units of text?\nA simple approach is to break the text into constituent words, take the embeddings\nfor individual words, and combine them to form the representation for the text.\nThere are various ways to combine them, the most popular being sum, average, etc.,\nbut these may not capture many aspects of the text as a whole, such as ordering. Sur‐\nprisingly, they work very well in practice (see Chapter 4). As a matter of fact, in\nCBOW, this was demonstrated by taking the sum of word vectors in context. The\nresulting vector represents the entire context and is used to predict the center word.\nIt’s always a good idea to experiment with this before moving to other representa‐\ntions. The following code shows how to obtain the vector representation for text by\naveraging word vectors using the library spaCy [16]:\nimport spacy\nimport en_core_web_sm\n# Load the spacy model. This takes a few seconds.\nnlp = en_core_web_sm.load()\n# Process a sentence using the model\ndoc = nlp(\"Canada is a large country\")\n#Get a vector for individual words\n#print(doc[0].vector) #vector for 'Canada', the first word in the text\nprint(doc.vector) #Averaged vector for the entire sentence\nBoth pre-trained and self-trained word embeddings depend on the vocabulary they\nsee in the training data. However, there is no guarantee that we will only see those\nwords in the production data for the application we’re building. Despite the ease of\nusing Word2vec or any such word embedding to do feature extraction from texts, we\ndon’t have a good way of handling OOV words yet. This has been a recurring prob‐\nlem in all the representations we’ve seen so far. What do we do in such cases?\nA simple approach that often works is to exclude those words from the feature extrac‐\ntion process so we don’t have to worry about how to get their representations. If we’re\nusing a model trained on a large corpus, we shouldn’t see too many OOV words any‐\nway. However, if a large fraction of the words from our production data isn’t present\nin the word embedding’s vocabulary, we’re unlikely to see good performance. This\nvocabulary overlap is a great heuristic to gauge the performance of an NLP model.\nIf the overlap between corpus vocabulary and embedding vocabu‐\nlary is less than 80%, we’re unlikely to see good performance from\nour NLP model.\n104 \n| \nChapter 3: Text Representation\n",
      "word_count": 422,
      "char_count": 2474,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "MinionPro-Regular (9.6pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 135,
      "text": "Even if the overlap is above 80%, the model can still do poorly depending on which\nwords fall in the 20%. If these words are important for the task, then this is very pos‐\nsible. For example, say we want to build a classifier that can classify medical docu‐\nments on cancer from medical documents on the heart. Now, in this case, certain\nterms like heart, cancer, etc., will become important for differentiating the two sets of\ndocuments. If these terms are not present in the word embedding’s vocabulary, our\nclassifier might still do poorly.\nAnother way to deal with the OOV problem for word embeddings is to create vectors\nthat are initialized randomly, where each component is between –0.25 to +0.25, and\ncontinue to use these vectors throughout the application we’re building [17, 18].\nFrom our own experience, this can give us a jump of 1–2% in performance.\nThere are also other approaches that handle the OOV problem by modifying the\ntraining process by bringing in characters and other subword-level linguistic compo‐\nnents. Let’s look at one such approach now. The key idea is that one can potentially\nhandle the OOV problem by using subword information, such as morphological\nproperties (e.g., prefixes, suffixes, word endings, etc.), or by using character represen‐\ntations. fastText [19], from Facebook AI research, is one of the popular algorithms\nthat follows this approach. A word can be represented by its constituent character n-\ngrams. Following a similar architecture to Word2vec, fastText learns embeddings for\nwords and character n-grams together and views a word’s embedding vector as an\naggregation of its constituent character n-grams. This makes it possible to generate\nembeddings even for words that are not present in the vocabulary. Say there’s a word,\n“gregarious,” that’s not found in the embedding’s word vocabulary. We break it into\ncharacter n-grams—gre, reg, ega, ….ous—and combine these embeddings of the n-\ngrams to arrive at the embedding of “gregarious.”\nPre-trained fastText models can be downloaded from their website [20], and gensim’s\nfastText wrapper can be used both for loading pre-trained models or training models\nusing fastText in a way similar to Word2vec. We leave that as an exercise for the\nreader. In Chapter 4, we’ll see how to use fastText embeddings for text classification.\nNow, we’ll take a look at some distributed representations that move beyond words.\nDistributed Representations Beyond Words\nand Characters\nSo far, we’ve seen two approaches to coming up with text representations using\nembeddings. Word2vec learned representations for words, and we aggregated them\nto form text representations. fastText learned representations for character n-grams,\nwhich were aggregated to form word representations and then text representations. A\npotential problem with both approaches is that they do not take the context of words\ninto account. Take, for example, the sentences “dog bites man” and “man bites dog.”\nDistributed Representations Beyond Words and Characters \n| \n105\n",
      "word_count": 478,
      "char_count": 3031,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 136,
      "text": "Both receive the same representation in these approaches, but they obviously have\nvery different meanings. Let’s look at another approach, Doc2vec, which allows us to\ndirectly learn the representations for texts of arbitrary lengths (phrases, sentences,\nparagraphs, and documents) by taking the context of words in the text into account.\nDoc2vec is based on the paragraph vectors framework [21] and is implemented in\ngensim. This is similar to Word2vec in terms of its general architecture, except that,\nin addition to the word vectors, it also learns a “paragraph vector” that learns a\nrepresentation for the full text (i.e., with words in context). When learning with a\nlarge corpus of many texts, the paragraph vectors are unique for a given text (where\n“text” can mean any piece of text of arbitrary length), while word vectors will be\nshared across all texts. The shallow neural networks used to learn Doc2vec embed‐\ndings (Figure 3-13) are very similar to the CBOW and SkipGram architecture of\nWord2vec. The two architectures are called distributed memory (DM) and distributed\nbag of words (DBOW). They are shown in Figure 3-13.\nOnce the Doc2vec model is trained, paragraph vectors for new texts are inferred\nusing the common word vectors from training. Doc2vec was perhaps the first widely\naccessible implementation for getting an embedding representation for the full text\ninstead of using a combination of individual word vectors. Since it models some form\nof context and can encode texts of arbitrary length into a fixed, low-dimensional,\ndense vector, it has found application in a wide range of NLP applications, such as\ntext classification, document tagging, text recommendation systems, and simple chat‐\nbots for FAQs. We’ll see an example of training a Doc2vec representation and using it\nfor text classification in the next chapter. Let’s look at other text representation meth‐\nods that extend this idea of taking full text into account.\nFigure 3-13. Doc2vec architectures: DM (left) and DBOW (right)\n106 \n| \nChapter 3: Text Representation\n",
      "word_count": 327,
      "char_count": 2057,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1419,
          "height": 498,
          "ext": "png",
          "size_bytes": 48140
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 137,
      "text": "Universal Text Representations\nIn all the representations we’ve seen so far, we notice that one word gets one fixed\nrepresentation. Can this be a problem? Well, to some extent, yes. Words can mean\ndifferent things in different contexts. For example, the sentences “I went to a bank to\nwithdraw money” and “I sat by the river bank and pondered about text representa‐\ntions” both use the word “bank.” However, they mean different things in each\nsentence. With the vectorization and embedding approaches that we’ve seen so far,\nthere’s no direct way to capture this information.\nIn 2018, researchers came up with the idea of contextual word representations, which\naddresses this issue. It uses “language modeling,” which is the task of predicting the\nnext likely word in a sequence of words. In its earliest form, it used the idea of n-\ngram frequencies to estimate the probability of the next word given a history of\nwords. The past few years have seen the advent of advanced neural language models\n(e.g., transformers [22, 23]) that make use of the word embeddings we discussed\nearlier but use complex architectures involving multiple passes through the text and\nmultiple reads from left to right and right to left to model the context of language use.\nNeural architectures such as recurrent neural networks (RNNs) and transformers\nwere used to develop large-scale models of language (ELMo [24], BERT [25]), which\ncan be used as pre-trained models to get text representations. The key idea is to lever‐\nage “transfer learning”—that is, to learn embeddings on a generic task (like language\nmodeling) on a massive corpus and then fine-tune learnings on task-specific data.\nThese models have shown significant improvements on some fundamental NLP\ntasks, such as question answering, semantic role labeling, named entity recognition,\nand coreference resolution, to name a few. We briefly described what some of these\ntasks are in Chapter 1. Interested readers can go through the references, including the\nupcoming book by Taher and Collados [26].\nIn the last three sections, we looked at the key ideas behind word embeddings, how to\ntrain them, and how to use pre-trained embeddings to get text representations. We’ll\nlearn more about how to use these representations in different NLP applications in\nthe coming chapters. These representations are very useful and popular in modern-\nday NLP. However, based on our experience, here are a few important aspects to keep\nin mind while using them in your project:\n• All text representations are inherently biased based on what they saw in training\ndata. For example, an embedding model trained heavily on technology news or\narticles is likely to identify Apple as being closer to, say, Microsoft or Facebook\nthan to an orange or pear. While this is anecdotal, such biases stemming from\ntraining data can have serious implications on the performance of NLP models\nand systems that rely on these representations. Understanding biases that may be\npresent in learned embeddings and developing methods for addressing them is\nUniversal Text Representations \n| \n107\n",
      "word_count": 501,
      "char_count": 3098,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 138,
      "text": "very important. An interested reader can look at Tolga et al. [27]. These biases are\nan important factor to consider in any NLP software development.\n• Unlike the basic vectorization approaches, pre-trained embeddings are generally\nlarge-sized files (several gigabytes), which may pose problems in certain deploy‐\nment scenarios. This is something we need to address while using them,\notherwise it can become an engineering bottleneck in performance. The\nWord2vec model takes ~4.5 GB RAM. One good hack is to use in-memory data‐\nbases like Redis [28] with a cache on top of them to address scaling and latency\nissues. Load your embeddings into such databases and use the embeddings as if\nthey’re available in RAM.\n• Modeling language for a real-world application is more than capturing the infor‐\nmation via word and sentence embeddings. We still need ways to encode specific\naspects of text, the relationships between sentences in it, and any other domain-\nand application-specific needs that may not be addressed by the embedding\nrepresentations themselves (yet!). For example, the task of sarcasm detection\nrequires nuances that are not yet captured well by embedding techniques.\n• As we speak, neural text representation is an evolving area in NLP, with rapidly\nchanging state of the art. While it’s easy to get carried away by the next big model\nin the news, a practitioner needs to exercise caution and consider practical issues\nsuch as return on investment from the effort, business needs, and infrastructural\nconstraints before trying to use them in production-grade applications.\nAn interested reader may refer to Smith [29] for a concise summary of the evolution\nof different word representations and the research challenges ahead for neural net‐\nwork models of text representation. Now, let’s move on to techniques for visualizing\nembeddings.\nVisualizing Embeddings\nSo far, we’ve seen various vectorization techniques for representing text. The vectors\nobtained are used as features for the NLP task at hand, be it text classification or a\nquestion-answering system. An important aspect of any ML project is feature explo‐\nration. How can we explore the vectors that we have to work with? Visual exploration\nis a very important aspect of any data-related problem. Is there a way to visually\ninspect word vectors? Even though embeddings are low-dimensional vectors, even\n100 or 300 dimensions are too high to visualize.\nEnter t-SNE [30], or t-distributed Stochastic Neighboring Embedding. It’s a technique\nused for visualizing high-dimensional data like embeddings by reducing them to two-\nor three-dimensional data. The technique takes in the embeddings (or any data) and\nlooks at how to best represent the input data using lesser dimensions, all while main‐\ntaining the same data distributions in original high-dimensional input space and low-\n108 \n| \nChapter 3: Text Representation\n",
      "word_count": 454,
      "char_count": 2895,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 139,
      "text": "dimensional output space. This, therefore, enables us to plot and visualize the input\ndata. It helps to get a feel for the space of word embedding.\nLet’s look at some visualizations using t-SNE. First, we look at feature vectors\nobtained from the MNIST digits dataset [31]. Here, the images are passed through a\nconvolution neural network and the final feature vectors. Figure 3-14 shows the two-\ndimensional plot of the vectors. It clearly shows that our feature vectors are very use‐\nful since vectors of the same class tend to cluster together.\nFigure 3-14. Visualizing MNIST data using t-SNE [32]\nLet’s now visualize word embeddings. In Figure 3-15, we show only a few words. The\ninteresting thing to note is that the words that have similar meanings tend to cluster\ntogether.\nVisualizing Embeddings \n| \n109\n",
      "word_count": 135,
      "char_count": 812,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 643,
          "height": 619,
          "ext": "png",
          "size_bytes": 98362
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 140,
      "text": "Figure 3-15. t-SNE visualizations of word embeddings (eft: numbers, right: job titles [33]\nLet’s look at another word embedding visualization, probably the most famous one in\nthe NLP community. Figure 3-16 shows two-dimensional visualization of embedding\nvectors for a subset of words: man, woman, uncle, aunt, king, queen. Figure 3-16\nshows not only the position of the vectors of these words, but also an interesting\nobservation between the vectors—the arrows capture the “relationship” between\nwords. t-SNE visualization helps greatly in coming up with such nice observations.\nFigure 3-16. t-SNE visualization shows some interesting relationships [7]\nt-SNE works equally well for visualizing document embeddings. For example, we\nmight take Wikipedia articles on various topics, obtain corresponding document vec‐\ntors for each article, then plot these vectors using t-SNE. The visualization in\nFigure 3-17 clearly shows that articles in a given category are grouped together.\nClearly, t-SNE is very useful for eyeballing the quality of feature vectors. We can use\ntools like the embedding projector from TensorBoard [34] to visualize embeddings in\nour day-to-day work. As shown in Figure 3-18, TensorBoard has a nice interface that\nis tailor made for visualizing embeddings. We leave it as an exercise for the reader to\nexplore it further. For more on t-SNE, you can read the excellent article on using t-\nSNE more effectively by Martin et al. [35].\n110 \n| \nChapter 3: Text Representation\n",
      "word_count": 230,
      "char_count": 1492,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1358,
          "height": 505,
          "ext": "png",
          "size_bytes": 24182
        },
        {
          "index": 1,
          "width": 641,
          "height": 474,
          "ext": "png",
          "size_bytes": 12430
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 141,
      "text": "Figure 3-17. Visualization of Wikipedia document vectors [36]\nFigure 3-18. Screenshot of TensorBoard interface for visualizing embeddings [37]\nVisualizing Embeddings \n| \n111\n",
      "word_count": 22,
      "char_count": 174,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 633,
          "height": 550,
          "ext": "png",
          "size_bytes": 213487
        },
        {
          "index": 1,
          "width": 1442,
          "height": 724,
          "ext": "png",
          "size_bytes": 420895
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 142,
      "text": "iv. Unless they’ve been fine-tuned for the task at hand.\nHandcrafted Feature Representations\nSo far in this chapter, we’ve seen various feature representation schemes that are\nlearned from a text corpus, large or small. These feature representations are mostly\nnot dependent on the NLP problem or application domain.iv The same approach\nworks whether we want text representation for information extraction or text classifi‐\ncation and whether we work with a corpus of tweets or scientific articles.\nHowever, in many cases, we do have some domain-specific knowledge about the\ngiven NLP problem, which we would like to incorporate into the model we’re build‐\ning. In such cases, we resort to handcrafted features. Let’s take an example of a real-\nworld NLP system: TextEvaluator [38]. It’s software developed by Educational Testing\nService (ETS). The goal of this tool is to help teachers and educators provide support\nin choosing grade-appropriate reading materials for students and identifying sources\nof comprehension difficulty in texts. Clearly, this is a very specific problem. Having\ngeneral-purpose word embeddings will not help much. It needs specialized features\nextracted from text to model some form of grade appropriateness. The screenshot in\nFigure 3-19 shows some of the specialized features that are extracted from text for\nvarious dimensions of text complexity they model. Clearly, measures such as “syntac‐\ntic complexity,” “concreteness,” etc., cannot be calculated by only converting text into\nBoW or embedding representations. They have to be designed manually, keeping in\nmind both the domain knowledge and the ML algorithms to train the NLP models.\nThis is why we call these handcrafted feature representations.\nFigure 3-19. TextEvaluator software output requiring handcrafted features [38]\n112 \n| \nChapter 3: Text Representation\n",
      "word_count": 279,
      "char_count": 1851,
      "fonts": [
        "MinionPro-Regular (8.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Regular (6.3pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1157,
          "height": 597,
          "ext": "png",
          "size_bytes": 77365
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 143,
      "text": "Another software tool from ETS that’s popular for grading is the automated essay\nscorer used in online exams, such as the Graduate Record Examination (GRE) and\nTest of English as a Foreign Language (TOEFL), to evaluate test-taker essays [39]. \nThis tool, too, requires handcrafted features. Evaluating an essay for various aspects\nof writing requires specialized features that address those needs. One cannot rely only\non n-grams or word embeddings. Another NLP application where one may need\nsuch specialized feature engineering is the spelling and grammar correction we use in\ntools such as Microsoft Word, Grammarly, etc. These are all examples of commonly\nused tools where we often need custom features to incorporate domain knowledge.\nClearly, custom feature engineering is much more difficult to formulate compared to\nother feature engineering schemes we’ve seen so far. It’s for this reason that vectoriza‐\ntion approaches are more accessible to get started with, especially when we don’t have\nenough understanding of the domain. Still, handcrafted features are very common in\nseveral real-world applications. In most industrial application scenarios, we end up\ncombining the problem-agnostic feature representations we saw earlier (basic vecto‐\nrization and distributed representations) with some domain-specific features to\ndevelop hybrid features. Recent studies from IBM Research [40] and Walmart [41]\nshow examples of how heuristics, handcrafted features, and ML are all used together\nin real-world industrial systems dealing with NLP problems. We’ll see some examples\nof how to use such handcrafted features in upcoming chapters, such as text classifica‐\ntion (Chapter 4) and information extraction (Chapter 5).\nWrapping Up\nIn this chapter, we saw different techniques for representing text, starting from the\nbasic approaches to state-of-the-art DL methods. A question that may arise organi‐\ncally at this point would be: when should we go for vectorization features and embed‐\ndings, and when should we look for handcrafted features? The answer is that it\ndepends on the task at hand. For some applications, such as text classification, it’s\nmore common to see vectorization approaches and embeddings as the go-to feature\nrepresentations for text. For some other applications, such as information extraction,\nor in the examples we saw in the previous section, it’s more common to look for\nhandcrafted, domain-specific features. Quite often, a hybrid approach that combines\nboth kinds of features are used in practice. Having said that, vectorization-based\napproaches are a great starting point.\nWe hope the discussion and various perspectives presented in this chapter gave you a\ngood idea about the role of text representation in NLP, different techniques for repre‐\nsenting text, and their respective pros and cons. In the next chapters, we’ll move on to\nsolving some of the essential NLP tasks (Chapters 4–7), where we’ll see different text\nrepresentations being put to use, starting with text representation.\nWrapping Up \n| \n113\n",
      "word_count": 463,
      "char_count": 3048,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 144,
      "text": "References\n[1] Bansal, Suraj. “Convolutional Neural Networks Explained”. December 28, 2019.\n[2] Manning, C., Hinrich Schütze, and Prabhakar Raghavan. Introduction to Informa‐\ntion \nRetrieval. \nCambridge: \nCambridge \nUniversity \nPress, \n2008. \nISBN:\n978-0-521-86571-5\n[3] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft), 2018.\n[4] scikit-learn.org. TFIDF vectorizer documentation. Last accessed June 15, 2020.\n[5] Firth, John R. “A Synopsis of Linguistic Theory 1930–1955.” Studies in Linguistic\nAnalysis (1968).\n[6] Ferrone, Lorenzo, and Fabio Massimo Zanzotto. “Symbolic, Distributed and Dis‐\ntributional Representations for Natural Language Processing in the Era of Deep\nLearning: A Survey”, (2017).\n[7] Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. “Efficient Estimation\nof Word Representations in Vector Space”, (2013).\n[8] Google. Word2vec pre-trained model. Last accessed June 15, 2020.\n[9] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. “GloVe: Global\nVectors for Word Representation”. Last accessed June 15, 2020.\n[10] Facebook. fastText pre-trained model. Last accessed June 15, 2020.\n[11] Ruder, Sebastian. Three-part blog series on word embeddings. https://oreil.ly/\nOkJnx, https://oreil.ly/bjygp, and https://oreil.ly/GHgg9. Last accessed June 15, 2020.\n[12] Rong, Xin. “word2vec parameter learning explained”, (2014).\n[13] Levy, Omer, Yoav Goldberg, and Ido Dagan. “Improving Distributional Similar‐\nity with Lessons Learned from Word Embeddings.” Transactions of the Association for\nComputational Linguistics 3 (2015): 211–225.\n[14] Levy, Omer and Yoav Goldberg. “Neural Word Embedding as Implicit Matrix\nFactorization.” Proceedings of the 27th International Conference on Neural Information\nProcessing Systems 2 (2014): 2177–2185.\n[15] RaRe Technologies. gensim: Topic Modelling for Humans, (GitHub repo). Last\naccessed June 15, 2020.\n[16] Explosion.ai. “spaCy: Industrial-Strength Natural Language Processing in\nPython”. Last accessed June 15, 2020.\n[17] word2vec-toolkit Google Group discussion. Last accessed June 15, 2020.\n114 \n| \nChapter 3: Text Representation\n",
      "word_count": 284,
      "char_count": 2154,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 145,
      "text": "[18] Code for: Kim, Yoon. “Convolutional neural networks for sentence classifica‐\ntion”. (2014).\n[19] Facebook Open Source. “fastText: Library for efficient text classification and rep‐\nresentation learning”. Last accessed June 15, 2020.\n[20] Facebook Open Source. “English word vectors”. Last accessed June 15, 2020.\n[21] Le, Quoc, and Tomas Mikolov. “Distributed Representations of Sentences and\nDocuments.” Proceedings of the 31st International Conference on Machine Learning\n(2014): 1188–1196.\n[22] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention is All You Need.”\nAdvances in Neural Information Processing Systems 30 (NIPS 2017): 5998–6008.\n[23] Wang, Chenguang, Mu Li, and Alexander J. Smola. “Language Models with\nTransformers”, (2019).\n[24] Allen Institute for AI. “ELMo: Deep contextualized word representations”. Last\naccessed June 15, 2020.\n[25] Google Research. bert: TensorFlow code and pre-trained models for BERT, (Git‐\nHub repo). Last accessed June 15, 2020.\n[26] Pilehvar, Mohammad Taher and Jose Camacho-Collados. “Embeddings in Natu‐\nral Language Processing: Theory and Advances in Vector Representation of Mean‐\ning.” Synthesis Lectures on Human Language Technologies. Morgan & Claypool, 2020.\n[27] Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam\nT. Kalai. “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing\nWord Embeddings.” Advances in Neural Information Processing Systems 29 (NIPS\n2016): 4349–4357.\n[28] Redis. Last accessed June 15, 2020.\n[29] Smith, Noah A. “Contextual Word Representations: A Contextual Introduction”,\n(2019).\n[30] Maaten, Laurens van der and Geoffrey Hinton. “Visualizing Data Using t-SNE.”\nJournal of Machine Learning Research 9, Nov. (2008): 2579–2605.\n[31] Le Cun, Yann, Corinna Cortes, and Christopher J.C. Burges. “The MNIST data‐\nbase of handwritten digits”. Last accessed June 15, 2020.\n[32] Rossant, Cyril. “An illustrated introduction to the t-SNE algorithm”. March 3,\n2015.\nWrapping Up \n| \n115\n",
      "word_count": 296,
      "char_count": 2084,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 146,
      "text": "[33] Turian, Joseph, Lev Ratinov, and Yoshua Bengio. “Word Representations: A Sim‐\nple and General Method for Semi-Supervised Learning.” Proceedings of the 48th\nAnnual Meeting of the Association for Computational Linguistics (2020): 384–394.\n[34] TensorFlow. “Word embeddings” tutorial. Last accessed June 15, 2020.\n[35] Wattenberg, Martin, Fernanda Viégas, and Ian Johnson. “How to Use t-SNE\nEffectively.” Distill 1.10 (2016): e2.\n[36] Dai, Andrew M., Christopher Olah, and Quoc V. Le. “Document Embedding\nwith Paragraph Vectors”, (2015).\n[37] TensorFlow. “Embedding Projector”. Last accessed June 15, 2020.\n[38] Educational Testing Service. “TextEvaluator”. Last accessed June 15, 2020.\n[39] Educational Testing Service. “Automated Scoring of Written Responses”, 2019.\n[40] Chiticariu, L., Yunyao Li, and Frederick R. Reiss. “Rule-Based Information\nExtraction is Dead! Long Live Rule-Based Information Extraction Systems!” Proceed‐\nings of the 2013 Conference on Empirical Methods in Natural Language Processing\n(2013): 827–832.\n[41] Suganthan G.C., Paul, Chong Sun, Haojun Zhang, Frank Yang, Narasimhan\nRampalli, Shishir Prasad, Esteban Arcaute, et al. “Why Big Data Industrial Systems\nNeed Rules and What We Can Do About It.” Proceedings of the 2015 ACM SIGMOD\nInternational Conference on Management of Data (2015): 265–276.\n116 \n| \nChapter 3: Text Representation\n",
      "word_count": 192,
      "char_count": 1368,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 147,
      "text": "PART II\nEssentials\n",
      "word_count": 3,
      "char_count": 19,
      "fonts": [
        "MyriadPro-SemiboldCond (28.4pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 148,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 149,
      "text": "CHAPTER 4\nText Classification\nOrganizing is what you do before you do something,\nso that when you do it, it is not all mixed up.\n—A.A. Milne\nAll of us check email every day, possibly multiple times. A useful feature of most\nemail service providers is the ability to automatically segregate spam emails away\nfrom regular emails. This is a use case of a popular NLP task known as text classifica‐\ntion, which is the focus of this chapter. Text classification is the task of assigning one\nor more categories to a given piece of text from a larger set of possible categories. In\nthe email spam–identifier example, we have two categories—spam and non-spam—\nand each incoming email is assigned to one of these categories. This task of categoriz‐\ning texts based on some properties has a wide range of applications across diverse\ndomains, such as social media, e-commerce, healthcare, law, and marketing, to name\na few. Even though the purpose and application of text classification may vary from\ndomain to domain, the underlying abstract problem remains the same. This invari‐\nance of the core problem and its applications in a myriad of domains makes text clas‐\nsification by far the most widely used NLP task in industry and the most researched\nin academia. In this chapter, we’ll discuss the usefulness of text classification and how\nto build text classifiers for our use cases, along with some practical tips for real-world\nscenarios.\nIn machine learning, classification is the problem of categorizing a data instance into\none or more known classes. The data point can be originally of different formats,\nsuch as text, speech, image, or numeric. Text classification is a special instance of the\nclassification problem, where the input data point(s) is text and the goal is to catego‐\nrize the piece of text into one or more buckets (called a class) from a set of pre-\ndefined buckets (classes). The “text” can be of arbitrary length: a character, a word, a\nsentence, a paragraph, or a full document. Consider a scenario where we want to\nclassify all customer reviews for a product into three categories: positive, negative,\n119\n",
      "word_count": 359,
      "char_count": 2126,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (9.3pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 150,
      "text": "and neutral. The challenge of text classification is to “learn” this categorization from a\ncollection of examples for each of these categories and predict the categories for new,\nunseen products and new customer reviews. This categorization need not always\nresult in a single category, though, and there can be any number of categories avail‐\nable. Let’s take a quick look at the taxonomy of text classification to understand this.\nAny supervised classification approach, including text classification, can be further\ndistinguished into three types based on the number of categories involved: binary,\nmulticlass, and multilabel classification. If the number of classes is two, it’s called\nbinary classification. If the number of classes is more than two, it’s referred to as mul‐\nticlass classification. Thus, classifying an email as spam or not-spam is an example of\nbinary classification setting. Classifying the sentiment of a customer review as nega‐\ntive, neutral, or positive is an example of multiclass classification. In both binary and\nmulticlass settings, each document belongs to exactly one class from C, where C is the\nset of all possible classes. In multilabel classification, a document can have one or\nmore labels/classes attached to it. For example, a news article on a soccer match may\nbelong to more than one category, such as “sports” and “soccer,” simultaneously,\nwhereas another news article on US elections may have the labels “politics,” “USA,”\nand “elections.” Thus, each document has labels that are a subset of C. Each article can\nbe in no class, exactly one class, multiple classes, or all of the classes. Sometimes, the\nnumber of labels in the set C can be very large (known as “extreme classification”). In\nsome other scenarios, we may have a hierarchical classification system, which may\nresult in each text getting different labels at different levels in the hierarchy. In this\nchapter, we’ll focus only on binary and multiclass classification, as those are the most\ncommon use cases of text classification in the industry.\nText classification is sometimes also referred to as topic classification, text categoriza‐\ntion, or document categorization. For the rest of this book, we’ll stick to the term “text\nclassification.” Note that topic classification is different from topic detection, which\nrefers to the problem of uncovering or extracting “topics” from texts, which we’ll\nstudy in Chapter 7.\nIn this chapter, we’ll take a closer look at text classification and build text classifiers\nusing different approaches. Our aim is to provide an overview of some of the most\ncommonly applied techniques along with practical advice on handling different sce‐\nnarios and decisions that have to be made when building text classification systems in\npractice. We’ll start by introducing some common applications of text classification,\nthen we’ll discuss what an NLP pipeline for text classification looks like and illustrate\nthe use of this pipeline to train and test text classifiers using different approaches,\nranging from the traditional methods to the state of the art. We’ll then tackle the\nproblem of training data collection/sparsity and different methods to handle it. We’ll\nend the chapter by summarizing what we learned in all these sections along with\nsome practical advice and a case study.\n120 \n| \nChapter 4: Text Classification\n",
      "word_count": 531,
      "char_count": 3369,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 151,
      "text": "Note that, in this chapter, we’ll only deal with the aspect of training and evaluating the\ntext classifiers. Issues related to deploying NLP systems in general and performing\nquality assurance will be discussed in Chapter 11.\nApplications\nText classification has been of interest in a number of application scenarios, ranging\nfrom identifying the author of an unknown text in the 1800s to the efforts of USPS in\nthe 1960s to perform optical character recognition on addresses and zip codes [1]. In\nthe 1990s, researchers began to successfully apply ML algorithms for text classifica‐\ntion for large datasets. Email filtering, popularly known as “spam classification,” is\none of the earliest examples of automatic text classification, which impacts our lives\nto this day. From manual analyses of text documents to purely statistical, computer-\nbased approaches and state-of-the-art deep neural networks, we’ve come a long way\nwith text classification. Let’s briefly discuss some of the popular applications before\ndiving into the different approaches to perform text classification. These examples\nwill also be useful in identifying problems that can be solved using text classification\nmethods in your organization.\nContent classification and organization\nThis refers to the task of classifying/tagging large amounts of textual data. This,\nin turn, is used to power use cases like content organization, search engines, and\nrecommendation systems, to name a few. Examples of such data include news\nwebsites, blogs, online bookshelves, product reviews, tweets, etc.; tagging product\ndescriptions in an e-commerce website; routing customer service requests in a\ncompany to the appropriate support team; and organizing emails into personal,\nsocial, and promotions in Gmail are all examples of using text classification for\ncontent classification and organization.\nCustomer support\nCustomers often use social media to express their opinions about and experien‐\nces of products or services. Text classification is often used to identify the tweets\nthat brands must respond to (i.e., those that are actionable) and those that don’t\nrequire a response (i.e., noise) [2, 3]. To illustrate, consider the three tweets about\nthe brand Macy’s shown in Figure 4-1.\nAlthough all three tweets mention the brand Macy’s explicitly, only the first one\nnecessitates a reply from Macy’s customer support team.\nApplications \n| \n121\n",
      "word_count": 363,
      "char_count": 2410,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 152,
      "text": "Figure 4-1. Tweets reaching out to brands: one is actionable, the other two are noise\nE-commerce\nCustomers leave reviews for a range of products on e-commerce websites like\nAmazon, eBay, etc. An example use of text classification in this kind of scenario is\nto understand and analyze customers’ perception of a product or service based\non their comments. This is commonly known as “sentiment analysis.” It’s used\nextensively by brands across the globe to better understand whether they’re get‐\nting closer to or farther away from their customers. Rather than categorizing cus‐\ntomer feedback as simply positive, negative, or neutral, over a period of time,\nsentiment analysis has evolved into a more sophisticated paradigm: “aspect”-\nbased sentiment analysis. To understand this, consider the customer review of a\nrestaurant shown in Figure 4-2.\nFigure 4-2. A review that praises some aspects and criticizes few\nWould you call the review in Figure 4-2 negative, positive, or neutral? It’s difficult\nto answer this—the food was great, but the service was bad. Practitioners and\nbrands working with sentiment analysis have realized that many products or\nservices have multiple facets. In order to understand overall sentiment, under‐\nstanding each and every facet is important. Text classification plays a major role\n122 \n| \nChapter 4: Text Classification\n",
      "word_count": 211,
      "char_count": 1354,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1199,
          "height": 677,
          "ext": "png",
          "size_bytes": 201052
        },
        {
          "index": 1,
          "width": 575,
          "height": 170,
          "ext": "png",
          "size_bytes": 22284
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 153,
      "text": "in performing such fine-grained analysis of customer feedback. We’ll discuss this\nspecific application in detail in Chapter 9.\nOther applications\nApart from the above-mentioned areas, text classification is also used in several\nother applications in various domains:\n• Text classification is used in language identification, like identifying the lan‐\nguage of new tweets or posts. For example, Google Translate has an auto‐\nmatic language identification feature.\n• Authorship attribution, or identifying the unknown authors of texts from a\npool of authors, is another popular use case of text classification, and it’s used\nin a range of fields from forensic analysis to literary studies.\n• Text classification has been used in the recent past for triaging posts in an\nonline support forum for mental health services [4]. In the NLP community,\nannual competitions are conducted (e.g., clpsych.org) for solving such text\nclassification problems originating from clinical research.\n• In the recent past, text classification has also been used to segregate fake\nnews from real news.\nNote that this section only serves as an illustration of the wide range of applications\nof text classification, and the list is not exhaustive, but we hope it gives you enough\nbackground to identify text classification problems in your workplace projects when\nyou encounter them. Let’s now look at how to build such text classification models.\nA Pipeline for Building Text Classification Systems\nIn Chapter 2, we discussed some of the common NLP pipelines. The text classifica‐\ntion pipeline shares some of its steps with the pipelines we learned in that chapter.\nOne typically follows these steps when building a text classification system:\n1. Collect or create a labeled dataset suitable for the task.\n2. Split the dataset into two (training and test) or three parts: training, validation\n(i.e., development), and test sets, then decide on evaluation metric(s).\n3. Transform raw text into feature vectors.\n4. Train a classifier using the feature vectors and the corresponding labels from the\ntraining set.\n5. Using the evaluation metric(s) from Step 2, benchmark the model performance\non the test set.\n6. Deploy the model to serve the real-world use case and monitor its performance.\nA Pipeline for Building Text Classification Systems \n| \n123\n",
      "word_count": 364,
      "char_count": 2325,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 154,
      "text": "Figure 4-3 shows these typical steps in building a text classification system.\nFigure 4-3. Flowchart of a text classification pipeline\nSteps 3 through 5 are iterated on to explore different variants of features and classifi‐\ncation algorithms and their parameters and to tune the hyperparameters before pro‐\nceeding to Step 6, deploying the optimal model in production.\nSome of the individual steps related to data collection and pre-processing were dis‐\ncussed in past chapters. For example, Steps 1 and 2 were discussed in detail in Chap‐\nter 2. Chapter 3 focused entirely on Step 3. Our focus in this chapter is on Steps 4\nthrough 5. Toward the end of this chapter, we’ll revisit Step 1 to discuss issues specific\nto text classification. We’ll deal with Step 6 in Chapter 11. To be able to perform Steps\n4 through 5 (i.e., to benchmark the performance of a model or compare multiple clas‐\nsifiers), we need the right measure(s) of evaluation. Chapter 2 discussed various gen‐\neral metrics used in evaluating NLP systems. For evaluating classifiers specifically,\namong the metrics introduced in Chapter 2, the following are used more commonly:\nclassification accuracy, precision, recall, F1 score, and area under ROC curve. In this\nchapter, we’ll use some of these measures to evaluate our models and also look at con‐\nfusion matrices to understand the model performance in detail.\nApart from these, when classification systems are deployed in real-world applications,\nkey performance indicators (KPIs) specific to a given business use case are also used\nto evaluate their impact and return on investment (ROI). These are often the metrics\n124 \n| \nChapter 4: Text Classification\n",
      "word_count": 272,
      "char_count": 1681,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1169,
          "height": 997,
          "ext": "png",
          "size_bytes": 64313
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 155,
      "text": "business teams care about. For example, if we’re using text classification to automati‐\ncally route customer service requests, a possible KPI could be the reduction in wait\ntime before the request is responded to compared to manual routing. In this chapter,\nwe’ll focus on the NLP evaluation measures. In Part III of the book, where we’ll dis‐\ncuss NLP use cases specific to industry verticals, we’ll introduce some KPIs that are\noften used in those verticals.\nBefore we start looking at how to build text classifiers using the pipeline we just dis‐\ncussed, let’s take a look at the scenarios where this pipeline is not at all necessary or\nwhere it’s not possible to use it.\nA Simple Classifier Without the Text Classification Pipeline\nWhen we talk about the text classification pipeline, we’re referring to a supervised\nmachine learning scenario. However, it’s possible to build a simple classifier without\nmachine learning and without this pipeline. Consider the following problem sce‐\nnario: we’re given a corpus of tweets where each tweet is labeled with its correspond‐\ning sentiment: negative or positive. For example, a tweet that says, “The new James\nBond movie is great!” is clearly expressing a positive sentiment, whereas a tweet that\nsays, “I would never visit this restaurant again, horrible place!!” has a negative senti‐\nment. We want to build a classification system that will predict the sentiment of an\nunseen tweet using only the text of the tweet. A simple solution could be to create\nlists of positive and negative words in English—i.e., words that have a positive or neg‐\native sentiment. We then compare the usage of positive versus negative words in the\ninput tweet and make a prediction based on this information. Further enhancements\nto this approach may involve creating more sophisticated dictionaries with degrees of\npositive, negative, and neutral sentiment of words or formulating specific heuristics\n(e.g., usage of certain smileys indicate positive sentiment) and using them to make\npredictions. This approach is called lexicon-based sentiment analysis.\nClearly, this does not involve any “learning” of text classification; that is, it’s based on\na set of heuristics or rules and custom-built resources such as dictionaries of words\nwith sentiment. While this approach may seem too simple to perform reasonably well\nfor many real-world scenarios, it may enable us to deploy a minimum viable product\n(MVP) quickly. Most importantly, this simple model can lead to better understanding\nof the problem and give us a simple baseline for our evaluation metric and speed.\nFrom our experience, it’s always good to start with such simpler approaches when\ntackling a new NLP problem, where possible. However, eventually, we’ll need ML\nmethods that can infer more insights from large collections of text data and perform\nbetter than the baseline approach.\nA Pipeline for Building Text Classification Systems \n| \n125\n",
      "word_count": 469,
      "char_count": 2938,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 156,
      "text": "Using Existing Text Classification APIs\nAnother scenario where we may not have to “learn” a classifier or follow this pipeline\nis when our task is more generic in nature, such as identifying a general category of a\ntext (e.g., whether it’s about technology or music). In such cases, we can use existing\nAPIs, such as Google Cloud Natural Language [5], that provide off-the-shelf content\nclassification models that can identify close to 700 different categories of text.\nAnother popular classification task is sentiment analysis. All major service providers\n(e.g., Google, Microsoft, and Amazon) serve sentiment analysis APIs [5, 6, 7] with\nvarying payment structures. If we’re tasked with building a sentiment classifier, we\nmay not have to build our own system if an existing API addresses our business\nneeds.\nHowever, many classification tasks could be specific to our organization’s business\nneeds. For the rest of this chapter, we’ll address the scenario of building our own clas‐\nsifier by considering the pipeline described earlier in this section.\nOne Pipeline, Many Classifiers\nLet’s now look at building text classifiers by altering Steps 3 through 5 in the pipeline\nand keeping the remaining steps constant. A good dataset is a prerequisite to start\nusing the pipeline. When we say “good” dataset, we mean a dataset that is a true rep‐\nresentation of the data we’re likely to see in production. Throughout this chapter,\nwe’ll use some of the publicly available datasets for text classification. A wide range of\nNLP-related datasets, including ones for text classification, are listed online [8]. Addi‐\ntionally, Figure Eight [9] contains a collection of crowdsourced datasets, some of\nwhich are relevant to text classification. The UCI Machine Learning Repository [10]\nalso contains a few text classification datasets. Google recently launched a dedicated\nsearch system for datasets for machine learning [11]. We’ll use multiple datasets\nthroughout this chapter instead of sticking to one to illustrate any dataset-specific\nissues you may come across.\nNote that our goal in this chapter is to give you an overview of different approaches.\nNo single approach is known to work universally well on all kinds of data and all clas‐\nsification problems. In the real world, we experiment with multiple approaches, eval‐\nuate them, and choose one final approach to deploy in practice.\nFor the rest of this section, we’ll use the “Economic News Article Tone and Rele‐\nvance” dataset from Figure Eight to demonstrate text classification. It consists of\n8,000 news articles annotated with whether or not they’re relevant to the US economy\n(i.e., a yes/no binary classification). The dataset is also imbalanced, with ~1,500 rele‐\nvant and ~6,500 non-relevant articles, which poses the challenge of guarding against\nlearning a bias toward the majority category (in this case, non-relevant articles).\nClearly, learning what a relevant news article is is more challenging with this dataset\n126 \n| \nChapter 4: Text Classification\n",
      "word_count": 475,
      "char_count": 3024,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 157,
      "text": "than learning what is irrelevant. After all, just guessing that everything is irrelevant\nalready gives us 80% accuracy!\nLet’s explore how a BoW representation (introduced in Chapter 3) can be used with\nthis dataset following the pipeline described earlier in this chapter. We’ll build classifi‐\ners using three well-known algorithms: Naive Bayes, logistic regression, and support\nvector machines. The notebook related to this section (Ch4/OnePipeline_ManyClassi‐\nfiers.ipynb) shows the step-by-step process of following our pipeline using these three\nalgorithms. We’ll discuss some of the important aspects in this section.\nNaive Bayes Classifier\nNaive Bayes is a probabilistic classifier that uses Bayes’ theorem to classify texts based\non the evidence seen in training data. It estimates the conditional probability of each\nfeature of a given text for each class based on the occurrence of that feature in that\nclass and multiplies the probabilities of all the features of a given text to compute the\nfinal probability of classification for each class. Finally, it chooses the class with maxi‐\nmum probability. A detailed step-by-step explanation of the classifier is beyond the\nscope of this book. However, a reader interested in Naive Bayes with a detailed\nexplanation in the context of text classification can look at Chapter 4 of Jurafsky and\nMartin [12]. Although simple, Naive Bayes is commonly used as a baseline algorithm\nin classification experiments.\nLet’s walk through the key steps of an implementation of the pipeline described ear‐\nlier for our dataset. For this, we use a Naive Bayes implementation in scikit-learn.\nOnce the dataset is loaded, we split the data into train and test data, as shown in the\ncode snippet below:\n#Step 1: train-test split\nX = our_data.text \n#the column text contains textual data to extract features from.\ny = our_data.relevance \n#this is the column we are learning to predict.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n#split X and y into training and testing sets. By default, \nit splits 75% #training and 25% test. random_state=1 for reproducibility.\nThe next step is to pre-process the texts and then convert them into feature vectors.\nWhile there are many different ways to do the pre-processing, let’s say we want to do\nthe following: lowercasing and removal of punctuation, digits and any custom\nstrings, and stop words. The code snippet below shows this pre-processing and con‐\nverting the train and test data into feature vectors using CountVectorizer in scikit-\nlearn, which is the implementation of the BoW approach we discussed in Chapter 3:\n#Step 2-3: Pre-process and Vectorize train and test data\nvect = CountVectorizer(preprocessor=clean) \n#clean is a function we defined for pre-processing, seen in the notebook.\nX_train_dtm = vect.fit_transform(X_train)\nOne Pipeline, Many Classifiers \n| \n127\n",
      "word_count": 445,
      "char_count": 2887,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 158,
      "text": "X_test_dtm = vect.transform(X_test)\nprint(X_train_dtm.shape, X_test_dtm.shape)\nOnce we run this in the notebook, we’ll see that we ended up having a feature vector\nwith over 45,000 features! We now have the data in a format we want: feature vectors.\nSo, the next step is to train and evaluate a classifier. The code snippet below shows\nhow to do the training and evaluation of a Naive Bayes classifier with the features we\nextracted above:\nnb = MultinomialNB() #instantiate a Multinomial Naive Bayes classifier\nnb.fit(X_train_dtm, y_train)#train the mode \ny_pred_class = nb.predict(X_test_dtm)#make class predictions for test data\nFigure 4-4 shows the confusion matrix of this classifier with test data.\nFigure 4-4. Confusion matrix for Naive Bayes classifier\nAs evident from Figure 4-4, the classifier is doing fairly well with identifying the non-\nrelevant articles correctly, only making errors 14% of the time. However, it does not\nperform well in comparison to the second category: relevance. The category is identi‐\nfied correctly only 42% of the time. An obvious thought may be to collect more data.\nThis is correct and often the most rewarding approach. But in the interest of covering\nother approaches, we assume that we cannot change it or collect additional data. This\nis not a far-fetched assumption—in industry, we often don’t have the luxury of col‐\nlecting more data; we have to work with what we have. We can think of a few possible\n128 \n| \nChapter 4: Text Classification\n",
      "word_count": 238,
      "char_count": 1488,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1385,
          "height": 1209,
          "ext": "png",
          "size_bytes": 42063
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 159,
      "text": "reasons for this performance and ways to improve this classifier. These are summar‐\nized in Table 4-1, and we’ll look into some of them as we progress in this chapter.\nTable 4-1. Potential reasons for poor classifier performance\nReason 1 Since we extracted all possible features, we ended up in a large, sparse feature vector, where most features are\ntoo rare and end up being noise. A sparse feature set also makes training hard.\nReason 2 There are very few examples of relevant articles (~20%) compared to the non-relevant articles (~80%) in the\ndataset. This class imbalance makes the learning process skewed toward the non-relevant articles category, as\nthere are very few examples of “relevant” articles.\nReason 3 Perhaps we need a better learning algorithm.\nReason 4 Perhaps we need a better pre-processing and feature extraction mechanism.\nReason 5 Perhaps we should look to tuning the classifier’s parameters and hyperparameters.\nLet’s see how to improve our classification performance by addressing some of the\npossible reasons for it. One way to approach Reason 1 is to reduce noise in the feature\nvectors. The approach in the previous code example had close to 40,000 features\n(refer to the Jupyter notebook for details). A large number of features introduce spar‐\nsity; i.e., most of the features in the feature vector are zero, and only a few values are\nnon-zero. This, in turn, affects the ability of the text classification algorithm to learn.\nLet’s see what happens if we restrict this to 5,000 and rerun the training and evalua‐\ntion process. This requires us to change the CountVectorizer instantiation in the\nprocess, as shown in the code snippet below, and repeat all the steps:\nvect = CountVectorizer(preprocessor=clean, max_features=5000) #Step-1\nX_train_dtm = vect.fit_transform(X_train)#combined step 2 and 3\nX_test_dtm = vect.transform(X_test)\nnb = MultinomialNB() #instantiate a Multinomial Naive Bayes model\n%time nb.fit(X_train_dtm, y_train)\n#train the model(timing it with an IPython \"magic command\")\ny_pred_class = nb.predict(X_test_dtm)\n#make class predictions for X_test_dtm\nprint(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_class))\nFigure 4-5 shows the new confusion matrix with this setting.\nNow, clearly, while the average performance seems lower than before, the correct\nidentification of relevant articles increased by over 20%. At that point, one may won‐\nder whether this is what we want. The answer to that question depends on the prob‐\nlem we’re trying to solve. If we care about doing reasonably well with non-relevant\narticle identification and doing as well as possible with relevant article identification,\nor doing equally well with both, we could conclude that reducing the feature vector\nsize with the Naive Bayes classifier was useful for this dataset.\nOne Pipeline, Many Classifiers \n| \n129\n",
      "word_count": 437,
      "char_count": 2849,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 160,
      "text": "Figure 4-5. Improved classification performance with Naive Bayes and feature selection\nConsider reducing the number of features if there are too many to\nreduce data sparsity.\nReason 2 in our list was the problem of skew in data toward the majority class. There\nare several ways to address this. Two typical approaches are oversampling the instan‐\nces belonging to minority classes or undersampling the majority class to create a bal‐\nanced dataset. Imbalanced-Learn [13] is a Python library that incorporates some of\nthe sampling methods to address this issue. While we won’t delve into the details of\nthis library here, classifiers also have a built-in mechanism to address such imbal‐\nanced datasets. We’ll see how to use that by taking another classifier, logistic regres‐\nsion, in the next subsection.\nClass imbalance is one of the most common reasons for a classifier\nto not do well. We must always check if this is the case for our task\nand address it.\n130 \n| \nChapter 4: Text Classification\n",
      "word_count": 166,
      "char_count": 998,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        },
        {
          "index": 1,
          "width": 1402,
          "height": 1209,
          "ext": "png",
          "size_bytes": 42682
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 161,
      "text": "To address Reason 3, let’s try using other algorithms, beginning with logistic\nregression.\nLogistic Regression\nWhen we described the Naive Bayes classifier, we mentioned that it learns the proba‐\nbility of a text for each class and chooses the one with maximum probability. Such a\nclassifier is called a generative classifier. In contrast, there’s a discriminative classifier\nthat aims to learn the probability distribution over all classes. Logistic regression is an\nexample of a discriminative classifier and is commonly used in text classification, as a\nbaseline in research, and as an MVP in real-world industry scenarios.\nUnlike Naive Bayes, which estimates probabilities based on feature occurrence in\nclasses, logistic regression “learns” the weights for individual features based on how\nimportant they are to make a classification decision. The goal of logistic regression is\nto learn a linear separator between classes in the training data with the aim of maxi‐\nmizing the probability of the data. This “learning” of feature weights and probability\ndistribution over all classes is done through a function called “logistic” function, and\n(hence the name) logistic regression [14].\nLet’s take the 5,000-dimensional feature vector from the last step of the Naive Bayes\nexample and train a logistic regression classifier instead of Naive Bayes. The code\nsnippet below shows how to use logistic regression for this task:\nfrom sklearn.linear_model import LogisticRegression \nlogreg = LogisticRegression(class_weight=\"balanced\")\nlogreg.fit(X_train_dtm, y_train) \ny_pred_class = logreg.predict(X_test_dtm)\nprint(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_class))\nThis results in a classifier with an accuracy of 73.7%. Figure 4-6 shows the confusion\nmatrix with this approach.\nOur logistic regression classifier instantiation has an argument class_weight, which\nis given a value “balanced”. This tells the classifier to boost the weights for classes in\ninverse proportion to the number of samples for that class. So, we expect to see better\nperformance for the less-represented classes. We can experiment with this code by\nremoving that argument and retraining the classifier, to witness a fall (by approxi‐\nmately 5%) in the bottom-right cell of the confusion matrix. However, logistic regres‐\nsion clearly seems to perform worse than Naive Bayes for this dataset.\nReason 3 in our list was: “Perhaps we need a better learning algorithm.” This gives\nrise to the question: “What is a better learning algorithm?” A general rule of thumb\nwhen working with ML approaches is that there is no one algorithm that learns well\non all datasets. A common approach is to experiment with various algorithms and\ncompare them.\nOne Pipeline, Many Classifiers \n| \n131\n",
      "word_count": 412,
      "char_count": 2762,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 162,
      "text": "Figure 4-6. Classification performance with logistic regression\nLet’s see if this idea helps us by replacing logistic regression with another well-known\nclassification algorithm that was shown to be useful for several text classification\ntasks, called the “support vector machine.”\nSupport Vector Machine\nWe described logistic regression as a discriminative classifier that learns the weights\nfor individual features and predicts a probability distribution over the classes. A sup‐\nport vector machine (SVM), first invented in the early 1960s, is a discriminative classi‐\nfier like logistic regression. However, unlike logistic regression, it aims to look for an\noptimal hyperplane in a higher dimensional space, which can separate the classes in\nthe data by a maximum possible margin. Further, SVMs are capable of learning even\nnon-linear separations between classes, unlike logistic regression. However, they may\nalso take longer to train.\nSVMs come in various flavors in sklearn. Let’s see how one of them is used by keep‐\ning everything else the same and altering maximum features to 1,000 instead of the\nprevious example’s 5,000. We restrict to 1,000 features, keeping in mind the time an\nSVM algorithm takes to train. The code snippet below shows how to do this, and\nFigure 4-7 shows the resultant confusion matrix:\n132 \n| \nChapter 4: Text Classification\n",
      "word_count": 210,
      "char_count": 1361,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1385,
          "height": 1209,
          "ext": "png",
          "size_bytes": 42171
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 163,
      "text": "from sklearn.svm import LinearSVC\nvect = CountVectorizer(preprocessor=clean, max_features=1000) #Step-1\nX_train_dtm = vect.fit_transform(X_train)#combined step 2 and 3\nX_test_dtm = vect.transform(X_test)\nclassifier = LinearSVC(class_weight='balanced') #notice the “balanced” option\nclassifier.fit(X_train_dtm, y_train) #fit the model with training data\ny_pred_class = classifier.predict(X_test_dtm)\nprint(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_class))\nFigure 4-7. Confusion matrix for classification with SVM\nWhen compared to logistic regression, SVMs seem to have done better with the rele‐\nvant articles category, although, among this small set of experiments we did, Naive\nBayes, with the smaller set of features, seems to be the best classifier for this dataset.\nAll the examples in this section demonstrate how changes in different steps affected\nthe classification performance and how to interpret the results. Clearly, we excluded\nmany other possibilities, such as exploring other text classification algorithms, chang‐\ning different parameters of various classifiers, coming up with better pre-processing\nmethods, etc. We leave them as further exercises for the reader, using the notebook as\na playground. A real-world text classification project involves exploring multiple\noptions like this, starting with the simplest approach in terms of modeling, deploy‐\nment, and scaling, and gradually increasing the complexity. Our eventual goal is to\nbuild the classifier that best meets our business needs given all the other constraints.\nOne Pipeline, Many Classifiers \n| \n133\n",
      "word_count": 210,
      "char_count": 1593,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1402,
          "height": 1209,
          "ext": "png",
          "size_bytes": 43420
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 164,
      "text": "Let’s now consider a part of Reason 4 in Table 4-1: better feature representation. So\nfar in this chapter, we’ve used BoW features. Let’s see how we can use other feature\nrepresentation techniques we saw in Chapter 3 for text classification.\nUsing Neural Embeddings in Text Classification\nIn the latter half of Chapter 3, we discussed feature engineering techniques using neu‐\nral networks, such as word embeddings, character embeddings, and document\nembeddings. The advantage of using embedding-based features is that they create a\ndense, low-dimensional feature representation instead of the sparse, high-\ndimensional structure of BoW/TF-IDF and other such features. There are different\nways of designing and using features based on neural embeddings. In this section,\nlet’s look at some ways of using such embedding representations for text\nclassification.\nWord Embeddings\nWords and n-grams have been used primarily as features in text classification for a\nlong time. Different ways of vectorizing words have been proposed, and we used one\nsuch representation in the last section, CountVectorizer. In the past few years, neu‐\nral network–based architectures have become popular for “learning” word represen‐\ntations, which are known as “word embeddings.” We surveyed some of the intuitions\nbehind this in Chapter 3. Let’s now take a look at how to use word embeddings as\nfeatures for text classification. We’ll use the sentiment-labeled sentences dataset from\nthe UCI repository, consisting of 1,500 positive-sentiment and 1,500 negative-\nsentiment sentences from Amazon, Yelp, and IMDB. All the steps are detailed in the\nnotebook Ch4/Word2Vec_Example.ipynb. Let’s walk through the important steps and\nwhere this approach differs from the previous section’s procedures.\nLoading and pre-processing the text data remains a common step. However, instead\nof vectorizing the texts using BoW-based features, we’ll now rely on neural embed‐\nding models. As mentioned earlier, we’ll use a pre-trained embedding model.\nWord2vec is a popular algorithm we discussed in Chapter 3 for training word embed‐\nding models. There are several pre-trained Word2vec models trained on large cor‐\npora available on the internet. Here, we’ll use the one from Google [15]. The\nfollowing code snippet shows how to load this model into Python using gensim:\ndata_path= \"/your/folder/path\"\npath_to_model = os.path.join(data_path,'GoogleNews-vectors-negative300.bin')\ntraining_data_path = os.path.join(data_path, \"sentiment_sentences.txt\")\n#Load W2V model. This will take some time.\nw2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\nprint('done loading Word2Vec')\n134 \n| \nChapter 4: Text Classification\n",
      "word_count": 384,
      "char_count": 2700,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 165,
      "text": "This is a large model that can be seen as a dictionary where the keys are words in the\nvocabulary and the values are their learned embedding representations. Given a\nquery word, if the word’s embedding is present in the dictionary, it will return the\nsame. How do we use this pre-learned embedding to represent features? As we dis‐\ncussed in Chapter 3, there are multiple ways of doing this. A simple approach is just\nto average the embeddings for individual words in text. The code snippet below\nshows a simple function to do this:\n# Creating a feature vector by averaging all embeddings for all sentences\ndef embedding_feats(list_of_lists):\n    DIMENSION = 300\n    zero_vector = np.zeros(DIMENSION)\n    feats = []\n    for tokens in list_of_lists:\n          feat_for_this =  np.zeros(DIMENSION)\n          count_for_this = 0\n          for token in tokens:\n                     if token in w2v_model:\n                          feat_for_this += w2v_model[token]\n                          count_for_this +=1\n          feats.append(feat_for_this/count_for_this)         \n    return feats\ntrain_vectors = embedding_feats(texts_processed)\nprint(len(train_vectors))\nNote that it uses embeddings only for the words that are present in the dictionary. It\nignores the words for which embeddings are absent. Also, note that the above code\nwill give a single vector with DIMENSION(=300) components. We treat the resulting\nembedding vector as the feature vector that represents the entire text. Once this fea‐\nture engineering is done, the final step is similar to what we did in the previous sec‐\ntion: use these features and train a classifier. We leave that as an exercise to the reader\n(refer to the notebook for the full code).\nWhen trained with a logistic regression classifier, these features gave a classification\naccuracy of 81% on our dataset (see the notebook for more details). Considering that\nwe just used an existing word embeddings model and followed only basic pre-\nprocessing steps, this is a great model to have as a baseline! We saw in Chapter 3 that\nthere are other pre-trained embedding approaches, such as GloVe, which can be\nexperimented with for this approach. Gensim, which we used in this example, also\nsupports training our own word embeddings if necessary. If we’re working on a cus‐\ntom domain whose vocabulary is remarkably different from that of the pre-trained\nnews embeddings we used here, it would make sense to train our own embeddings to\nextract features.\nIn order to decide whether to train our own embeddings or use pre-trained embed‐\ndings, a good rule of thumb is to compute the vocabulary overlap. If the overlap\nbetween the vocabulary of our custom domain and that of pre-trained word\nUsing Neural Embeddings in Text Classification \n| \n135\n",
      "word_count": 420,
      "char_count": 2770,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 166,
      "text": "embeddings is greater than 80%, pre-trained word embeddings tend to give good\nresults in text classification.\nAn important factor to consider when deploying models with embedding-based fea‐\nture extraction approaches is that the learned or pre-trained embedding models have\nto be stored and loaded into memory while using these approaches. If the model itself\nis bulky (e.g., the pre-trained model we used takes 3.6 GB), we need to factor this into\nour deployment needs.\nSubword Embeddings and fastText\nWord embeddings, as the name indicates, are about word representations. Even off-\nthe-shelf embeddings seem to work well on classification tasks, as we saw earlier.\nHowever, if a word in our dataset was not present in the pre-trained model’s vocabu‐\nlary, how will we get a representation for this word? This problem is popularly known\nas out of vocabulary (OOV). In our previous example, we just ignored such words\nfrom feature extraction. Is there a better way?\nWe discussed fastText embeddings [16] in Chapter 3. They’re based on the idea of\nenriching word embeddings with subword-level information. Thus, the embedding\nrepresentation for each word is represented as a sum of the representations of indi‐\nvidual character n-grams. While this may seem like a longer process compared to just\nestimating word-level embeddings, it has two advantages:\n• This approach can handle words that did not appear in training data (OOV).\n• The implementation facilitates extremely fast learning on even very large\ncorpora.\nWhile fastText is a general-purpose library to learn the embeddings, it also supports\noff-the-shelf text classification by providing end-to-end classifier training and testing;\ni.e., we don’t have to handle feature extraction separately. The remaining part of this\nsubsection shows how to use the fastText classifier [17] for text classification. We’ll\nwork with the DBpedia dataset [18]. It’s a balanced dataset consisting of 14 classes,\nwith 40,000 training and 5,000 testing examples per class. Thus, the total size of the\ndataset is 560,000 training and 70,000 testing data points. Clearly, this is a much\nlarger dataset than what we saw before. Can we build a fast training model using fast‐\nText? Let’s check it out!\nThe training and test sets are provided as CSV files in this dataset. So, the first step\ninvolves reading these files into your Python environment and cleaning the text to\nremove extraneous characters, similar to what we did in the pre-processing steps for\nthe other classifier examples we’ve seen so far. Once this is done, the process to use\nfastText is quite simple. The code snippet below shows a simple fastText model. The\n136 \n| \nChapter 4: Text Classification\n",
      "word_count": 432,
      "char_count": 2705,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 167,
      "text": "step-by-step process is detailed in the associated Jupyter notebook (Ch4/Fast‐\nText_Example.ipynb):\n## Using fastText for feature extraction and training\nfrom fasttext import supervised\n\"\"\"fastText expects and training file (csv), a model name as input arguments.\nlabel_prefix refers to the prefix before label string in the dataset.\ndefault is __label__. In our dataset, it is __class__.\nThere are several other parameters which can be seen in:\nhttps://pypi.org/project/fasttext/\n\"\"\"\nmodel = supervised(train_file, 'temp', label_prefix=\"__class__\")\nresults = model.test(test_file)\nprint(results.nexamples, results.precision, results.recall)\nIf we run this code in the notebook, we’ll notice that, despite the fact that this is a\nhuge dataset and we gave the classifier raw text and not the feature vector, the train‐\ning takes only a few seconds, and we get close to 98% precision and recall! As an exer‐\ncise, try to build a classifier using the same dataset but with either BoW or word\nembedding features and algorithms like logistic regression. Notice how long it takes\nfor the individual steps of feature extraction and classification learning!\nWhen we have a large dataset, and when learning seems infeasible with the\napproaches described so far, fastText is a good option to use to set up a strong work‐\ning baseline. However, there’s one concern to keep in mind when using fastText, as\nwas the case with Word2vec embeddings: it uses pre-trained character n-gram\nembeddings. Thus, when we save the trained model, it carries the entire character n-\ngram embeddings dictionary with it. This results in a bulky model and can result in\nengineering issues. For example, the model stored with the name “temp” in the above\ncode snippet has a size close to 450 MB. However, fastText implementation also\ncomes with options to reduce the memory footprint of its classification models with\nminimal reduction in classification performance [19]. It does this by doing vocabu‐\nlary pruning and using compression algorithms. Exploring these possibilities could\nbe a good option in cases where large model sizes are a constraint.\nfastText is extremely fast to train and very useful for setting up\nstrong baselines. The downside is the model size.\nWe hope this discussion gives a good overview of the usefulness of fastText for text\nclassification. What we showed here is a default classification model without any tun‐\ning of the hyperparameters. fastText’s documentation contains more information on\nthe different options to tune your classifier and on training custom embedding repre‐\nsentations for a dataset you want. However, both of the embedding representations\nwe’ve seen so far learn a representation of words and characters and collect them\nUsing Neural Embeddings in Text Classification \n| \n137\n",
      "word_count": 432,
      "char_count": 2797,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "MinionPro-Regular (9.6pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 168,
      "text": "together to form a text representation. Let’s see how to learn the representation for a\ndocument directly using the Doc2vec approach we discussed in Chapter 3.\nDocument Embeddings\nIn the Doc2vec embedding scheme, we learn a direct representation for the entire\ndocument (sentence/paragraph) rather than each word. Just as we used word and\ncharacter embeddings as features for performing text classification, we can also use\nDoc2vec as a feature representation mechanism. Since there are no existing pre-\ntrained models that work with the latest version of Doc2vec [20], let’s see how to\nbuild our own Doc2vec model and use it for text classification.\nWe’ll use a dataset called “Sentiment Analysis: Emotion in Text” from figure-\neight.com [9], which contains 40,000 tweets labeled with 13 labels signifying different\nemotions. Let’s take the three most frequent labels in this dataset—neutral, worry,\nhappiness—and build a text classifier for classifying new tweets into one of these\nthree classes. The notebook for this subsection (Ch4/Doc2Vec_Example.ipynb) walks\nyou through the steps involved in using Doc2vec for text classification and provides\nthe dataset.\nAfter loading the dataset and taking a subset of the three most frequent labels, an\nimportant step to consider here is pre-processing the data. What’s different here com‐\npared to previous examples? Why can’t we just follow the same procedure as before?\nThere are a few things that are different about tweets compared to news articles or\nother such text, as we briefly discussed in Chapter 2 when we talked about text pre-\nprocessing. First, they are very short. Second, our traditional tokenizers may not\nwork well with tweets, splitting smileys, hashtags, Twitter handles, etc., into multiple\ntokens. Such specialized needs prompted a lot of research into NLP for Twitter in the\nrecent past, which resulted in several pre-processing options for tweets. One such sol‐\nution is a TweetTokenizer, implemented in the NLTK [21] library in Python. We’ll\ndiscuss more on this topic in Chapter 8. For now, let’s see how we can use a TweetTo\nkenizer in the following code snippet:\ntweeter = TweetTokenizer(strip_handles=True,preserve_case=False)\nmystopwords = set(stopwords.words(\"english\"))\n#Function to pre-process and tokenize tweets\ndef preprocess_corpus(texts):\n    def remove_stops_digits(tokens):\n    #Nested function to remove stopwords and digits\n          return [token for token in tokens if token not in mystopwords \n                  and not token.isdigit()]\n    return [remove_stops_digits(tweeter.tokenize(content)) for content in texts]\nmydata = preprocess_corpus(df_subset['content'])\nmycats = df_subset['sentiment']\n138 \n| \nChapter 4: Text Classification\n",
      "word_count": 393,
      "char_count": 2730,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 169,
      "text": "The next step in this process is to train a Doc2vec model to learn tweet representa‐\ntions. Ideally, any large dataset of tweets will work for this step. However, since we\ndon’t have such a ready-made corpus, we’ll split our dataset into train-test and use the\ntraining data for learning the Doc2vec representations. The first part of this process\ninvolves converting the data into a format readable by the Doc2vec implementation,\nwhich can be done using the TaggedDocument class. It’s used to represent a document\nas a list of tokens, followed by a “tag,” which in its simplest form can be just the file‐\nname or ID of the document. However, Doc2vec by itself can also be used as a nearest\nneighbor classifier for both multiclass and multilabel classification problems using .\nWe’ll leave this as an exploratory exercise for the reader. Let’s now see how to train a\nDoc2vec classifier for tweets through the code snippet below:\n#Prepare training data in doc2vec format:\nd2vtrain = [TaggedDocument((d),tags=[str(i)]) for i, d in enumerate(train_data)]\n#Train a doc2vec model to learn tweet representations. Use only training data!!\nmodel = Doc2Vec(vector_size=50, alpha=0.025, min_count=10, dm =1, epochs=100)\nmodel.build_vocab(d2vtrain)\nmodel.train(d2vtrain, total_examples=model.corpus_count, epochs=model.epochs)\nmodel.save(\"d2v.model\")\nprint(\"Model Saved\")\nTraining for Doc2vec involves making several choices regarding parameters, as seen\nin the model definition in the code snippet above. vector_size refers to the dimen‐\nsionality of the learned embeddings; alpha is the learning rate; min_count is the min‐\nimum frequency of words that remain in vocabulary; dm, which stands for distributed\nmemory, is one of the representation learners implemented in Doc2vec (the other is\ndbow, or distributed bag of words); and epochs are the number of training iterations.\nThere are a few other parameters that can be customized. While there are some\nguidelines on choosing optimal parameters for training Doc2vec models [22], these\nare not exhaustively validated, and we don’t know if the guidelines work for tweets.\nThe best way to address this issue is to explore a range of values for the ones that mat‐\nter to us (e.g., dm versus dbow, vector sizes, learning rate) and compare multiple mod‐\nels. How do we compare these models, as they only learn the text representation? One\nway to do it is to start using these learned representations in a downstream task—in\nthis case, text classification. Doc2vec’s infer_vector function can be used to infer the\nvector representation for a given text using a pre-trained model. Since there is some\namount of randomness due to the choice of hyperparameters, the inferred vectors\ndiffer each time we extract them. For this reason, to get a stable representation, we\nrun it multiple times (called steps) and aggregate the vectors. Let’s use the learned\nmodel to infer features for our data and train a logistic regression classifier:\n#Infer the feature representation for training and test data using \n#the trained model\nmodel= Doc2Vec.load(\"d2v.model\")\n#Infer in multiple steps to get a stable representation\ntrain_vectors =  [model.infer_vector(list_of_tokens, steps=50)\nUsing Neural Embeddings in Text Classification \n| \n139\n",
      "word_count": 498,
      "char_count": 3263,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 170,
      "text": "              for list_of_tokens in train_data]\ntest_vectors = [model.infer_vector(list_of_tokens, steps=50)\n              for list_of_tokens in test_data]\nmyclass = LogisticRegression(class_weight=\"balanced\") \n#because classes are not balanced\nmyclass.fit(train_vectors, train_cats)\npreds = myclass.predict(test_vectors)\nprint(classification_report(test_cats, preds))\nNow, the performance of this model seems rather poor, achieving an F1 score of 0.51\non a reasonably large corpus, with only three classes. There are a couple of interpreta‐\ntions for this poor result. First, unlike full news articles or even well-formed senten‐\nces, tweets contain very little data per instance. Further, people write with a wide\nvariety in spelling and syntax when they tweet. There are a lot of emoticons in differ‐\nent forms. Our feature representation should be able to capture such aspects. While\ntuning the algorithms by searching a large parameter space for the best model may\nhelp, an alternative could be to explore problem-specific feature representations, as\nwe discussed in Chapter 3. We’ll see how to do this for tweets in Chapter 8. An\nimportant point to keep in mind when using Doc2vec is the same as for fastText: if we\nhave to use Doc2vec for feature representation, we have to store the model that\nlearned the representation. While it’s not typically as bulky as fastText, it’s also not as\nfast to train. Such trade-offs need to be considered and compared before we make a\ndeployment decision.\nSo far, we’ve seen a range of feature representations and how they play a role for text\nclassification using ML algorithms. Let’s now turn to a family of algorithms that\nbecame popular in the past few years, known as “deep learning.”\nDeep Learning for Text Classification\nAs we discussed in Chapter 1, deep learning is a family of machine learning algo‐\nrithms where the learning happens through different kinds of multilayered neural\nnetwork architectures. Over the past few years, it has shown remarkable improve‐\nments on standard machine learning tasks, such as image classification, speech recog‐\nnition, and machine translation. This has resulted in widespread interest in using\ndeep learning for various tasks, including text classification. So far, we’ve seen how to\ntrain different machine learning classifiers, using BoW and different kinds of embed‐\nding representations. Now, let’s look at how to use deep learning architectures for text\nclassification.\nTwo of the most commonly used neural network architectures for text classification\nare convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\nLong short-term memory (LSTM) networks are a popular form of RNNs. Recent\napproaches also involve starting with large, pre-trained language models and fine-\ntuning them for the task at hand. In this section, we’ll learn how to train CNNs and\n140 \n| \nChapter 4: Text Classification\n",
      "word_count": 436,
      "char_count": 2907,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 171,
      "text": "LSTMs and how to tune a pre-trained language model for text classification using the\nIMDB sentiment classification dataset [23]. Note that a detailed discussion on how\nneural network architectures work is beyond the scope of this book. Interested read‐\ners can read the textbook by Goodfellow et al. [24] for a general theoretical discussion\nand Goldberg’s book [25] for NLP-specific uses of neural network architectures.\nJurafsky and Martin’s book [12] also provides a short but concise overview of differ‐\nent neural network methods for NLP.\nThe first step toward training any ML or DL model is to define a feature representa‐\ntion. This step has been relatively straightforward in the approaches we’ve seen so far,\nwith BoW or embedding vectors. However, for neural networks, we need further pro‐\ncessing of input vectors, as we saw in Chapter 3. Let’s quickly recap the steps involved\nin converting training and test data into a format suitable for the neural network\ninput layers:\n1. Tokenize the texts and convert them into word index vectors.\n2. Pad the text sequences so that all text vectors are of the same length.\n3. Map every word index to an embedding vector. We do that by multiplying word\nindex vectors with the embedding matrix. The embedding matrix can either be\npopulated using pre-trained embeddings or it can be trained for embeddings on\nthis corpus.\n4. Use the output from Step 3 as the input to a neural network architecture.\nOnce these are done, we can proceed with the specification of neural network archi‐\ntectures and training classifiers with them. The Jupyter notebook associated with this\nsection (Ch4/DeepNN_Example.ipynb) will walk you through the entire process from\ntext pre-processing to neural network training and evaluation. We’ll use Keras, a\nPython-based DL library. The code snippet below illustrates Steps 1 and 2:\n#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer.\n#Tokenizer is fit on training data only, and that is used to tokenize both train \n#and test data.\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(train_texts)\ntrain_sequences = tokenizer.texts_to_sequences(train_texts) \ntest_sequences = tokenizer.texts_to_sequences(test_texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n#Converting this to sequences to be fed into neural network. Max seq. len is \n#1000 as set earlier. Initial padding of 0s, until vector is of \n#size MAX_SEQUENCE_LENGTH\ntrainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\ntrainvalid_labels = to_categorical(np.asarray(train_labels))\ntest_labels = to_categorical(np.asarray(test_labels))\nDeep Learning for Text Classification \n| \n141\n",
      "word_count": 400,
      "char_count": 2797,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 172,
      "text": "i. There are other such pre-trained embeddings available. Our choice in this case is arbitrary.\nStep 3: If we want to use pre-trained embeddings to convert the train and test data\ninto an embedding matrix like we did in the earlier examples with Word2vec and\nfastText, we have to download them and use them to convert our data into the input\nformat for the neural networks. The following code snippet shows an example of how\nto do this using GloVe embeddings, which were introduced in Chapter 3. GloVe\nembeddings come with multiple dimensionalities, and we chose 100 as our dimen‐\nsion here. The value of dimensionality is a hyperparameter, and we can experiment\nwith other dimensions as well:i\nembeddings_index = {}\nwith open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n    for line in f:\n          values = line.split()\n          word = values[0]\n          coefs = np.asarray(values[1:], dtype='float32')\n          embeddings_index[word] = coefs\nnum_words = min(MAX_NUM_WORDS, len(word_index)) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i > MAX_NUM_WORDS:\n          continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n          embedding_matrix[i] = embedding_vector\nStep 4: Now, we’re ready to train DL models for text classification! DL architectures\nconsist of an input layer, an output layer, and several hidden layers in between the\ntwo. Depending on the architecture, different hidden layers are used. The input layer\nfor textual input is typically an embedding layer. The output layer, especially in the\ncontext of text classification, is a softmax layer with categorical output. If we want to\ntrain the input layer instead of using pre-trained embeddings, the easiest way is to call\nthe Embedding layer class in Keras, specifying the input and output dimensions. How‐\never, since we want to use pre-trained embeddings, we should create a custom\nembedding layer that uses the embedding matrix we just built. The following code\nsnippet shows how to do that:\nembedding_layer = Embedding(num_words, EMBEDDING_DIM,\n                        embeddings_initializer=Constant(embedding_matrix),\n                        input_length=MAX_SEQUENCE_LENGTH,\n                        trainable=False)\nprint(\"Preparing of embedding matrix is done\")\nThis will serve as the input layer for any neural network we want to use (CNN or\nLSTM). Now that we know how to pre-process the input and define an input layer,\n142 \n| \nChapter 4: Text Classification\n",
      "word_count": 356,
      "char_count": 2552,
      "fonts": [
        "MinionPro-Regular (8.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MinionPro-Regular (6.3pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 173,
      "text": "let’s move on to specifying the rest of the neural network architecture using CNNs\nand LSTMs.\nCNNs for Text Classification\nLet’s now look at how to define, train, and evaluate a CNN model for text classifica‐\ntion. CNNs typically consist of a series of convolution and pooling layers as the hid‐\nden layers. In the context of text classification, CNNs can be thought of as learning\nthe most useful bag-of-words/n-grams features instead of taking the entire collection\nof words/n-grams as features, as we did earlier in this chapter. Since our dataset has\nonly two classes—positive and negative—the output layer has two outputs, with the\nsoftmax activation function. We’ll define a CNN with three convolution-pooling lay‐\ners using the Sequential model class in Keras, which allows us to specify DL models\nas a sequential stack of layers—one after another. Once the layers and their activation\nfunctions are specified, the next task is to define other important parameters, such as\nthe optimizer, loss function, and the evaluation metric to tune the hyperparameters of\nthe model. Once all this is done, the next step is to train and evaluate the model. The\nfollowing code snippet shows one way of specifying a CNN architecture for this task\nusing the Python library Keras and prints the results with the IMDB dataset for this\nmodel:\nprint('Define a 1D CNN model.')\ncnnmodel = Sequential()\ncnnmodel.add(embedding_layer)\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(MaxPooling1D(5))\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(MaxPooling1D(5))\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(GlobalMaxPooling1D())\ncnnmodel.add(Dense(128, activation='relu'))\ncnnmodel.add(Dense(len(labels_index), activation='softmax'))\ncnnmodel.compile(loss='categorical_crossentropy',\n                    optimizer='rmsprop',\n                    metrics=['acc'])\ncnnmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=1, validation_data=(x_val, y_val))\nscore, acc = cnnmodel.evaluate(test_data, test_labels)\nprint('Test accuracy with CNN:', acc)\nAs you can see, we made a lot of choices in specifying the model, such as activation\nfunctions, hidden layers, layer sizes, loss function, optimizer, metrics, epochs, and\nbatch size. While there are some commonly recommended options for these, there’s\nno consensus on one combination that works best for all datasets and problems. A\ngood approach while building your models is to experiment with different settings\n(i.e., hyperparameters). Keep in mind that all these decisions come with some\nDeep Learning for Text Classification \n| \n143\n",
      "word_count": 348,
      "char_count": 2631,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 174,
      "text": "associated cost. For example, in practice, we have the number of epochs as 10 or\nabove. But that also increases the amount of time it takes to train the model. Another\nthing to note is that, if you want to train an embedding layer instead of using pre-\ntrained embeddings in this model, the only thing that changes is the line cnnmo\ndel.add(embedding_layer). Instead, we can specify a new embedding layer as, for\nexample, cnnmodel.add(Embedding(Param1, Param2)). The code snippet below\nshows the code and model performance for the same:\nprint(\"Defining and training a CNN model, training embedding layer on the fly \n      instead of using pre-trained embeddings\")\ncnnmodel = Sequential()\ncnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n…\n...\ncnnmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=1, validation_data=(x_val, y_val))\nscore, acc = cnnmodel.evaluate(test_data, test_labels)\nprint('Test accuracy with CNN:', acc)\nIf we run this code in the notebook, we’ll notice that, in this case, training the embed‐\nding layer on our own dataset seems to result in better classification on test data.\nHowever, if the training data were substantially small, sticking to the pre-trained\nembeddings, or using the domain adaptation techniques we’ll discuss later in this\nchapter, would be a better choice. Let’s look at how to train similar models using an\nLSTM.\nLSTMs for Text Classification\nAs we saw briefly in Chapter 1, LSTMs and other variants of RNNs in general have\nbecome the go-to way of doing neural language modeling in the past few years. This\nis primarily because language is sequential in nature and RNNs are specialized in\nworking with sequential data. The current word in the sentence depends on its con‐\ntext—the words before and after. However, when we model text using CNNs, this\ncrucial fact is not taken into account. RNNs work on the principle of using this con‐\ntext while learning the language representation or a model of language. Hence, they’re\nknown to work well for NLP tasks. There are also CNN variants that can take such\ncontext into account, and CNNs versus RNNs is still an open area of debate. In this\nsection, we’ll see an example of using RNNs for text classification. Now that we’ve\nalready seen one neural network in action, it’s relatively easy to train another! Just\nreplace the convolutional and pooling parts with an LSTM in the prior two code\nexamples. The following code snippet shows how to train an LSTM model using the\nsame IMDB dataset for text classification:\nprint(\"Defining and training an LSTM model, training embedding layer on the fly\")\nrnnmodel = Sequential()\nrnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n144 \n| \nChapter 4: Text Classification\n",
      "word_count": 423,
      "char_count": 2710,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 175,
      "text": "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nrnnmodel.add(Dense(2, activation='sigmoid'))\nrnnmodel.compile(loss='binary_crossentropy',\n               optimizer='adam',\n               metrics=['accuracy'])\nprint('Training the RNN')\nrnnmodel.fit(x_train, y_train,\n          batch_size=32,\n          epochs=1,\n          validation_data=(x_val, y_val))\nscore, acc = rnnmodel.evaluate(test_data, test_labels,\n                          batch_size=32)\nprint('Test accuracy with RNN:', acc)\nNotice that this code took much longer to run than the CNN example. While LSTMs\nare more powerful in utilizing the sequential nature of text, they’re much more data\nhungry as compared to CNNs. Thus, the relative lower performance of the LSTM on\na dataset need not necessarily be interpreted as a shortcoming of the model itself. It’s\npossible that the amount of data we have is not sufficient to utilize the full potential of\nan LSTM. As in the case of CNNs, several parameters and hyperparameters play\nimportant roles in model performance, and it’s always a good practice to explore mul‐\ntiple options and compare different models before finalizing on one.\nText Classification with Large, Pre-Trained Language Models\nIn the past two years, there have been great improvements in using neural network–\nbased text representations for NLP tasks. We discussed some of these in “Universal\nText Representations” on page 107. These representations have been used successfully\nfor text classification in the recent past by fine-tuning the pre-trained models to the\ngiven task and dataset. BERT, which was mentioned in Chapter 3, is a popular model\nused in this way for text classification. Let’s take a look at how to use BERT for text\nclassification using the IMDB dataset we used earlier in this section. The full code is\nin the relevant notebook (Ch4/BERT_Sentiment_Classification_IMDB.ipynb).\nWe’ll use ktrain, a lightweight wrapper to train and use pre-trained DL models using\nthe TensorFlow library Keras. ktrain provides a straightforward process for all steps,\nfrom obtaining the dataset and the pre-trained BERT to fine-tuning it for the classifi‐\ncation task. Let’s see how to load the dataset first through the code snippet below:\ndataset = tf.keras.utils.get_file(\nfname=\"aclImdb.tar.gz\",   \norigin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n extract=True,)\nOnce the dataset is loaded, the next step is to download the BERT model and pre-\nprocess the dataset according to BERT’s requirements. The following code snippet\nshows how to do this with ktrain’s functions:\nDeep Learning for Text Classification \n| \n145\n",
      "word_count": 355,
      "char_count": 2637,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 176,
      "text": "(x_train, y_train), (x_test, y_test), preproc = \n                       text.texts_from_folder(IMDB_DATADIR,maxlen=500,                            \n   preprocess_mode='bert',train_test_names=['train','test'],\nThe next step is to load the pre-trained BERT model and fine-tune it for this dataset.\nHere’s the code snippet to do this:\nmodel = text.text_classifier('bert', (x_train, y_train), preproc=preproc)\nlearner=ktrain.get_learner(model,train_data=(x_train,y_train),    \n                          val_data=(x_test, y_test), batch_size=6)\nlearner.fit_onecycle(2e-5, 4)\nThese three lines of code will train a text classifier using the BERT pre-trained model.\nAs with other examples we’ve seen so far, we would need to do parameter tuning and\na lot of experimentation to pick the best-performing model. We leave that as an exer‐\ncise for the reader.\nIn this section, we introduced the idea of using DL for text classification using two\nneural network architectures—CNN and LSTM—and showed how we can tune a\nstate-of-the-art, pre-trained language model (BERT) for a given dataset and classifica‐\ntion task. There are several variants to these architectures, and new models are being\nproposed every day by NLP researchers. We saw how to use one pre-trained language\nmodel, BERT. There are other such models, and this is a constantly evolving area in\nNLP research; the state of the art keeps changing every few months (or even weeks!).\nHowever, in our experience as industry practitioners, several NLP tasks, especially\ntext classification, still widely use several of the non-DL approaches we described ear‐\nlier in the chapter. Two primary reasons for this are a lack of the large amounts of\ntask-specific training data that neural networks demand and issues related to com‐\nputing and deployment costs.\nDL-based text classifiers are often nothing but condensed repre‐\nsentations of the data they were trained on. These models are often\nas good as the training dataset. Selecting the right dataset becomes\nall the more important in such cases.\nWe’ll end this section by reiterating what we mentioned earlier when we discussed the\ntext classification pipeline: in most industrial settings, it always makes sense to start\nwith a simpler, easy-to-deploy approach as your MVP and go from there incremen‐\ntally, taking customer needs and feasibility into account.\n146 \n| \nChapter 4: Text Classification\n",
      "word_count": 338,
      "char_count": 2396,
      "fonts": [
        "UbuntuMono-Regular (8.5pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 177,
      "text": "We’ve seen several approaches to building text classification models so far. Unlike\nheuristics-based approaches where the predictions can be justified by tracing back the\nrules applied on the data sample, ML models are treated as a black box while making\npredictions. However, in the recent past, the topic of interpretable ML started to gain\nprominence, and programs that can “explain” an ML model’s predictions exist now.\nLet’s take a quick look at their application for text classification.\nInterpreting Text Classification Models\nIn the previous sections, we’ve seen how to train text classifiers using multiple\napproaches. In all these examples, we took the classifier predictions as is, without\nseeking any explanations. In fact, most real-world use cases of text classification may\nbe similar—we just consume the classifier’s output and don’t question its decisions.\nTake spam classification: we generally don’t look for explanations of why a certain\nemail is classified as spam or regular email. However, there may be scenarios where\nsuch explanations are necessary.\nConsider a scenario where we developed a classifier that identifies abusive comments\non a discussion forum website. The classifier identifies comments that are objectiona‐\nble/abusive and performs the job of a human moderator by either deleting them or\nmaking them invisible to users. We know that classifiers aren’t perfect and can make\nerrors. What if the commenter questions this moderation decision and asks for an\nexplanation? Some method to “explain” the classification decision by pointing to\nwhich feature’s presence prompted such a decision can be useful in such cases. Such a\nmethod is also useful to provide some insights into the model and how it may per‐\nform on real-world data (instead of train/test sets), which may result in better, more\nreliable models in the future.\nAs ML models started getting deployed in real-world applications, interest in the\ndirection of model interpretability grew. Recent research [26, 27] resulted in usable\ntools [28, 29] for interpreting model predictions (especially for classification). Lime\n[28] is one such tool that attempts to interpret a black-box classification model by\napproximating it with a linear model locally around a given training instance. The\nadvantage of this is that such a linear model is expressed as a weighted sum of its fea‐\ntures and is easy to interpret for humans. For example, if there are two features, f1\nand f2, for a given test instance of a binary classifier with classes A and B, a Lime lin‐\near model around this instance could be something like -0.3 × f1 + 0.4 × f2 with a\nprediction B. This indicates that the presence of feature f1 will negatively affect this\nprediction (by 0.3) and skew it toward A. [26] explains this in more detail. Let’s now\nlook at how Lime [28] can be used to understand the predictions of a text classifier.\nInterpreting Text Classification Models \n| \n147\n",
      "word_count": 474,
      "char_count": 2944,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 178,
      "text": "Explaining Classifier Predictions with Lime\nLet’s take a model we already built earlier in this chapter and see how Lime can help\nus interpret its predictions. The following code snippet uses the logistic regression\nmodel we built earlier using the “Economy News Article Tone and Relevance” dataset,\nwhich classifies a given news article as being relevant or non-relevant and shows how\nwe can use Lime (the full code can be accessed in the notebook Ch4/Lime‐\nDemo.ipynb):\nfrom lime import lime_text\nfrom lime.lime_text import LimeTextExplainer\nfrom sklearn.pipeline import make_pipeline\ny_pred_prob = classifier.predict_proba(X_test_dtm)[:, 1]\nc = make_pipeline(vect, classifier)\nmystring = list(X_test)[221] #Take a string from test instance\nprint(c.predict_proba([mystring])) #Prediction is a \"No\" here, i.e., not relevant\nclass_names = [\"no\", \"yes\"] #not relevant, relevant\nexplainer = LimeTextExplainer(class_names=class_names)\nexp = explainer.explain_instance(mystring, c.predict_proba, num_features=6)\nexp.as_list()\nThis code shows six features that played an important role in making this prediction.\nThey’re as follows:\n[('YORK', 0.23416984139912805),\n ('NEW', -0.22724581340890154),\n ('showing', -0.12532906927967377),\n ('AP', -0.08486610147834726),\n ('dropped', 0.07958281943957331),\n ('trend', 0.06567603359316518)]\nThus, the output of the above code can be seen as a linear sum of these six features.\nThis would mean that, if we remove the features “NEW” and “showing,” the predic‐\ntion should move toward the opposite class, i.e., “relevant/Yes,” by 0.35 (the sum of\nthe weights of these two features). Lime also has functions to visualize these predic‐\ntions. Figure 4-8 shows a visualization of the above explanation.\nAs shown in the figure, the presence of three words—York, trend, and dropped—\nskews the prediction toward Yes, whereas the other three words skew the prediction\ntoward No. Apart from some uses we mentioned earlier, such visualizations of classi‐\nfiers can also help us if we want to do some informed feature selection.\nWe hope this brief introduction gave you an idea of what to do if you have to explain\na classifier’s predictions. We also have a notebook (Ch4/Lime_RNN.ipynb) that\nexplains an LSTM model’s predictions using Lime, and we leave this detailed explora‐\ntion of Lime as an exercise for the reader.\n148 \n| \nChapter 4: Text Classification\n",
      "word_count": 338,
      "char_count": 2384,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 179,
      "text": "Figure 4-8. Visualization of Lime’s explanation of a classifier’s prediction\nLearning with No or Less Data and Adapting to\nNew Domains\nIn all the examples we’ve seen so far, we had a relatively large training dataset avail‐\nable for the task. However, in most real-world scenarios, such datasets are not readily\navailable. In other cases, we may have an annotated dataset available, but it might not\nbe large enough to train a good classifier. There can also be cases where we have a\nlarge dataset of, say, customer complaints and requests for one product suite, but\nwe’re asked to customize our classifier to another product suite for which we have a\nvery small amount of data (i.e., we’re adapting an existing model to a new domain). In\nthis section, let’s discuss how to build good classification systems for these scenarios\nwhere we have no or little data or have to adapt to new domain training data.\nNo Training Data\nLet’s say we’re asked to design a classifier for segregating customer complaints for our\ne-commerce company. The classifier is expected to automatically route customer\ncomplaint emails into a set of categories: billing, delivery, and others. If we’re fortu‐\nnate, we may discover a source of large amounts of annotated data for this task within\nthe organization in the form of a historical database of customer requests and their\ncategories. If such a database doesn’t exist, where should we start to build our\nclassifier?\nThe first step in such a scenario is creating an annotated dataset where customer\ncomplaints are mapped to the set of categories mentioned above. One way to\napproach this is to get customer service agents to manually label some of the com‐\nplaints and use that as the training data for our ML model. Another approach is\ncalled “bootstrapping” or “weak supervision.” There can be certain patterns of infor‐\nmation in different categories of customer requests. Perhaps billing-related requests\nLearning with No or Less Data and Adapting to New Domains \n| \n149\n",
      "word_count": 334,
      "char_count": 2004,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1216,
          "height": 552,
          "ext": "png",
          "size_bytes": 23006
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 180,
      "text": "mention variants of the word “bill,” amounts in a currency, etc. Delivery-related\nrequests talk about shipping, delays, etc. We can get started with compiling some\nsuch patterns and using their presence or absence in a customer request to label it,\nthereby creating a small (perhaps noisy) annotated dataset for this classification task.\nFrom here, we can build a classifier to annotate a larger collection of data. Snorkel\n[30], a recent software tool developed by Stanford University, is useful for deploying\nweak supervision for various learning tasks, including classification. Snorkel was used\nto deploy weak supervision–based text classification models at industrial scale at\nGoogle [31]. They showed that weak supervision could create classifiers comparable\nin quality to those trained on tens of thousands of hand-labeled examples! [32] shows\nan example of how to use Snorkel to generate training data for text classification\nusing a large amount of unlabeled data.\nIn some other scenarios where large-scale collection of data is necessary and feasible,\ncrowdsourcing can be seen as an option to label the data. Websites like Amazon\nMechanical Turk and Figure Eight provide platforms to make use of human intelli‐\ngence to create high-quality training data for ML tasks. A popular example of using\nthe wisdom of crowds to create a classification dataset is the “CAPTCHA test,” which\nGoogle uses to ask if a set of images contain a given object (e.g., “Select all images that\ncontain a street sign”).\nLess Training Data: Active Learning and Domain Adaptation\nIn scenarios like the one described earlier, where we collected small amounts of data\nusing human annotations or bootstrapping, it may sometimes turn out that the\namount of data is too small to build a good classification model. It’s also possible that\nmost of the requests we collected belonged to billing and very few belonged to the\nother categories, resulting in a highly imbalanced dataset. Asking the agents to spend\nmany hours doing manual annotation is not always feasible. What should we do in\nsuch scenarios?\nOne approach to address such problems is active learning, which is primarily about\nidentifying which data points are more crucial to be used as training data. It helps\nanswer the following question: if we had 1,000 data points but could get only 100 of\nthem labeled, which 100 would we choose? What this means is that, when it comes to\ntraining data, not all data points are equal. Some data points are more important as\ncompared to others in determining the quality of the classifier trained. Active learn‐\ning converts this into a continuous process.\nUsing active learning for training a classifier can be described as a step-by-step\nprocess:\n1. Train the classifier with the available amount of data.\n2. Start using the classifier to make predictions on new data.\n150 \n| \nChapter 4: Text Classification\n",
      "word_count": 468,
      "char_count": 2891,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 181,
      "text": "3. For the data points where the classifier is very unsure of its predictions, send\nthem to human annotators for their correct classification.\n4. Include these data points in the existing training data and retrain the model.\nRepeat Steps 1 through 4 until a satisfactory model performance is reached.\nTools like Prodigy [33] have active learning solutions implemented for text classifica‐\ntion and support the efficient usage of active learning to create annotated data and\ntext classification models quickly. The basic idea behind active learning is that the\ndata points where the model is less confident are the data points that contribute most\nsignificantly to improving the quality of the model, and therefore only those data\npoints get labeled.\nNow, imagine a scenario for our customer complaint classifier where we have a lot of\nhistorical data for a range of products. However, we’re now asked to tune it to work\non a set of newer products. What’s potentially challenging in this situation? Typical\ntext classification approaches rely on the vocabulary of the training data. Hence,\nthey’re inherently biased toward the kind of language seen in the training data. So, if\nthe new products are very different (e.g., the model is trained on a suite of electronic\nproducts and we’re using it for complaints on cosmetic products), the pre-trained\nclassifiers trained on some other source data are unlikely to perform well. However,\nit’s also not realistic to train a new model from scratch on each product or product\nsuite, as we’ll again run into the problem of insufficient training data. Domain adap‐\ntation is a method to address such scenarios; this is also called transfer learning. Here,\nwe “transfer” what we learned from one domain (source) with large amounts of data\nto another domain (target) with less labeled data but large amounts of unlabeled data.\nWe already saw one example of how to use BERT for text classification earlier in this\nchapter.\nThis approach for domain adaptation in text classification can be summarized as\nfollows:\n1. Start with a large, pre-trained language model trained on a large dataset of the\nsource domain (e.g., Wikipedia data).\n2. Fine-tune this model using the target language’s unlabeled data.\n3. Train a classifier on the labeled target domain data by extracting feature repre‐\nsentations from the fine-tuned language model from Step 2.\nULMFit [34] is another popular domain adaptation approach for text classification.\nIn research experiments, it was shown that this approach matches the performance of\ntraining from scratch with 10 to 20 times more training examples and only 100\nlabeled examples in text classification tasks. When unlabeled data was used to fine-\ntune the pre-trained language model, it matched the performance of using 50 to 100\nLearning with No or Less Data and Adapting to New Domains \n| \n151\n",
      "word_count": 465,
      "char_count": 2862,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 182,
      "text": "times more labeled examples when trained from scratch, on the same text classifica‐\ntion tasks. Transfer learning methods are currently an active area of research in NLP.\nTheir use for text classification has not yet shown dramatic improvements on stan‐\ndard datasets, nor are they the default solution for all classification scenarios in\nindustry setups yet. But we can expect to see this approach yielding better and better\nresults in the near future.\nSo far, we’ve seen a range of text classification methods and discussed obtaining\nappropriate training data and using different feature representations for training the\nclassifiers. We also briefly touched on how to interpret the predictions made by some\ntext classification models. Let’s now consolidate what we’ve learned so far using a\nsmall case study of building a text classifier for a real-world scenario.\nCase Study: Corporate Ticketing\nLet’s consider a real-world scenario and learn how we can apply some of the concepts\nwe’ve discussed in this section. Imagine we’re asked to build a ticketing system for our\norganization that will track all the tickets or issues people face in the organization and\nroute them to either internal or external agents. Figure 4-9 shows a representative\nscreenshot for such a system; it’s a corporate ticketing system called Spoke.\nFigure 4-9. A corporate ticketing system\nNow let’s say our company has recently hired a medical counsel and partnered with a\nhospital. So our system should also be able to pinpoint any medical-related issue and\nroute it to the relevant people and teams. But while we have some past tickets, none\n152 \n| \nChapter 4: Text Classification\n",
      "word_count": 267,
      "char_count": 1661,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1438,
          "height": 825,
          "ext": "png",
          "size_bytes": 188836
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 183,
      "text": "of them are labeled as health related. In the absence of these labels, how will we go\nabout building such a health issue–related classification system?\nLet’s explore a couple of options:\nUse existing APIs or libraries\nOne option is to start with a public API or library and map its classes to what’s\nrelevant to us. For instance, the Google APIs mentioned earlier in the chapter can\nclassify content into over 700 categories. There are 82 categories associated with\nmedical or health issues. These include categories like /Health/Health Condi‐\ntions/Pain Management, /Health/Medical Facilities & Services/Doctors’ Offices, /\nFinance/Insurance/Health Insurance, etc.\nWhile not all categories are relevant to our organization, some could be, and we\ncan map these accordingly. For example, let’s say our company doesn’t consider\nsubstance abuse and obesity issues as relevant for medical counsel. We can\nignore /Health/Substance Abuse and /Health/Health Conditions/Obesity in this\nAPI. Similarly, whether insurance should be a part of HR or referred outside can\nalso be handled with these categories.\nUse public datasets\nWe can also adopt public datasets for our needs. For example, 20 Newsgroups is a\npopular text classification dataset, and it’s also part of the sklearn library. It has a\nrange of topics, including sci.med. We can also use it to train a basic classifier,\nclassifying all other topics in one category and sci.med in another.\nUtilize weak supervision\nWe have a history of past tickets, but they’re not labeled. So, we can consider\nbootstrapping a dataset out of it using the approaches described earlier in this\nsection. For example, consider having a rule: “If the past ticket contains words\nlike fever, diarrhea, headache, or nausea, put them in the medical counsel cate‐\ngory.” This rule can create a small amount of data, which we can use as a starting\npoint for our classifier.\nActive learning\nWe can use tools like Prodigy to conduct data collection experiments where we\nask someone working at the customer service desk to look at ticket descriptions\nand tag them with a preset list of categories. Figure 4-10 shows an example of\nusing Prodigy for this purpose.\nCase Study: Corporate Ticketing \n| \n153\n",
      "word_count": 357,
      "char_count": 2223,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 184,
      "text": "Figure 4-10. Active learning with Prodigy\nLearning from implicit and explicit feedback\nThroughout the process of building, iterating, and deploying this solution, we’re\ngetting feedback that we can use to improve our system. Explicit feedback could\nbe when the medical counsel or hospital says explicitly that the ticket was not rel‐\nevant. Implicit feedback could be extracted from other dependent variables like\nticket response times and ticket response rates. All of these could be factored in\nto improve our model using active learning techniques.\nA sample pipeline summarizing these ideas may look like what’s shown in\nFigure 4-11. We start with no labeled data and use either a public API or a model cre‐\nated with a public dataset or weak supervision as the first baseline model. Once we\nput this model to production, we’ll get explicit and implicit signals on where it’s\nworking or failing. We use this information to refine our model and active learning to\nselect the best set of instances that need to be labeled. Over time, as we collect more\ndata, we can build more sophisticated and deeper models.\n154 \n| \nChapter 4: Text Classification\n",
      "word_count": 190,
      "char_count": 1150,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1429,
          "height": 995,
          "ext": "png",
          "size_bytes": 96823
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 185,
      "text": "Figure 4-11. A pipeline for building a classifier when there’s no training data\nIn this section, we started looking at a practical scenario of not having enough train‐\ning data for building our own text classifier for our custom problem. We discussed\nseveral possible solutions to address the issue. Hopefully, this helps you foresee and\nprepare for some of the scenarios related to data collection and creation in your\nfuture projects related to text classification.\nPractical Advice\nSo far, we’ve shown a range of different methods for building text classifiers and\npotential issues you may run into. We’d like to end this chapter with some practical\nadvice that summarizes our observations and experience with building text classifica‐\ntion systems in industry. Most of these are generic enough to be applied to other top‐\nics in the book as well.\nEstablish strong baselines\nA common fallacy is to start with a state-of-the-art algorithm. This is especially\ntrue in the current era of deep learning, where every day, new approaches/algo‐\nrithms keep coming up. However, it’s always good to start with simpler\napproaches and try to establish strong baselines first. This is useful for three\nmain reasons:\nPractical Advice \n| \n155\n",
      "word_count": 199,
      "char_count": 1232,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1225,
          "height": 992,
          "ext": "png",
          "size_bytes": 62177
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 186,
      "text": "a. It helps us get a better understanding of the problem statement and key chal‐\nlenges.\nb. Building a quick MVP helps us get initial feedback from end users and stake‐\nholders.\nc. A state-of-the-art research model may give us only a minor improvement\ncompared to the baseline, but it might come with a huge amount of technical\ndebt.\nBalance training data\nWhile working with classification, it’s very important to have a balanced dataset\nwhere all categories have an equal representation. An imbalanced dataset can\nadversely impact the learning of the algorithm and result in a biased classifier.\nWhile we cannot always control this aspect of the training data, there are various\ntechniques to fix class imbalance in the training data. Some of them are collecting\nmore data, resampling (undersample from majority classes or oversample from\nminority classes), and weight balancing.\nCombine models and humans in the loop\nIn practical scenarios, it makes sense to combine the outputs of multiple classifi‐\ncation models with handcrafted rules from domain experts to achieve the best\nperformance for the business. In other cases, it’s practical to defer the decision to\na human evaluator if the machine is not sure of its classification decision. Finally,\nthere could also be scenarios where the learned model has to change with time\nand newer data. We’ll discuss some solutions for such scenarios in Chapter 11,\nwhich focuses on end-to-end systems.\nMake it work, make it better\nBuilding a classification system is not just about building a model. For most\nindustrial settings, building a model is often just 5% to 10% of the total project.\nThe rest consists of gathering data, building data pipelines, deployment, testing,\nmonitoring, etc. It is always good to build a model quickly, use it to build a sys‐\ntem, then start improvement iterations. This helps us to quickly identify major\nroadblocks and the parts that need the most work, and it’s often not the modeling\npart.\nUse the wisdom of many\nEvery text classification algorithm has its own strengths and weaknesses. There is\nno single algorithm that always works well. One way to circumvent this is via\nensembling: training multiple classifiers. The data is passed through every classi‐\nfier, and the predictions generated are combined (e.g., majority voting) to arrive\nat a final class prediction. An interested reader can look at the work of Dong et al.\n[35, 36] for a deep dive into ensemble methods for text classification.\n156 \n| \nChapter 4: Text Classification\n",
      "word_count": 412,
      "char_count": 2520,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 187,
      "text": "Wrapping Up\nIn this chapter, we saw how to address the problem of text classification from multi‐\nple viewpoints. We discussed how to identify a classification problem, tackle the vari‐\nous stages in a text classification pipeline, collect data to create relevant datasets, use\ndifferent feature representations, and train several classification algorithms. With\nthis, we hope you’re now well-equipped and ready to solve text classification prob‐\nlems for your use case and scenario and understand how to use existing solutions,\nbuild our own classifiers using various methods, and tackle the roadblocks you may\nface in the process. We focused on only one aspect of building text classification sys‐\ntems in industry applications: building the model. Issues related to the end-to-end\ndeployment of NLP systems will be dealt with in Chapter 11. In the next chapter, we’ll\nuse some of the ideas we learned here to tackle a related but different NLP problem:\ninformation extraction.\nReferences\n[1] United States Postal Service. The United States Postal Service: An American His‐\ntory, 57–60. ISBN: 978-0-96309-524-4. Last accessed June 15, 2020.\n[2] Gupta, Anuj, Saurabh Arora, Satyam Saxena, and Navaneethan Santhanam.\n“Noise reduction and smart ticketing for social media-based communication sys‐\ntems.” US Patent Application 20190026653, filed January 24, 2019.\n[3] Spasojevic, Nemanja and Adithya Rao. “Identifying Actionable Messages on Social\nMedia.” 2015 IEEE International Conference on Big Data: 2273–2281.\n[4] CLPSYCH: Computational Linguistics and Clinical Psychology Workshop. Shared\nTasks 2019.\n[5] Google Cloud. “Natural Language”. Last accessed June 15, 2020.\n[6] Amazon Comprehend. Last accessed June 15, 2020.\n[7] Azure Cognitive Services. Last accessed June 15, 2020.\n[8] Iderhoff, Nicolas. nlp-datasets: Alphabetical list of free/public domain datasets\nwith text data for use in Natural Language Processing (NLP), (GitHub repo). Last\naccessed June 15, 2020.\n[9] Kaggle. “Sentiment Analysis: Emotion in Text”. Last accessed June 15, 2020.\n[10] UC Irvine Machine Learning Repository. A collection of repositories for\nmachine learning. Last accessed June 15, 2020.\n[11] Google. “Dataset Search”. Last accessed June 15, 2020.\nWrapping Up \n| \n157\n",
      "word_count": 334,
      "char_count": 2258,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 188,
      "text": "[12] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft), 2018.\n[13] Lemaître, Guillaume, Fernando Nogueira, and Christos K. Aridas. “Imbalanced-\nlearn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine\nLearning”. The Journal of Machine Learning Research 18.1 (2017): 559–563.\n[14] For a detailed mathematical description of logistic regression, refer to Chapter 5\nin [12].\n[15] Google. Pre-trained word2vec model. Last accessed June 15, 2020.\n[16] Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov.\n“Enriching Word Vectors with Subword Information.” Transactions of the Association\nfor Computational Linguistics 5 (2017): 135–146.\n[17] Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. “Bag of\nTricks for Efficient Text Classification”. (2016).\n[18] Ramesh, Sree Harsha. torchDatasets, (GitHub repo). Last accessed June 15, 2020.\n[19] Joulin, Armand, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve\nJégou, and Tomas Mikolov. “Fasttext.zip: Compressing text classification models”.\n(2016).\n[20] For older Doc2vec versions, there are some pre-trained models; e.g., https://\noreil.ly/kt0U0 (last accessed June 15, 2020).\n[21] Natural Language Toolkit. “NLTK 3.5 documentation”. Last accessed June 15,\n2020.\n[22] Lau, Jey Han and Timothy Baldwin. “An Empirical Evaluation of doc2vec with\nPractical Insights into Document Embedding Generation”. (2016).\n[23] Stanford Artificial Intelligence Laboratory. “Large Movie Review Dataset”. Last\naccessed June 15, 2020.\n[24] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. Cam‐\nbridge: MIT Press, 2016. ISBN: 978-0-26203-561-3\n[25] Goldberg, Yoav. “Neural Network Methods for Natural Language Processing.”\nSynthesis Lectures on Human Language Technologies 10.1 (2017): 1–309.\n[26] Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “‘Why Should I Trust\nYou?’ Explaining the Predictions of Any Classifier.” Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining (2016):\n1135–1144.\n158 \n| \nChapter 4: Text Classification\n",
      "word_count": 292,
      "char_count": 2129,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 189,
      "text": "[27] Lundberg, Scott M. and Su-In Lee. “A Unified Approach to Interpreting Model\nPredictions.” Advances in Neural Information Processing Systems 30 (NIPS 2017):\n4765–4774.\n[28] Marco Tulio Correia Ribeiro. Lime: Explaining the predictions of any machine\nlearning classifier, (GitHub repo). Last accessed June 15, 2020.\n[29] Lundberg, Scott. shap: A game theoretic approach to explain the output of any\nmachine learning model, (GitHub repo).\n[30] Snorkel. “Programmatically Building and Managing Training Data”. Last\naccessed June 15, 2020.\n[31] Bach, Stephen H., Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cas‐\nsandra Xia, Souvik Sen et al. “Snorkel DryBell: A Case Study in Deploying Weak\nSupervision at Industrial Scale”. (2018).\n[32] Snorkel. “Snorkel Intro Tutorial: Data Labeling”. Last accessed June 15, 2020.\n[33] Prodigy. Last accessed June 15, 2020.\n[34] Fast.ai. “Introducing state of the art text classification with universal language\nmodels”. Last accessed June 15, 2020.\n[35] Dong, Yan-Shi and Ke-Song Han. “A comparison of several ensemble methods\nfor text categorization.” IEEE International Conference on Services Computing (2004):\n419–422.\n[36] Caruana, Rich, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes.\n“Ensemble Selection from Libraries of Models.” Proceedings of the Twenty-First Inter‐\nnational Conference on Machine Learning (2004): 18.\nWrapping Up \n| \n159\n",
      "word_count": 201,
      "char_count": 1404,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 190,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 191,
      "text": "CHAPTER 5\nInformation Extraction\nWhat’s in a name? A rose\nby any other name would smell as sweet.\n—William Shakespeare\nWe deal with a lot of textual content every day, be it short messages on the phone or\ndaily emails or longer texts we read for fun or at work or to catch up on current\naffairs. Such text documents are a rich source of information for us. Depending on\nthe context, “information” can mean multiple things, such as key events, people, or\nrelationships between people, places, or organizations, etc. Information extraction\n(IE) refers to the NLP task of extracting relevant information from text documents.\nAn example of IE put to use in real-world applications are the short blurbs we see to\nthe right when we search for a popular figure’s name on Google.\nWhen compared to structured information sources like databases or tables or semi-\nstructured sources such as webpages (which have some markup), text is a form of\nunstructured data. For example, in a database, we know where to look for something\nbased on its schema. However, to a large extent, text documents typically comprise\nfree-flowing text without a set schema. This makes IE a challenging problem. Texts\nmay contain various kinds of information. In most cases, extracting information that\nhas a fixed pattern (e.g., addresses, phone numbers, dates, etc.) is relatively straight‐\nforward using pattern-based extraction techniques like regular expressions, even\nthough the text itself is considered unstructured data. However, extracting other\ninformation (e.g., names of people, relations between different entities in the text,\ndetails for a calendar event, etc.) may require more advanced language processing.\nIn this chapter, we’ll discuss various IE tasks and the methods for implementing them\nfor our applications. We’ll start with a brief historical background, followed by an\noverview of different IE tasks and applications of IE in the real world. We’ll then\nintroduce the typical NLP processing pipeline for solving any IE task and move on to\n161\n",
      "word_count": 327,
      "char_count": 2034,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (9.3pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 192,
      "text": "discuss how to solve specific IE tasks—key phrase extraction, named entity recogni‐\ntion, named entity disambiguation and linking, and relationship extraction—along\nwith some practical advice on implementing them in your projects. We’ll then present\na case study of how IE is used in a real-world scenario and briefly cover other\nadvanced IE tasks. With this introduction, let’s explore IE, starting with a brief\nhistory.\nApproaches for extracting different kinds of information from documents like scien‐\ntific papers and medical reports have been proposed in the past in the research com‐\nmunity. However, Message Understanding Conferences organized by the US Navy\n(1987–1998) [1] can be considered the starting point for modern-day research on\ninformation extraction from text. This was followed by the Automatic Content\nExtraction Program (1999–2008) [2] and the Text Analysis Conference (2009–2018)\nseries organized by NIST [3], which introduced competitions for extracting different\nkinds of information from text, from recognizing names of different entities to con‐\nstructing large, queryable knowledge bases. Existing libraries and methods for\nextracting various forms of information from text and their use in real-world applica‐\ntions trace their origins back to the research that started in these conference series.\nBefore we start looking at what the methods and libraries for IE are, let’s first take a\nlook at some examples of where IE is used in real-world applications.\nIE Applications\nIE is used in a wide range of real-world applications, from news articles, to social\nmedia, and even receipts. Here, we’ll cover the details of a few of them:\nTagging news and other content\nThere’s a lot of text generated about various events happening around the world\nevery day. In addition to classifying text using methods discussed in Chapter 4,\nit’s useful for some applications, such as search engines and recommendation sys‐\ntems, if such texts are tagged with important entities mentioned within them. For\nexample, look at Figure 5-1, which shows a screenshot from the Google News [4]\nhomepage.\nPeople (e.g., Jean Vanier), organizations (e.g., Progressive Conservative Party of\nOntario), locations (e.g., Canada), and events (e.g., Brexit) currently in the news\nare extracted and shown to the reader so that they can go directly to news about a\nspecific entity. This is one example of information extraction at work in a popular\napplication.\n162 \n| \nChapter 5: Information Extraction\n",
      "word_count": 385,
      "char_count": 2496,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 193,
      "text": "Figure 5-1. Screenshot from the Google News homepage\nChatbots\nA chatbot needs to understand the user’s question in order to generate/retrieve a\ncorrect response. For example, consider the question, “What are the best cafes\naround the Eiffel Tower?” The chatbot needs to understand that “Eiffel Tower”\nand “cafe” are locations, then identify cafes within a certain distance of the Eiffel\nTower. IE is useful in extracting such specific information from a pool of avail‐\nable data. We’ll discuss more on chatbots in Chapter 6.\nApplications in social media\nA lot of information is disseminated through social media channels like Twitter.\nExtracting informative excerpts from social media text may help in decision\nmaking. An example use case is extracting time-sensitive, frequently updated\ninformation, such as traffic updates and disaster relief efforts, based on tweets.\nNLP for Twitter is one of the most useful applications that utilizes the abundant\ninformation present in social media. We’ll touch on some of these applications in\nChapter 8.\nExtracting data from forms and receipts\nMany banking apps nowadays have the feature to scan a check and deposit the\nmoney directly into the user’s account. Whether you’re an individual, small busi‐\nness, or larger business enterprise, it’s not uncommon to use apps that scan bills\nand receipts. Along with optical character recognition (OCR), information\nextraction techniques play an important role in these apps [5, 6]. We won’t dis‐\ncuss this aspect in this chapter, as OCR is the primary step in such applications\nand isn’t part of the processing pipelines in this book.\nNow that we have an idea of what IE is and where it’s useful, let’s move on to under‐\nstanding what the different tasks covered under IE are.\nIE Applications \n| \n163\n",
      "word_count": 288,
      "char_count": 1787,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 375,
          "height": 276,
          "ext": "png",
          "size_bytes": 14506
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 194,
      "text": "IE Tasks\nIE is a term that’s used to refer to a range of different tasks of varying complexity. The\noverarching goal of IE is to extract “knowledge” from text, and each of these tasks\nprovides different information to do that. To understand what these tasks are, con‐\nsider the snippet from a New York Times article shown in Figure 5-2.\nFigure 5-2. A New York Times article from April 30, 2019 [7]\nAs human readers, we find several useful pieces of information in this blurb. For\nexample, we know that the article is about Apple, the company (and not the fruit),\nand that it mentions a person, Luca Maestri, who is the finance chief of the company.\nThe article is about the buyback of stock and other issues related to it. For a machine\nto understand all this involves different levels of IE.\nIdentifying that the article is about “buyback” or “stock price” relates to the IE task of\nkeyword or keyphrase extraction (KPE). Identifying Apple as an organization and Luca\nMaestri as a person comes under the IE task of named entity recognition (NER). Rec‐\nognizing that Apple is not a fruit, but a company, and that it refers to Apple, Inc. and\nnot some other company with the word “apple” in its name is the IE task of named\nentity disambiguation and linking. Extracting the information that Luca Maestri is the\nfinance chief of Apple refers to the IE task of relation extraction.\nThere are a few advanced IE tasks beyond those mentioned above. Identifying that\nthis article is about a single event (let’s call it “Apple buys back stocks”) and being able\nto link it to other articles talking about the same event over time refers to the IE task\nof event extraction. A related task is temporal information extraction, which aims to\n164 \n| \nChapter 5: Information Extraction\n",
      "word_count": 313,
      "char_count": 1771,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 562,
          "height": 416,
          "ext": "png",
          "size_bytes": 23928
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 195,
      "text": "extract information about times and dates, which is also useful for developing calen‐\ndar applications and interactive personal assistants. Finally, many applications, such\nas automatically generating weather reports or flight announcements, follow a stan‐\ndard template with some slots that need to be filled based on extracted data. This IE\ntask is known as template filling.\nEach of these tasks requires different levels of language processing. A range of rule-\nbased methods as well as supervised, unsupervised, and semi-supervised machine\nlearning (including state-of-the-art deep learning approaches) can be used for devel‐\noping solutions to solve these tasks. However, considering that IE is very much\ndependent on the application domain (e.g., finance, news, airlines, etc.), IE in indus‐\ntry is generally implemented as a hybrid system incorporating rule-based and\nlearning-based approaches [8, 9]. IE is still a very active area of research, and not all\nthese tasks are considered “solved” or matured enough to have standard approaches\nthat can be used in real-world application scenarios. Tasks such as KPE and NER are\nmore widely studied than others and have some tried-and-tested solutions. The rest\nof the tasks are relatively more challenging, and it’s more common to rely on pay-as-\nyou-use services from large providers like Microsoft, Google, and IBM.\nAn important point to note regarding IE is that the datasets needed to train IE models\nare typically more specialized than what we saw, for example, in Chapter 4, where all\nwe needed to get started was a collection of texts mapped to some categories. Hence,\nreal-world use cases of IE may not always require us to train models from scratch,\nand we can make use of external APIs for some tasks. Before moving on to specific\ntasks, let’s first take a look at the general NLP pipeline for any IE task.\nThe General Pipeline for IE\nThe general pipeline for IE requires more fine-grained NLP processing than what we\nsaw for text classification (Chapter 4). For example, to identify named entities (per‐\nsons, organizations, etc.), we would need to know the part-of-speech tags of words.\nFor relating multiple references to the same entity (e.g., Albert Einstein, Einstein, the\nscientist, he, etc.), we would need coreference resolution. Note that none of these are\nmandatory steps for building a text classification system. Thus, IE is a task that is\nmore NLP intensive than text classification. Figure 5-3 shows a typical NLP pipeline\nfor IE tasks. Not all steps in the pipeline are necessary for all IE tasks, and the figure\ndemonstrates which IE tasks require what levels of analysis.\nWe discussed the details of the different processing steps illustrated in this figure in\nChapters 1 and 2. As the figure shows, key phrase extraction is the task requiring\nminimal NLP processing (some algorithms also do POS tagging before extracting\nkeyphrases), whereas, other than named entity recognition, all the other IE tasks\nrequire deeper NLP pre-processing followed by models developed for those specific\ntasks. IE tasks are typically evaluated in terms of precision, recall, and F1 scores using\nThe General Pipeline for IE \n| \n165\n",
      "word_count": 511,
      "char_count": 3194,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 196,
      "text": "standard evaluation sets. Considering the different levels of NLP pre-processing\nrequired, IE tasks are also affected by the accuracy of these processing steps them‐\nselves. Collecting relevant training data and training our own models for IE, if neces‐\nsary, should take all these aspects into account. With this background, let’s now start\nlooking at each of the IE tasks, one by one.\nFigure 5-3. IE pipeline illustrating NLP processing needed for some IE tasks\nKeyphrase Extraction\nConsider a scenario where we want to buy a product, which has a hundred reviews,\non Amazon. There’s no way we’re going to read all of them to get an idea of what\nusers think about the product. To facilitate this, Amazon has a filtering feature: “Read\nreviews that mention.” This presents a bunch of keywords or phrases that several peo‐\nple used in these reviews to filter the reviews, as shown in Figure 5-4. This is a good\nexample of where KPE can be useful in an application we all use.\nKeyword and phrase extraction, as the name indicates, is the IE task concerned\nwith extracting important words and phrases that capture the gist of the text from a\ngiven text document. It’s useful for several downstream NLP tasks, such as\n166 \n| \nChapter 5: Information Extraction\n",
      "word_count": 214,
      "char_count": 1256,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1205,
          "height": 1139,
          "ext": "png",
          "size_bytes": 55125
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 197,
      "text": "search/information retrieval, automatic document tagging, recommendation systems,\ntext summarization, etc.\nFigure 5-4. “Read reviews that mention” on Amazon.ca\nKPE is a well-studied problem in the NLP community, and the two most commonly\nused methods to solve it are supervised learning and unsupervised learning. Super‐\nvised learning approaches require corpora with texts and their respective keyphrases\nand use engineered features or DL techniques [10]. Creating such labeled datasets for\nKPE is a time- and cost-intensive endeavor. Hence, unsupervised approaches that do\nnot require a labeled dataset and are largely domain agnostic are more popular for\nKPE. These approaches are also more commonly used in real-world KPE applica‐\ntions. Recent research has also shown that state-of-the-art DL methods for KPE don’t\nperform any better than unsupervised approaches [11].\nAll the popular unsupervised KPE algorithms are based on the idea of representing\nthe words and phrases in a text as nodes in a weighted graph where the weight indi‐\ncates the importance of that keyphrase. Keyphrases are then identified based on how\nconnected they are with the rest of the graph. The top-N important nodes from the\ngraph are then returned as keyphrases. Important nodes are those words and phrases\nthat are frequent enough and also well connected to different parts of the text. The\ndifferent graph-based KPE approaches differ in the way they select potential words/\nphrases from the text (from a large set of possible words and phrases in the entire\ntext) and the way these words/phrases are scored in the graph.\nThere’s a huge body of work on this topic, with some working implementations avail‐\nable. In most cases, existing approaches are a great starting point to meet your\nrequirements. How can we use these to implement a keyphrase extractor in our\nproject? Let ‘s look at an example.\nImplementing KPE\nThe Python library textacy [12], built on top of the well-known library spaCy [13],\ncontains implementations for some of the common graph-based keyword and phrase\nextraction algorithms. The notebook associated with this section (Ch5/KPE.ipynb)\nillustrates the use of textacy to extract keyphrases using two algorithms, TextRank\nKeyphrase Extraction \n| \n167\n",
      "word_count": 351,
      "char_count": 2257,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 623,
          "height": 187,
          "ext": "png",
          "size_bytes": 7753
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 198,
      "text": "[14] and SGRank. We’ll use a text file that talks about the history of NLP as our test\ndocument. The code snippet below illustrates KPE with textacy:\nfrom textacy import *\nimport textacy.ke\nmytext = open(“nlphistory.txt”).read()\nen = textacy.load_spacy_lang(\"en_core_web_sm\", disable=(\"parser\",))\ndoc = textacy.make_spacy_doc(mytext, lang=en)\nprint(\"Textrank output: \", [kps for kps, weights in \ntextacy.ke.textrank(doc, normalize=\"lemma\",  topn=5)])\nprint(\"SGRank output: \", [kps for kps, weights in \ntextacy.ke.sgrank(mydoc, n_keyterms=5)])\nOutput: \nTextrank output:  ['successful natural language processing system', \n'statistical machine translation system', 'natural language system', \n'statistical natural language processing', 'natural language task']\nSGRank output:  ['natural language processing system', \n'statistical machine translation', 'research', 'late 1980', 'early']\nThere are numerous options for how long our n-grams should be in these phrases;\nwhat POS tags should be considered or ignored; what pre-processing should be done\na priori; how to eliminate overlapping n-grams, such as statistical machine transla‐\ntion and machine translation in the above example; and so on. Some of these are\nexplored in the notebook, and we leave the rest as exercises for the reader.\nWe showed one example of implementing KPE with textacy. There are other options,\nthough. For example, the Python library gensim has a keyword extractor based on\nTextRank [15]. [16] shows how to implement TextRank from scratch. You can\nexplore multiple library implementations and compare them before choosing one.\nPractical Advice\nWe’ve seen how keyphrase extraction can be implemented using spaCy and textacy\nand how we can modify it to suit our needs. From a practical point of view, there are\na few caveats to keep in mind when using such graph-based algorithms in produc‐\ntion, though. We’ll list a few of them below, along with some suggestions for working\naround them based on our experience with adding KPE as a feature in software\nproducts:\n• The process of extracting potential n-grams and building the graph with them is\nsensitive to document length, which could be an issue in a production scenario.\nOne approach to dealing with it is to not use the full text, but instead try using\n168 \n| \nChapter 5: Information Extraction\n",
      "word_count": 342,
      "char_count": 2324,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 199,
      "text": "the first M% and the last N% of the text, since we would expect that the introduc‐\ntory and concluding parts of the text should cover the main summary of the text.\n• Since each keyphrase is independently ranked, we sometimes end up seeing over‐\nlapping keyphrases (e.g., “buy back stock” and “buy back”). One solution for this\ncould be to use some similarity measure (e.g., cosine similarity) between the top-\nranked keyphrases and choose the ones that are most dissimilar to one another.\ntextacy already implements a function to address this issue, as shown in the\nnotebook.\n• Seeing counterproductive patterns (e.g., a keyphrase that starts with a preposition\nwhen you don’t want that) is another common problem. This is relatively\nstraightforward to handle by tweaking the implementation code for the algo‐\nrithm and explicitly encoding information about such unwanted word patterns.\n• Improper text extraction can affect the rest of the KPE process, especially when\ndealing with formats such as PDF or scanned images. This is primarily because\nKPE is sensitive to sentence structure in the document. Hence, it’s always a good\nidea to add some post-processing to the extracted key phrases list to create a\nfinal, meaningful list without noise.\nA custom solution could be a combination of an existing graph-based KPE algorithm\nthat addresses the above-mentioned issues and a domain-specific list of heuristics, if\navailable. From our experience, this covers the issues most commonly encountered\nwith KPE in typical NLP projects.\nIn this section, we saw how to use KPE algorithms to extract important words and\nphrases from any document and some ways to overcome potential challenges. While\nsuch keyphrases can potentially capture the names of important entities in the text,\nwe’re not specifically looking for them when we use KPE algorithms. Let’s now look at\nthe next—and perhaps most popular—IE task, which is designed to look specifically\nfor the presence of named entities in the text.\nNamed Entity Recognition\nConsider a scenario where the user asks a search query—“Where was Albert Einstein\nborn?”—using Google search. Figure 5-5 shows a screenshot of what we see before a\nlist of search results.\nTo be able to show “Ulm, Germany” for this query, the search engine needs to deci‐\npher that Albert Einstein is a person before going on to look for a place of birth. This\nis an example of NER in action in a real-world application.\nNamed Entity Recognition \n| \n169\n",
      "word_count": 405,
      "char_count": 2471,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 200,
      "text": "Figure 5-5. Screenshot of a Google search result\nNER refers to the IE task of identifying the entities in a document. Entities are typi‐\ncally names of persons, locations, and organizations, and other specialized strings,\nsuch as money expressions, dates, products, names/numbers of laws or articles, and\nso on. NER is an important step in the pipeline of several NLP applications involving\ninformation extraction. Figure 5-6 illustrates the function of NER using the displaCy\nvisualizer by explosion.ai [17].\nAs seen in the figure, for a given text, NER is expected to identify person names, loca‐\ntions, dates, and other entities. Different categories of entities identified here are some\nof the ones commonly used in NER system development [18]. NER is a prerequisite\nfor being able to do other IE tasks, such as relation extraction or event extraction,\nwhich were introduced earlier in this chapter and will be discussed in greater detail\nlater on. NER is also useful in other applications like machine translation, as names\nneed not necessarily be translated while translating a sentence. So, clearly, there’s a\nrange of scenarios in NLP projects where NER is a major component. It’s one of the\ncommon tasks you’re likely to encounter in NLP projects in industry. How do we\nbuild such an NER system? The rest of this section focuses on this question, consider‐\ning three cases: building our own NER system, using existing libraries, and using\nactive learning.\n170 \n| \nChapter 5: Information Extraction\n",
      "word_count": 244,
      "char_count": 1507,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1029,
          "height": 629,
          "ext": "png",
          "size_bytes": 361956
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 201,
      "text": "Figure 5-6. NER example using the displaCy visualizer\nBuilding an NER System\nA simple approach to building an NER system is to maintain a large collection of per‐\nson/organization/location names that are the most relevant to our company (e.g.,\nnames of all clients, cities in their addresses, etc.); this is typically referred to as a\ngazetteer. To check whether a given word is a named entity or not, just do a lookup in\nthe gazetteer. If a large number of entities present in our data are covered by a gazet‐\nteer, then it’s a great way to start, especially when we don’t have an existing NER sys‐\ntem available. There are a few questions to consider with such an approach. How\ndoes it deal with new names? How do we periodically update this database? How does\nit keep track of aliases, i.e., different variations of a given name (e.g., USA, United\nStates, etc.)?\nAn approach that goes beyond a lookup table is rule-based NER, which can be based\non a compiled list of patterns based on word tokens and POS tags. For example, a\npattern “NNP was born,” where “NNP” is the POS tag for a proper noun, indicates\nthat the word that was tagged “NNP” refers to a person. Such rules can be program‐\nmed to cover as many cases as possible to build a rule-based NER system. Stanford\nNLP’s RegexNER [19] and spaCy’s EntityRuler [20] provide functionalities to imple‐\nment your own rule-based NER.\nA more practical approach to NER is to train an ML model, which can predict the\nnamed entities in unseen text. For each word, a decision has to be made whether or\nnot that word is an entity, and if it is, what type of the entity it is. In many ways, this is\nvery similar to the classification problems we discussed in detail in Chapter 4. The\nNamed Entity Recognition \n| \n171\n",
      "word_count": 319,
      "char_count": 1763,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1243,
          "height": 697,
          "ext": "png",
          "size_bytes": 81118
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 202,
      "text": "only difference here is that NER is a “sequence labeling” problem [21]. The typical\nclassifiers we saw in Chapter 4 predict labels for texts independent of their surround‐\ning context. Consider a classifier that classifies sentences in a movie review into posi‐\ntive/negative/neutral categories based on their sentiment. This classifier does not\n(usually) take into account the sentiment of previous (or subsequent) sentences when\nclassifying the current sentence. In a sequence classifier, such context is important. A\ncommon use case for sequence labeling is POS tagging, where we need information\nabout the parts of speech of surrounding words to estimate the part of speech of the\ncurrent word. NER is traditionally modeled as a sequence classification problem,\nwhere the entity prediction for the current word also depends on the context. For\nexample, if the previous word was a person name, there’s a higher probability that the\ncurrent word is also a person name if it’s a noun (e.g., first and last names).\nTo illustrate the difference between a normal classifier and a sequence classifier, con‐\nsider the following sentence: “Washington is a rainy state.” When a normal classifier\nsees this sentence and has to classify it word by word, it has to make a decision as to\nwhether Washington refers to a person (e.g., George Washington) or the State of\nWashington without looking at the surrounding words. It’s possible to classify the\nword “Washington” in this particular sentence as a location only after looking at the\ncontext in which it’s being used. It’s for this reason that sequence classifiers are used\nfor training NER models.\nConditional random fields (CRFs) is one of the popular sequence classifier training\nalgorithms. The notebook associated with this section (Ch5/NERTraining.ipynb)\nshows how we can use CRFs to train an NER system. We’ll use CONLL-03, a popular\ndataset used for training NER systems [22], and an open source sequence labeling\nlibrary called sklearn-crfsuite [23], along with a set of simple word- and POS tag–\nbased features, which provide contextual information we need for this task.\nTo perform sequence classification, we need data in a format that allows us to model\nthe context. Typical training data for NER looks like Figure 5-7, which is a sentence\nfrom the CONLL-03 dataset.\nThe labels in the figure follow what’s known as a BIO notation: B indicates the begin‐\nning of an entity; I, inside an entity, indicates when entities comprise more than one\nword; and O, other, indicates non-entities. Peter Such is a name with two words in the\nexample shown in Figure 5-7. Thus, “Peter” gets tagged as a B-PER, and “Such” gets\ntagged as an I-PER to indicate that Such is a part of the entity from the previous\nword. The remaining entities in this example, Essex, Yorkshire, and Headingley, are\nall one-word entities. So, we only see B-ORG and B-LOC as their tags. Once we\nobtain a dataset of sentences annotated in this form and we have a sequence classifier\nalgorithm, how should we train an NER system?\n172 \n| \nChapter 5: Information Extraction\n",
      "word_count": 508,
      "char_count": 3087,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 203,
      "text": "The steps are the same as those for the text classifiers we saw in Chapter 4:\n1. Load the dataset\n2. Extract the features\n3. Train the classifier\n4. Evaluate it on a test set\nFigure 5-7. NER training data format example\nNamed Entity Recognition \n| \n173\n",
      "word_count": 47,
      "char_count": 253,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 276,
          "height": 801,
          "ext": "png",
          "size_bytes": 21047
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 204,
      "text": "Loading the dataset is straightforward. This particular dataset is also already split into\na train/dev/test set. So, we’ll train the model using the training set. We saw a range of\nfeature representation techniques in Chapter 3. Let’s look at an example using hand‐\ncrafted features this time. What features seem intuitively relevant for this task? To\nidentify names of people or places, for example, patterns such as whether the word\nstarts with an uppercase character or whether it’s preceded or succeeded by a verb/\nnoun, etc., can be used as starting points to train an NER model. The following code\nsnippet shows a function that extracts the previous and next words’ POS tags for a\ngiven sentence. The notebook has a more elaborate feature set:\ndef sent2feats(sentence):\n    feats = []\n    sen_tags = pos_tag(sentence)\n    for i in range(0,len(sentence)):\n         word = sentence[i]\n         wordfeats = {}\n         #POS tag features: current tag, previous and next 2 tags.\n         wordfeats['tag'] = sen_tags[i][1]\n         if i == 0:\n          wordfeats[\"prevTag\"] = \"<S>\"\n         elif i == 1:\n          wordfeats[\"prevTag\"] = sen_tags[0][1]\n         else:\n          wordfeats[\"prevTag\"] = sen_tags[i - 1][1]\n         if i == len(sentence) - 2:\n          wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n         elif i == len(sentence) - 1:\n          wordfeats[\"nextTag\"] = \"</S>\"\n         else:\n          wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n         feats.append(wordfeats)\n    return feats\nAs you can see from the wordfeats variable in this code sample, each word is trans‐\nformed into a dictionary of features, and therefore each sentence will look like a list of\ndictionaries (the variable feats in the code), which will be used by the CRF classifier.\nThe following code snippet shows a function to train an NER system with a CRF\nmodel and evaluates the model performance on the development set:\n#Train a sequence model\ndef train_seq(X_train,Y_train,X_dev,Y_dev):\n    crf = CRF(algorithm='lbfgs', c1=0.1, c2=10, max_iterations=50)\n    crf.fit(X_train, Y_train)\n    labels = list(crf.classes_)\n    y_pred = crf.predict(X_dev)\n    sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n    print(metrics.flat_f1_score(Y_dev,y_pred,average='weighted',\n                          labels=labels))\n174 \n| \nChapter 5: Information Extraction\n",
      "word_count": 312,
      "char_count": 2361,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 205,
      "text": "Training this CRF model gave an F1 score of 0.92 on the development data, which is a\nvery good score! The notebook shows more detailed evaluation measures and how to\ncalculate them. Here, we showed some of the most commonly used features in learn‐\ning an NER system and used a popular training method and a publicly available data‐\nset. Clearly, there’s a lot to be done in terms of tuning the model and developing\n(even) better features; this example only serves to illustrate one way of developing an\nNER model quickly using one particular library in case you need to and you have a\nrelevant dataset. MITIE [24] is another such library to train NER systems.\nRecent advances in NER research either exclude or augment the kind of feature engi‐\nneering we did in this example with neural network models. NCRF++ [25] is another\nlibrary that can be used to train your own NER using different neural network archi‐\ntectures. A notebook that uses the BERT model for training an NER system using the\nsame dataset is available in the GitHub repo (Ch5/BERT_CONLL_NER.ipynb). We\nleave working through that as an exercise for the reader.\nWe took a quick tour of how to train our own NER system. However, in real-world\nscenarios, using the trained model by itself won’t be sufficient, as the data keeps\nchanging and new entities keep getting added, and there will also be some domain-\nspecific entities or patterns that were not seen in generic training datasets. Hence,\nmost NER systems deployed in real-world scenarios use a combination of ML mod‐\nels, gazetteers, and some pattern matching–based heuristics to improve their perfor‐\nmance [26]. [24] shows an example of how Rasa, a company that builds intelligent\nchatbots, improves its entity extraction using lookup tables.\nClearly, to build these NER systems ourselves, we need large, annotated datasets in a\nformat similar to the one shown in Figure 5-7. While datasets like CONLL-03 are\navailable, they work with a limited set of entities (person, organization, location, mis‐\ncellaneous, other) and in limited domains. There are other such datasets, such as\nOntoNotes [27], which are much larger and cover different kinds of text. However,\nthey’re not freely available and usually need to be purchased under expensive license\nagreements, which may not always be supported by our organizations’ budgets. So,\nwhat should we do?\nNER Using an Existing Library\nWhile all this discussion about training an NER system may make building and\ndeploying it look like a long process (starting with procuring a dataset), thankfully,\nNER has been well researched over the past few decades, and we have off-the-shelf\nlibraries to start with. Stanford NER [28], spaCy, and AllenNLP [29] are some well-\nknown NLP libraries that can be used to incorporate a pre-trained NER model into a\nsoftware product. The code snippet below illustrates using NER from spaCy:\nNamed Entity Recognition \n| \n175\n",
      "word_count": 484,
      "char_count": 2924,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 206,
      "text": "import spacy\nnlp = spacy.load(\"en_core_web_lg\")\ntext_from_fig = \"On Tuesday, Apple announced its plans for another major chunk \n                 of the money: It will buy back a further $75 billion in stock.\"\ndoc = nlp(text_from_fig)\nfor ent in doc.ents:\n    if ent.text:\n         print(ent.text, \"\\t\", ent.label_)\nRunning this code snippet will show Tuesday as DATE, Apple as ORG, and $75 bil‐\nlion as MONEY. Considering that spaCy’s NER is based on a state-of-the-art neural\nmodel coupled with some pattern matching and heuristics, it’s a good starting point.\nHowever, we may run into two issues:\n1. As mentioned earlier, we may be using NER in a specific domain, and the pre-\ntrained models may not capture the specific nature of our own domain.\n2. Sometimes, we may want to add new categories to the NER system without hav‐\ning to collect a large dataset for all the common categories.\nWhat should we do in such cases?\nNER Using Active Learning\nFrom our experience, the best approach to NER when we want customized solutions\nbut don’t want to train everything from scratch is to start with an off-the-shelf prod‐\nuct and either augment it with customized heuristics for our problem domain (using\ntools such as RegexNER or EntityRuler) and/or use active learning using tools like\nProdigy (like we saw in Chapter 4 for text classification). This allows us to improve an\nexisting pre-trained NER model by manually tagging a few example sentences con‐\ntaining new NER categories or correct a few model predictions manually and use\nthese to retrain the model. [30] shows some examples of going through this process\nusing Prodigy.\nIn general, in most cases, we don’t always have to think about developing an NER sys‐\ntem from scratch. If we do have to develop an NER system from scratch, the first\nthing we would need, as we saw in this section, is a large collection of annotated data\nof sentences where each word/token is tagged with its category (entity type or other).\nOnce such a dataset is available, the next step is to use it to obtain handcrafted and/or\nneural feature representations and feed them to a sequence labeling model. Chapters\n8 and 9 in [31] deal with specific methods to learn from such sequences. In the\nabsence of such data, rule-based NER is the first step.\n176 \n| \nChapter 5: Information Extraction\n",
      "word_count": 390,
      "char_count": 2323,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 207,
      "text": "Start with a pre-trained NER model and enhance it with heuristics,\nactive learning, or both.\nPractical Advice\nSo far, we’ve taken a quick look at how to use existing NER systems, discussed some\nways of augmenting them, and discussed how to train our own NER from scratch.\nDespite the fact that state-of-the-art NER is highly accurate (with F1 scores over 90%\nusing standard evaluation frameworks for NER in NLP research), there are several\nissues to keep in mind when using NER in our own software applications. Here are a\ncouple caveats based on our own experience with developing NER systems:\n• NER is very sensitive to the format of its input. It’s more accurate with well-\nformatted plain text than with, say, a PDF document from which plain text needs\nto be extracted first. While it’s possible to build custom NER systems for specific\ndomains or for data like tweets, the challenge with PDFs comes from the failure\nto be 100% accurate in extracting text from them while preserving the structure.\n[32] illustrates some of the challenges with PDF-to-text extraction. Why do we\nneed to be so accurate in properly extracting the structure from PDFs, though?\nIn PDFs, partial sentences, headings, and formatting are common, and they can\nall mess up NER accuracy. There’s no single solution for this. One approach is to\ndo custom post-processing of PDFs to extract blobs of text, then run NER on the\nblobs.\nIf you’re working with documents, such as reports, etc., pre-\nprocess them to extract text blobs, then run NER on them.\n• NER is also very sensitive to the accuracy of the prior steps in its processing pipe‐\nline: sentence splitting, tokenization, and POS tagging (refer back to Figure 5-2).\nTo understand how improper sentence splitting can result in poor NER results,\ntry taking the content from the screenshot back in Figure 5-1 and looking at the\noutput from spaCy (see the notebook Ch5/NERIssues.ipynb for a short illustra‐\ntion). So, some amount of pre-processing may be necessary before passing a\npiece of text into an NER model to extract entities.\nDespite such shortcomings, NER is immensely useful for many IE scenarios, such as\ncontent tagging, search, and mining social media to identify customer feedback about\nspecific products, to name a few. While NER (and KPE) serve the useful task of\nNamed Entity Recognition \n| \n177\n",
      "word_count": 394,
      "char_count": 2343,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 208,
      "text": "identifying important words, phrases, and entities in documents, some NLP applica‐\ntions require further analysis of language, which leads us to more advanced NLP\ntasks. One such IE task is entity disambiguation or entity linking, and it’s the topic of\nthe next section.\nNamed Entity Disambiguation and Linking\nConsider a scenario where we’re working on the data science team of a large newspa‐\nper publication (say, The New York Times). We’re charged with the task of building a\nsystem that creates visual representation of news stories by connecting different enti‐\nties mentioned in the stories to what they refer to in the real world, as shown in\nFigure 5-8.\nFigure 5-8. Entity linking by IBM [33]\nDoing this requires knowledge of several IE tasks beyond what we’ve seen with NER\nand KPE. As a first step, we have to know what these entities or keywords actually\nrefer to in the real world. Let’s take another example to illustrate why this could be\nchallenging. Consider this sentence: “Lincoln drives a Lincoln Aviator and lives on\nLincoln Way.” All three mentions of “Lincoln” here refer to different entities and dif‐\nferent types of entities: the first Lincoln is a person, the second one is a vehicle, and\nthe third is a location. How can we reliably link the three Lincolns to their correct\nWikipedia pages like in Figure 5-8?\n178 \n| \nChapter 5: Information Extraction\n",
      "word_count": 233,
      "char_count": 1380,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 698,
          "height": 416,
          "ext": "png",
          "size_bytes": 273035
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 209,
      "text": "Named entity disambiguation (NED) refers to the NLP task of achieving exactly this:\nassigning a unique identity to entities mentioned in the text. It’s also the first step in\nmoving toward more sophisticated tasks to address the scenario mentioned above by\nidentifying relationships between entities. NER and NED together are known as\nnamed entity linking (NEL). Some other NLP applications that would need NEL\ninclude question answering and constructing large knowledge bases of connected\nevents and entities, such as the Google Knowledge Graph [34].\nSo, how do we build an IE system for performing NEL? Just as NER identifies entities\nand their spans using contextual information encoded by a range of features, NEL\nalso relies on context. However, it requires going beyond POS tagging in terms of the\nNLP pre-processing needed. At a minimum, NEL needs some form of parsing to\nidentify linguistic items like subject, verb, and object. Additionally, it may also need\ncoreference resolution to resolve and link multiple references to the same entity (e.g.,\nAlbert Einstein, the scientist, Einstein, etc.) to the same reference in a large, encyclo‐\npedic knowledge base (e.g., Wikipedia). This is typically modeled as a supervised ML\nproblem and evaluated in terms of precision, recall, and F1 scores on standard test\nsets.\nState-of-the-art NEL uses a range of different neural network architectures [35].\nClearly, learning an NEL model requires the presence of a large, annotated dataset as\nwell as some kind of encyclopedic resource to link to. Further, NEL is a much more\nspecialized NLP task compared to what we’ve seen so far (text representation, text\nclassification, NER, KPE). In our experience as industry practitioners, it’s more com‐\nmon to use off-the-shelf, pay-as-you-use services offered by big providers such as\nIBM (Watson) and Microsoft (Azure) for NEL rather than developing an in-house\nsystem. Let’s look at an example of using one such service.\nNEL Using Azure API\nThe Azure Text Analytics API is one of the popular APIs for NEL. DBpedia Spotlight\n[36] is a freely available tool to do the same. The following code snippet (Ch5/\nEntityLinking-AzureTextAnalytics.ipynb) shows how to access the Azure API to per‐\nform entity linking on a text. Azure comes with a seven-day free trial, which is a good\nway to explore the API to understand whether it meets your requirements:\nimport requests\nmy_api_key = 'XXXXXXX'\ndef print_entities(text):\n    url = \"https://westcentralus.api.cognitive.microsoft.com/text/analytics/\\\n    v2.1/entities\"\n    documents = {'documents':[{'id':'1', 'language':'en', 'text':text}]}\n    headers = {'Ocp-Apim-Subscription-Key': my_api_key}\n    response = requests.post(url, headers=headers, json=documents)\n    entities = response.json()\n    return entities\nNamed Entity Disambiguation and Linking \n| \n179\n",
      "word_count": 420,
      "char_count": 2849,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 210,
      "text": "mytext = open(\"nytarticle.txt\").read() #file is in the github repo.\nentities = print_entities(mytext)\nfor document in entities['documents']:\n    print(\"Entities in this document: \")\n    for entity in document['entities']:\n          if entity['type'] in [\"Person\", \"Location\", \"Organization\"]:\n                     print(entity['name'], \"\\t\", entity['type'])\n                     if 'wikipediaUrl' in entity.keys():\n                          print(entity['wikipediaUrl'])\nThe result of running this code using the Azure API is shown in Figure 5-9; it lists\nentities in the text along with their Wikipedia links wherever available.\nFigure 5-9. Output of entity linking for a news article from The New York Times\nWe see that San Francisco is a location, but a specific location, which is indicated by\nits Wikipedia page. Alex Jones is not any other Alex Jones, but an American TV show\nhost, as can be seen from the Wikipedia page. This is clearly much more informative\nthan stopping at NER, and it can be used for better information extraction. This\ninformation can then be used for understanding the relationship between these enti‐\nties, which we’ll see later in this chapter.\nSo, we now have a way to incorporate NEL in our NLP system. How good is this solu‐\ntion? Based on our experience using off-the-shelf NEL systems, there are a few\nimportant things to keep in mind while using NEL in your project:\n• Existing NEL approaches are not perfect, and they’re unlikely to fare well with\nnew names or domain-specific terms. Since NEL also requires further linguistic\nprocessing, including syntactic parsing, its accuracy is also affected by how well\nthe different processing steps are done.\n• Like with other IE tasks, the first step in any NLP pipeline—text extraction and\ncleanup—affects what we see as output for NEL as well. When we use third-party\n180 \n| \nChapter 5: Information Extraction\n",
      "word_count": 284,
      "char_count": 1893,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 411,
          "height": 255,
          "ext": "png",
          "size_bytes": 21234
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 211,
      "text": "services, we have little control over adapting them to our domain, if needed, or\nunderstanding their internal workings to modify them to our needs.\nWith this overview, now that we know how to introduce NEL into our project’s NLP\npipeline if necessary, let’s move on to the next IE task that has NEL as a prerequisite:\nrelationship extraction.\nRelationship Extraction\nImagine we’re working at a company that mines tons of news articles to derive, say,\nfinancial insights. To be able to do such analysis on thousands of news texts every day,\nwe would need a constantly updated knowledge base that connects different people,\norganizations, and events based on the news content. A use case for this knowledge\nbase could be to analyze stock markets based on documents released by companies\nand the news articles about them. How will we get started building such a tool? The\nIE tasks we’ve seen so far—KPE, NER, and NEL—are all useful to a certain extent in\nhelping identify entities, events, keyphrases, etc. But how do we go from there to the\nnext step of “connecting” them with some relation? What exactly are the relations?\nHow will we extract them? Let’s revisit Figure 5-2, which shows a screenshot of a New\nYork Times article. One relation that can be extracted is: (Luca Maestri, finance chief,\nApple). Here, we connect Luca Maestri to Apple with the relationship of finance\nchief.\nRelationship extraction (RE) is the IE task that deals with extracting entities and rela‐\ntionships between them from text documents. It’s an important step in building a\nknowledge base, and it’s also useful in improving search and developing question-\nanswering systems. Figure 5-10 shows an example of a working RE system by Rosette\nText Analytics [37] for the following text snippet [38]:\nSatya Narayana Nadella is an Indian-American business executive. He currently serves\nas the Chief Executive Officer (CEO) of Microsoft, succeeding Steve Ballmer in 2014.\nBefore becoming chief executive, he was Executive Vice President of Microsoft’s Cloud\nand Enterprise Group, responsible for building and running the company’s computing\nplatforms.\nThis output shows that Narayana Nadella is a person related to Microsoft as an\nemployee, related to India and America as a citizen, and so on. How does one pro‐\nceed with extracting such relationships from a piece of text? Clearly, it’s more chal‐\nlenging than the other IE tasks we’ve seen so far in this chapter and requires deeper\nknowledge of language processing as compared to every other task we’ve covered in\nthis book so far. Apart from identifying what entities there are and disambiguating\nthem, we need to model the process of extracting the relationships between them by\nconsidering the words connecting the entities in a sentence, their sense of usage, and\nso on. Further, an important question that needs to be resolved is: what constitutes a\n“relation”? Relations can be specific to a given domain. For example, in the medical\nRelationship Extraction \n| \n181\n",
      "word_count": 491,
      "char_count": 3000,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 212,
      "text": "domain, relations could include type of injury, location of injury, cause of injury,\ntreatment of injury, etc. In the financial domain, relations could mean something\ncompletely different. A few generic relations between people, locations, and organiza‐\ntions are: located in, is a part of, founder of, parent of, etc. How do we extract them?\nFigure 5-10. Relation extraction demo\nApproaches to RE\nIn NLP, RE is a well-researched topic, and—starting from handwritten patterns to\ndifferent forms of supervised, semi-supervised, and unsupervised learning—various\nmethods have been explored (and are still being used) for building RE systems.\nHand-built patterns consist of regular expressions that aim to capture specific rela‐\ntionships. For example, a pattern such as “PER, [something] of ORG” can indicate a\nsort of “is-a-part-of” relation between that person and organization. Such patterns\nhave the advantage of high precision, but they often have less coverage, and it could\nbe challenging to create such patterns to cover all possible relations within a domain.\n182 \n| \nChapter 5: Information Extraction\n",
      "word_count": 167,
      "char_count": 1109,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1346,
          "height": 1163,
          "ext": "png",
          "size_bytes": 55522
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 213,
      "text": "Hence, RE is often treated as a supervised classification problem. The datasets used to\ntrain RE systems contain a set of pre-defined relations, similar to classification data‐\nsets. This consists of modeling it as a two-step classification problem:\n1. Whether two entities in a text are related (binary classification).\n2. If they are related, what is the relation between them (multiclass classification)?\nThese are treated as a regular text classification problem, using handcrafted features,\ncontextual features like in NER (e.g., words around a given entity), syntactic structure\n(e.g., a pattern such as NP VP NP, where NP is a noun phrase and VP is a verb\nphrase), and so on. Neural models typically use different embedding representations\n(which we saw in Chapter 3) followed by an architecture like recurrent neural net‐\nworks (which we saw in Chapter 4).\nBoth supervised approaches and pattern-based approaches are typically domain spe‐\ncific, and getting large amounts of annotated data each time we start on a new\ndomain can be both challenging and expensive. As we saw in Chapter 4, bootstrap‐\nping can be used in such scenarios, starting with a small set of seed patterns and gen‐\neralizing by learning new patterns based on the sentences extracted using these seed\npatterns. An extension of such weak supervision approaches is called distant supervi‐\nsion. In this, instead of using a small set of seed patterns, large databases such as\nWikipedia, Freebase, etc., are used to first collect thousands of examples of many rela‐\ntions (e.g., using Wikipedia infoboxes), thereby creating a large dataset of relations.\nThis can then be followed by a regular supervised relation extraction approach. Even\nthis works only when such large databases exist. [39] illustrates how to use Snorkel,\nwhich we saw in Chapters 2 and 4, to learn specific relations in the absence of any\ntraining data. We leave its exploration as an exercise for the reader.\nIn scenarios where we cannot procure training data for supervised approaches, we\ncan resort to unsupervised approaches. Unsupervised RE (also known as “open IE”)\naims to extract relations from the web without relying on any training data or any list\nof relations. The relations extracted are in the form of <verb, argument1, argument2>\ntuples. Sometimes, a verb may have more arguments. Figure 5-11 shows an example\nof the output of such an open IE system by AllenNLP [40, 41].\nRelationship Extraction \n| \n183\n",
      "word_count": 400,
      "char_count": 2465,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 214,
      "text": "Figure 5-11. Open IE demo by AllenNLP\nIn this example, we see the relation as a verb and its three arguments <published,\nalbert einstein, the theory of relativity, in 1915>. We can also extract the relation\ntuples <published, albert einstein, the theory of relativity>, <published, albert ein‐\nstein, in 1915>, and <published, theory of relativity, 1915>. Obviously, in such a sys‐\ntem, we see at least as many (typically, more) such tuples/quadruples as the number\nof verbs. While this is an advantage in the sense that it can extract all such relations,\nthe challenge with this approach lies in mapping the extracted versions to some stan‐\ndardized set of relations (e.g., fatherOf, motherOf, inventorOf, etc.) from a database.\nTo then extract specific relations from this information (if we need to), we would\nhave to devise our own procedures combining the outputs of NER/NEL, coreference\nresolution, and open IE.\nRE with the Watson API\nRE is a hard problem, and it would be challenging and time consuming to develop\nour own relation extraction systems from scratch. A solution commonly used in NLP\nprojects in the industry is to rely on the Natural Language Understanding service\nprovided by IBM Watson [42]. The next code snippet (Ch5/REWatson.ipynb) shows\nhow to extract relationships between entities with IBM Watson using a text snippet\nfrom the Wikipedia page referenced earlier in this section:\nmytext3 = \"\"\"Nadella attended the Hyderabad Public School, Begumpet [12] before \nreceiving a bachelor's in electrical engineering[13] from the Manipal Institute \nof Technology (then part of Mangalore University)in Karnataka in 1988.\"\"\"\nresponse = natural_language_understanding.analyze(text=mytext3,\n            features=Features(relations=RelationsOptions())).get_result()\nfor item in response['relations']:\n         print(item['type'])\n         for subitem in item['arguments']:\n          print(subitem['entities'])\nFigure 5-12 shows the output of this code in terms of the relations it extracted. The\nrelations are extracted using a supervised model and contain a preset list of relations\n[43]. Thus, anything outside that list of relations will not be extracted.\n184 \n| \nChapter 5: Information Extraction\n",
      "word_count": 320,
      "char_count": 2215,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1214,
          "height": 412,
          "ext": "png",
          "size_bytes": 27273
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 215,
      "text": "Figure 5-12. Relation extraction output from IBM Watson\nThis output information, showing relations between different entities, can then be\nused to construct a knowledge base for the organization’s data. As we can see, RE is\nnot a completely solved problem yet, and the performance of the approach is also\ndomain dependent. What worked for a Wikipedia article may not work for a general\nnews article or, say, social media text. [44] summarizes the state of the art in RE.\nStart with pattern-based approaches and use some form of weak\nsupervision in scenarios where pre-trained supervised models may\nnot work.\nWe hope this gave an overview of where RE is useful and how to approach the prob‐\nlem if you encounter it at your workplace. Let’s now take a look at a few other IE tasks\nbefore concluding the discussion on this topic.\nOther Advanced IE Tasks\nSo far, we’ve discussed different information extraction tasks, where they’re useful,\nand how we can build them into our NLP projects if required. While this list of tasks\nis by no means exhaustive, they’re the tasks most commonly used across industry use\ncases. In this section, let’s take a quick look at a few more specialized IE tasks. These\nare not very common and are used sparingly in NLP projects in the industry, so we’ll\nonly introduce them briefly in this section. We would advise the reader to start with\n[26] to get further guidance on the different approaches for solving these tasks. Let’s\nlook at an overview of three other IE tasks: temporal IE, event extraction, and tem‐\nplate filling.\nOther Advanced IE Tasks \n| \n185\n",
      "word_count": 273,
      "char_count": 1588,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        },
        {
          "index": 1,
          "width": 880,
          "height": 307,
          "ext": "png",
          "size_bytes": 30243
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 216,
      "text": "Temporal Information Extraction\nConsider an email text: “Let us meet at 3 p.m. today and decide on what to present at\nthe meeting on Friday.” Say we’re working on an application to identify and populate\ncalendars with events extracted from such conversations, much like we see in Gmail.\nFigure 5-13 shows a screenshot of this utility in Gmail.\nTo build a similar application, in addition to extracting date and time information (3\npm, today, Friday) from the text, we should also convert the extracted data into some\nkind of standard form (e.g., mapping the expression “on Friday” to the exact date,\nbased on context, and “today” to today’s date). While extracting date and time infor‐\nmation can be done using a collection of handcrafted patterns in the form of regex, or\nby applying supervised sequence labeling techniques like we did for NER, normaliza‐\ntion of extracted date and time into a standard date-time format can be challenging.\nTogether, these tasks are referred to as temporal IE and normalization. Contemporary\napproaches to such temporal expression normalization are primarily rule based and\ncoupled with semantic analysis [26].\nFigure 5-13. Identifying and extracting temporal events from emails [45]\nDuckling [46] is a Python library recently released by Facebook’s bots team that was\nused to build bots for Facebook Messenger. The package is designed to parse text and\nget structured data. Among the many tasks it can do, it can process the natural lan‐\nguage text data to extract temporal events. Figure 5-14 shows the output when we run\nthe sentence “Let us meet at 3 p.m. today and decide on what to present at the meet‐\ning on Friday” through Duckling. It’s able to map “3 p.m. today” to the correct time\non a given day.\nDuckling supports multiple languages. From our experience, it works very well and is\na great off-the-shelf package to begin with if you want to incorporate some form of\ntemporal IE into your project. Other packages, such as SUTime [47] by Stanford NLP,\n186 \n| \nChapter 5: Information Extraction\n",
      "word_count": 341,
      "char_count": 2040,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 572,
          "height": 309,
          "ext": "png",
          "size_bytes": 26047
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 217,
      "text": "Natty [48], Parsedatetime [49], and Chronic [50], are also capable of processing\nhuman-readable dates and times. We leave it to the reader to explore these packages\nfurther and see how useful they are for temporal IE. Now, let’s move on to the next IE\ntask: event extraction.\nFigure 5-14. Sample output of temporal IE via Duckling\nEvent Extraction\nIn the email-text example we discussed in the previous section, the aim of extracting\ntemporal expressions is to eventually extract information about an “event.” Events can\nbe anything that happens at a certain point in time: meetings, increase in fuel prices\nin a region at a certain time, presidential elections, the rise and fall of stocks, life\nevents like birth, marriage, and demise, and so on. Event extraction is the IE task that\ndeals with identifying and extracting events from text data. Figure 5-15 shows an\nexample of extracting life events from people’s Twitter feeds.\nThere are many business applications of event extraction. Consider a finance-lending\ncompany that reaches out to people for education loans. Wouldn’t they love to have a\nsystem that can scan Twitter feeds and identify “university admission” events? Or\nconsider a trade analyst in a hedge fund who needs to keep tabs on major events\naround the world. It’s believed that Bloomberg Terminal [51] has a submodule that\nreports major events that are identified from thousands of news sources and social\nchannels like Twitter in real time across the globe. A popular, fun application of event\nextraction is the congratsbot [52]. The bot reads through tweets and responds with a\n“congrats” message if it sees any event that one should be congratulated on. See\nFigure 5-16 for an example.\nOther Advanced IE Tasks \n| \n187\n",
      "word_count": 287,
      "char_count": 1743,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 992,
          "height": 547,
          "ext": "png",
          "size_bytes": 42964
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 218,
      "text": "Figure 5-15. Examples of extracting life events from Twitter data [53]\nFigure 5-16. Congratsbot in action\nSo, how should we approach this problem? Event extraction is treated as a supervised\nlearning problem in NLP literature. Contemporary approaches use sequence tagging\nand multilevel classifiers, much like we saw earlier with relationship extraction. The\nultimate goal is to identify various events over time periods, connect them, and create\na temporally ordered event graph. This is still an active area of research, and working\nsolutions for event extraction like those mentioned previously only work for specific\nscenarios; i.e., there are no relatively generic solutions like we saw for RE, NER, etc.\nTo the best of our knowledge, there are no off-the-shelf services or packages for this\ntask. If you end up doing a project that requires event extraction, the best way for‐\nward is to first start with a rule-based approach based on domain knowledge, then\nfollow it up with weak supervision. As you start accumulating more data, you can\nmove toward ML approaches.\n188 \n| \nChapter 5: Information Extraction\n",
      "word_count": 177,
      "char_count": 1115,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1418,
          "height": 538,
          "ext": "png",
          "size_bytes": 59575
        },
        {
          "index": 1,
          "width": 623,
          "height": 235,
          "ext": "png",
          "size_bytes": 36090
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 219,
      "text": "Template Filling\nIn some application scenarios, such as weather forecasts and financial reports, the\ntext format is fairly standard, and what changes are the specific details pertaining to\nthat situation. For example, consider a scenario where we work in an organization\nthat sends reports on companies’ stock prices on a daily basis. The format of these\nreports will be similar for most companies. An example of one such “template” sen‐\ntence is: “Company X’s stock is up by Y% since yesterday,” where X and Y change but\nthe sentence pattern remains the same. If we’re asked to automate the report genera‐\ntion process, how should we approach it? Such scenarios are good use cases for an IE\ntask called template filling, where the task is to model text generation as a slot-filling\nproblem. Figure 5-17 shows an example of template filling and how it can be used to\nbuild an entity graph.\nFigure 5-17. Example of template filling [54]\nOther Advanced IE Tasks \n| \n189\n",
      "word_count": 165,
      "char_count": 968,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1129,
          "height": 1317,
          "ext": "png",
          "size_bytes": 92789
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 220,
      "text": "Generally, the templates to fill are pre-defined. This is typically modeled as a two-\nstage, supervised ML problem, similar to relation extraction. The first step involves\nidentifying whether a template is present in a given sentence, and the second step\ninvolves identifying slot fillers for that template, with a separate classifier trained for\neach slot. Work is being done in the direction of automatically inducing templates.\nSince this is a specialized, domain-dependent case, we’re not aware of any off-the-\nshelf service provider for this task. As with other tasks in this section, we recommend\nyou start with the chapter on IE in [31] to gain further understanding.\nA recent real-world example of template filling–based text generation is the BBC’s\ncoverage of the 2019 UK elections. BBC created a template and created news stories\nautomatically for all of the UK’s 650 electoral areas. [55] and [56] discuss this project\nin greater detail.\nWith this, we conclude our discussion of most IE tasks. So far, we’ve seen a wide\nrange of IE tasks and how to incorporate some of them individually in your code.\nHow do these tasks connect with one another in a real-world application? Let’s dis‐\ncuss a case study.\nCase Study\nImagine we work for a large, traditional enterprise. We communicate via email and\nenterprise messaging platforms like Slack or Yammer. A lot of discussions about\nmeetings happen as part of email threads. There are the three main types of meetings:\nteam meeting, one-on-one meeting, and talk/presentation, plus their associated ven‐\nues. Say we’re tasked with building a system that automatically finds relevant meet‐\nings, books the venue or conference hall, and notifies people. Let’s look at how the IE\ntasks we’ve discussed would be useful in this scenario. We’ll assume that there’s only\none meeting per email. Look at the email exchange in Figure 5-18 for our scenario\ndescription. How would we go about starting to build that?\nAs a caveat, we might need to restrict what we’re building at the start and solve a\nmore focused problem. For instance, an email my contain multiple meeting men‐\ntions, like in this example: “MountLogan was a good venue. Let us meet there tomor‐\nrow and have an all hands in MountRainer on Thursday.” Let’s assume there’s only\none meeting per email in our case study and start thinking about how to approach\nthe problem of building a simple system as an MVP to get started.\nFirst, we’ll need some amount of labeled data. We can start building labeled data in\nmultiple ways. Imagine we have access to past calendar and conference booking\ninformation as well as email. Does comparing booking information and the emails\nyield positive matches? If so, we could try hardcoded weak supervision, similar to the\none described in Chapter 4. Alternatively, we could try bootstrapping with pre-built\nservices like Google Cloud NLP or AWS Comprehend. For example, Google Cloud\n190 \n| \nChapter 5: Information Extraction\n",
      "word_count": 489,
      "char_count": 2967,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 221,
      "text": "NLP has an entity extraction service that returns events, and we can use it to generate\na dataset. However, as such automatically created datasets may not be perfect, we’ll\nneed manual verification.\nLet’s say we’re dealing with the following entities and have collected some data with\nthese annotations: Room Name (Meeting Location), Meeting Date, Meeting Time,\nMeeting Type (derived field), Meeting Invitees. For our first model, we can use a\nsequence labeling model like conditional random fields (CRFs), which are also used\nfor NER. To classify the type of meeting, we can start with a rule-based classifier\nbased on features such as room size (larger rooms may generally imply larger meet‐\nings), number of invitees, etc.\nFigure 5-18. Meeting information extraction from email (representative image)\nCase Study \n| \n191\n",
      "word_count": 129,
      "char_count": 823,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 560,
          "height": 763,
          "ext": "png",
          "size_bytes": 70067
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 222,
      "text": "Once our system is in deployment, we can start collecting feedback in the form of\nexplicit tagging or more implicit feedback. These may include meeting accept/reject\nrates and meeting conflict rates on the calendar and for the room. All this informa‐\ntion can be used to collect more data so we can apply more sophisticated models.\nOnce we have enough data (5–10K labeled sentences from emails), we can start\nexploring more powerful language understanding models. If enough compute power\nis available, we may take advantage of a powerful pre-trained model like BERT and\ncan fine-tune it on the new labeled dataset. The pipeline for this process is depicted in\nFigure 5-19.\nFigure 5-19. Pipeline for meeting information extraction system development\n192 \n| \nChapter 5: Information Extraction\n",
      "word_count": 126,
      "char_count": 791,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 995,
          "height": 1553,
          "ext": "png",
          "size_bytes": 93857
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 223,
      "text": "Now let’s consider the more complex case we discussed at the beginning, where we\nmay have mentions of multiple entities (room names) and also loose mentions of\nmultiple meetings happening at different times. We want to tackle this problem as a\nmulticlass, multilabel classification problem. The linguistic ambiguity could be hard\nto decipher via handcrafted feature engineering like the presence of some specific\nentity, fixed vocabulary, etc. A reasonable way to approach this problem would be to\nuse a deeper neural network with recurrence, such as an LSTM or a GRU network.\nThese networks will model contextual information around each word and encode\nthat knowledge into the hidden vectors we would use to finally classify the email.\nWhile all this discussion is specific to one real-world IE problem, it’s possible to\nincrementally implement and improve a solution to any IE problem using the\napproach outlined in this section.\nWrapping Up\nIn this chapter, we looked at information extraction and its usefulness in different\nreal-world scenarios and discussed how to implement solutions for different IE tasks,\nincluding keyphrase extraction, named entity recognition, named entity linking, and\nrelationship extraction. We also introduced the tasks of temporal information extrac‐\ntion, event extraction, and template filling. Compared to what we saw with text classi‐\nfication, an important difference with IE is that these tasks rely on resources beyond\nlarge annotated corpora and also require more domain knowledge. Hence, in a prac‐\ntical scenario, it’s more common to use pre-trained models and solutions from large\nservice providers rather than developing IE systems of our own from scratch, unless\nwe’re working on a super-specialized domain that needs custom solutions. Another\nimportant point to note, which we also reiterated several times throughout the chap‐\nter, is the role that a good text extraction and cleanup process plays in all these tasks.\nWhile we didn’t take up specific end-to-end examples involving multiple IE tasks\n(some of which will be covered in Part III of the book), we hope that this chapter\ngives you enough of an idea about IE and the things to keep in mind when imple‐\nmenting IE tasks in your projects. In the next chapter, we’ll take a look at how to\nbuild chatbots for different use cases you may encounter in your workplace.\nReferences\n[1] Wikipedia. “Message Understanding Conference”. Last modified November 20,\n2019.\n[2] Linguistic Data Consortium. “ACE”. Last accessed June 15, 2020.\n[3] NIST. “Text Analysis Conference”. Last accessed June 15, 2020.\n[4] Google News. Last accessed June 15, 2020.\nWrapping Up \n| \n193\n",
      "word_count": 422,
      "char_count": 2667,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 224,
      "text": "[5] Sarno, Adrian. “Information Extraction from Receipts with Graph Convolutional\nNetworks”, Nanonets (blog), 2020.\n[6] Sensibill. Last accessed June 15, 2020.\n[7] Nicas, Jack. “Apple’s Plan to Buy $75 Billion of Its Stock Fuels Spending Debate”.\nNew York Times, April 30, 2019.\n[8] Chiticariu, Laura, Yunyao Li, and Frederick Reiss. “Rule-Based Information\nExtraction is Dead! Long Live Rule-Based Information Extraction Systems!” Proceed‐\nings of the 2013 Conference on Empirical Methods in Natural Language Processing\n(2013): 827–832.\n[9] Chiticariu, L. et al. “Web Information Extraction”. In Liu, L. and Özsu, M.T. (eds),\nEncyclopedia of Database Systems, New York: Springer, 2018.\n[10] Hasan, Kazi Saidul and Vincent Ng. “Automatic Keyphrase Extraction: A Survey\nof the State of the Art.” Proceedings of the 52nd Annual Meeting of the Association for\nComputational Linguistics 1: (2014): 1262–1273.\n[11] Çano, Erion and Ondřej Bojar. “Keyphrase Generation: A Text Summarization\nStruggle.” Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies 1 (2019):\n666–672.\n[12] Chartbeat Labs Projects. textacy: NLP, before and after spaCy, (GitHub repo).\nLast accessed June 15, 2020.\n[13] Explosion.ai. “SpaCy: Industrial-Strength Natural Language Processing in\nPython”. Last accessed June 15, 2020.\n[14] Mihalcea, Rada and Paul Tarau. “Textrank: Bringing Order into Text.” Proceed‐\nings of the 2004 Conference on Empirical Methods in Natural Language Processing\n(2004): 404–411.\n[15] Gensim. “summarization.keywords—Keywords for TextRank summarization\nalgorithm”. Last accessed June 15, 2020.\n[16] Chowdhury, Jishnu Ray. “Implementation of TextRank”. Last accessed June 15,\n2020.\n[17] Explosion.ai. “displaCy Named Entity Visualizer”. Last accessed June 15, 2020.\n[18] spaCy. Common entity categories in NER development. Last accessed June 15,\n2020.\n[19] The Stanford Natural Language Processing Group. “Stanford RegexNER”. Last\naccessed June 15, 2020.\n[20] Explosion.ai. spacy’s EntityRuler. Last accessed June 15, 2020.\n194 \n| \nChapter 5: Information Extraction\n",
      "word_count": 300,
      "char_count": 2150,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 225,
      "text": "[21] Wikipedia. “Sequence labeling”. Last modified January 18, 2017.\n[22] Sang, Erik F. and Fien De Meulder. “Introduction to the CoNLL-2003 Shared\nTask: Language-Independent Named Entity Recognition.” Proceedings of the Seventh\nConference on Natural Language Learning at HLT-NAACL (2003).\n[23] Team HG-Memex. sklearn-crfsuite: scikit-learn inspired API for CRFsuite, (Git‐\nHub repo). Last accessed June 15, 2020.\n[24] MIT-NLP. MITIE: library and tools for information extraction, (GitHub repo).\nLast accessed June 15, 2020.\n[25] Yang, Jie. NCRF++: a Neural Sequence Labeling Toolkit, (GitHub repo). Last\naccessed June 15, 2020.\n[26] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft), 2018, Chapter 18.\n[27] Linguistic Data Consortium. “OntoNotes Release 5.0”. Last accessed June 15,\n2020.\n[28] The Stanford Natural Language Processing Group. “Stanford Named Entity Rec‐\nognizer (NER)”. Last accessed June 15, 2020.\n[29] Allen Institute for AI. “AllenNLP: An open-source NLP research library, built on\nPyTorch”. Last accessed June 15, 2020.\n[30] Explosion.ai. Prodigy’s NER Recipes. Last accessed June 15, 2020.\n[31] Jurafsky, Daniel and James H. Martin. Speech and Language Processing: An Intro‐\nduction to Natural Language Processing, Computational Linguistics and Speech Recog‐\nnition. Upper Saddle River, NJ: Prentice Hall, 2008.\n[32] FilingDB. “What’s so hard about PDF text extraction?” Last accessed June 15,\n2020.\n[33] IBM Research Editorial Staff. “Making sense of language. Any language”. Octo‐\nber 28, 2016.\n[34] Wikipedia. “Knowledge Graph”. Last modified April 12, 2020.\n[35] NLP-progress. “Entity Linking”. Last accessed June 15, 2020.\n[36] DBpedia Spotlight. “Shedding light on the web of documents”. Last accessed\nJune 15, 2020.\n[37] Rosette Text Analytics. “Relationship Extraction”. Last accessed June 15, 2020.\n[38] Wikipedia. “Satya Nadella”. Last modified April 10, 2020.\n[39] Snorkel. “Detecting spouse mentions in sentences”. Last accessed June 15, 2020.\nWrapping Up \n| \n195\n",
      "word_count": 294,
      "char_count": 2039,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 226,
      "text": "[40] Allen Institute for AI. “Reading Comprehension: Demo”. Last accessed June 15,\n2020.\n[41] AllenNLP’s GitHub repository. Last accessed June 15, 2020.\n[42] IBM Cloud. “Watson Natural Language Understanding”. Last accessed June 15,\n2020.\n[43] IBM Cloud. Relation types. Last accessed June 15, 2020.\n[44] NLP-progress. “Relationship Extraction”. Last accessed June 15, 2020.\n[45] BetterCloud. “Hidden Shortcuts for Creating Calendar Events Right from\nGmail”. Last accessed June 15, 2020.\n[46] Wit.ai. Duckling. Last accessed June 15, 2020.\n[47] The Stanford Natural Language Processing Group. “Stanford Temporal Tagger”.\nLast accessed June 15, 2020.\n[48] Stelmach, Joe. “Natty”. Last accessed June 15, 2020.\n[49] Taylor, Mike. “parsedatetime”. Last accessed June 15, 2020.\n[50] Preston-Warner, Tom. Chronic: a pure Ruby natural language date parser, (Git‐\nHub repo). Last accessed June 15, 2020.\n[51] Bloomberg Professional Services. “Event-Driven Feeds”. Last accessed June 15,\n2020.\n[52] Twitter. Congratulatron (@congratsbot). Last accessed June 15, 2020.\n[53] Li, Jiwei, Alan Ritter, Claire Cardie, and Eduard Hovy. “Major Life Event Extrac‐\ntion from Twitter based on Congratulations/Condolences Speech Acts”. Proceedings\nof the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP) (2014): 1997–2007.\n[54] Jean-Louis, Ludovic, Romaric Besançon, and Olivier Ferret. “Text Segmentation\nand Graph-based Method for Template Filling in Information Extraction.” Proceed‐\nings of 5th International Joint Conference on Natural Language Processing (2011): 723–\n731.\n[55] Molumby, Conor and Joe Whitwell. “General Election 2019: Semi-Automation\nMakes It a Night of 689 Stories”. BBC News Labs, December 13, 2019.\n[56] Reiter, Ehud. “Election Results: Lessons from a Real-World NLG System”, Ehud\nReiter’s Blog, December 23, 2019.\n196 \n| \nChapter 5: Information Extraction\n",
      "word_count": 265,
      "char_count": 1889,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 227,
      "text": "CHAPTER 6\nChatbots\nOne machine can do the work of fifty ordinary men.\nNo machine can do the work of one extraordinary man.\n—Elbert Green Hubbard\nChatbots are interactive systems that allow users to interact in natural language. They\ngenerally interact via text but can also use speech interfaces. Early 2016 saw the intro‐\nduction of the first wave of chatbots that soon became ubiquitous. Platforms like\nFacebook Messenger, Google Assistant, and Amazon Alexa are some examples of\nchatbots. There are now tools that allow developers to create custom chatbots [1] for\ntheir brand or service so that consumers can carry out some of their daily actions\nfrom within their messaging platforms.\nThe introduction of chatbots into society has brought us to the beginning of a new\nera in technology: the era of the conversational interface. It’s an interface that soon\nwon’t require a screen or a mouse to use. There will be no need to click or swipe; just\nthe use of voice will be enough. This interface will be completely conversational, and\nthose conversations will be indistinguishable from the conversations we have with\nour friends and family. Since chatbots deal with text under the hood, it’s all about\nunderstanding the text responses coming from users and producing reasonable\nreplies. From understanding to generation, NLP plays a significant role, which we’ll\nsee throughout this chapter.\nThe history of chatbots and of artificial intelligence in general are pretty intertwined.\nIn the 1950s and ’60s, computer scientists Alan Turing and Joseph Weizenbaum con‐\ntemplated the concept of computers communicating like humans do. Later, in 1966,\nJoseph Weizebaum built Eliza [2], the first chatterbot ever coded, using only 200 lines\nof code. Eliza imitated the language of a Rogerian Psychotherapist using regular\nexpressions and rules. Humans knew they were interacting with a computer program,\n197\n",
      "word_count": 303,
      "char_count": 1900,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (9.3pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 228,
      "text": "and yet, through the emotional responses Eliza would offer, still grew emotionally\nattached to the program during trials.\nLater, in the advent of powerful signal processing tools, researchers focused on build‐\ning spoken dialog tools with the goal of improving user experience. Many spoken dia‐\nlog systems were built between 1980 and 2000 and started as military-based projects\n(by DARPA) aimed mainly at improving automated communication with soldiers.\nThe systems were used to provide instructions, which later translated into chatbots\ncapable of helping users get answers to frequently asked questions for various serv‐\nices. The bots were still handcrafted such that responses they generated were fixed,\nand the bots were not good at handling the context provided in the conversation.\nIn recent years, chatbots have become more feasible and useful, both due to the ubiq‐\nuity of smartphones and recent advances in ML and DL. In addition to APIs to create\nchatbots on popular messaging platforms like Facebook Messenger, we now have var‐\nious platforms to create the AI and logic behind the chatbots. This has allowed folks\nand companies with limited AI background and experience to deploy their own chat‐\nbots easily.\nThis chapter aims to cover the underlying systems and theory of chatbots, along with\npractical, hands-on experience building chatbots using different scenarios. We’ll end\nwith some state-of-the-art research that may bring major advances to this entire para‐\ndigm. We’ll motivate our readers by introducing popular applications of chatbots.\nApplications\nChatbots can be used for many different tasks in many different industries, from\nretail, to news, and even the medical field. We’ll briefly discuss various applications of\nchatbots. Many of these use cases have become more mature in recent years, while\nsome are still in their infancy. These use cases include:\nShopping and e-commerce\nRecently, chatbots are being used for various e-commerce operations, including\nplacing or modifying an order, payment, etc. Bots for recommending various\nitems are also of great interest to the e-commerce industry. Industries are focused\non building conversational recommendation systems to provide a more seamless\nuser experience.\nNews and content discovery\nSimilar to e-commerce, chatbots can be used in news and content discovery.\nUsers may specify various nuances of their search in a conversational manner,\nand the bot should be able respond with relevant articles.\n198 \n| \nChapter 6: Chatbots\n",
      "word_count": 385,
      "char_count": 2510,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 229,
      "text": "Customer service\nCustomer service is another area where bots are used heavily. They’re used to\nlodge complaints, help answer FAQs, and navigate queries in pre-defined conver‐\nsational flows set by the business requirements.\nMedical\nIn health and medical applications, FAQ bots are of great use. These bots can\nhelp patients fetch relevant information quickly based on their symptoms.\nRecently, there has also been interest in building chatbots that elicit useful infor‐\nmation from patients, especially older patients, regarding their health conditions\nby asking relevant questions.\nLegal\nIn legal applications, bots can also be used to serve FAQs for users. They can even\nbe used for more complex goals, such as asking follow-up questions. For exam‐\nple, if a user asks for legal articles to follow up on a case, a bot might ask specific\nquestions regarding the nature of the case to find a more appropriate match.\nHere’s a more elaborate example of an FAQ bot, which is common in many service\nplatforms, to help users by providing answers to frequently asked questions.\nA Simple FAQ Bot\nA FAQ bot is generally a search-based system where, given a question, it looks for\ncorrect answers and provides them to the user. It’s essentially a bot that allows a user\nto ask questions in different ways to get a response. Such bots are quite useful for pro‐\nviding a conversational interface to a complex set of questions.\nAs an example, we’ll consider a subset of Amazon Machine Learning Frequently\nAsked Questions. A machine needs to learn to provide the correct answer given simi‐\nlar questions, so it’s a good idea to have some paraphrases of each question. See\nTable 6-1 for some input-output examples for such a chatbot.\nTable 6-1. Amazon ML FAQ to be used for a FAQ bot [3]\nQuestions\nAnswer\nWhat can I do with Amazon Machine Learning?\nHow can I use Amazon Machine Learning?\nWhat can Amazon Machine Learning do?\nYou can use Amazon Machine Learning to create a wide variety of\npredictive applications. For example, you can use Amazon Machine\nLearning to help you build applications that flag suspicious transactions,\ndetect fraudulent orders, forecast demand, etc.\nWhat algorithm does Amazon Machine Learning\nuse to generate models? How does Amazon\nMachine Learning build models?\nAmazon Machine Learning currently uses an industry-standard logistic\nregression algorithm to generate models.\nAre there limits to the size of the dataset I can\nuse for training?\nWhat is the maximum size of training dataset?\nAmazon Machine Learning can train models on datasets up to 100 GB in\nsize.\nApplications \n| \n199\n",
      "word_count": 428,
      "char_count": 2598,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MinionPro-It (9.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 230,
      "text": "Figure 6-1 is a working version of such an FAQ bot. Later in the chapter, we’ll learn\nhow to build such a bot for various applications step by step.\nFigure 6-1. An FAQ bot\nNow, we’ll transition to the taxonomy of chatbots and explain various categories of\nchatbot based on their usage.\nA Taxonomy of Chatbots\nLet’s expand on chatbots of various uses and their applicability to various domains.\nChatbots can be classified in many ways, which affects how they’re built and where\nthey’re used. A way of looking at these chatbots is how they interact with the user:\nExact answer or FAQ bot with limited conversations\nThese chatbots are linked to a fixed set of responses and retrieve a correct\nresponse based on understanding the user’s query. For example, if we build an\nFAQ bot, the bot has to understand the question and retrieve a fixed, correct\nanswer for it. Generally, one response from the user does not depend on the pre‐\nvious responses. Take a look at Figure 6-2. In the FAQ bot example, we see that,\nin the first two turns, the bot provides a fixed response to similar questions that\nare asked with slight variations. For a different question, it pulls out a different\nanswer.\n200 \n| \nChapter 6: Chatbots\n",
      "word_count": 214,
      "char_count": 1213,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 394,
          "height": 515,
          "ext": "png",
          "size_bytes": 102285
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 231,
      "text": "Flow-based bot\nFlow-based conversational bots are generally more complex than FAQ bots in\nterms of the variability of their responses. Users may gradually express their\nopinions or requests over the course of conversations. For example, when order‐\ning a pizza, a user may express their requested toppings, pizza size, and other\nnuances gradually. The bot should understand and track this information\nthroughout the conversation to successfully generate a response every time. In\nFigure 6-2, for the flow-based bot, we see that the bot asks a specific set of ques‐\ntions to achieve the goal of making a pizza order. This flow was pre-defined, and\nthe bot asks relevant questions to fulfill the order. We’ll discuss such a flow-based\nbot in greater detail later in this chapter.\nOpen-ended bot\nOpen-ended bots are intended mainly for entertainment, where the bot is sup‐\nposed to converse with the user about various topics. The bot doesn’t have to\nmaintain specific directions or flows of the conversation. In Figure 6-2, the open-\nended bot carries out a conversation without any pre-existing template or fixed\nquestion-answer pairs. It transitions fluently from one topic to another to main‐\ntain the interesting conversation. This example of an open-ended bot was built by\none of the authors for a popular digital assistant platform.\nFigure 6-2. Types of chatbots\nA Taxonomy of Chatbots \n| \n201\n",
      "word_count": 225,
      "char_count": 1398,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1411,
          "height": 1012,
          "ext": "png",
          "size_bytes": 217088
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 232,
      "text": "Chatbots are classified into two broad categories: (1) goal-oriented dialogs and (2)\nchitchats. FAQ bots and flow-based bots fall into the first category, whereas open-\nended bots are mainly chitchat types. Both of these types of bots are used heavily in\nindustry and are also in the active area of research in academia.\nGoal-Oriented Dialog\nThe natural human purpose of having a conversation is to accomplish a goal via rele‐\nvant information seeking. In the similar line of thought, it’s easy to design any chatbot\nor conversation agent for a specific use case where the end goal is known. Most of the\nchatbots we’ve discussed so far (those typically used in research or industry) are goal-\noriented chatbots. The user interacting with the chatbot should have complete infor‐\nmation about what they want to achieve after the conversation. For example, looking\nfor a movie recommendation or booking flight reservations through chatbots or con‐\nversational agents are examples of goal-oriented dialog where the goal is to watch a\nmovie or book a flight.\nNow, by definition, the goal-oriented systems are domain-specific, which requires\ndomain-specific knowledge in the system. This hampers the generalizability and scal‐\nability of the chatbot framework. Research from Facebook [4] recently presented an\nend-to-end framework for training all components from the dialogs themselves to\nmitigate that limitation. This research proposes an automatic manipulation of the\ndata—for example, question-answer pairs to carry out a meaningful conversation via\nrequired API calls. This is one of the newest approaches that researchers and industry\npractitioners have started to follow.\nChitchats\nApart from goal-oriented conversations, humans also engage in unstructured, open-\ndomain conversations without any specific goals. These human-human conversations\ninvolve free-form, opinionated discussions about various topics. Having a conversa‐\ntional agent that can have a chitchat with a human is challenging due to the absence\nof objective goals. A conversational agent must generate coherent, on-topic, and fac‐\ntually correct responses to make the dialog more natural.\nThe application of chitchat bots is futuristic but holds immense potential. For exam‐\nple, these bots could be used to elicit useful but sensitive information in the case of a\nmedical emergency for geriatric care. The free-form conversational bot could also be\nused to address the long-standing issue of loneliness and depression among teenagers\nand elderly people. Some of the market-leader companies, such as Amazon, Apple,\nand Google, to name a few, are investing heavily in building such bots for worldwide\ncustomers.\n202 \n| \nChapter 6: Chatbots\n",
      "word_count": 411,
      "char_count": 2710,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 233,
      "text": "So far, we’ve discussed various kinds of chatbots and their usage in various industries.\nThis will allow us to appreciate various components of chatbots based on usage and\nalso help us implement some of the components as we need them. Now, we’ll deep-\ndive into the chatbot development pipeline and discuss details of various\ncomponents.\nA Pipeline for Building Dialog Systems\nWe discussed various NLP tasks, such as classification and entity detection, through‐\nout Chapters 4 and 5. Now, we’ll utilize some of them to describe an example pipeline\nto build a dialog system. Figure 6-3 depicts a complete pipeline of a dialog system\nwith various components. We’ll discuss the utility of each component and data flow\nthrough the pipeline.\nFigure 6-3. Pipeline for a dialog system\nSpeech recognition\nUsually, the dialog system works as an interface between human and machine, so\nthe input into the dialog system is human speech. Speech recognition algorithms\ntranscribe speech to natural text. In industrial dialog systems, state-of-the-art [5]\nspeech-to-text models are used, which is beyond the scope of this book. If you’re\ninterested in speech models, refer to [5] for an overall view.\nNatural language understanding (NLU)\nAfter transcribing, the system tries to analyze and “understand” the transcribed\ntext. This module encompasses various natural language understanding tasks.\nExamples of such tasks are sentiment detection, named entity extraction, corefer‐\nence resolution, etc. This module is primarily responsible for gathering all possi‐\nble information that is implicitly (sentiment) or explicitly (named entities)\npresent in the input text.\nA Pipeline for Building Dialog Systems \n| \n203\n",
      "word_count": 260,
      "char_count": 1700,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1390,
          "height": 515,
          "ext": "png",
          "size_bytes": 39445
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 234,
      "text": "Dialog and task manager\nOnce we obtain information from the input, a dialog manager, as shown in the\nfigure, gathers and systematically decides which pieces of information are impor‐\ntant or not. A dialog manager is a module that controls and guides the flow of the\nconversation. Imagine this as a table containing information extracted in NLU\nsteps and stored concurrently for all utterances in the ongoing conversation. The\ndialog manager develops a strategy via rules or other complex mechanisms, such\nas reinforcement learning, to effectively utilize the information obtained from\nthe input. Dialog managers are mostly prevalent in goal-oriented dialogs since\nthere’s a definite objective to reach via the conversation.\nNatural language generation\nFinally, as the dialog manager decides a strategy for responding, the natural lan‐\nguage generation module generates a response in a human-readable form\naccording to the strategy devised by the dialog manager. The response generator\ncould be template based or a generative model learned from data. After this, a\nspeech synthesis module converts the text back to speech to the end user. For\nmore information on speech synthesis tasks, take a look at [6] and [7].\nAny chatbot can be built using such a pipeline. For text-based chat‐\nbots, we can remove the speech processing components. While the\nNLU and generation component can be complex, a dialog manager\ncould simply be rules routing the bot to an appropriate response\ngenerator.\nAlthough the pipeline in Figure 6-3 assumes the chatbot is voice based, a similar\npipeline without the speech processing modules will work for text-based chatbots.\nBut in all industrial applications, we’re moving toward eventually having more and\nmore voice-based systems, so the pipeline discussed here is more general, and it\napplies to a variety of applications we described previously (including the case study\nin Chapter 1). Now that we’ve briefly discussed the various components of a chatbot\nand how a conversation flow takes place, let’s deep dive to understand these compo‐\nnents in detail.\nDialog Systems in Detail\nThe main idea of a dialog system or chatbot is to understand a user’s query or input\nand to provide an appropriate response. This is different from typical question-\nanswering systems where, given a question, there has to be an answer. In a dialog\nsetup, users may ask their queries in “turns.” In each turn, a user reveals their interest\nabout the topic based on what the bot may have responded with. So, in a dialog sys‐\ntem, the most important thing is understanding nuances from the user’s input in a\nturn-by-turn way and storing them in context to generate responses.\n204 \n| \nChapter 6: Chatbots\n",
      "word_count": 438,
      "char_count": 2711,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 235,
      "text": "Before we get into the details of bots and dialog systems, we’ll cover the terminology\nused in dialog systems and chatbot development more broadly.\nDialog act or intent\nThis is the aim of a user command. In traditional systems, the intent is a primary\ndescriptor. Often, several other things, such as sentiment, can be linked to the\nintent. The intent is also called a “dialog act” in some literature. In the first exam‐\nple in Figure 6-4, orderPizza is the intent of the user command. Similarly, in the\nsecond example, the user wants to know about a stock, so the intent is getStock‐\nQuote. These intents are usually pre-defined based on the chatbot’s domain of\noperation.\nSlot or entity\nThis is the fixed ontological construct that holds information regarding specific\nentities related to the intent. The information related to each slot that’s surfaced\nin the original utterance is “value.” The slots and value together are sometimes\ndenoted as an “entity.” Figure 6-4 shows two examples of entities. The first exam‐\nple looks for specific attributes of the pizza to be ordered: “medium” and “extra\ncheese.” On the other hand, the second example looks for the related entities for\ngetStockQuote: the stock name and the time period the chatbot is asked for.\nDialog state or context\nA dialog state is an ontological construct that contains both the information\nabout the dialog act as well as state-value pairs. Similarly, context can be viewed\nas a set of dialog states that also captures previous dialog states as history.\nFigure 6-4. Example of different terminology used in chatbots\nDialog Systems in Detail \n| \n205\n",
      "word_count": 270,
      "char_count": 1621,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 602,
          "height": 407,
          "ext": "png",
          "size_bytes": 72875
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 236,
      "text": "Now, let’s complete a walkthrough using a cloud API called Dialogflow [8] for a fic‐\ntional pizza shop to enable users to converse with a chatbot to order pizza. This is a\ngoal-oriented system where the goal is to accommodate the user’s request and order a\npizza.\nPizzaStop Chatbot\nDialogflow is a conversational agent–building platform by Google. By providing the\ntools to understand and generate natural language and manage the conversation, Dia‐\nlogflow enables us to easily create conversational experiences. While there are many\nother tools available, we chose this one because it’s easy to use, mature, and is being\nimproved constantly.\nImagine there’s a fictional pizza shop called PizzaStop, and we have to build a chatbot\nthat can take an order from a customer. A pizza can have multiple toppings (like\nonions, tomatoes, and peppers), and it can come in different sizes. An order can also\ncontain one or more items from the sides, appetizers, and/or beverages section of the\nmenu. Now that we understand the requirements, let’s begin building our bot using\nthe Dialogflow framework.\nBuilding our Dialogflow agent\nBefore we begin creating our agent, we need to create an account and set up a few\nthings. For this, open the official Dialogflow website [9], log in with your Google\naccount, and provide the required permissions. Navigate to V2 of the API [10]. Click\non “try it for free” and you’ll be directed to the free tier of Google Cloud Services,\nthen you can follow the registration process.\n1. First, we need to create an agent. Click the Create Agent button, then enter the\nname of the agent. You can provide any name, but it’s good practice to provide a\nname that gives an idea of what the agent is used for. For our PizzaStop project,\nwe’ll name our agent “Pizza.” Now, set the time zone and click the Create button.\nFigure 6-5 shows the UI you’ll see while creating an agent.\n206 \n| \nChapter 6: Chatbots\n",
      "word_count": 329,
      "char_count": 1923,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 237,
      "text": "Figure 6-5. Creating an agent using Dialogflow\n2. You’ll then be redirected to another page with options that allow you to create the\nbot. Figure 6-6 shows the UI of Dialogflow, which we’ll use multiple times while\ncreating our agent. By default, we’ll already have two intents: Default Fallback\nIntent and Default Welcome Intent. Default Fallback Intent is the default response\nif some internal API fails and Default Welcome Intent will generate a welcome\nmessage.\nFigure 6-6. Dialogflow UI after creating an agent\nDialog Systems in Detail \n| \n207\n",
      "word_count": 89,
      "char_count": 549,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 997,
          "height": 327,
          "ext": "png",
          "size_bytes": 28190
        },
        {
          "index": 1,
          "width": 1362,
          "height": 661,
          "ext": "png",
          "size_bytes": 62398
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 238,
      "text": "3. Now, we need to add the intents and entities we care about to our agent. To add\nan intent, hover over the Intents block and click the + button. You’ll see some‐\nthing similar to Figure 6-7. These intents and entities are what we defined earlier\nin the section.\nFigure 6-7. Dialogflow UI after clicking the “+” button\n4. Now, we’ll create the first intent: orderPizza. As we create a new intent, we have\nto provide training examples, called “training phrases,” to enable the bot to detect\nvariations of responses that belong to the intent. We also need to provide “con‐\ntext”: a piece of information that can be remembered over the span of a conversa‐\ntion and that will be used for subsequent intent detection.\nExamples of training phrases are “I want to order a pizza” or “medium with\ncheese please.” The first one denotes a simple intent of pizza ordering, whereas\nthe second one consists of entities that are useful to remember, such as medium\nsize and cheese topping.\n208 \n| \nChapter 6: Chatbots\n",
      "word_count": 177,
      "char_count": 1003,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1161,
          "height": 905,
          "ext": "png",
          "size_bytes": 30937
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 239,
      "text": "Figure 6-8 shows sample training phrases added to the agent.\nFigure 6-8. Adding training phrases for intents\n5. Since we’ve included intent, we need to add the respective entities to remember\nimportant information provided by the user. Create an entity named pizzaSize,\nenable “fuzzy matching” (which matches entities even if they’re only approxi‐\nmately the same), and provide the necessary values. Similarly, create a pizzaTop‐\nping entity, but this time, also enable “Define synonyms” (this lets us define\nsynonyms while allowing us to match several words, defined as synonyms, to the\nsame entity).\nThese two together will help us detect “medium size” and “cheese toppings,” as\nshown in Figures 6-9 and 6-10.\nDialog Systems in Detail \n| \n209\n",
      "word_count": 118,
      "char_count": 745,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 715,
          "height": 530,
          "ext": "png",
          "size_bytes": 19874
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 240,
      "text": "Figure 6-9. Creating the pizzaSize entity\nFigure 6-10. Creating the pizzaTopping entity\n6. Now, let’s go back to the Intents block to add additional information to the\nAction and Parameters section. We need both the topping and size to complete\nthe order, so we need to check the Required box on those. One pizza can’t be\nmultiple sizes, but one pizza can have multiple toppings. So, enable the isList\noption for toppings to allow it to have multiple values.\nA user might only mention the size or the topping. To gather the complete infor‐\nmation, we need to add a prompt that asks follow-up questions, such as, “What\nsize of pizza would you like?” as a prompt for pizzaSize. This is shown in\nFigure 6-11.\n210 \n| \nChapter 6: Chatbots\n",
      "word_count": 130,
      "char_count": 734,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 723,
          "height": 326,
          "ext": "png",
          "size_bytes": 11540
        },
        {
          "index": 1,
          "width": 761,
          "height": 444,
          "ext": "png",
          "size_bytes": 21125
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 241,
      "text": "Figure 6-11. Actions and parameters for orderPizza intent\n7. We also need to provide sample responses, as shown in Figure 6-12, that the\nagent will give the user. We can ask the user if they need drinks, appetizers, or\nsides. If we were creating something like a billing intent, we could end the con‐\nversation after it by enabling the “Set this intent as end of conversation” slider in\nthe Responses block.\nFigure 6-12. Adding the appropriate responses our agent should use\n8. So far, we’ve added a simple intent and entities. Now we can look at a complex\nentity with context. Consider the statement, “I want to order 2 L of juice and 3\nwings.” Our agent needs to recognize the quantity and the item ordered. This is\ndone by adding a custom entity in Dialogflow. We’ve created an entity called\ncompositeSide, and it can handle all of these combinations. For example, in\n“@sys.number-integer:number-integer @appetizer:appetizer”, the first entity\ndeals with recognizing how many of the appetizers are ordered, and the next one\nDialog Systems in Detail \n| \n211\n",
      "word_count": 179,
      "char_count": 1060,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 731,
          "height": 312,
          "ext": "png",
          "size_bytes": 17082
        },
        {
          "index": 1,
          "width": 737,
          "height": 356,
          "ext": "png",
          "size_bytes": 13857
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 242,
      "text": "deals with the type of appetizer, as shown in Figures 6-13 and 6-14. As you can\nsee, the signatures of these entities are given as regular expressions.\nFigure 6-13. Creating the compositeSide entity\nFigure 6-14. Example of a complex statement with multiple entities and context\n9. We can add many more intents and entities to make our agent robust. In\nFigure 6-15 and Figure 6-16, take a look at examples of other intents and entities\nwe added to enrich and enhance the user’s pizza-buying experience.\n212 \n| \nChapter 6: Chatbots\n",
      "word_count": 90,
      "char_count": 530,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 775,
          "height": 323,
          "ext": "png",
          "size_bytes": 21204
        },
        {
          "index": 1,
          "width": 291,
          "height": 501,
          "ext": "png",
          "size_bytes": 16578
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 243,
      "text": "Figure 6-15. All the intents for this agent\nFigure 6-16. All the entities for this agent\nNow that we’ve gone through the steps to build a bot for PizzaStop, we’ll test our bot\nto see how it works in various scenarios.\nDialog Systems in Detail \n| \n213\n",
      "word_count": 47,
      "char_count": 251,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 769,
          "height": 336,
          "ext": "png",
          "size_bytes": 10288
        },
        {
          "index": 1,
          "width": 766,
          "height": 420,
          "ext": "png",
          "size_bytes": 12862
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 244,
      "text": "Testing our agent\nNow, let’s test our agent in a website setting. For this, we need to open it in “web\ndemo” mode. Click the Integrations block and scroll down until you reach Web\nDemo. Click the link in the pop-up window, and that’s it! Feel free to test your agent\nto your heart’s content. Figure 6-17 shows snippets of the one we built. Testing our\nbot is important for validating that it’s working. We’ll analyze a few cases of varying\ndifficulty.\nFigure 6-17. Making a simple order using our agent\nWe can see in Figure 6-17 that our bot is able to handle simple queries to order a\npizza. As we have tested the bot end to end, we can also test various components of it\nindividually. Testing individual components helps to prototype quickly and catch\nedges cases before the end-to-end testing.\nNow, let’s go through a more complex example, which will be tested with an integra‐\ntion of this bot with Google Assistant. In the example shown in Figure 6-17, our\nagent identifies the intent to order a pizza and recognizes the toppings we ordered.\nThe pizzaSize entity is not fulfilled, so it asks a question regarding the size of the pizza\nto fulfill the entity’s requirement. With the orderPizza intent fulfilled, the agent then\nproceeds to ask us about sides and appetizers. Based on the statement we provided,\nthe agent needs to fulfill the orderSize intent and should be able to identify the quan‐\ntity of juice and the appetizer. This shows that the agent is able to handle complex\nentities. Finally, we move on to the conversation for selecting the type of payment.\n214 \n| \nChapter 6: Chatbots\n",
      "word_count": 282,
      "char_count": 1600,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 932,
          "ext": "png",
          "size_bytes": 121421
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 245,
      "text": "Figures 6-18 and 6-19 show how internal state and extracted entities work in another\nconversation.\nFigure 6-18. Texting complex statements with multiple entities\nFigure 6-19. Testing with a complex entity and context\nDialog Systems in Detail \n| \n215\n",
      "word_count": 38,
      "char_count": 250,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1442,
          "height": 723,
          "ext": "png",
          "size_bytes": 152381
        },
        {
          "index": 1,
          "width": 290,
          "height": 483,
          "ext": "png",
          "size_bytes": 16160
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 246,
      "text": "Dialogflow allows us to build goal-oriented chatbots. It’s important\nto have an extensive ontology (possible slots and intents) for our\ndomain, as it will make our bot rich in responding to varied user\nqueries.\nWe’ve shown how to build a fully functional chatbot using the Dialogflow API. We\nlearned about intents and entities—the two main building blocks of understanding\ndialog. Now, we’ll delve deep into building custom models for intent/dialog act classi‐\nfication and entity/slot identification.\nDeep Dive into Components of a Dialog System\nSo far, we’ve seen how to build a chatbot using Dialogflow and how to add various\nfeatures to handle complex entities and context. Now, we want to deep-dive into the\nmachine learning aspect of the internals of a dialog system. As we discussed while\ndescribing the pipeline for a dialog system, understanding the context (i.e., the user\nresponse) in light of conversation history is one of the most important tasks for\nbuilding a dialog system.\nUnderstanding context can be broken down into understanding the user’s intent and\ndetecting corresponding entities for that particular intent. These internal components\ncorrespond to the natural language understanding component in the chatbot pipe‐\nline. To illustrate this, we’ll go through a sample of a conversation on restaurant book‐\ning and describe how to model different components for context understanding.\nFigure 6-20 shows an example of a user looking for a restaurant reservation. As we\ncan see, there are labels available for each response. The labels indicate intents and\nentities for these responses. We want to use such annotations to train our ML models.\nFigure 6-20. Conversation about restaurant booking [11]\n216 \n| \nChapter 6: Chatbots\n",
      "word_count": 274,
      "char_count": 1748,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        },
        {
          "index": 1,
          "width": 564,
          "height": 444,
          "ext": "png",
          "size_bytes": 57525
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 247,
      "text": "Before we go into the model, we’ll formally define two natural understanding tasks\nrelated to context understanding for dialogs. Since this involves understanding the\nnuances of language underneath, these are also attributed as natural language under‐\nstanding (NLU) tasks.\nDialog Act Classification\nDialog act classification is a task to identify how the user utterance plays a role in the\ncontext of dialog. This informs what “act” the user is performing. For example, a sim‐\nple example of dialog acts would be to identify a “yes/no” question. If the user asks,\n“Are you going to school today?”, this would be classified as a yes/no question. On the\nother hand, if the user asks, “What is the depth of the ocean?”, that may not be classi‐\nfied as a yes/no question. We’ve seen that intents or dialog acts are important for\nbuilding a chatbot, even in Cloud APIs. Identifying intent helps to understand what\nthe user is asking for and to take actions accordingly.\nBuilding dialog act classification and slot identification from\nscratch can be a complex and data-consuming process. Doing so\nmakes sense when our dialog acts and slots are more open-ended\nin nature than a Cloud API or existing framework can solve. Hav‐\ning complete control of dialog internals can yield better results over\ntime in such problems.\nThis can be reframed as a classification problem: given a dialog utterance, classify it\ninto dialog acts or labels. In our example from Figure 6-20, we define a dialog act pre‐\ndiction task where labels include inform, request, etc. The utterance “Where is it?”\ncan be classified as a dialog act “request.” On the other hand, the utterance “I’m look‐\ning for a cheaper restaurant” can be classified as an “inform” dialog act. Drawing on\nwhat we learned in Chapter 4, we can use any classifier we like to solve this task. We’ll\ndiscuss the models pertaining to this task with a complete dataset example in “Dialog\nExamples with Code Walkthrough” on page 219.\nIdentifying Slots\nOnce we’ve extracted the intents, we want to move on to extracting entities. Extract‐\ning entities is also important for generating correct and appropriate responses to the\nuser’s input. We also saw in our Dialogflow example that extracting entities along\nwith the intents creates a full understanding of the user’s input.\nIn the example in Figure 6-20—”I’m looking for a cheaper restaurant”—we want to\nidentify “cheaper” as a price slot and take its value verbatim—i.e., the value of the slot\nis “cheaper.” If we know ontologies for slot-value pairs, a more normalized form can\nultimately be restored, such as “cheaper” -> “cheap.” We have seen similar tasks in\nChapter 5, where we learned how to extract entities from sentences. We can take a\nDeep Dive into Components of a Dialog System \n| \n217\n",
      "word_count": 467,
      "char_count": 2788,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 248,
      "text": "similar approach (i.e., a sequence labeling approach) here as well to extract these enti‐\nties.\nPreviously, in our Dialogflow examples, we saw that slots have to be pre-defined\nbeforehand. But here, we want to build this component on our own using an ML\nalgorithm. Recall the algorithms discussed in the context of NER in Chapter 5. We’ll\nuse similar algorithms for slot detection and labeling. We’ll use an open source\nsequence labeling library called sklearn-crfsuite [12], which we introduced in Chap‐\nter 5, for this task. We’ll discuss details of this experiment in a later section.\nWe can choose a range of ontologies for annotating entities. Imag‐\nine we’re building a travel bot. The choice of entity for the destina‐\ntion can be city or airport. To make it robust, we must detect\nairports as an entity since one city can have multiple airports. On\nthe other hand, in the case of a restaurant-booking bot, detecting\ncities as an entity is probably suitable.\nOne of the disadvantages of these methods is that they need a lot of labeled data for\nboth intent and entity detection. Also, we need dedicated models for both of the tasks.\nThis can make the system slow during deployment. Getting fine-grained labels for\nentities is also expensive. These issues limit the scalability of the pipeline for more\ndomains.\nRecent research [11] on spoken language understanding revealed that joint under‐\nstanding and tracking is better than individual classification and sequence labeling\nparts. This joint model is lightweight at deployment as compared to individual mod‐\nels. For joint modeling, we can utilize dialog states, which is “inform(price - cheap)”\nin our example in Figure 6-20. We can aim to rank or score each candidate pair\njointly with dialog act (in combination, a dialog state) to jointly determine the state.\nJoint determination is more complex and requires better representation learning\ntechniques, which are beyond the scope of this book. Interested readers can learn\nmore about this at [11]. Now that we’ve discussed NLU components, let’s move on to\nresponse generation.\nResponse Generation\nOnce we identify the slots and intent, the final step is for a dialog system to generate\nan appropriate response. There are many ways to generate a response: fixed respon‐\nses, using templates, and automatic generation.\nFixed responses\nFAQ bots mainly use fixed responses. Based on the intent and values for the slots,\na dictionary lookup is made on a pool of responses and retrieves the best\nresponse. A simple case would be to discard the slot information and have one\n218 \n| \nChapter 6: Chatbots\n",
      "word_count": 429,
      "char_count": 2609,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 249,
      "text": "response per intent. For more complex retrieval, a ranking mechanism can be\nbuilt that ranks the pool of responses based on the detected intent and slot-value\npairs (or the dialog state).\nUse of templates\nTo make responses dynamic, a templates-based approach is often taken. Tem‐\nplates are very useful when the follow-up response is a clarifying question. Slot\nvalues can be used to come up with a follow-up question or a fact-driven answer.\nFor example, “The House serves cheap Thai food” can be constructed using a\ntemplate as <restaurant name> serves <price-value> <food-value> food. Once we\nidentify slots and their values, we populate this template to finally generate an\nappropriate response.\nAutomatic generation\nMore natural and fluent generation can be learned using a data-driven approach.\nUpon obtaining the dialog state, a conditional generative model can be built that\ntakes a dialog state as an input and generates the next response for the agent.\nThese models can be graphical models or DL-based language models. Later, we’ll\nbriefly cover end-to-end approaches for dialogs that are similar to automatic gen‐\nerations.\nWhile automatic generation is robust, template generation has\nadvantages over it. It might be hard to distinguish between the two,\nespecially when the template variety is high, Template-based\nresponses contain fewer grammatical errors and are easier to train.\nNow that we’ve deep-dived into various components of a dialog system, let’s walk\nthrough examples of dialog act classification and slot predictions.\nDialog Examples with Code Walkthrough\nNow, we’ll go through instances of various real-world dialog datasets that are publicly\navailable and discuss their usage to model various aspects of a dialog system. Then\nwe’ll use two of those datasets to show how to implement models for two tasks we\ndescribed for context understanding: dialog act prediction or intent classification and\nslot identification or entity detection. We’ll explore a couple of models for each task\nand show via comparisons how these models can be improved gradually. All the\nmodels are inspired from the NLU tasks (classification and information extraction)\nwe discussed in Chapters 4 and 5.\nDeep Dive into Components of a Dialog System \n| \n219\n",
      "word_count": 352,
      "char_count": 2258,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 250,
      "text": "Datasets\nTable 6-2 is a brief summary of various datasets that are used for benchmarking algo‐\nrithms for goal-oriented dialog tasks. As we’re interested in various NLU tasks in dia‐\nlogs, we present four datasets for goal-oriented dialogs that act as benchmarks for\ndialog-based NLU tasks.\nTable 6-2. Goal-oriented datasets from various domains and their usage\nDataset\nDomain\nUsage\nATIS [13]\nAir Ticket\nBooking\nBenchmark for intent classification and slot filling. This is a single-domain dataset, hence\nentities and intents are restricted to one domain.\nSNIPS [14]\nMultidomain\nBenchmark for intent classification and slot filling. This is a multidomain dataset, hence the\nentities belong to multiple domains. Multiple-domain datasets are challenging to model\ndue their variability.\nDSTC [15]\nRestaurants\nBenchmark for dialog state tracking or joint determination of intent and slots. This is\nsimilarly a single-domain dataset, but the entities are expressed more in terms of\nannotations and contain more metadata.\nMultiWoZ\n[16]\nMultidomain\nBenchmark for dialog state tracking or joint determination of intent and slots that spans\nover multiple domains. For the similar reason of variability, modeling this dataset is more\nchallenging than modeling single-domain ones.\nIn addition to these datasets, several datasets of varying scale (i.e., number of sample\nconversations) are available [17] for various other subtasks in a dialog pipeline. Later\nin this section, we’ll discuss how to gather such a dataset and apply it to a domain-\nspecific scenario. For now, we’ll focus on goal-oriented dialogs since they have direct\nusage in industry and the state-of-the-art research is well established.\nDespite the existence of many open source datasets, there are only\na few datasets that reflect the naturalness of human conversation.\nDatasets collected by online annotators like Mechanical Turkers\nsuffer from templatish and forced conversation, which affects the\ndialog quality. Also, domain-specific dialog datasets are still not\navailable for many domains, such as healthcare, law, etc.\nDialog act prediction\nDialog act classification or intent detection is the task we described in the previous\nsection as a part of the NLU component in a dialog system. This is a classification\ntask, and we’ll follow our classification pipeline from Chapter 4 to solve it.\nLoading the dataset.   We’ll use ATIS (Airline Travel Information Systems) for the\nintent detection task. ATIS is a dataset that’s used heavily for spoken language under‐\nstanding and performing various NLU tasks. The dataset consists of 4,478 training\nutterances and 893 test utterances with a total of 21 intents. We’ve chosen 17 intents,\n220 \n| \nChapter 6: Chatbots\n",
      "word_count": 414,
      "char_count": 2726,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "MinionPro-Regular (9.6pt)",
        "MyriadPro-SemiboldCond (11.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 394,
          "height": 514,
          "ext": "png",
          "size_bytes": 7986
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 251,
      "text": "which appear in both the train and test set. Hence, our task is a 17-class classification\ntask. An instance of the dataset looks like the following code:\nQuery text: BOS please list the flights from charlotte to long beach arriving \n after lunch time EOS\nIntent label:  flight\nModels.   Since it’s a classification task, we’ll use one of the DL techniques we used in\nChapter 4 directly: a CNN model. Using CNN is useful here because it captures the n-\ngram features via its dense representations. N-grams such as “list of flights” is indica‐\ntive of a “flight” label:\natis_cnnmodel = Sequential()\natis_cnnmodel.add(embedding_layer)\natis_cnnmodel.add(Conv1D(128, 5, activation='relu'))\natis_cnnmodel.add(MaxPooling1D(5))\natis_cnnmodel.add(Conv1D(128, 5, activation='relu'))\natis_cnnmodel.add(MaxPooling1D(5))\natis_cnnmodel.add(Conv1D(128, 5, activation='relu'))\natis_cnnmodel.add(GlobalMaxPooling1D())\natis_cnnmodel.add(Dense(128, activation='relu'))\natis_cnnmodel.add(Dense(num_classes), activation='softmax'))\natis_cnnmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics= ['acc'])\nWe obtain an accuracy of 72% with the use of a CNN on the test, averaged over all\nclasses. If we use an RNN model, the accuracy shoots up to 96%. We believe that\nRNN is able to capture the interdependency of words across the input sentence. RNN\ncaptures the importance of a word with respect to the context it’s seen before. The\nelaborate details of these models and the dataset code are given in ch6/\nCNN_RNN_ATIS_intents.ipynb:\natis_rnnmodel = Sequential()\natis_rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\natis_rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\natis_rnnmodel.add(Dense(num_classes), activation='sigmoid'))\natis_rnnmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics= ['accuracy'])\nAs we know, recent transformer pre-trained models (such as BERT) are more power‐\nful. So, we’ll try to use BERT to improve the obtained performance so far. BERT can\ncapture the context better and has more parameters, so it’s more expressive and mod‐\nels the intricacies of the language. To use BERT, we use a BERT-style input tokeniza‐\ntion scheme:\n# For data:\nsentence = \" [CLS] \" + query + \" [SEP]\"\nTokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n                                         do_lower_case=True)\nDeep Dive into Components of a Dialog System \n| \n221\n",
      "word_count": 300,
      "char_count": 2465,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MyriadPro-SemiboldCond (11.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 252,
      "text": "tokenizer.tokenize(sentence)\n# For model:\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                     num_labels=num_classes)\nSince BERT is pre-trained, the representation of content is much better than any\nmodels we train from scratch, such as CNNs or RNNs. We see that BERT achieves\n98.8% accuracy, beating both CNN and RNN for the dialog act prediction task. Fol‐\nlow the notebook ch6/BERT_ATIS_intents.ipynb for the complete code for model and\ndata preparation.\nSlot identification\nSlot identification is another task we described in the previous section as a part of the\nNLU component in a dialog system. We described why we can pose this as a sequence\nlabeling task. We need to find the slot values given the input, and we’ll follow our\nsequence labelling pipeline from Chapter 5 to solve this task.\nLoading the dataset.   We’ll use SNIPS for this slot identification task. SNIPS is a dataset\ncurated by Snips, an AI voice platform for connected devices. It contains 16,000\ncrowdsourced queries and is a popular benchmark for slot identification tasks. We’ll\nload both training and test examples, and an instance of the dataset looks like the\ncode below:\nQuery text: [Play, Magic, Sam, from, the, thirties]  # tokenized\nSlots: [O, artist-1, artist-2, O, O, year-1]\nAs we discussed in Chapter 5, we’re using the BIO scheme to annotate the slots. Here,\nO denotes “other,” and artist-1 and artist-2 denote the two words for artist name.\nThe same goes for the year.\nModels.   Since a slot identification task can be viewed as a sequence labeling task, we’ll\nuse one of the popular techniques we used in Chapter 5: a CRF++ model from the\nsklearn package. We also use word vectors instead of creating handcrafted features to\nfeed into a CRF. CRFs are a popular sequence labeling technique and are used heavily\nin information extraction.\nWe use word features that will be useful for this particular task. We see that the con‐\ntext for each word is important in addition to the meaning of the word itself. So, we\nuse the previous two words and next two words for a given word as features. We also\nuse the word embedding vectors retrieved from GloVe pre-trained embeddings (dis‐\ncussed in Chapter 3) as additional features. Features for each word are concatenated\nacross words in an input. This input representation is passed to a CRF model for\nsequence labeling:\n222 \n| \nChapter 6: Chatbots\n",
      "word_count": 392,
      "char_count": 2457,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "MyriadPro-SemiboldCond (11.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 253,
      "text": "def sent2feats(sentence):\n    feats = []\n    sen_tags = pos_tag(sentence) #This format is specific to this POS tagger!\n    for i in range(0,len(sentence)):\n        word = sentence [i]\n        wordfeats = {}\n        #word features: word, prev 2 words, next 2 words in the sentence.\n        wordfeats ['word'] = word\n        if i == 0:\n            wordfeats [\"prevWord\"] = wordfeats [\"prevSecondWord\"] = \"<S>\"\n        elif i==1:\n            wordfeats [\"prevWord\"] = sentence [0]\n            wordfeats [\"prevSecondWord\"] = \"</S>\"\n        else:\n            wordfeats [\"prevWord\"] = sentence [i-1]\n            wordfeats [\"prevSecondWord\"] = sentence [i-2]\n        #next two words as features\n        if i == len(sentence)-2:\n            wordfeats [\"nextWord\"] = sentence [i+1]\n            wordfeats [\"nextNextWord\"] = \"</S>\"\n        elif i==len(sentence)-1:\n            wordfeats [\"nextWord\"] = \"</S>\"\n            wordfeats [\"nextNextWord\"] = \"</S>\"\n        else:\n            wordfeats [\"nextWord\"] = sentence [i+1]\n            wordfeats [\"nextNextWord\"] = sentence [i+2]\n        #Adding word vectors\n        vector = get_embeddings(word)\n        for iv,value in enumerate(vector):\n            wordfeats ['v{}'.format(iv)]=value\n        feats.append(wordfeats)\n    return feats\n# training\ncrf = CRF(algorithm='lbfgs', c1=0.1, c2=10, max_iterations=50)\n# Fit on training data\ncrf.fit(X_train, Y_train)\nWe obtain an F1 of 85.5 with the use of a CRF++ model. More details can be found in\nthe notebook ch6/CRF_SNIPS_slots.ipynb. Similar to the previous classification task,\nwe’ll try to use BERT to improve the performance obtained so far. BERT can capture\nthe context better, even in the case of a sequence labeling task. We use all the hidden\nrepresentations for all the words in the query to predict a label for each. Hence, at the\nend, we input a sequence of words into the model and obtain a sequence of labels (of\nthe same length as the input), which can be inferred as predicted slots with the words\nas values:\n# For data:\nsentence = \" [CLS] \" + query + \" [SEP]\"\nDeep Dive into Components of a Dialog System \n| \n223\n",
      "word_count": 280,
      "char_count": 2114,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 254,
      "text": "Tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n                                         do_lower_case=True)\ntokenizer.tokenize(sentence)\n# For model:\nmodel = BertForTokenClassification.from_pretrained(\"bert-base-uncased\",\n                                                  num_labels=num_tags)\nBut, we find that BERT achieves only 73 F1. This could be due to the presence of\nmany named entities in the input that were not well represented by the original BERT\nparameters. On the other hand, the features we obtained for the CRF were strong\nenough for this dataset to capture the necessary patterns. This is an interesting exam‐\nple where a simpler model beats BERT. See the complete model details in the note‐\nbook ch6/BERT_SNIPS_slots.ipynb.\nAs we’ve seen before and here as well, pre-trained models help get\nbetter performance over other DL models learned from scratch.\nThere could be exceptions, as pre-trained models are sensitive to\nthe size of the data. Pre-trained models may overfit on smaller\ndatasets, and handcrafted features may generalize well in those\ncases.\nSo far, we’ve learned how to build various NLU components for a goal-oriented dia‐\nlog using popular datasets. We’ve seen how various DL models perform relatively well\nin these tasks. With these, we’ll be able to run such custom models in our own dataset\nand explore various models to pick the best one. We also introduced four datasets\nthat are popular benchmarks for goal-oriented dialog modeling. They can be used for\nprototyping newer models to verify their performance against state-of-the-art mod‐\nels. Now, we’ll transition to other dialog models that are generally used beyond goal-\noriented settings, and we’ll discuss their advantages and disadvantages.\nOther Dialog Pipelines\nSo far, we’ve discussed the modular pipeline we introduced in Figure 6-3. But there\nare many other pipelines that can be used in various scenarios, especially in the case\nof an open-ended chatbot. The initial pipeline in Figure 6-3 sometimes lacks in terms\nof ease of trainability due to multiple components, as each of them has to be individ‐\nually trained and they need separate annotated datasets for each component. Besides\nthat, in a modular pipeline, one needs to define the ontology explicitly and it does not\ncapture latent patterns from the data. That is why we will briefly touch upon other\nexisting pipelines that may be promising in future.\n224 \n| \nChapter 6: Chatbots\n",
      "word_count": 363,
      "char_count": 2457,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "UbuntuMono-Italic (8.5pt)",
        "MinionPro-Regular (9.6pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 394,
          "height": 514,
          "ext": "png",
          "size_bytes": 7986
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 255,
      "text": "End-to-End Approach\nSequence-to-sequence models (we’ll call it seq2seq) have seen huge acceptance in\ncritical NLP tasks such as neural machine translation, named entity recognition, etc.\nThe seq2seq models generally take a sequence as input and output another sequence.\nIn a translation task, imagine our input sentence is in one language and output is in\nthe language we want to translate it to.\nSimilar to other tasks, we can build a chatbot using seq2seq models. Imagine that the\ninput of the model is the user utterance: a sequence of words. As the output, it gener‐\nates another sequence of words, which is the response from the bot. Seq2seq models\nare end-to-end trainable, so we don’t have to maintain multiple modules, and they are\ngenerally LSTM based. Recently, state-of-the-art transformers have been used for\nseq2seq tasks, so they can also be applied in the case of dialog.\nUsually, we use tokenization to create word tokens and create a sequence out of a\nquestion. Seq2seq is capable of capturing the inherent order of the token in the\nsequence—this is important, as it ensures that we capture the right meaning of the\nquestion in order to answer it correctly. See Figure 6-21 for some examples from a\nwork by Google [11] on such an end-to-end model. They input the questions to the\nmodel, and the model generated the corresponding outputs.\nFigure 6-21. Example of work done by Google on seq2seq models [18]\nDeep Reinforcement Learning for Dialogue Generation\nIf you’re wondering how a machine would generate a diverse set of answers given any\nkind of question, you’re not alone. [19] studied the drawbacks of typical seq2seq\nmodels and discovered that they often kept generating the generic output, “I don’t\nknow.” These models generated utterances without considering how to respond in\nOther Dialog Pipelines \n| \n225\n",
      "word_count": 301,
      "char_count": 1833,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1330,
          "height": 726,
          "ext": "png",
          "size_bytes": 86495
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 256,
      "text": "order to have a good conversation. Doing so requires futuristic knowledge about the\ngoodness of the conversation, which will ultimately help the user achieve their goal.\nThe concept of goodness is abstract, so it’s typically defined based on the objective of\nthe conversation. For example, with a goal-oriented dialog setup, we have a defined\ngoal to achieve, whereas in a chitchat setup, goodness is defined by how interesting\nthe conversation will be.\nHere, we see the combination of two ideas: goal-oriented dialog and seq2seq-based\ngeneration. Reinforcement learning can help us here. Each time the machine utters a\nresponse is nothing but it performing a specific action. A set of such actions can be\nmade in a way that ensures the goal is finally achieved via the conversation. In rein‐\nforcement learning based on exploration and exploitation, the machine tries to learn\nto generate the best response based on a futuristic reward defined by the user, which\nis directly related to how likely the current response is to achieve the final goal.\nFigure 6-22 shows how the reinforcement learning–based model performed well\ncompared to the typical seq2seq-based model. On the right-hand side, you can see\nthat the reinforcement learning–based model generated a more diverse response\ninstead of collapsing into a generic default response.\nFigure 6-22. Comparison of deep reinforcement learning and a seq2seq model [19]\nHuman-in-the-Loop\nSo far, we’ve talked about machines generating answers in response to questions\nasked, without human intervention. The machine may improve its performance if\nhumans intervene in its learning process and reward or penalize based on the correct\nor incorrect response. These rewards or penalties act as feedback for the model.\nAnswering a natural language query typically follows three steps: understand the\nquery, perform an action, and respond to utterances. While doing this, the machine\n226 \n| \nChapter 6: Chatbots\n",
      "word_count": 304,
      "char_count": 1953,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1212,
          "height": 568,
          "ext": "png",
          "size_bytes": 103526
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 257,
      "text": "might need human intervention in various scenarios—for example, if the question is\nout of the chatbot’s scope, if the action it took was not correct, or if the understanding\nof the query was wrong. Typically, when humans intervene in a machine’s learning\nprocess, it’s termed as human-in-the-loop.\nIn the context of chatbots, Facebook has performed an exercise [20] of using humans\nto inject partial rewards when the bot is learning in a reinforcement learning setup.\nAs we discussed in the previous subsection, the ultimate goal of the bot is to fulfill the\nuser’s needs. But with human-in-the-loop, while exploring various actions, the bot\nreceives additional input from a human “teacher,” which clearly improves the quality\nof the response, as shown in Figure 6-23.\nFigure 6-23. Humans providing additional signals during dialog learning [20]\nHuman-in-the-loop is ultimately a more practical system to deploy\nthan a completely automated dialog generation system. End-to-end\nmodels are efficient to train, but they may not be reliable in pro‐\nducing factually correct outputs. Hence, a hybrid system with the\ncombination of end-to-end dialog generation framework and with\nhuman resources will be more reliable and robust.\nWe’ve discussed various techniques beyond goal-oriented dialog. Many of these\nmethods are built by industry and are usable in practical settings. These end-to-end\nmodels can grow large in terms of parameters (via the use of new transformer archi‐\ntecture) and can therefore become infeasible to deploy in small-scale applications.\nBut we also saw here that even LSTM models can generate reasonable outputs.\nHuman-in-the-loop is also a feasible technique that can be adopted regardless of the\ncomputing power available.\nRasa NLU\nSo far, we’ve discussed how to build two main components of a dialog system: dialog\nact prediction and slot filling. Beyond these two components, there are several inte‐\ngration steps to tie them into a complete pipeline for dialog. Also, we can build wrap‐\nping logic around these components and create a comprehensive dialog experience\nfor users.\nRasa NLU \n| \n227\n",
      "word_count": 330,
      "char_count": 2118,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        },
        {
          "index": 1,
          "width": 1059,
          "height": 225,
          "ext": "png",
          "size_bytes": 51101
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 258,
      "text": "Building such a complete dialog system requires significant engineering work. But\nthe good news is there are frameworks available that allow us to build custom NLP\nmodels as various components of the system and that provide overhead engineering\ntools and supports to build a functioning bot. One example of such a framework is\nRasa. Rasa offers a suite of features [21] that can be essential in building a chatbot for\nindustrial use. Figure 6-24 shows the Rasa chatbot interface along with its interactive\nlearning framework, which we’ll discuss later.\nFigure 6-24. Rasa chatbot interface and interactive learning framework [21]\nWe’ll briefly touch on Rasa’s available features and discuss how they can be used to\nimprove the user’s experience with a chatbot:\nContext-based conversations\nThe Rasa framework allows users to capture and utilize the conversation context\nor dialog state. Internally, Rasa performs NLU and captures required slots and\ntheir values, which can be utilized in response generation.\nInteractive learning\nRasa offers an interactive interface that can be used for two purposes. One is to\ncreate more training data for the internal models by chatting with the bot. The\nsecond is to provide feedback when the models make mistakes. This feedback\ncan be used as negative samples for the model to improve performance in chal‐\nlenging cases.\nData annotation\nRasa presents a highly interactive and easy-to-use interface to annotate more data\nto improve the model training. Data annotation can be done from scratch or\nmodified from examples where labels are already predicted by the existing\n228 \n| \nChapter 6: Chatbots\n",
      "word_count": 258,
      "char_count": 1634,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 727,
          "ext": "png",
          "size_bytes": 241369
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 259,
      "text": "models. See Figure 6-25 for an example of the data annotation step in Rasa.\nWrapper frameworks are built on Rasa NLU, which eases the data annotation\nprocess to generate large-scale dialog datasets. Once such framework is Chatette\n[22], which is a tool that accepts templates and then spawns dialog instances\nusing those templates at scale.\nFigure 6-25. Data annotation and API integrations\nAPI integration\nFinally, the dialog service can also be integrated with other APIs as well as chat\nplatforms like Slack, Facebook, Google Home, and Amazon Alexa. The next sec‐\ntion includes a case study where we’ll produce recipe recommendations via con‐\nversations and integrate a faceted search API endpoint into the bot to facilitate\nthe recommendation process.\nRasa NLU \n| \n229\n",
      "word_count": 123,
      "char_count": 773,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 1937,
          "ext": "png",
          "size_bytes": 328255
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 260,
      "text": "Customize your models in Rasa\nApart from the framework, Rasa also allows us to customize our models by\nchoosing from a pool of models. For example, for intent/dialog act detection, we\ncan choose “sklearn classifier” [23] or “mitie classifier” [24], or we can write our\nown classifier and add that to the building pipeline for Rasa to use it. Various\noptions for embeddings such as spaCy and Rasa’s own are available with the\nframework.\nWe can also harness the power of transformer models as we see performance\nimprovement while building our individual components. Rasa provides BERT\n(and various distilled versions to improve on latency) for both classification and\nsequence labeling tasks [25, 26]. Overall, this makes Rasa a very powerful tool for\nbuilding a dialog system from scratch.\nRasa enables us to build our chatbot in a modular way. For exam‐\nple, we can start with existing pre-trained models and later use cus‐\ntom models built on our specific datasets as needed. Similarly, we\ncan start default API integrations and conversation channels and\nmodify them when needed.\nNow, let’s go over a complete case study with a real scenario discussing what steps are\nnecessary to create a conversational system from scratch in an industrial setup,\nincluding data setup, model building, and deployment.\nA Case Study: Recipe Recommendations\nCooks often look for specific recipes tailored to their culinary and dietary preferen‐\nces. A conversational interface where cooks can find their recipe of choice by fleshing\nout their preferences via a conversation with the agent would be a good user experi‐\nence. In this case study, we’ll discuss all the components we’ve covered in this chapter\nalong with the frameworks required to build them. We’ll see the evolving need for\ndata and modeling complexity of the business problem and address them via various\ntools we’ve learned about in this chapter.\nImagine we’re part of a recipe and food aggregator site. We’ve been tasked with build‐\ning a chatbot. Users can talk about the kind of food they’re craving or want to cook.\nThis is an uncharted problem, so how will we go about building this? Figure 6-26\nshows some example suggestions of recipes for various user preferences.\nWe need to convert this business problem into a technical problem with objectives\nand constraints. As a user will interact with the system, our goal is to create a fully\ndefined query that can fetch a suitable recipe. The recipe can come from an API end‐\npoint or a generative model. This query is made of a set of attributes that define the\ndish, such as ingredients, cuisine, calorie level, cooking time, etc. We also know that\n230 \n| \nChapter 6: Chatbots\n",
      "word_count": 447,
      "char_count": 2681,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 261,
      "text": "users can reveal their preferences through turns in a conversation, so we need to track\ntheir preferences and update the internal dialog state as the conversation proceeds.\nFigure 6-26. Example of a recipe-suggestion site: Allrecipes.com\nUtilizing Existing Frameworks\nWe’ll start with Dialogflow, the cloud API we described earlier in the chapter since it’s\neasy to build. Before we start, we need to define entities like we did before, such as\ningredients, cuisine, calorie level, cooking time. We can build an ontology for the\ncooking domain and identify the number of slots we’d like our chatbot to support.\nA Case Study: Recipe Recommendations \n| \n231\n",
      "word_count": 104,
      "char_count": 656,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 960,
          "height": 996,
          "ext": "png",
          "size_bytes": 929715
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 262,
      "text": "Initially, it will be good to keep an exhaustive list of these entities. Here are some\nexamples of training instances that capture nuances in this early phase of bot\nbuilding:\n• I want a low calorie dessert that is vegan.\n• I have peas, carrots, and chicken in my kitchen. What can I make with it in 30\nminutes?\nDialogflow is capable of handling the user’s preference and identifying the slots and\nvalues necessary to look for a correct recipe. Also, due to the conversational nature of\nthe user’s interaction, the bot will maintain its dialog state or context to fully under‐\nstand the user’s input. We’ll assume a database of recipes has been pre-defined and\nprefilled. Now, once the entities are captured via the bot, we need to feed them into an\nAPI endpoint. This endpoint will do a faceted search on the database and retrieve the\nbest-ranked recipes.\nAs we collect more data, Dialogflow will slowly become better. But due to its lack of\ncustom models, it can’t solve more complex conversations related to this task. Some\nexamples where a Dialogflow-based bot will eventually fail are:\n• I have a chicken with me, what can I cook with it besides chicken lasagna?\n• Give me a recipe for a chocolate dessert that can be made in just 10 mins instead\nof the regular half an hour.\nThese examples show a presence of more than one value for one slot, and only one of\nthem is correct—for example, “10 mins” is correct, while “half an hour” isn’t.\nMatching-based methods in Dialogflow will fail in such cases. That’s why we need to\nbuild custom models so that these examples can be added as adversarial examples in\ntheir training pipeline. In a Rasa pipeline with custom models, we can add such\nadversarial examples in order for the model to learn to identify correct slots and their\nvalues. It’s also possible to generate such adversarial examples from the data we’ve\ngathered using data augmentation techniques and including them though the data\nannotation techniques of the Rasa framework, as shown in Figure 6-27.\nWith this updated training data, new custom models will be able to pick the correct\nvalues to fully describe the user’s ask for the recipe. Once slots and values are cap‐\ntured, the rest of the process will be similar to how it was before (i.e., an API end‐\npoint can use this information to query an appropriate recipe).\n232 \n| \nChapter 6: Chatbots\n",
      "word_count": 414,
      "char_count": 2364,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 263,
      "text": "Figure 6-27. How Rasa can facilitate complex annotations\nOpen-Ended Generative Chatbots\nOur solution is good enough to be deployed on a real website where millions of users\ninteract regularly. Now we can focus on solving more challenging tasks with the\nobjective of improving the user experience even more. So far, we’ve been providing\nusers with specific recipes that are stored in a datastore beforehand. What if we want\nto make the chatbot more open ended by generating recipes instead of searching for\nthem from a pre-existing pool? The advantage of such systems is their ability to han‐\ndle unknown attribute values and customize recipes to fit the personalized tastes of\nthe users.\nOpen-ended chatbots are generally harder to evaluate because\nmany variants of a response can be correct given the context.\nHuman evaluation seems to be most efficient, but it’s irreproduci‐\nble and therefore harder to compare to other systems. A mix of\nautomatic and human evaluation is the right way to evaluate gener‐\native dialog systems.\nHere, we can utilize powerful seq2seq generative models that can condition their gen‐\neration on the various desired attributes the user has described for the recipe prefer‐\nence. Researchers (including one of the authors) have shown [27] that these seq2seq\nmodels are capable of generating personalized recipes based on preferences and\nA Case Study: Recipe Recommendations \n| \n233\n",
      "word_count": 225,
      "char_count": 1412,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        },
        {
          "index": 1,
          "width": 1442,
          "height": 901,
          "ext": "png",
          "size_bytes": 206589
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 264,
      "text": "previous recipe interactions. These models are capable of incorporating nuances and\npotentially generating a novel recipe that’s valid but unique to the user’s culinary\ntaste. Figure 6-28 shows such an example of a newly generated recipe incorporating a\nuser’s preference. The user’s preference can be just a list of recipes that they’ve inter‐\nacted with before. For example, in this figure, the user had previously interacted with\nmojito, martini, and Bloody Mary. The personalized model added an extra garnishing\nstep (highlighted in gray) to make it more personalized.\nFigure 6-28. Recipes generated personalized to user’s preferences [27]\nMerging such generative models with other dialog components can really boost the\nuser experience. While we’ve discussed one specific recipe-recommendation problem,\nsimilar approaches can be taken in developing similar applications. We’ve discussed\nnecessary tools and models that can be used together to build a bot according to the\nbusiness problem at hand. We started with a very simple approach using Dialogflow\nand gradually added more complexity to tackle dialog nuances in the way users may\nexpress their queries and choices. Finally, we went the extra mile to build an end-to-\nend personalized chatbot.\nWrapping Up\nIn this chapter, we discussed chatbots and their applicability in various domains. We\nwent through a pipeline approach and delved deep into its various components. We\ntalked about a complete flow-based bot with a cloud-based API, then implemented\nML components of NLU modules. Finally, we analyzed a business problem and pro‐\nvided some pathways to approach it incrementally.\nBut as far as dialog systems and chatbots are concerned, there are many challenges\nthat are still unsolved. Hence, this is a very active area of research in the NLP com‐\nmunity. In addition to academic research, industrial research groups are also looking\nfor scalable solutions to existing approaches so that chatbots can be built reliably and\n234 \n| \nChapter 6: Chatbots\n",
      "word_count": 312,
      "char_count": 2015,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 568,
          "ext": "png",
          "size_bytes": 274162
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 265,
      "text": "deployed to users. Still today, many industrial chatbots fail to be robust and suffer in\nthe issue of natural language understanding and natural language generation. We\nmention these challenges in order to provide a broader picture of the domain of\nchatbots.\nThe major problem right now in building dialog systems is a lack of datasets that\nreflect natural conversations. Many times, personal data can’t be collected for privacy\nreasons. Other times, a lack of such conversational interfaces hinders data collection\ncapability. Also, existing datasets, especially ones that claim to be real-world datasets,\nlack naturalness. These datasets are created mainly by online annotators, and most of\nthe time, they sound scripted due to the nature of objective data collection. This\nproblem is very different from other NLP tasks. For instance, annotating a correct\nclass to a datapoint in a classification task or pointing out the relevant information in\nan information extraction task is more objective and easy to get than labels via\ncrowd-sourced online annotators. In the case of dialog, many times the task is subjec‐\ntive hence the data collection process becomes complex.\nFurthermore, the current generative models are not capable enough to generate fac‐\ntually correct statements, which becomes a critical problem in the case of chatbots. In\nthe short span of a conversation, factually incorrect generation may hinder the qual‐\nity of the conversation. Hence, future research and industrial efforts should be toward\nboth gathering better representative datasets and improving both natural language\nunderstanding and generation models that can be used in a chatbot pipeline.\nIn summary, we discussed the foundations of dialog systems, starting with an overall\npipeline, and developed a dialog system using Dialogflow, a cloud API; dove deep\ninto building custom models for understanding dialog context; and finally, used all of\nthem to solve a case study. While we anticipate that the area will continue evolving\nand improving, this chapter will be a good start for you to adapt to the new solutions\nthat keep coming. Now let’s turn to a few other common NLP problem scenarios in\nthe next chapter.\nReferences\n[1] ParlAI. Last accessed June 15, 2020.\n[2] Wallace, Michal and George Dunlop. Eliza, The Rogerian Therapist. Last accessed\nJune 15, 2020.\n[3] Amazon. “Build a Machine Learning Model”. Last accessed June 15, 2020.\n[4] Miller, Alexander H., Will Feng, Adam Fisch, Jiasen Lu, Dhruv Batra, Antoine\nBordes, Devi Parikh, and Jason Weston. “ParlAI: A Dialog Research Software Plat‐\nform.” Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations (2017): 79–84.\nWrapping Up \n| \n235\n",
      "word_count": 427,
      "char_count": 2739,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 266,
      "text": "[5] Pratap, Vineel, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Syn‐\nnaeve, Vitaliy Liptchinsky, and Ronan Collobert. “wav2letter++: The Fastest Open-\nsource Speech Recognition System”, (2018).\n[6] Google Cloud. “Cloud Text-to-Speech”. Last accessed June 15, 2020.\n[7] van den Oord, Aäron and Dieleman, Sander. “WaveNet: A Generative Model for\nRaw Audio”, DeepMind (blog), September 8, 2016.\n[8] Dialogflow. Last accessed June 15, 2020.\n[9] Dialogflow login page. Last accessed June 15, 2020.\n[10] Google Cloud. Dialogflow V2 API. Last accessed June 15, 2020.\n[11] Mrkšić, Nikola, Diarmuid O. Séaghdha, Tsung-Hsien Wen, Blaise Thomson, and\nSteve Young. “Neural Belief Tracker: Data-Driven Dialogue State Tracking.” Proceed‐\nings of the 55th Annual Meeting of the Association for Computational Linguistics 1\n(2016): 1777–1788.\n[12] Team HG-Memex. “sklearn-crfsuite: scikit-learn inspired API for CRFsuite”.\nLast accessed June 15, 2020.\n[13] Hemphill, Charles T., John J. Godfrey, and George R. Doddington. “The ATIS\nSpoken Language Systems Pilot Corpus.” Speech and Natural Language: Proceedings of\na Workshop Held at Hidden Valley, Pennsylvania, June 24–27, 1990.\n[14] Coucke, Alice, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier,\nDavid Leroy, Clément Doumouro et al. “Snips Voice Platform: an embedded Spoken\nLanguage Understanding system for private-by-design voice interfaces”, (2018).\n[15] Williams, Jason, Antoine Raux, and Matthew Henderson. “The Dialog State\nTracking Challenge Series: A Review.” Dialogue & Discourse 7.3 (2016): 4–33.\n[16] Budzianowski, Paweł, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva,\nStefan Ultes, Osman Ramadan, and Milica Gašić. “MultiWOZ - A Large-Scale Multi-\nDomain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling”, (2018).\n[17] Serban, Iulian Vlad, Ryan Lowe, Peter Henderson, Laurent Charlin, and Joelle\nPineau. “A Survey of Available Corpora for Building Data-Driven Dialogue Systems”,\n(2015).\n[18] Vinyals, Oriol and Quoc Le. “A Neural Conversational Model”, (2015).\n[19] Li, Jiwei, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Juraf‐\nsky. “Deep Reinforcement Learning for Dialogue Generation”, (2016).\n[20] Weston, Jason E. “Dialog-Based Language Learning.” Proceedings of the 30th\nInternational Conference on Neural Information Processing Systems (2016): 829–837.\n[21] Rasa. Last accessed June 15, 2020.\n236 \n| \nChapter 6: Chatbots\n",
      "word_count": 340,
      "char_count": 2433,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 267,
      "text": "[22] SimGus. Chatette: A powerful dataset generator for Rasa NLU, inspired by Cha‐\ntito, (GitHub repo). Last accessed June 15, 2020.\n[23] scikit-learn. “Classifier comparison.” Last accessed June 15, 2020.\n[24] MIT-NLP. MITIE: library and tools for information extraction, (GitHub repo).\nLast accessed June 15, 2020.\n[25] Sucik, Sam. “Compressing BERT for faster prediction”. Rasa (blog), August 8,\n2019.\n[26] Ganesh, Prakhar, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Deming\nChen, Marianne Winslett, Hassan Sajjad, and Preslav Nakov. “Compressing Large-\nScale Transformer-Based Models: A Case Study on BERT”, (2020).\n[27] Majumder, Bodhisattwa Prasad, Shuyang Li, Jianmo Ni, and Julian McAuley.\n“Generating Personalized Recipes from Historical User Preferences”, (2019).\nWrapping Up \n| \n237\n",
      "word_count": 114,
      "char_count": 798,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 268,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 269,
      "text": "CHAPTER 7\nTopics in Brief\nThe problems are solved, not by giving new information,\nbut by arranging what we have known since long.\n—Ludwig Wittgenstein, Philosophical Investigations\nSo far in Part II of this book, we’ve discussed a few common application scenarios of\nNLP: text classification, information extraction, and chatbots (Chapters 4 through 6).\nWhile these are the most common use cases for NLP we’re likely to encounter in\nindustry projects, there are many other NLP tasks that are relevant in building real-\nworld applications involving large collections of documents. We’ll take a quick look at\nsome of these topics in this chapter. Let’s first start with a few largely unrelated sce‐\nnarios you may encounter in your workplace projects. We’ll discuss them in more\ndetail throughout the chapter.\nIf someone asks us to find out what NLP is and we have no idea, where do we start?\nIn the pre-internet era, we would’ve hit the nearest library to do some research. How‐\never, now the first place we’d go is to a search engine. Search involves a lot of human-\ncomputer interaction using natural language, so it gives rise to very interesting use\ncases for NLP.\nOur client is a big law firm. When a new case comes up, they sometimes have to\nresearch lots and lots of documents related to the case to get a bigger picture of what\nit’s about. Many times, there isn’t enough time for a thorough manual review. Our cli‐\nent wants us to develop software that can provide a quick overview of the topics dis‐\ncussed in large document collections. Topic modeling is a technique that’s used to\naddress this problem of finding latent topics in a large collection of documents.\nThe same client’s firm has another problem: case report documents they receive are\nusually quite long, and it’s difficult even for an experienced lawyer to get the gist\nquickly. So our client wants a solution to automatically create summaries of text\n239\n",
      "word_count": 330,
      "char_count": 1928,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (9.3pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 270,
      "text": "documents. Text summarization approaches are used to address such use cases in the\nindustry.\nMany of us read news online every day. A common feature of many news websites is\nthe “related articles” feature, which shows articles that are topically related to the arti‐\ncle we’re reading. Consider a related scenario where we’re shown jobs related to a\ngiven job based on the profile descriptions. Recommendation methods using NLP are\nkey to building solutions for such use cases.\nWe live in an increasingly multicultural world, and many organizations have clients\nor customers across the globe. This results in the need to translate documents (at\nscale) in all supported languages in the organization. Machine translation (MT) is use‐\nful in such scenarios. Streaming services like Amazon, Netflix, and YouTube use MT\nextensively for generating subtitles in various languages. Tools like Google Translate\nhelp tourists across the globe communicate in local languages.\nWe use search engines for many reasons in day-to-day life. Sometimes, we want to\nknow answers to questions. Try asking a factual question such as, “Who wrote Ani‐\nmal Farm?” to your favorite search engine. Google shows “George Orwell” as its top\nresult along with some biographical details about him, followed by other regular\nsearch results. Try asking a somewhat descriptive question, say, “How do I calm down\na crying baby?” Among the answers, you’ll also see a blurb from some website that\nlists a number of ways to calm a baby. This is an example of question answering, where\nthe task is to locate the most appropriate answer to the user query instead of showing\na collection of documents. Note that this is slightly different from the FAQ chatbot we\nsaw in Chapter 6, where the scope of answers lies within a much smaller dataset (i.e.,\nFAQs) instead of a large collection of documents (such as the web).\nThese are the topics we’ll discuss in this chapter. While they may seem very different\nfrom one another, we’ll see the similarities between them as we progress through the\nchapter. This collection is not an exhaustive list, but these are some common scenar‐\nios encountered when developing NLP-based solutions for industrial applications.\nThe first four tasks (search, topic modeling, text summarization, and recommenda‐\ntion) are more common in real-world applied NLP scenarios, so we’ll discuss them in\ngreater detail than the other two. With large-scale question answering and machine\ntranslation, you’re unlikely to encounter a scenario where you’d have to develop solu‐\ntions from scratch, so we’ll only introduce them so you’ll know where to get started to\nbuild an MVP quickly. Table 7-1 summarizes the topics we’ll cover in this chapter,\nalong with example usage scenarios and the kind of data they work on.\n240 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 461,
      "char_count": 2828,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 271,
      "text": "Table 7-1. List of the topics covered in this chapter\nNLP task\nUse\nNature of data\nSearch\nFind relevant content for a given user query.\nWorld wide web/large collection of\ndocuments\nTopic modeling\nFind topics and hidden patterns in a set of documents.\nLarge collection of documents\nText summarization\nCreate a shorter version of the text with the most\nimportant content\nTypically a single document\nRecommendations\nShowing related articles\nLarge collection of documents\nMachine translation\nTranslate from one language to another\nA single document\nQuestion answering\nsystem\nGet answers to queries directly instead of a set of\ndocuments.\nA single document or a large\ncollection of documents\nWith this overview, let us start introducing these topics one by one in a little bit more\ndetail. Our first topic is search and information retrieval.\nSearch and Information Retrieval\nA search engine is an important component of everyone’s online activity. We search\nfor information to decide on the best items to purchase, nice places to eat out, and\nbusinesses to frequent, just to name a few examples. We also rely heavily on search to\nsift through our emails, documents, and financial transactions. A lot of these search\ninteractions happen through text (or speech converted to text in voice input). This\nmeans that a lot of language processing happens inside a search engine. Thus, we can\nsay that NLP plays an important role in modern search engines.\nLet’s start with a quick look into what happens when we search. When a user searches\nusing a query, the search engine collects a ranked list of documents that matches the\nquery. For this to happen, an “index” of documents and vocabulary used in them\nshould be constructed first and is then used to search and rank results. One popular\nform of indexing textual data and ranking search results for search engines is some‐\nthing we studied in Chapter 3: TF-IDF. Recent developments in DL models for NLP\ncan also be used for this purpose. For example, Google recently started ranking\nsearch results and showing search snippets using the BERT model. They claim that\nthis has improved the quality and relevance of their search results [1]. This is an\nimportant example of NLP’s usefulness in a modern-day search engine.\nApart from this major function of storing data and ranking search results, several fea‐\ntures in a modern search engine involve NLP. For example, consider the screenshot of\na Google search result shown in Figure 7-1, which illustrates some features that use\nNLP.\nSearch and Information Retrieval \n| \n241\n",
      "word_count": 422,
      "char_count": 2561,
      "fonts": [
        "MyriadPro-Cond (9.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 272,
      "text": "Figure 7-1. Screenshot of a Google search query\n1. Spelling correction: The user entered an incorrect spelling, and the search engine\noffered a suggestion showing the correct spelling.\n2. Related queries: The “People also ask” feature shows other related questions peo‐\nple ask about Marie Curie.\n3. Snippet extraction: All the search results show a text snippet involving the query.\n4. Biographical information extraction: On the right-hand side, there’s a small snip‐\npet showing Marie Curie’s biographical details along with some specific informa‐\ntion extracted from text. There are also some quotes and a list of people related to\nher in some way.\n5. Search results classification: On top, there are categories of search results: all,\nnews, images, videos, and so on.\nHere, we see a range of concepts we’ve learned about in this book being put to use.\nWhile these are by no means the only places NLP is used in search engines, they are\nexamples of where NLP is useful in the user interface aspect of search. However,\nthere’s much more to search than NLP, and building a search engine seems like a mas‐\nsive endeavor requiring a lot of infrastructure. This may make one wonder: when do\nyou need to build a search engine, and how? Do we always build search engines as\nmassive as Google? Let’s take a look at two scenarios to answer these questions.\n242 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 237,
      "char_count": 1387,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 892,
          "height": 763,
          "ext": "png",
          "size_bytes": 460617
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 273,
      "text": "Imagine we work for a company like Broad Reader. Our company wants to develop a\nsearch engine that crawls forums and discussion boards from all over the web and lets\nusers query this large collection. Consider another scenario: let’s say our client is a law\nfirm where loads of legal documents from clients and other legal sources are\nuploaded every day. We’re asked to develop a custom search engine for the client to\nsearch through their database. How are these two scenarios different?\nThe first scenario requires that we build what we call a generic search engine, where\nwe would have to set up a way to scrape different websites, keep looking for new con‐\ntent and new websites, and constantly build and update our “index.” The second sce‐\nnario is an example of an enterprise search engine, as we don’t have to scout for\ncontent to index. Thus, these two types of search engines are distinguished as follows:\n• Generic search engines, such as Google and Bing, that crawl the web and aim to\ncover as much as possible by constantly looking for new webpages\n• Enterprise search engines, where our search space is restricted to a smaller set of\nalready existing documents within an organization\nIn our experience, the second form of search is the most common use case you may\nencounter at your workplace, so we’ll only briefly introduce a generic search engine\nby discussing a few basic components that are also relevant to enterprise search.\nComponents of a Search Engine\nHow does a search engine work? What are some of the basic components? We briefly\nintroduce them through Figure 7-2, taken from the now-famous 1998 research paper\non the architecture of Google [2].\nSearch and Information Retrieval \n| \n243\n",
      "word_count": 293,
      "char_count": 1713,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 274,
      "text": "Figure 7-2. Early architecture of the Google search engine [2]\nThere are several small and large components inside a search engine, as shown in the\nfigure. The three major components that can be considered its building blocks (and a\nfourth component that is now also common) are:\nCrawler\nCollects all the content for the search engine. The crawler’s job is to traverse the\nweb following a bunch of seed URLs and build its collection of URLs through\nthem in a breadth-first way. It visits each URL, saves a copy of the document,\ndetects the outgoing hyperlinks, then adds them to the list of URLs to be visited\nnext. Typical decisions that need to be made when designing a crawler include\nidentifying what to crawl, when to stop crawling, when to re-crawl, what to re-\ncrawl, and how to make sure we don’t crawl duplicate content. From our experi‐\nence, even when you have to develop some sort of generic search engine (say, a\nblog search engine), you’re unlikely to encounter a scenario where you should\ndesign your own crawler. Production-ready crawlers, such as Apache Nutch [3]\nand Scrapy [4], can be customized and used for your project in such scenarios.\n244 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 207,
      "char_count": 1195,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1091,
          "height": 1114,
          "ext": "png",
          "size_bytes": 130861
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 275,
      "text": "Indexer\nParses and stores the content that the crawler collects and builds an “index” so it\ncan be searched and retrieved efficiently. While it’s possible to index videos,\naudio, images, etc., text indexing is the most common type of indexing in real-\nworld projects. Data structures for a search engine index are developed keeping\nin mind the need for fast and efficient search of its crawl in response to a user\nquery. An example of a popular indexing algorithm used in web search engines is\nan “inverted index,” which stores the list of documents associated with each word\nin its vocabulary. As with crawlers, you’re unlikely to encounter a situation where\nyou have to develop your own indexer. Software like Apache Solr [5] and Elastic‐\nsearch [6] are typically used in the industry to build an index and search over it.\nSearcher\nSearches the index and ranks the search results for the user query based on the\nrelevance of the results to the query. A typical search query on Google or Bing\nwill likely yield hundreds and thousands of results. As users, we can’t go through\nthem manually to decide whether the result is relevant to our query. This is\nwhere a ranking of search results becomes important. An intuitive approach to\nranking based on what we’ve seen so far in this book is to obtain a vector repre‐\nsentation of the result document and user query and rank the documents based\non some measure of similarity. In fact, as we mentioned at the start of the chap‐\nter, TF-IDF, which we saw in detail in Chapter 3 and used for text classification\nin Chapter 4, is one of the popular methods of searching and ranking search\nresults.\nFeedback\nA fourth component, which is now common in all search engines, that tracks and\nanalyzes user interactions with the search engine, such as click-throughs, time\nspent on searching and on each clicked result, etc., and uses it for continuous\nimprovement of the search system.\nWe hope this short discussion gave a quick glance into what a typical search engine is\nmade up of. Information retrieval is a major research area in itself, and search engine\ndevelopment is a massive undertaking involving a lot of computation and infrastruc‐\nture. All the topics discussed above are not completely solved problems yet. In this\nsection, we only provided an overview of how a search engine works in order to lead\ninto a discussion on where NLP comes into the picture and how to develop custom\nsearch engines. Interested readers can refer to [7] for a detailed discussion on the\nalgorithms and data structures behind search engine development.\nWith this introduction, let’s move on to what a typical search engine pipeline looks\nlike in the use cases you may encounter in your workplace and what NLP methods\nwe’ve learned so far can be put to use in this pipeline.\nSearch and Information Retrieval \n| \n245\n",
      "word_count": 490,
      "char_count": 2842,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 276,
      "text": "A Typical Enterprise Search Pipeline\nSay we work for a large newspaper and are tasked with developing a search engine for\nits website. We already mentioned that Solr and ElasticSearch are typically used for\naddressing such scenarios. How will we use them? Let’s do a step-by-step walk‐\nthrough and also discuss which NLP tools we’ll need in this process.\nCrawling/content acquisition\nWe don’t really need a crawler in this case, as we don’t need data from external\nwebsites. What we need is a way to read data from the location where all the news\narticles are stored (e.g., in a local database or in some cloud location).\nText normalization\nOnce we collect the content, depending on its format, we start by first extracting\nthe main text and discarding additional information (e.g., newspaper headers).\nIt’s also common to do some pre-processing steps, such as tokenizing, lowercas‐\ning, stop word removal, stemming, etc., before vectorizing.\nIndexing\nFor indexing, we have to vectorize the text. TF-IDF is a popular scheme for this,\nas we discussed earlier. However, like Google, we can also use BERT instead. How\ndo we use BERT for search? We can use BERT to get a vector representation of\nthe query and documents and generate a ranked list of closest documents for a\ngiven query in terms of vector distance. [8] shows how we can use such text\nembeddings to index and search using Elasticsearch.\nIn addition to indexing the entire content of an article, we can also add additional\nfields/facets to the index for each document and later search by these facets. For\nexample, for a newspaper, this can be the news category, other tags like the state\ninvolved (e.g., California for a news article about something in the USA), and so on.\nText classification approaches we saw in Chapter 4 can be used to get such categories\nand tags, if necessary. At the time of displaying the search results, we can combine\nthis with filters like date to enrich the user experience. We’ll see an example of such a\nfaceted search in Chapter 9.\nSo, let’s assume we’ve built our search engine following the above process. What next?\nWhat happens when the user types a query? At this point, the pipeline typically con‐\nsists of the following steps:\n1. Query processing and execution: The search query is passed through the text nor‐\nmalization process as above. Once the query is framed, it’s executed, and results\nare retrieved and ranked according to some notion of relevance. Search engine\nlibraries like Elasticsearch even provide custom scoring functions to modify the\nranking of documents retrieved for a given query [9].\n246 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 446,
      "char_count": 2641,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 277,
      "text": "2. Feedback and ranking: To evaluate search results and make them more relevant to\nthe user, user behavior is recorded and analyzed, and signals such as click action\non result and time spent on a result page are used to improve the ranking algo‐\nrithm. An example in our newspaper’s case could be to learn the reader’s prefer‐\nence (e.g., the reader prefers reading local news from Region X) and show them a\npersonalized ranking of suggested articles.\nWe hope this newspaper use case shows what a typical enterprise search engine devel‐\nopment pipeline looks like. Like with many software applications, recent develop‐\nments in the field of machine learning have also influenced enterprise search. We\nbriefly mentioned how BERT and other such embedding-based text representations\ncan be used with Elasticsearch. Amazon Kendra [10], an enterprise search engine\npowered by machine learning, is a recent addition to this space.\nSetting Up a Search Engine: An Example\nNow that we have an idea of the components of a search engine and how they work\ntogether in an example scenario, let’s take a quick look at building a small search\nengine using Elasticsearch’s Python API. We’ll use the CMU Book Summaries dataset\n[11], which consists of plot summaries of over 16,000 books extracted from Wikipe‐\ndia pages. We’ll illustrate the process using 500 documents, but the notebook associ‐\nated with this section (Ch7/ElasticSearch.ipynb) can be used to build a search engine\nwith the full dataset. We already have our content in place, so we don’t need a crawler.\nTaking a simple use case that doesn’t involve additional pre-processing (no stemming,\nfor example), the following code snippet shows how to build an index using Elastic‐\nsearch:\n#Build an index from booksummaries dataset, using only 500 documents.\npath = \"../booksummaries/booksummaries.txt\"\ncount = 1\nfor line in open(path):\n    fields = line.split(\"\\t\")\n    doc = {'id' : fields[0],\n          'title': fields[2],\n          'author': fields[3],\n          'summary': fields[6]\n          }\n      #Index is called myindex\n    res = es.index(index=\"myindex\", id=fields[0], body=doc)\n    count = count+1\n    if count%100 == 0:\n          print(\"indexed 100 documents\")\n    if count == 501:\n          break\nres = es.search(index=\"myindex\", body={\"query\": {\"match_all\": {}}})\nprint(\"Your index has %d entries\" % res['hits']['total']['value'])\nSearch and Information Retrieval \n| \n247\n",
      "word_count": 361,
      "char_count": 2431,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 278,
      "text": "This code builds an index with four fields per document—id, title, author, and sum\nmary—which are all available in the dataset itself. Once the index is built, it runs a\nquery to check the size of the index. In this case, the output will show as 500 entries.\nOnce the index is built, we have to figure out how to use it to perform search. While\nwe won’t go into the user interface design aspects of the search process, the following\ncode snippet illustrates how to search with Elasticsearch:\n#match query works as a OR query when the query string has multiple words\n#match_phrase looks for exact matches. So using that here.\nwhile True:\n    query = input(\"Enter your search query: \")\n    if query == \"STOP\":\n        break\n    res = es.search(index=\"myindex\", body={\"query\": {\"match_phrase\":\n                                            {\"summary\": query}}})\n    print(\"Your search returned %d results:\"\n                     %res['hits']['total']['value'])\n    for hit in res[\"hits\"][\"hits\"]:\n          print(hit[\"_source\"][\"title\"])\n          #to get a snippet 100 characters before and after the match\n          loc = hit[\"_source\"][\"summary\"].lower().index(query)\n          print(hit[\"_source\"][\"summary\"][:100])\n          print(hit[\"_source\"][\"summary\"][loc-100:loc+100])\nThis snippet keeps asking the user to enter a search query until the word STOP is\ntyped and shows the search results, along with a short snippet containing the search\nphrase. For example, if the user searches for the word “countess,” the results look as\nfollows:\nEnter your search query: countess\nYour search returned 7 results:\nAll's Well That Ends Well\n71\n Helena, the orphan daughter of a famous physician, is the ward of the Countess\n of Rousillon, and ho\n…\n…\n…\nEnter your search query: STOP\nElasticsearch has many features to alter the scoring function, to change the search\nprocess in terms of query formulation (e.g., exact match versus fuzzy match), to add\npre-processing steps like stemming during the indexing process, and so on. We leave\nthem as further exercises for the reader. Now, let’s look at a case study of building an\nenterprise search engine from scratch and improving it.\n248 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 320,
      "char_count": 2203,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 279,
      "text": "A Case Study: Book Store Search\nImagine a scenario where we have a new e-commerce store focused on books and we\nhave to build its search pipeline. We have metadata like author, title, and summary.\nThe search functionality we saw earlier can serve as the baseline at the start. We can\nset up our own search engine backend or use online services like Elasticsearch [12] or\nElastic on Azure [13].\nThis default search output might have a bunch of issues. For instance, it may show\nthe results with exact query matches in title or summary to be higher than more rele‐\nvant results that aren’t an exact match. Some of the exact matches might be poorly\nwritten books with bad reviews, which we’re not accounting for in our search rank‐\ning. For example, consider these two books on Marie Curie: Marie Curie Biography\nand The Life of Marie Curie. The latter is an authoritative biography on Marie Curie,\nwhile the former is a new and poorly reviewed book. But while querying for “marie\ncurie biography,” the less-relevant book, Marie Curie Biography, is ranked higher than\nthe popular The Life of Marie Curie.\nWe can incorporate real-world metrics that account for this into our search engine.\nFor instance, the number of times a book is viewed and sold, the number of reviews,\nand the book’s rating can all be incorporated into the search ranking function. In\nElasticsearch, this can be done by using function scoring and giving a manually\nselected weightage to number of ratings, number of books sold, and average rating.\nSo, we might want to give more weightage to books sold than the number of times it\nwas viewed. These heuristics will provide more relevant results as more books get\nsold and reviewed. This method of manually defining search relevance weights can be\na good starting point when there’s no data or the data is limited.\nWe should start collecting user interactions with the search engine to improve it fur‐\nther. These interactions can include the search query, the kind of user, and their\nactions on the books. When recording such granular search information, various pat‐\nterns can be found—for example, when searching for “science books for children,”\nscientists’ biographies get purchased at a higher rate even when they’re ranked lower.\nOver time, with increasing amounts of data, we can learn relevance ranking from\nthese logs. We can use a tool like Elasticsearch Learning to Rank [14] to learn this\ninformation and improve search relevance. Over time, more advanced techniques\nlike neural embeddings can also be incorporated into search query analysis [15].\nAs more user information is collected, search results can also be personalized based\non the user’s past preferences. Generally, such systems are built as a layer over the ini‐\ntial ranking retrieved from the search engine.\nAnother point to consider in this journey of building an advanced search engine is\nhow important it is to keep complete control of the system and data. If such a search\nengine is not a core part of your offering and the organization is comfortable with\nSearch and Information Retrieval \n| \n249\n",
      "word_count": 521,
      "char_count": 3095,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 280,
      "text": "data sharing, many of these features also come as a managed service. These managed\nsearch engine services include Algolia [16] and Swiftype [17].\nSince the implementation of a search engine involves many other factors beyond NLP\nand is typically reserved for larger-sized datasets, we’re not showing a running exam‐\nple covering all the aspects of a search engine in this book. However, we hope this\nshort introduction gave you an overview of how to get started developing custom\nsearch engines involving textual data and of where the NLP techniques you learned\nso far may play a role. For more details on implementing search engines with Elastic‐\nsearch, refer to [18]. Now, let’s move on to the second topic of this chapter: topic\nmodeling.\nTopic Modeling\nTopic modeling is one of the most common applications of NLP in industrial use\ncases. For analyzing different forms of text from news articles to tweets, from visual‐\nizing word clouds (see Chapter 8) to creating graphs of connected topics and docu‐\nments, topic models are useful for a range of use cases. Topic models are used\nextensively for document clustering and organizing large collections of text data.\nThey’re also useful for text classification.\nBut what is topic modeling? Say we’re given a large collection of documents, and we’re\nasked to “make sense” out of it. What will we do? Clearly, the task is not well defined.\nGiven the large volume of documents, going through each of them manually is not an\noption. One way to approach it is to bring out some words that best describe the cor‐\npus, like the most common words in the corpus. This is called a word cloud. The key\nto a good word cloud is to remove stop words. If we take any English text corpus and\nlist out the most frequent k words, we won’t get any meaningful insights, as the most\nfrequent words will be stop words (the, is, are, am, etc.). After doing appropriate pre-\nprocessing, the word cloud may yield some meaningful insights depending on the\ndocument collection.\nAnother approach is to break the documents into words and phrases, then group\nthese words and phrases together based on some notion of similarity between them.\nThe resulting groups of words and phrases can then be used to build some under‐\nstanding of what the corpus is about. Intuitively, if we pick one word from each\ngroup, then the set of selected words represents (in a semantic sense) what the corpus\nis about. Another possibility is to use TF-IDF (See Chapter 3). Consider a corpus of\ndocuments wherein some documents are on farming. Then, terms like “farm,”\n“crops,” “wheat,” and “agriculture” should form the “topics” in the documents on\nfarming. What’s the easiest way to find these terms that occur frequently in a docu‐\nment but do not occur much in other documents in the corpus?\n250 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 487,
      "char_count": 2833,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 281,
      "text": "Topic modeling operationalizes this intuition. It tries to identify the “key” words\n(called “topics”) present in a text corpus without prior knowledge about it, unlike the\nrule-based text mining approaches that use regular expressions or dictionary-based\nkeyword searching techniques. Figure 7-3 shows a visualization of a topic model’s\nresults for a humanities corpus.\nFigure 7-3. Illustration a of topic modeling visualization [19].\nIn this figure, we see a collection of keywords for individual humanities disciplines—\nand also how some keywords overlap between disciplines—obtained through a topic\nmodel. This is an example of how we can use a topic model to discover what the top‐\nics in a large corpus are about. It has to be noted that there’s no single topic model.\nTopic modeling generally refers to a collection of unsupervised statistical learning\nmethods to discover latent topics in a large collection of text documents. Some of the\npopular topic modeling algorithms are latent Dirichlet allocation (LDA), latent\nsemantic analysis (LSA), and probabilistic latent semantic analysis (PLSA). In prac‐\ntice, the technique that’s most commonly used is LDA.\nTopic Modeling \n| \n251\n",
      "word_count": 182,
      "char_count": 1188,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 682,
          "height": 596,
          "ext": "png",
          "size_bytes": 513447
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 282,
      "text": "What does LDA do? Let’s start with a toy corpus [20]. Say we have a collection of\ndocuments, D1 to D5, and each document consists of a single sentence:\n• D1: I like to eat broccoli and bananas.\n• D2: I ate a banana and salad for breakfast.\n• D3: Puppies and kittens are cute.\n• D4: My sister adopted a kitten yesterday.\n• D5: Look at this cute hamster munching on a piece of broccoli.\nLearning a topic model on this collection using LDA may produce an output like this:\n• Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching\n• Topic B: 20% puppies, 20% kittens, 20% cute, 15% hamster\n• Document 1 and 2: 100% Topic A\n• Document 3 and 4: 100% Topic B\n• Document 5: 60% Topic A, 40% Topic B\nThus, topics are nothing but a mixture of keywords with a probability distribution,\nand documents are made up of a mixture of topics, again with a probability distribu‐\ntion. A topic model only gives a collection of keywords per topic. What exactly the\ntopic represents and what it should be named is typically left to human interpretation\nin an LDA model. Here, we might look at Topic A and say, “it is about food.” Likewise,\nfor topic B, we might say, “it is about pets.”\nHow does LDA achieve this? LDA assumes that the documents under consideration\nare produced from a mixture of topics. It further assumes the following process gen‐\nerates these documents: at the start, we have a list of topics with a probability distri‐\nbution. For every topic, there’s an associated list of words with a probability\ndistribution. We sample k topics from topic distribution. For each of the k topics\nselected, we sample words from the corresponding distribution. This is how each\ndocument in the collection is generated.\nNow, given a set of documents, LDA tries to backtrack the generation process and fig‐\nure out what topics would generate these documents in the first place. The topics are\ncalled “latent” because they’re hidden and must be discovered. How does LDA do this\nbacktracking? It does so by factorizing a document-term matrix (M) that keeps count\nof words across all documents. It has all the m documents D1, D2, D3 … Dm arranged\nalong the rows and all the n words W1,W2, ..,Wn in the corpus vocabulary arranged as\ncolumns. M[i,j] is the frequency count of word Wj in Document Di. Figure 7-4 shows\none such matrix for a hypothetical corpus consisting of five documents, with a\nvocabulary of six words.\n252 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 439,
      "char_count": 2441,
      "fonts": [
        "MinionPro-It (6.3pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Regular (6.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 283,
      "text": "Figure 7-4. Document–term matrix (M)\nNote that if each word in the vocabulary represents a unique dimension and the total\nvocabulary is of size n, then the ith row of this matrix is a vector that represents the ith\ndocument in this n-dimensional space. LDA factorizes M into two submatrices: M1\nand M2. M1 is a document–topics matrix and M2 is a topic–terms matrix, with\ndimensions (M, K) and (K, N), respectively. With four topics (K1–K4), the submatri‐\nces for M may look like the ones shown in Figure 7-5. Here, k is the number of topics\nwe’re interested in finding.\nFigure 7-5. Factorized matrices\nk, the number of topics, is a hyperparameter. The optimal value for\nk is found by trial and error.\nTopic Modeling \n| \n253\n",
      "word_count": 128,
      "char_count": 724,
      "fonts": [
        "MinionPro-It (9.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Regular (6.3pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        },
        {
          "index": 1,
          "width": 1117,
          "height": 385,
          "ext": "png",
          "size_bytes": 16107
        },
        {
          "index": 2,
          "width": 1117,
          "height": 758,
          "ext": "png",
          "size_bytes": 27224
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 284,
      "text": "These submatrices can then be used to understand the topic structure of a document\nand the keywords a topic is made up of. Now that we have some idea of what happens\nbehind the scenes when we train a topic model, let’s look at how to build one.\nTraining a Topic Model: An Example\nWe’ve seen the intuition behind LDA. How do we build one ourselves? Here, we’ll use\nan LDA implementation from the Python library gensim [21] and the CMU Book\nSummary Dataset [11] we used earlier for demonstrating how to create a search\nengine. The notebook associated with this section (Ch5/TopicModeling.ipynb) con‐\ntains more details. The following code snippet shows how to train a topic model\nusing LDA:\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom gensim.models import LdaModel\nfrom gensim.corpora import Dictionary\nfrom pprint import pprint\n#tokenize, remove stopwords, non-alphabetic words, lowercase\ndef preprocess(textstring):\n   stops =  set(stopwords.words('english'))\n   tokens = word_tokenize(textstring)\n   return [token.lower() for token in tokens if token.isalpha() \n          and token not in stops]\ndata_path = \"/PATH/booksummaries/booksummaries.txt\"\nsummaries = []\nfor line in open(data_path, encoding=\"utf-8\"):\n   temp = line.split(\"\\t\")\n   summaries.append(preprocess(temp[6]))\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(summaries)\n# Filter infrequent or too frequent words.\ndictionary.filter_extremes(no_below=10, no_above=0.5)\ncorpus = [dictionary.doc2bow(summary) for summary in summaries]\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n# Train the topic model\nmodel = LdaModel(corpus=corpus, id2word=id2word,iterations=400, num_topics=10)\ntop_topics = list(model.top_topics(corpus))\npprint(top_topics)\nIf we visually inspect the topics, one of them shows words such as police, case, mur‐\ndered, killed, death, body, etc. While topics themselves will not get names in a topic\nmodel, in looking at the keywords, we may infer that this relates to the topic of crime/\nthriller novels.\n254 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 301,
      "char_count": 2173,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 285,
      "text": "How do you evaluate the results? Given the topic–term matrix for LDA, we sort each\ntopic from highest to lowest term weights and then select the first n terms for each\ntopic. We then measure the coherence for terms in each topic, which essentially meas‐\nures how similar these words are to one another. Additionally, in this example, we\nmade a few choices for the model parameters, such as number of iterations, number\nof topics, and so on, and did not do any fine-tuning. The notebook associated with\nthis section (Ch7/TopicModeling.ipynb) shows how to evaluate the coherence of topic\nmodels.\nAs with any real-world project, we need to experiment with different parameters and\ntopic models before choosing a final model to deploy. Gensim’s tutorial on LDA [22]\nprovides more information on how to build, tune, and evaluate a topic model.\nRemoving words with low frequency or keeping only those words\nthat are nouns and verbs are some ways of improving a topic\nmodel. If the corpus is big, divide it into batches of fixed sizes and\nrun topic modeling for each batch. The best output comes from the\nintersection of topics from each batch.\nWhat’s Next?\nNow that we know how to build a topic model, how exactly can we use it? In our\nexperience, some of the use cases for topic models are:\n• Summarizing documents, tweets, etc., in the form of keywords based on learned\ntopic distributions\n• Detecting social media trends over a period of time\n• Designing recommender systems for text\nAlso, the distribution of topics for a given document can be used as a feature vector\nfor text classification.\nAlthough there is clearly a range of use cases for topic models in industry projects,\nthere are a few challenges associated with their use. The evaluation and interpretation\nof topic models is still challenging, and there’s no consensus on it yet. Parameter tun‐\ning for topic models can also take a lot of time. In the above example, we provided\nthe number of topics manually. As mentioned previously, there’s no straightforward\nprocedure to know the number of topics; we explore with multiple values based on\nour estimates about the topics in the dataset. Another thing to keep in mind is that\nmodels like LDA typically work only with long documents and perform poorly on\nshort documents, such as a corpus of tweets.\nDespite all these challenges, topic models are an important tool in any NLP engineer’s\ntoolbox, and they have a wider reach in terms of where they can be used. We hope we\nTopic Modeling \n| \n255\n",
      "word_count": 432,
      "char_count": 2505,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 286,
      "text": "gave you enough information to help you identify its suitable use cases at your work‐\nplace. An interested reader can start at [23] to delve deeper into this topic. Let’s move\non to the next topic of this chapter: text summarization.\nText Summarization\nText summarization refers to the task of creating a summary of a longer piece of text.\nThe goal of this task is to create a coherent summary that captures the key ideas in the\ntext. It’s useful to do a quick read of large documents, store only relevant information,\nand facilitate better retrieval of information. NLP research on the problem of auto‐\nmatic text summarization was actively pursued by different research groups around\nthe world starting in the early 2000s as a part of the Document Understanding Con‐\nference [24] series. This series of conferences held competitions to solve several sub‐\ntasks within the larger realm of text summarization. Some of them are listed below:\nExtractive versus abstractive summarization\nExtractive summarization refers to selecting important sentences from a piece of\ntext and showing them together as a summary. Abstractive summarization refers\nto the task of generating an abstract of the text; i.e., instead of picking sentences\nfrom within the text, a new summary is generated.\nQuery-focused versus query-independent summarization\nQuery-focused summarization refers to creating the summary of the text\ndepending on the user query, whereas query-independent summarization creates\na general summary.\nSingle-document versus multi-document summarization\nAs the names indicate, single-document summarization is the task of creating a\nsummary from a single document, whereas multi-document summarization cre‐\nates a summary from a collection of documents.\nWe’ll look at some use cases to help you understand how these can be applied to\nactual tasks.\nSummarization Use Cases\nIn our experience, the most common use case for text summarization is a single-\ndocument, query-independent, extractive summarization. This is typically used to\ncreate short summaries of longer documents for human readers or a machine (e.g., in\na search engine to index summaries instead of full texts). A well-known example of\nsuch a summarizer in action in a real-world product is the autotldr bot on Reddit\n[25], a screenshot of which is shown in Figure 7-6. The autotldr bot summarizes long\nReddit posts by selecting and ranking the most important sentences in the post.\n256 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 389,
      "char_count": 2480,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 287,
      "text": "Figure 7-6. Screenshot of Reddit’s autotldr bot\nTwo other use cases one of the authors implemented in their past workplaces are:\n• An automatic sentence highlighter for news articles that colors “summary” sen‐\ntences (i.e., sentences that capture the gist of the text) instead of creating full-\nlength summaries.\n• A text summarizer to index only the summaries of documents instead of the full\ncontent, with the goal of reducing the size of a search engine’s index.\nYou may encounter similar scenarios for implementing a text summarizer at your\nworkplace. Let’s look at an example of how we can leverage existing libraries to imple‐\nment a single-document, query-independent, extractive summarizer.\nSetting Up a Summarizer: An Example\nResearch in this area has explored rule-based, supervised, and unsupervised\napproaches and, more recently, DL-based architectures. However, popular extractive\nsummarization algorithms used in real-world scenarios use a graph-based sentence-\nranking approach. Each sentence in a document is given a score based on its relation\nto other sentences in the text, and this is captured differently in different algorithms.\nThe Top N sentences are then returned as a summary. Sumy [26] is a Python library\nthat contains implementations of several popular query-independent, extractive sum‐\nmarization algorithms. The code snippet below shows an example of how to use\nText Summarization \n| \n257\n",
      "word_count": 216,
      "char_count": 1421,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1204,
          "height": 738,
          "ext": "png",
          "size_bytes": 108774
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 288,
      "text": "sumy’s implementation of a popular summarization algorithm, TextRank [27], to\nsummarize a Wikipedia page:\nfrom sumy.parsers.html import HtmlParser\nfrom sumy.nlp.tokenizers import Tokenizer\nfrom sumy.summarizers.text_rank import TextRankSummarizer\nurl = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\nparser = HtmlParser.from_url(url, Tokenizer(\"english\"))\nsummarizer = TextRankSummarizer()\nfor sentence in summarizer(parser.document, 5):\n    print(sentence)\nThis library takes care of HTML parsing and tokenization for the given URL, then\nuses TextRank to choose the most important sentences as the summary of the text.\nRunning this code shows the five most important sentences in the Wikipedia page on\nautomatic summarization.\nSumy is not the only library with such implementations of summarization algo‐\nrithms. Another popular library is gensim, which implements an improvised version\nof TextRank [28]. The following code snippet shows how to use gensim’s summarizer\nto summarize a given text:\nfrom gensim.summarization import summarize\ntext = \"some text you want to summarize\"\nprint(summarize(text))\nNote that, unlike sumy, gensim does not come with an HTML parser, so we’ll have to\nincorporate an HTML parsing step if we want to parse web pages. Gensim’s summa‐\nrizer also allows us to experiment with the length of the summaries. We’ll leave the\nexploration of other summarization algorithms in sumy and further investigation of\ngensim as exercises for the reader.\nSo, now we know how to implement a summarizer in our projects. However, there\nare a few things to keep in mind when using these libraries to deploy a working sum‐\nmarizer. Let’s take a look at some of them based on our experiences with building\nsummarizers for various application scenarios.\nPractical Advice\nIf you encounter a scenario where you have to deploy a summarizer as a product fea‐\nture, there are a few things to keep in mind. It’s very likely that you’ll use one of the\noff-the-shelf summarizers like in the example above rather than implementing your\nown summarizer from scratch. However, if existing algorithms don’t suit your project\nscenario or if they perform poorly, you may have to develop your own summarizer. A\nmore common reason to work on your own summarizer is if you’re in an R&D orga‐\nnization, working toward pushing the state of the art in summarization systems. So,\nassuming you’re using off-the-shelf summarizers, how do you compare the multiple\n258 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 375,
      "char_count": 2492,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 289,
      "text": "summarization algorithms available and choose the one that works best for your use\ncase?\nIn research, summarization approaches are evaluated using a common dataset of ref‐\nerence summaries created by humans. Recall-Oriented Understudy for Gisting Evalu‐\nation (ROUGE) [29] is a common set of metrics based on n-gram overlaps used for\nevaluating automatic summarization systems. However, such datasets may or may not\nsuit your exact use case. Hence, the best way to compare different approaches is to\ncreate your own evaluation set or ask human annotators to rate the summaries pro‐\nduced by different algorithms in terms of coherence, accuracy of the summary, etc.\nThere are a few practical issues to keep in mind when deploying a summarizer:\n• Pre-processing steps like sentence splitting (or HTML parsing in the above\nexample) play a very important role in what comes out as output summary. Most\nlibraries have built-in sentence splitters, but even those can do erroneous sen‐\ntence splitting for different input data (e.g., what if there’s a news article with a\nletter quoted in the middle?). To our knowledge, there’s no one-stop solution for\nsuch issues, and you may need to develop custom solutions for the data formats\nyou encounter in your project.\n• Most summarization algorithms are sensitive to the size of the text given as\ninput. For example, TextRank runs in polynomial time, so it can easily take up a\nlot of computing time to generate summaries for larger pieces of text. You need\nto be aware of this limitation when using a summarizer with very large texts. A\nworkaround could be to run the summarizer on partitions of the large text and\nstringing the summaries together. Another alternative could be to run the sum‐\nmarizer on the top M% and bottom N% of the text instead of the whole text\n(assuming that these parts contain the gist of a long document).\nSummarizers are sensitive to text length. So, it may make sense to\nrun a summarizer on selected parts of the text.\nSo far, we’ve only seen examples of extractive summarization. In comparison, abstrac‐\ntive summarization is more of a research topic than a practical application. Three\ninteresting use cases that come up frequently in abstractive summarization research\nare: news headline generation, news summary generation, and question answering.\nDeep learning and reinforcement learning approaches have shown some promising\nresults for abstractive summarization in the recent past [30]. Because this topic has so\nfar been primarily a research bastion and is restricted to academics and organizations\nwith dedicated AI teams, we won’t discuss it in further detail in this book. However,\nText Summarization \n| \n259\n",
      "word_count": 437,
      "char_count": 2688,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 290,
      "text": "we hope this discussion gave you enough of an overview about summarization to get\nstarted with an MVP in case you need one. Now, let’s take a look at another interest‐\ning problem where NLP is useful: offering recommendations for textual data.\nRecommender Systems for Textual Data\nWe’re all familiar with seeing related searches, related news articles, related jobs,\nrelated products, and other such features on the various websites we browse in our\nday-to-day lives, and it’s not unusual for clients to request them. How do these\n“related texts” features work?\nNews articles, job descriptions, product descriptions, and search queries all contain a\nlot of text. Hence, textual content and the similarities or relatedness between different\ntexts is important to consider when developing recommender systems for textual\ndata. A common approach to building recommendation systems is a method called\ncollaborative filtering. It shows recommendations to users based on their past history\nand on what users with similar profiles preferred in the past. For example, Netflix\nrecommendations use this type of approach at a large scale.\nIn contrast, there are content-based recommendation systems. An example of one\nsuch recommendation is the “related articles” feature on newspaper websites. Look at\nan example from CBC, a Canadian news website, shown in Figure 7-7.\nBelow the article text, we see a collection of related stories that are topically similar to\nthe source article, which is titled “How Desmond Cole wrote a bestselling book about\nbeing black in Canada.” As you can see, the related stories cover black history and\nracism in Canada and list another article about Desmond Cole. How do we build\nsuch a feature based on content similarity among texts? One approach to building\nsuch a content-based recommendation system is to use a topic model like we saw ear‐\nlier in this chapter. Texts similar to the current text in terms of topic distribution can\nbe shown as “related” texts. However, the advent of neural text representations has\nchanged the ways we can show such recommendations. Let’s take a look at how we\ncan use a neural text representation to show related text recommendations.\n260 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 359,
      "char_count": 2228,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 291,
      "text": "Figure 7-7. Screenshot showing the related stories feature on cbc.ca [31]\nCreating a Book Recommender System: An Example\nWe’ve seen a few examples of neural network–based text representations (Chapter 3)\nand how some of them can be useful for text classification (Chapter 4). One of the\nrepresentations we saw was Doc2vec. The following code snippet shows how to use\nDoc2vec for serving related book recommendations using the CMU Book Summary\nDataset we used earlier in this chapter for topic modeling and the Python libraries\nNLTK (for tokenization) and gensim (for Doc2vec implementation):\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n# Read the dataset’s README to understand the data format.\ndata_path = \"/DATASET_FOLDER_PATH/booksummaries.txt\"\nmydata = {} #titles-summaries dictionary object\nfor line in open(data_path, encoding=\"utf-8\"):\n    temp = line.split(\"\\t\")\n    mydata[temp[2]] = temp[6]\n# Prepare the data for doc2vec, build and save a doc2vec model.\nd2vtrain = [TaggedDocument((word_tokenize(mydata[t])), tags=[t]) \n                          for t in mydata.keys()]\nmodel = Doc2Vec(vector_size=50, alpha=0.025, min_count=10, dm =1, epochs=100)\nRecommender Systems for Textual Data \n| \n261\n",
      "word_count": 166,
      "char_count": 1258,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 624,
          "height": 517,
          "ext": "png",
          "size_bytes": 50740
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 292,
      "text": "model.build_vocab(train_doc2vec)\nmodel.train(train_doc2vec, total_examples=model.corpus_count, \n  epochs=model.epochs)\nmodel.save(\"d2v.model\")\n# Use the model to look for similar texts.\nmodel= Doc2Vec.load(\"d2v.model\")\n# This is a sentence from the summary of “Animal Farm” on Wikipedia:\n# https://en.wikipedia.org/wiki/Animal_Farm\nsample = \"\"\"\nNapoleon enacts changes to the governance structure of the farm, replacing\nmeetings with a committee of pigs who will run the farm.\n \"\"\"\nnew_vector = model.infer_vector(word_tokenize(sample))\nsims = model.docvecs.most_similar([new_vector]) #gives 10 most similar titles\nprint(sims)\nThis prints the output as:\n[('Animal Farm', 0.6960548758506775), (\"Snowball's Chance\", 0.6280543208122253), \n('Ponni', 0.583295464515686), ('Tros of Samothrace', 0.5764356255531311), \n('Payback: Debt and the Shadow Side of Wealth', 0.5714253783226013),\n('Settlers in Canada', 0.5685930848121643), ('Stone Tables', \n0.5614138245582581), ('For a New Liberty: The Libertarian Manifesto', \n0.5510331988334656), ('The God Boy', 0.5497804284095764), \n('Snuff', 0.5480046272277832)]\nNote that we just tokenized the text in this example and did not do any other pre-\nprocessing, nor did we do any model tuning. This is just an example of how we can\napproach the development of a recommendation system, not a detailed analysis.\nMore recent approaches to implementing such systems use BERT or other such mod‐\nels to calculate document similarity. We also briefly mentioned text similarity–based\nsearch options in Elasticsearch earlier in this section; that’s another option for imple‐\nmenting a recommender system for our use case. We’ll leave exploring them further\nas an exercise for the reader.\nNow that we have an idea of how to build a recommendation system for textual data,\nlet’s take a look at some practical advice for building such recommendation systems\nbased on our past experiences.\nPractical Advice\nWe just saw a simple example of a textual recommendation system. This kind of\napproach will work for some use cases, such as recommending related news articles.\nHowever, we may have to consider aspects beyond text in many applications where\nwe need to provide more personalized recommendations or where other non-textual\naspects of the item need to be considered. An example of such a case is similar listing\nrecommendations in Airbnb, where they combine embedding-based neural text\n262 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 337,
      "char_count": 2448,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 293,
      "text": "representations with other information, such as location, price, etc., to provide per‐\nsonalized recommendations [32].\nHow do we know our recommendation system is working? In a real-world project,\nthe impact of recommendations can be measured by performance indicators, such as\nuser click-through rates, conversion into a purchase (if relevant), customer engage‐\nment on the website, etc. A/B tests where different groups of users are exposed to dif‐\nferent recommendations are used to compare these performance indicators. A third\n(and perhaps more time consuming) way is to conduct carefully designed user stud‐\nies where participants are shown specific recommendations and asked to rate them.\nFinally, if we have a small test set with appropriate recommendations for a given item,\nwe can evaluate a recommendation system by comparing it to this test set. In our\nexperience, a combination of these indicators, along with an analytics platform like\nGoogle Analytics, is used in evaluating industry-scale recommendation systems.\nLast but not least, our pre-processing decisions play a significant role in the recom‐\nmendations served by our system. So, we need to know what we want before going\nahead with an approach. In the example above, we just did plain tokenization. In the\nreal world, it’s not uncommon to see lowercasing, removal of special characters, etc.,\nas parts of the pre-processing pipeline.\nThis concludes our overview of text recommendation systems. We hope this provides\nyou enough information to identify suitable use cases at your workplace and build\nrecommendation systems for them. Let’s move to the next topic of this chapter:\nmachine translation.\nMachine Translation\nMachine translation (MT)—translating text from one language to another automati‐\ncally—is one of the original problems of NLP research. Early MT systems employed\nrule-based approaches that required a lot of linguistic knowledge, including the\ngrammars of source and target languages, to be explicitly coded along with resources\nlike dictionaries between languages. This was followed by several years of research\nand application development using statistical methods that relied on the existence of\nlots and lots of parallel data between languages. Such datasets were usually collected\nfrom resources where texts were translated into multiple languages, such as European\nparliamentary proceedings. The past five years have seen explosive growth in DL-\nbased neural MT approaches, which have become the state of the art in both research\nand production-scale MT systems. Google Translate is a popular example. However,\nowing to the amount of data and resources required to build them, research and\ndevelopment of such systems has been primarily the bastion of large organizations.\nMachine Translation \n| \n263\n",
      "word_count": 423,
      "char_count": 2798,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 294,
      "text": "Clearly, MT is a large research area, and building MT systems seems like a large\neffort. Where is MT useful in the industry? Here are two example scenarios where\nMT may be required to develop solutions:\n• Our client’s products are used by people around the world who leave reviews on\nsocial media in multiple languages. Our client wants to know the general senti‐\nment of those reviews. For this, instead of looking for sentiment analysis tools in\nmultiple languages, one option is to use an MT system, translate all the reviews\ninto one language, and run sentiment analysis for that language.\n• We work with a lot of social media data (e.g., tweets) on a regular basis and\nnotice that it’s unlike the kind of text we encounter in typical text documents. For\nexample, consider the sentence, “am gud,” which, in formal, well-formed English\nis, “I am good.” (More details on how social media text differs from normal, well-\nformed text are in Chapter 8.) MT can be used to map these two sentences by\ntreating the conversion from “am gud” to “I am good” as an informal-to-\ngrammatical English translation problem.\nWhile we may or may not develop our own MT systems, there are many scenarios\nwhere we may need to implement an MT solution in our NLP projects. [33] discusses\nsome of the industry use cases of MT. So what should we do, then, if we face a similar\nsituation? Let’s look at an example of how to set up an MT system in our project.\nUsing a Machine Translation API: An Example\nBuilding an MT system from scratch is a time- and resource-consuming exercise. A\nmore common way to set up an MT system for a project is to use one of the pay-per-\nuse translation services APIs provided by large research organizations such as Google\nor Microsoft, which are powered by state-of-the-art neural MT models. The follow‐\ning code snippet shows how to use the Bing Translate API [34] (after obtaining the\nsubscription key and the endpoint URL by registering) to translate from English to\nGerman:\nimport os, requests, uuid, json\nsubscription_key = \"XXXXX\"\nendpoint = \"YYYYY\"\npath = '/translate?api-version=3.0'\nparams = '&to=de' #From English to German (de)\nconstructed_url = endpoint + path + params\nheaders = {\n    'Ocp-Apim-Subscription-Key': subscription_key,\n    'Content-type': 'application/json',\n    'X-ClientTraceId': str(uuid.uuid4())\n}\n264 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 397,
      "char_count": 2374,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 295,
      "text": "body = [{'text' : 'How good is Machine Translation?'}]\nrequest = requests.post(constructed_url, headers=headers, json=body)\nresponse = request.json()\nprint(json.dumps(response, sort_keys=True, indent=4, separators=(',', ': ')))\nThis example requests a translation of the sentence “How good is Machine Transla‐\ntion?” from English to German. The output in JSON format is shown below:\n[\n    {\n    \"detectedLanguage\": {\n          \"language\": \"en\",\n          \"score\": 1.0\n    },\n    \"translations\": [\n          {\n               \"text\": \"Wie gut ist maschinelle Übersetzung?\",\n               \"to\": \"de\"\n          }\n    ]\n    }\n]\nThis shows the translated sentence in German as “Wie gut ist maschinelle Überset‐\nzung?” We can use the service as we need it by calling the Bing Translate API. Similar\nsetups exist for other providers of such services. Before concluding this topic, let’s\ntake a look at some practical advice for readers who want to incorporate MT into an\nNLP project.\nPractical Advice\nFirst, as we explained earlier, don’t build your own MT system if you don’t have to. It’s\nmore practical to make use of translation APIs. When using such APIs, it’s important\nto pay close attention to pricing policies. Considering the costs involved, it might be a\ngood idea to store the translations of frequently used text (called a translation mem‐\nory or a translation cache).\nMaintain a translation memory, which can be used for translations\nthat repeat frequently.\nWhen working with an entirely new language, or, say, a new domain where existing\ntranslation APIs do poorly, it makes sense to start with a domain knowledge–based,\nrule-based translation system addressing the restricted scenario we’re dealing with.\nAnother approach to addressing such data-scarce scenarios is to augment our\nMachine Translation \n| \n265\n",
      "word_count": 266,
      "char_count": 1818,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 296,
      "text": "training data by doing “back translation.” Let’s say we want to translate from English\nto the Navajo language. English is a popular language for MT, while Navajo is not, but\nwe do have a few examples of English–Navajo translation. In such a case, we can\nbuild an MT model between Navajo and English, then use this system to translate a\nfew Navajo sentences into English. At this point, these machine-translated Navajo–\nEnglish pairs can be added as additional training data to the English–Navajo MT sys‐\ntem. This results in a translation system with more examples to train on (even though\nsome of these examples are synthetic). In general, though, if accuracy of translation is\nparamount, it might make sense to form a hybrid MT system that combines the neu‐\nral models with rules and some form of post-processing.\nData augmentation is a useful approach to collect more training\ndata for building an MT system.\nMT is a large area of research with dedicated annual conferences, journals, and data-\ndriven competitions where academics and industry groups involved in MT research\ncompete and evaluate their systems. We’ve only scratched the surface to give you\nsome idea about the topic. A collection of learning materials on MT [35] are available\nfor readers interested in further study. With this overview of MT, let’s move on to the\nnext topic of this chapter: question-answering systems.\nQuestion-Answering Systems\nWhen searching online with a search engine such as Google or Bing, for some of the\nqueries, we see “answers” along with a bunch of search results. These answers can be\na few words or a listing or definition. In Chapter 5, we saw some examples of one\nsuch query to illustrate named entity recognition’s role in search. Let’s now go a little\nbit farther than that. Consider the screenshot in Figure 7-8 from Google search for\nthe query “who invented penicillin.”\n266 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 321,
      "char_count": 1913,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 297,
      "text": "Figure 7-8. Screenshot for the query “who invented penicillin”\nHere, the search engine performs an additional task of question answering along with\ninformation retrieval. If we follow the search engine pipeline described earlier with\nthe aim of answering such a question, the processing steps look like the ones shown\nin Figure 7-9.\nClearly, NLP plays an important role in understanding the user query, deciding what\nkind of question it is and what kind of answer is needed, and identifying where the\nanswers are in a given document after retrieving documents relevant to the query.\nWhile this is an example of a large, generic search engine, we may also encounter sce‐\nnarios where we have to implement a question-answering system for internal con‐\nsumption, using a company’s data or some other custom setting. Following the\npipeline approach mentioned earlier in “Search and Information Retrieval” on page\n241 can lead us toward a solution in such cases.\nThere may be other relatively simpler scenarios of question answering in the work‐\nplace, too. A common scenario is an FAQ-answering system. We saw how this works\nin Chapter 6. Let’s briefly discuss one more scenario, based on one of the author’s past\nexperiences at their workplace.\nQuestion-Answering Systems \n| \n267\n",
      "word_count": 206,
      "char_count": 1277,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1156,
          "height": 692,
          "ext": "png",
          "size_bytes": 94176
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 298,
      "text": "Figure 7-9. Answer extraction\nDeveloping a Custom Question-Answering System\nLet’s say we’re asked to develop a question-answering system that answers all user\nquestions about computers. We’ve identified a few websites with question-and-\nanswer discussions (e.g., Stack Overflow) and have a crawler in place. At this point,\nhow can we get started with the first version of the question-answering system? One\nway to build an MVP is to start looking at the markup structure of the websites. Gen‐\nerally, the questions and answers are distinguished using different HTML elements.\nCollecting this information and using it specifically to build an index of question–\nanswer pairs will get us started on a question-answering system for this task. The next\nstep could be using text embeddings and performing a similarity-based search using\nElasticsearch.\nLooking for Deeper Answers\nIn the approaches described above, we would still expect the user question to have a\nsignificant amount of exact overlap with the indexed question and answer. However,\nDL-based text embeddings, which we’ve seen in different chapters throughout this\n268 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 178,
      "char_count": 1158,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 986,
          "height": 1163,
          "ext": "png",
          "size_bytes": 60675
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 299,
      "text": "book so far, are capable of going beyond exact matches and capturing semantic simi‐\nlarities. Such a neural question-answering approach looks for the answer span in a\ntext by comparing the question’s embedding with that of the text’s subunits (words,\nsentences, and paragraphs). Question answering using deep neural networks is very\nmuch an active area of research and is typically studied as a supervised ML problem\nusing specific datasets designed for this task, such as the SQuAD [36] dataset.\nDeepQA, which is a part of Allen NLP [37], is a popular library for developing exper‐\nimental question-answering systems using DL architectures.\nAnother approach to question answering is knowledge-based question answering,\nwhich relies on the presence of a huge knowledge database and a way to map user\nqueries to the database. This is typically used for answering short, factual questions.\nReal-world question-answering systems like IBM Watson, which beat human partici‐\npants in the popular quiz show Jeopardy!, use a combination of both approaches. Bing\nAnswer Search API [38], which allows subscribed users to query the system for\nanswers, is an example of a research system that follows such a hybrid approach.\nDeveloping any such question-answering system that can model deeper knowledge at\nweb scale requires a substantial amount of data and computing resources coupled\nwith a lot of experimentation. It’s not yet a common scenario in a typical software\ncompany working on NLP projects, so we won’t discuss it further in this book. To get\na historical overview of question answering along with the most recent developments\nin research, we recommend reading Chapter 25 of the upcoming edition of the popu‐\nlar NLP textbook, Speech and Language Processing [39]. If you want to implement a\nDL-based question-answering system for your own dataset (e.g., internal documents\nin an organization), libraries such as CDQA-Suite [40] provide the backbone to get\nstarted.\nAs can be seen from this discussion, question answering is an area of search that has a\nwide-ranging array of solutions, ranging from simple and straightforward approaches\nlike extracting markup, to complex, DL-based solutions. We hope this overview pro‐\nvided you with enough examples of the use cases you may encounter in your work‐\nplace to develop question-answering systems.\nWrapping Up\nIn this chapter, we saw how NLP plays a role in a range of problem scenarios, starting\nfrom search engines to question answering. We saw how some of the topics we\nlearned earlier in the book can be used to address these problems. While these topics\nseem disparate at first glance, some of them are also related to one another—for\nexample, search, recommendation systems, and question answering are all some form\nof information retrieval. Even summarization can be treated as such, as we retrieve\nrelevant sentences from a given text. Additionally, all of them, except machine trans‐\nlation, typically do not require large, annotated datasets. Thus, we can see some\nWrapping Up \n| \n269\n",
      "word_count": 483,
      "char_count": 3042,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 300,
      "text": "similarities among these topics. Note that each of the topics we discussed are still\nactive research questions in NLP, and a lot of new developments happen every day, so\nthe treatment of topics in this chapter is not exhaustive. However, we hope this gave\nyou enough of an overview to get started should you encounter a related use case at\nwork.\nWith this, we’ve reached the end of the “Essentials” part of the book. In the next part,\nwe’ll take a look at how all these different topics come together in specific domains.\nReferences\n[1] Nayak, Pandu. “Understanding Searches Better than Ever Before”. The Keyword\n(blog), October 25, 2019.\n[2] Brin, Sergey and Lawrence Page. “The Anatomy of a Large-Scale Hypertextual\nWeb Search Engine.” Computer Networks and ISDN Systems 30.1–7 (1998): 107–117.\n[3] Apache Nutch. Last accessed June 15, 2020.\n[4] Scrapy, a fast and powerful scraping and web crawling framework. Last accessed\nJune 15, 2020.\n[5] Apache Solr, an open source search engine. Last accessed June 15, 2020.\n[6] Elasticsearch, an open source search engine. Last accessed June 15, 2020.\n[7] Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. Introduc‐\ntion to Information Retrieval. Cambridge: Cambridge University Press, 2008. ISBN:\n978-0-52186-571-5\n[8] Tibshirani, Julie. “Text similarity search with vector fields”. Elastic (blog), August\n27, 2019.\n[9] Elasticsearch. “Function score query” documentation. Last accessed June 15, 2020.\n[10] Amazon Kendra. Last accessed June 15, 2020.\n[11] Bamman, David and Noah Smith. “CMU Book Summary Dataset”, 2013.\n[12] Amazon Elasticsearch Service. Last accessed June 15, 2020.\n[13] Elastic on Azure. Last accessed June 15, 2020.\n[14] Elasticsearch. “Elasticsearch Learning to Rank: the documentation”. Last\naccessed June 15, 2020.\n[15] Mitra, Bhaskar and Nick Craswell. “An introduction to neural information\nretrieval.” Foundations and Trends in Information Retrieval 13.1 (2018): 1–126.\n[16] Search engine services by Algolia. Last accessed June 15, 2020.\n270 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 316,
      "char_count": 2061,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 301,
      "text": "[17] Search engine services by Swiftype, and Amazon Kendra. Last accessed June 15,\n2020.\n[18] Gormley, Clinton and Zachary Tong. Elasticsearch: The Definitive Guide. Boston:\nO’Reilly, 2015. ISBN: 978-1-44935-854-9\n[19] “EH Topic Modeling II”. Last accessed June 15, 2020.\n[20] Keshet, Joseph. “Latent Dirichlet Allocation”. Lecture from Advanced Techni‐\nques in Machine Learning (89654), Bar Ilan University, 2016.\n[21] RaRe Consulting. “Genism: topic modelling for humans”. Last accessed June 15,\n2020.\n[22] Gensim’s LDA tutorial. Last accessed June 15, 2020.\n[23] Topic modeling is a broad area, with entire books written on the topic, so we\nwon’t discuss how they work in this book. Interested readers can refer to the follow‐\ning article as a starting point: Blei, David M. “Probabilistic Topic Models.” Communi‐\ncations of the ACM 55.4 (2012): 77–84.\n[24] NIST. Document Understanding Conference series. Last accessed June 15, 2020.\n[25] Reddit. autotldr bot. Last accessed June 15, 2020.\n[26] Sumy, an automatic text summarizer. Last accessed June 15, 2020.\n[27] Mihalcea, Rada and Paul Tarau. “TextRank: Bringing Order into Text.” Proceed‐\nings of the 2004 Conference on Empirical Methods in Natural Language Processing\n(2004): 404–411.\n[28] Mortensen, Ólavur. “Text Summarization with Gensim”. RARE Technologies\n(blog), August 24, 2015.\n[29] Wikipedia. “ROUGE (metric)”. Last updated September 3, 2019.\n[30] Paulus, Romain, Caiming Xiong, and Richard Socher. “Your TLDR by an ai: a\nDeep Reinforced Model for Abstractive Summarization”. Salesforce Research (blog),\n2017.\n[31] Patrick, Ryan B. “How Desmond Cole Wrote a Bestselling Book about Being\nBlack in Canada”. CBC, February 27, 2020.\n[32] Grbovic, Mihajlo et al. “Listing Embeddings in Search Ranking”. Airbnb Engi‐\nneering & Data Science (blog), March 13, 2018.\n[33] Way, Andy. “Traditional and Emerging Use-Cases for Machine Translation.” Pro‐\nceedings of Translating and the Computer 35 (2013): 12.\n[34] Azure Cognitive Services. Translator Text API v3.0. Last accessed June 15, 2020.\n[35] Machine Translation courses. Last accessed June 15, 2020.\nWrapping Up \n| \n271\n",
      "word_count": 320,
      "char_count": 2134,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 302,
      "text": "[36] SQuAD2.0. “The Stanford Question Answering Dataset”. Last accessed June 15,\n2020.\n[37] Allen Institute for AI. AllenNLP. Last accessed June 15, 2020.\n[38] Microsoft. Project Answer Search API. Last accessed June 15, 2020.\n[39] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft). 2018.\n[40] CDQA-Suite, a library to help build a QA system for your dataset. Last accessed\nJune 15, 2020.\n272 \n| \nChapter 7: Topics in Brief\n",
      "word_count": 75,
      "char_count": 465,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 303,
      "text": "PART III\nApplied\n",
      "word_count": 3,
      "char_count": 17,
      "fonts": [
        "MyriadPro-SemiboldCond (28.4pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 304,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 305,
      "text": "CHAPTER 8\nSocial Media\nIn today’s world, we don’t need to speak English\nbecause we have social media.\n—Vir Das\nSocial media platforms (Twitter, Facebook, Instagram, WhatsApp, etc.) have revolu‐\ntionized the way we communicate with individuals, groups, communities, corpora‐\ntions, government agencies, media houses, etc. This, in turn, has changed established\nnorms and etiquette and the day-to-day practices of how businesses and government\nagencies carry out things like sales, marketing, public relations, and customer sup‐\nport. Given the huge volume and variety of data generated daily on social media plat‐\nforms, there’s a huge body of work focused on building intelligent systems to\nunderstand communication and interaction on these platforms. Since a large part of\nthis communication happens in text, NLP has a fundamental role to play in building\nsuch systems. In this chapter, we’ll focus on how NLP is useful for analyzing social\nmedia data and how to build such systems.\nTo give an idea of the volume of data that’s generated on these platforms [1, 2, 3],\nconsider the following numbers:\nVolume: 152 million monthly active users on Twitter; for Facebook, it’s 2.5 billion\nVelocity: 6,000 tweets/second; 57,000 Facebook posts/second\nVariety: Topic, language, style, script\nThe infographic shown in Figure 8-1 presents how much data is generated per\nminute across different platforms [4].\n275\n",
      "word_count": 217,
      "char_count": 1404,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (9.3pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 306,
      "text": "Figure 8-1. Data generated in one minute on various social platforms\nGiven these numbers, social platforms have to be the largest generators of unstruc‐\ntured natural language data. It’s not possible to manually analyze even a fraction of\nthis data. Since a lot of content is text, the only way forward is to design NLP-based\nintelligent systems that can work with social data and bring out insights. This is the\nfocus of this chapter. We’ll look at some important business applications, such as\ntopic detection, sentiment analysis, customer support, and fake news detection, to\nname a few. A large part of this chapter will be about how the text from social plat‐\nforms is different from other sources of data and how we can design subsystems to\nhandle these differences. Let’s begin by looking at some of the important applications\nthat use NLP to extract insights from social media data.\n276 \n| \nChapter 8: Social Media\n",
      "word_count": 156,
      "char_count": 923,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1361,
          "height": 1447,
          "ext": "png",
          "size_bytes": 390951
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 307,
      "text": "Applications\nThere’s a wide variety of NLP applications that use data from social platforms, includ‐\ning sentiment detection, customer support, and opinion mining, to name a few. This\nsection will briefly discuss some of the popular ones to give an idea of where we could\nbegin applying these applications for our own needs:\nTrending topic detection\nThis deals with identifying the topics that are currently getting the most traction\non social networks. Trending topics tell us what content people are attracted to\nand what they think is noteworthy. This information is of immense importance\nto media houses, retailers, first responders, government entities, and many more.\nIt helps them fine-tune their strategies of engaging with their users. Imagine the\ninsights it could bring when done at the level of specific geolocations.\nOpinion mining\nPeople often use social media to express their opinions about a product, service,\nor policy. Gathering this information and making sense of it is of great value to\nbrands and organizations. It’s impossible to go through thousands of tweets and\nposts manually to understand the larger opinion of the masses. In such scenarios,\nbeing able to summarize thousands of social posts and extract the key insights is\nhighly valuable.\nSentiment detection\nSentiment analysis of social media data has to be by far the most popular applica‐\ntion of NLP on social data. Brands rely extensively on using signals from social\nmedia to better understand their users’ sentiments about their products and serv‐\nices and that of their competitors. They use it to better understand their custom‐\ners, from using sentiment to identify the cohorts of customers they should\nengage with to understanding the shift in the sentiment of its customer base over\na long duration of time.\nRumor/fake news detection\nGiven their fast and far reach, social networks are also misused to spread false\nnews. In the past few years, there have been instances where social networks were\nused to sway the opinion of masses using false propaganda. There is a lot of work\ngoing on toward understanding and identifying fake news and rumors. This is\npart of both preventive and corrective measures to control this menace.\nAdult content filtering\nSocial media also suffers from people using social networks to spread inappropri‐\nate content. NLP is used extensively to identify and filter out inappropriate con‐\ntent, such as nudity, profanity, racism, threats, etc.\nApplications \n| \n277\n",
      "word_count": 398,
      "char_count": 2485,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 308,
      "text": "Customer support\nOwing to the widespread use of social media and its public visibility, customer\nsupport on social media has evolved into a must-have for every brand across the\nglobe. Users reach out to brands with their complaints and concerns via social\nchannels. NLP is used extensively to understand, categorize, filter, prioritize, and\nin some cases even automatically respond to the complaints.\nThere are many other applications that we haven’t dug into, such as geolocation\ndetection, sarcasm detection, event and topic detection, emergency situation aware‐\nness, and rumor detection, to name a few. Our aim here is to give you a good idea of\nthe landscape of applications that can be built using social media text data (SMTD).\nNow, let’s look into why building NLP applications using SMTD is not a straightfor‐\nward application of the concepts we’ve learned so far in this book and why SMTD\nrequires special treatment.\nUnique Challenges\nUntil now, we’ve (implicitly) assumed that the input text (most of the time, if not\nalways) follows the basic tenets of any language, namely:\n• Single language\n• Single script\n• Formal\n• Grammatically correct\n• Few or no spelling errors\n• Mostly text-like (very few non-textual elements, such as emoticons, images,\nsmileys, etc.)\nThese assumptions essentially stem from the properties and characteristics of the\ndomain(s) from which the input text data comes. Standard NLP systems assume that\nthe language they deal with is highly structured and formal. When it comes to text\ndata coming from social platforms, most of the above assumptions go for a toss. This\nis because users can be extremely terse when posting on social media; this extreme\nbrevity is a hallmark of social posts. For example, users may write “are” as “r,” “we” as\n“v,” “laugh out loud” as “lol,” etc. This brevity has given rise to a new recipe for lan‐\nguage: one that’s very informal and consists of nonstandard spellings, hashtags, emo‐\nticons, new words and acronyms, code-mixing, transliteration, etc. These\ncharacteristics make the language used on social platforms so unique that it’s alto‐\ngether considered a new language—the “language of social.”\n278 \n| \nChapter 8: Social Media\n",
      "word_count": 357,
      "char_count": 2204,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 309,
      "text": "Because of this, the NLP tools and techniques designed for standard text data don’t\nwork well with SMTD. To illustrate this point better, let’s look at some sample tweets,\nshown in Figures 8-2 and 8-3. Notice how the language used here is very different\nfrom the language used in newspapers, blog posts, emails, book chapters, etc.\nFigure 8-2. Examples of new words being introduced in vocabulary\nFigure 8-3. New recipe for language: nonstandard spellings, emoticons, code-mixing,\ntransliteration [5]\nUnique Challenges \n| \n279\n",
      "word_count": 82,
      "char_count": 527,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1184,
          "height": 718,
          "ext": "png",
          "size_bytes": 191030
        },
        {
          "index": 1,
          "width": 353,
          "height": 432,
          "ext": "png",
          "size_bytes": 172804
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 310,
      "text": "These differences pose challenges to standard NLP systems. Let’s look at the key dif‐\nferences in detail:\nNo grammar\nAny language is known to strictly follow the rules of grammar. However, conver‐\nsations on social media don’t follow any grammar and are characterized by\ninconsistent (or absent) punctuation and capitalization, emoticons, incorrect or\nnonstandard spelling, repetition of the same character multiple times, and ram‐\npant abbreviations. This departure from standard languages makes basic pre-\nprocessing steps like tokenization, POS tagging, and identification of sentence\nboundaries difficult. Modules specialized to work with SMTD are required to\nachieve these tasks.\nNonstandard spelling\nMost languages have a single way of writing any word, so writing a word in any\nother way is a spelling mistake. In SMTD, words can have many spelling varia‐\ntions. As an example, consider the following different ways in which the English\nword “tomorrow” is written on social [6]—tmw, tomarrow, 2mrw, tommorw,\n2moz, tomorro, tommarrow, tomarro, 2m, tomorrw, tmmrw, tomoz, tommorow,\ntmrrw, tommarow, 2maro, tmrow, tommoro, tomolo, 2mor, 2moro, 2mara, 2mw,\ntomaro, tomarow, tomoro, 2morr, 2mro, tmoz, tomo, 2morro, 2mar, 2marrow,\ntmr, tomz, tmorrow, 2mr, tmo, tmro, tommorrow, tmrw, tmrrow, 2mora, tomm‐\nrow, tmoro, 2ma, 2morrow, tomrw, tomm, tmrww, 2morow, 2mrrw, tomorow.\nFor an NLP system to work well, it needs to understand that all these words refer\nto the same word.\nMultilingual\nTake any article from a newspaper or a book, and you’ll probably find it’s written\nin a single language. Seldom will you see where large parts of it are written in\nmore than one language. On social media, people often mix languages. Consider\nthe following example from a social media website [7]:\nYaar tu to, GOD hain. tui\nJU te ki korchis? Hail u man!\nIt means, “Buddy you are GOD. What are you doing in JU? Hail u man!” The text\nis a mix of three languages: English (normal font), Hindi (italics), and Bengali\n(boldface). For Bengali and Hindi, phonetic typing has been used.\nTransliteration\nEach language is written in its own script, which refers to how the characters are\nwritten. However, on social media, people often write the characters of one script\nusing characters of another script. This is called “transliteration.” For example,\nconsider the Hindi word “आप” (devanagari script, pronounced as “aap”). In\n280 \n| \nChapter 8: Social Media\n",
      "word_count": 386,
      "char_count": 2439,
      "fonts": [
        "ArialUnicodeMS (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MinionPro-Bold (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 311,
      "text": "English, it means “you” (roman script). But people often write it in roman script\nas “aap.” Transliteration is common in SMTD, usually due to the typing interface\n(keyboard) being in roman script but the language of communication being non-\nEnglish.\nSpecial characters\nSMTD is characterized by the presence of many non-textual entities, such as spe‐\ncial characters, emojis, hashtags, emoticons, images and gifs, non-ASCII charac‐\nters, etc. For example, look at the tweets shown in Figure 8-4. From an NLP\nstandpoint, one needs modules in the pre-processing pipelines to handle such\nnon-textual entities.\nFigure 8-4. Special characters in social media data\nEver-evolving vocabulary\nMost languages add either no or very few new words to their vocabulary every\nyear. But when it comes to the language of social, the vocabulary increases at a\nvery fast rate. New words pop up every single day. This means that any NLP sys‐\ntem processing SMTD sees a lot of new words that were not part of the vocabu‐\nlary of the training data. This has an adverse impact on the performance of the\nNLP system and is known as the out of vocabulary (OOV) problem.\nIn order to get an idea of the severity of this problem, look at the infographic\nshown in Figure 8-5. We did this experiment [8] a few years ago, where we collec‐\nted a large corpus of tweets and quantified the amount of “new vocabulary” seen\nUnique Challenges \n| \n281\n",
      "word_count": 241,
      "char_count": 1412,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1276,
          "height": 832,
          "ext": "png",
          "size_bytes": 271604
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 312,
      "text": "on a month-by-month basis. The figure shows the fraction of new words seen in\na month as compared to the previous month’s data. As evident from the image,\nwhen compared to the vocabulary of the previous month, there are 10–15% new\nwords every month.\nFigure 8-5. Fraction of new vocabulary words seen every month [8]\n282 \n| \nChapter 8: Social Media\n",
      "word_count": 61,
      "char_count": 348,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1427,
          "height": 1882,
          "ext": "png",
          "size_bytes": 107891
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 313,
      "text": "Length of text\nThe average length of text on social media platforms is much smaller compared\nto other channels of communication like blogs, product reviews, emails, etc. The\nreason is that shorter text can be typed quickly while preserving understandabil‐\nity. This was driven primarily by Twitter’s 140-character restriction. For example,\n“This is an example for texting language” might be written as “dis is n eg 4 txtin\nlang.” Both mean the same, but the former is 39 characters long while the latter\nhas only 24 characters. As Twitter’s popularity and adoption grew, being terse on\nsocial platforms became the norm. This way of condensed writing has become so\npopular that now it can be seen in every informal communication, such as mes‐\nsages and chats.\nNoisy data\nSocial media posts are full of spam, ads, promoted content, and all manner of\nother unsolicited, irrelevant, or distracting content. Thus, we cannot take raw\ndata from social platforms and consume it as is. Filtering out noisy data is a vital\nstep. For example, imagine we’re collecting data for an NLP task (say, sarcasm\ndetection) from a Twitter handle or Facebook page, either by scraping or using\nthe Twitter API. It’s important to run a check that no spam, ads, or irrelevant\ncontent has come into our dataset.\nIn short, text data from social media is highly informal compared to text data from\nblogs, books, etc., and this lack of formality can manifest in the various ways\ndescribed above. All of these can have adverse impacts on the performance of NLP\nsystems that don’t have built-in ways to handle them. Figure 8-6 [5] shows the spec‐\ntrum of formalism in text data and where different sources of text data appear on it.\nFigure 8-6. Spectrum of formalism in text data depending on data sources [5]\nOwing to the characteristics and peculiarities that stem from the informal nature of\nthe language of social, standard NLP tools and techniques face difficulties when\napplied to SMTD. NLP for SMTD relies on either converting the text from social to\nstandard text (normalization) or building systems that are specifically designed to\ntackle SMTD. We’ll see how to go about doing this while building various applica‐\ntions in the next section.\nUnique Challenges \n| \n283\n",
      "word_count": 377,
      "char_count": 2246,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1357,
          "height": 363,
          "ext": "png",
          "size_bytes": 19430
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 314,
      "text": "It’s important to identify, understand, and address the language\npeculiarities found in SMTD. Building submodules that can handle\nthese peculiarities often goes a long way toward improving the per‐\nformance of models working with SMTD.\nNow, let’s focus on building business applications using SMTD.\nNLP for Social Data\nWe’ll now take a deep dive into applying NLP to SMTD to build some interesting\napplications that we can apply to a variety of problems. We may need to know how\ncustomers are responding to a particular announcement or product we’ve released, or\nbe able to identify user demographics, for example. We’ll start with simple applica‐\ntions like word clouds and ramp up to more complex ones, like understanding senti‐\nment in posts on social media platforms like Twitter.\nWord Cloud\nA word cloud is a pictorial way of capturing the most significant words in a given\ndocument or corpus. It’s nothing but an image composed of words (in different sizes)\nfrom the text under consideration, where the size of the word is proportional to its\nimportance (frequency) in the text corpus. It’s a quick way to understand the key\nterms in a corpus. If we run a word cloud algorithm on this book, we’re likely to get a\nword cloud similar to one shown in Figure 8-7.\nFigure 8-7. Word cloud for Chapter 4 of this book\n284 \n| \nChapter 8: Social Media\n",
      "word_count": 232,
      "char_count": 1348,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        },
        {
          "index": 1,
          "width": 710,
          "height": 730,
          "ext": "png",
          "size_bytes": 471865
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 315,
      "text": "Words like NLP, natural language processing, and linguistics occur many times com‐\npared to other words in the book, so they show up prominently in the corresponding\nword cloud. So, how do we create word clouds from a collection of tweets? What’s the\nNLP pipeline for this?\nHere’s a step-by-step process for building a word cloud:\n1. Tokenize a given corpus or document\n2. Remove stop words\n3. Sort the remaining words in descending order of frequency\n4. Take the top k words and plot them “aesthetically”\nThe following code snippet illustrates how to implement this pipeline in practice (the\ncomplete code can be found in Ch8/wordcloud.ipynb). For this, we’ll use a library\ncalled wordcloud [9] that has a built-in function for generating word clouds:\n    from wordcloud import WordCloud\ndocument_file_path = ‘./twitter_data.txt’\ntext_from_file = open(document_file_path).read()\nstop_words = set(nltk.corpus.stopwords.words('english'))\nword_tokens = twokenize(text_from_file)\nfiltered_sentence = [w for w in word_tokens if not w in stop_words]\nwl_space_split = \" \".join(filtered_sentence)\nmy_wordcloud = WordCloud().generate(wl_space_split)\nplt.imshow(my_wordcloud)\nplt.axis(\"off\")\nplt.show()\nDepending on the styling, we can generate word clouds in various shapes to suit our\napplication [10], as shown in Figure 8-8.\nFigure 8-8. The same word cloud in various shapes\nNLP for Social Data \n| \n285\n",
      "word_count": 200,
      "char_count": 1398,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 508,
          "ext": "png",
          "size_bytes": 379568
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 316,
      "text": "Tokenizer for SMTD\nOne of the key steps in the above process is to correctly tokenize the text data. For\nthis, we used twokenize [11] to get tokens from the text corpus. This is a specialized\nfunction for getting tokens from tweets’ text data. This function is part of a set of NLP\ntools specially designed to work with SMTD [12, 13]. Now, we might ask: why do we\nneed a specialized tokenizer, and why not use the standard tokenizer available in\nNLTK? We discussed this briefly in Chapters 3 and 4, but it’s worth spending time on\nagain. The answer lies in the fact that the tokenizer available in NLTK is designed to\nwork with standard English language. Specifically in the English language, two words\nare separated by space. This might not necessarily be true for English used on Twitter.\nThis suggests that a tokenizer that uses space as a way to identify word boundaries\nmight not do well on SMTD. Let’s understand this with an example. Consider the fol‐\nlowing tweet: “Hey @NLPer! This is a #NLProc tweet :-D”. The ideal way to tokenize\nthis text is as follows: ['Hey', '@NLPer', '!', ‘This', ‘is', ‘a', '#NLProc', ‘tweet', ':-D']. Using\na tokenizer designed for the English language, like nltk.tokenize.word_tokenize, we’ll\nget the following tokens: ['Hey', '@', ‘NLPer', '!', ‘This', ‘is', ‘a', '#', ‘NLProc', ‘tweet', ':', '-\nD']. Clearly, the set of tokens given by the tokenizer in NLTK is not correct. It’s\nimportant to use a tokenizer that gives correct tokens. twokenize is specifically\ndesigned to deal with SMTD.\nOnce we have the correct set of tokens, frequency counting is straightforward. There\nare a number of specialized tokenizers available to work with SMTD. Some of the\npopular ones are nltk.tokenize.TweetTokenizer [14], Twikenizer [15], Twokenizer by\nARK at CMU [12], and twokenize [11]. For a given input tweet, each of them can give\nslightly different output. Use the one that gives the best output for your corpus and\nuse case.\nNow, we’ll move on to the next application, where we’ll try to extract topics that are\ntrending.\nTrending Topics\nJust a couple of years ago, keeping yourself updated with the latest topics was pretty\nstraightforward—pick up the day’s newspaper, read through the headlines, and you’re\ndone. Social media has changed this. Given the volume of traffic, what is trending can\n(and often does) change within a few hours. Keeping track of what’s trending by the\nhour may not be so important for an individual, but for a business entity, it can be\nvery important.\nHow can we keep track of trending topics? In the lingo of social media, any conversa‐\ntion around a topic is often associated with a hashtag. Thus, finding trending topics is\nall about finding the most popular hashtags in a given time window. In Figure 8-9, we\nshow a snapshot of trending topics in the area of New York.\n286 \n| \nChapter 8: Social Media\n",
      "word_count": 489,
      "char_count": 2865,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 317,
      "text": "Figure 8-9. Snapshot of trending topics on Twitter [16]\nSo how do we implement a system that can collect trending topics? One of the sim‐\nplest ways to do this is using a Python API called Tweepy [17]. Tweepy gives a simple\nfunction, trends_available, to fetch the trending topics. It takes the geolocation\n(WOEID, or Where On Earth Identifier) as an input and returns the trending topics\nin that geolocation. The function trends_available returns the top-10 trending top‐\nics for a given WOEID, on the condition that the trending information for the given\nWOEID is available. The response of this function call is an array of objects that are\n“trending.” In response, each object encodes the following information: name of the\ntopic that’s trending, the corresponding query parameters that can be used to search\nfor the topic using Twitter search, and the URL to Twitter search. Below is a code\nsnippet that demonstrates how we can use Tweepy to fetch trending topics (full code\nat Ch8/TrendingTopics.ipynb):\nimport tweepy, json\nCONSUMER_KEY = 'key'\nCONSUMER_SECRET = 'secret'\nACCESS_KEY = 'key'\nACCESS_SECRET = 'secret'\nauth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\nauth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\nNLP for Social Data \n| \n287\n",
      "word_count": 193,
      "char_count": 1257,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 281,
          "height": 537,
          "ext": "png",
          "size_bytes": 37758
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 318,
      "text": "api = tweepy.API(auth)\n# Where On Earth ID for the entire world is 1.\n# See https://dev.twitter.com/docs/api/1.1/get/trends/place and\n# http://developer.yahoo.com/geo/geoplanet/\nWORLD_WOE_ID = 1\nCANADA_WOE_ID = 23424775 # WOEID for Canada\nworld_trends = api.t\ntrends_place(_id=WORLD_WOE_ID)\ncanada_trends = api.trends_place(_id=CANADA_WOE_ID )\nworld_trends_set = set([trend['name'] for trend in world_trends[0]['trends']])\ncanada_trends_set = set([trend['name'] for trend incanada_trends[0]['trends']])\n# This gives the top trending hashtags for both world and Canada.\ncommon_trends = world_trends_set.intersection(us_trends_set)\ntrend_queries = [trend['query'] for trend in results[0]['trends']]\nfor trend_query in trend_queries:\n    print(api.search(q=trend_query))\n# this will return the tweets for each of the trending topic\nThis small snippet of code will give us the live top trends for a given location. The\nonly problem is that Tweepy is a free API, so it has rate limits. Twitter imposes rate\nlimits on how many requests an application can make to any given API resource\nwithin a given time window—you can’t make thousands of requests. Twitter’s rate\nlimits are well documented. In case you need to make calls beyond the rate limits,\nlook at Gnip [18], a paid data hosepipe from Twitter.\nNow, let’s see how to implement another popular NLP application: sentiment analysis\nwith social media data.\nUnderstanding Twitter Sentiment\nWhen it comes to NLP and social media, one of the most popular applications has to\nbe sentiment analysis. For businesses and brands across the globe, it’s crucial to listen\nto what people are saying about them and their products and services. It’s even more\nimportant to know whether people’s opinion is positive or negative and if this senti‐\nment polarity is changing over time. In the pre-social era, this was done using cus‐\ntomer surveys, including door-to-door visits. In today’s world, social media is a great\nway to understand people’s sentiment about a brand. Even more important is how\nthis sentiment changes over time. Figure 8-10 shows how sentiment changes over\ntime for a given organization. Visualizations like these provide great insights to mar‐\n288 \n| \nChapter 8: Social Media\n",
      "word_count": 325,
      "char_count": 2232,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 319,
      "text": "keting teams and organizations—dissecting their audience’s reactions to their cam‐\npaigns and events helps them plan strategically for future campaigns and content.\nFigure 8-10. Tracking change in sentiment over time [19]\nIn this section, we’ll focus on building sentiment analysis for Twitter data using a\ndataset from the public domain. There are many datasets available on the internet—\nfor example, the University of Michigan Sentiment Analysis competition on Kaggle\n[20] and Twitter Sentiment Corpus by Niek Sanders [21].\nHow is sentiment analysis for Twitter different from the sentiment analysis models\nwe built in Chapter 4? The key difference lies in the dataset. In Chapter 4, we used the\nIMDB dataset, which consists of sentences that are well formed and have a structure\nto them. On the other hand, the data in the Twitter sentiment corpus consists of\ntweets written informally. This leads to the various issues we discussed in “Unique\nChallenges” on page 278. These issues, in turn, impact the performance of the model.\nA great experiment is to run the sentiment analysis pipeline from Chapter 4 on our\nTwitter corpus and take a deep dive into the kind of mistakes the model makes. We\nleave this as an exercise for the reader.\nNLP for Social Data \n| \n289\n",
      "word_count": 209,
      "char_count": 1268,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1321,
          "height": 1050,
          "ext": "png",
          "size_bytes": 62856
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 320,
      "text": "We’ll move forward by building a system for sentiment analysis and setting up a base‐\nline. For this, we’ll use TextBlob [22], which is a Python-based NLP toolkit built on\ntop of NLTK and Pattern. It comes with an array of modules for text processing, text\nmining, and text analysis. All it takes is five lines of code to get a basic sentiment\nclassifier:\nfrom textblob import TextBlob\nfor tweet_text in tweets_text_collection:\n    print(tweet_text)\n    analysis = TextBlob(tweet_text)\n    print(analysis.sentiment)\nThis will give us polarity and subjectivity values of each of the tweets in the corpus.\nPolarity is a value between [–1.0, 1.0] and tells how positive or negative the text is.\nSubjectivity is within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very\nsubjective.\nIt uses a simple idea: tokenize the tweet and compute polarity and subjectivity for\neach of the tokens. Then combine the polarity and subjectivity numbers to arrive at a\nsingle value for the whole sentence. We leave it to the reader to get into the finer\ndetails. This simple sentiment classifier might not work well, primarily because of the\ntokenizer used by TextBlob. Our data comes from social media, so it will most likely\nnot follow formal English. Thus, after tokenization, many of the tokens may not be\nstandard words found in the English dictionary, so we won’t have the polarity and\nsubjectivity for all such tokens.\nSay we’ve been asked to improve our classifier. We can try various techniques and\nalgorithms we learned in Chapter 4. However, we might not see a great improvement\nin performance because of the noise present in the data (discussed in “Unique Chal‐\nlenges” on page 278). Thus, the key to improving the system lies in better cleaning\nand pre-processing of the text data. This is crucial for SMTD. Below, we’ll discuss\nsome important parts of pre-processing for SMTD. For the rest of the pipeline, we\ncan follow the pipeline discussed in Chapter 4.\nPre-processing and data cleaning are crucial when working with\nSMTD. This step is likely to provide the most gains in model per‐\nformance.\nPre-Processing SMTD\nMost NLP systems that work with SMTD have a rich pre-processing pipeline that\nincludes many steps. In this section, we’ll cover some of the steps that come up often\nin dealing with SMTD.\n290 \n| \nChapter 8: Social Media\n",
      "word_count": 390,
      "char_count": 2343,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 321,
      "text": "Removing markup elements\nIt’s not surprising to see markup elements (HTML, XML, XHTML, etc.) in SMTD,\nand it’s important to remove them. A great way to achieve this is to use a library\ncalled Beautiful Soup [23]:\nfrom bs4 import BeautifulSoup\nmarkup = '<a href=\"http://nlp.com/\">\\nI love <i>nlp</i>\\n</a>'\nsoup = BeautifulSoup(markup)\nsoup.get_text()\nThis gives the output \\nI love nlp\\n.\nHandling non-text data\nSMTD is often full of symbols, special characters, etc., and they’re often in encodings\nsuch as Latin and Unicode. In order to understand them, it’s important to convert the\nsymbols present in the data to simple and easier-to-understand characters. This is\noften done by converting to a standard encoding format like UTF-8. In the example\nbelow, we see how the entire text is converted into a machine-readable form:\ntext = 'I love Pizza 🍕!  Shall we book a cab 🚕 to gizza?'\nText = text.encode(\"utf-8\")\nprint(Text)\nb'I love Pizza \\xf0\\x9f\\x8d\\x95!  \nShall we book a cab \\xf0\\x9f\\x9a\\x95 to get pizza?'\nHandling apostrophes\nAnother hallmark of SMTD is the use of the apostrophe; it’s quite common to see sce‐\nnarios like ‘s, ‘re, ‘r, etc. The way to handle this is to expand apostrophes. This\nrequires a dictionary that can map apostrophes to full forms:\nApostrophes_expansion = {\n“'s\" : \" is\",\n\"'re\" : \" are\",\n\"'r\" : \" are\", ...} ## Given such a dictionary\nwords = twokenize(tweet_text)\nprocessed_tweet_text = [Apostrophes_expansion[word] if word \n                       in Apostrophes_expansion else word for word in words]\nprocessed_tweet_text = \" \".join(processed_tweet_text)\nTo the best of our knowledge, such a mapping between apostrophes and their expan‐\nsion is not available anywhere off the shelf, so it needs to be created manually.\nNLP for Social Data \n| \n291\n",
      "word_count": 280,
      "char_count": 1782,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "Symbola (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 322,
      "text": "Handling emojis\nEmojis are at the very core of communication over social channels. One small image\ncan completely describe one or more human emotions. However, they pose a huge\nchallenge for machines. How can we design subsystems that can understand the\nmeaning of an emoji? A naive thing to do during pre-processing would be to remove\nall emojis. This could result in significant loss of meaning.\nA good way to achieve this is to replace the emoji with corresponding text explaining\nthe emoji. For example, replace “” with “fire”. To do so, we need a mapping\nbetween emojis and their corresponding elaboration in text. Demoji [24] is a Python\npackage that does exactly this. It has a function, findall(), that gives a list of all\nemojis in the text along with their corresponding meanings.\ntweet = \"#startspreadingthenews yankees win great start by \n going 5strong\ninnings with 5k’s\n \n solo homerun \n with 2 solo homeruns \nand\n 3run homerun… \n \n \n with rbi’s … \n \n and \n \nto close the game\n!!!….WHAT A GAME!! \"\ndemoji.findall(tweet)\n{\n    \"\n\": \"fire\",\n    \"\n\": \"volcano\",\n    \"\n\": \"man judge: medium skin tone\",\n    \"\n\": \"Santa Claus: medium-dark skin tone\",\n    \"\n\": \"flag: Mexico\",\n    \"\n\": \"ogre\",\n    \"\n\": \"clown face\",\n    \"\n\": \"flag: Nicaragua\",\n    \"\n\": \"person rowing boat: medium-light skin tone\",\n    \"\n\": \"ox\",\n}\nWe can use the output of findall() to replace all emojis in a text with their corre‐\nsponding meaning in words.\nSplit-joined words\nAnother peculiarity of SMTD is that users sometimes combine multiple words into a\nsingle word, where the word disambiguation is done by using capital letters, for\nexample GoodMorning, RainyDay, PlayingInTheCold, etc. This is simple to handle.\nThe following code snippet does the job for us:\n    processed_tweet_text = “ “.join(re.findall(‘[A-Z][^A-Z]*’, tweet_text))\nFor GoodMorning, this will return “Good Morning.”\n292 \n| \nChapter 8: Social Media\n",
      "word_count": 303,
      "char_count": 1906,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 154,
          "height": 154,
          "ext": "png",
          "size_bytes": 2212
        },
        {
          "index": 1,
          "width": 154,
          "height": 113,
          "ext": "png",
          "size_bytes": 1778
        },
        {
          "index": 2,
          "width": 146,
          "height": 154,
          "ext": "png",
          "size_bytes": 1809
        },
        {
          "index": 3,
          "width": 154,
          "height": 116,
          "ext": "png",
          "size_bytes": 939
        },
        {
          "index": 4,
          "width": 154,
          "height": 154,
          "ext": "png",
          "size_bytes": 4624
        },
        {
          "index": 5,
          "width": 154,
          "height": 150,
          "ext": "png",
          "size_bytes": 4842
        },
        {
          "index": 6,
          "width": 154,
          "height": 154,
          "ext": "png",
          "size_bytes": 2913
        },
        {
          "index": 7,
          "width": 154,
          "height": 131,
          "ext": "png",
          "size_bytes": 1556
        },
        {
          "index": 8,
          "width": 154,
          "height": 140,
          "ext": "png",
          "size_bytes": 3308
        },
        {
          "index": 9,
          "width": 154,
          "height": 113,
          "ext": "png",
          "size_bytes": 2425
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 323,
      "text": "Removal of URLs\nAnother common feature of SMTD is the use of URLs. Depending on the application,\nwe might want to remove the URL all together. The code snippet replaces all URLs\nwith a constant; in this case, constant_url. While in simpler cases, we could use a\nregex, such as http\\S+, in most cases, we’ll have to write a custom regex like the one\nshown in the following snippet. This code is complex because some social posts con‐\ntain tiny URLs instead of full URLs:\ndef process_URLs(tweet_text):\n    '''\n    replace all URLs in the tweet text\n    '''\n    UrlStart1 = regex_or('https?://', r'www\\.')\n    CommonTLDs  = regex_or( 'com','co\\\\.uk','org','net','info','ca','biz',\n                          'info','edu','in','au')\n    UrlStart2 = r'[a-z0-9\\.-]+?' + r'\\.' + CommonTLDs + \n                pos_lookahead(r'[/ \\W\\b]')\n    # * not + for case of:  \"go to bla.com.\" -- don't want period\n    UrlBody = r'[^ \\t\\r\\n<>]*?'\n    UrlExtraCrapBeforeEnd = '%s+?' % regex_or(PunctChars, Entity)\n    UrlEnd = regex_or( r'\\.\\.+', r'[<>]', r'\\s', '$')\n    Url =      (optional(r'\\b') +\n          regex_or(UrlStart1, UrlStart2) +\n          UrlBody +\n    pos_lookahead( optional(UrlExtraCrapBeforeEnd) + UrlEnd))\n    Url_RE = re.compile(\"(%s)\" % Url, re.U|re.I)\n    tweet_text = re.sub(Url_RE, \" constant_url \", tweet_text)\n    # fix to handle unicodes in URL\n    URL_regex2 = r'\\b(htt)[p\\:\\/]*([\\\\x\\\\u][a-z0-9]*)*'\n    tweet_text = re.sub(URL_regex2, \" constant_url \", tweet_text)\n    return tweet_text\nNonstandard spellings\nOn social media, people often write words that are technically spelling mistakes. For\nexample, people often write one or more characters multiple times, as in “yessss” or\n“ssssh” (instead of “yes” or “ssh”). This repetition of characters is very common in\nSMTD. Below is a simple way to fix this. We use the fact that, in the English language,\nthere are hardly any words that have the same character three times consecutively. So\nwe trim accordingly:\ndef prune_multple_consecutive_same_char(tweet_text):\n    '''\n    yesssssssss  is converted to yes\n    ssssssssssh is converted to ssh\n    '''\n          tweet_text = re.sub(r'(.)\\1+', r'\\1\\1', tweet_text)\n          return tweet_text\nNLP for Social Data \n| \n293\n",
      "word_count": 296,
      "char_count": 2229,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 324,
      "text": "This gives the output yess ssh.\nAnother idea is to use spelling-correction libraries. Most of them use some form of\ndistance metric, such as edit distance or Levenshtein distance. TextBlob itself has\nsome spelling-correction capabilities:\nfrom textblob import TextBlob\ndata = \"His sellection is bery antresting\"\noutput = TextBlob(data).correct()\nprint(output)\nThis gives the output: His selection is very interesting.\nWe hope this gives you a good idea of why, when it comes to SMTD, pre-processing is\nso important, and of how it can be accomplished. This is by no means an exhaustive\nlist of pre-processing steps. Now, we’ll focus on the next step in our NLP pipeline\n(from back in Figure 2-1): feature engineering.\nText Representation for SMTD\nPreviously, we saw how to make a simple sentiment classifier for tweets using Text‐\nBlob [22]. Now, let’s try to build a more sophisticated classifier. Let’s say we’ve imple‐\nmented all the pre-processing steps we discussed in the previous section. Now what?\nNow we need to break the text into tokens and then represent them mathematically.\nFor tokenization, we use twokenize [11], which is a specialized tokenizer designed to\nwork with Twitter data. How do we represent the tokens we get? We can try various\ntechniques we learned in Chapter 3.\nIn our experience, basic vectorization approaches like BoW and TF-IDF do not work\nwell with SMTD, primarily due to noise and variation in text data (e.g., the variations\nof “tomorrow” we discussed earlier in this chapter). The noise and variations lead to\nextremely sparse vectors. This leaves us with the option of using embeddings. As we\nsaw in Chapter 3, training our own embeddings is very expensive. So, we can begin\nby using pre-trained embeddings. In Chapter 4, we saw how to use Google’s pre-\ntrained word embeddings to build a sentiment classifier. Now, if we run the same\ncode on our dataset collected from social media platforms, we may not get impressive\nnumbers like we got there. One of the reasons may be that the vocabulary of our data‐\nset is dramatically different from the vocabulary of the Word2vec model. To verify\nthis, we just tokenize our text corpus and build a set on all tokens, then compare it\nwith the vocabulary of Word2vec. The following code snippet does this:\ncombined = tokenizer(train_test_X)\n# This is one way to create vocab set from our dataset.\nflat_list = chain(*combined)\ndataset_vocab = set(flat_list)\nlen(dataset_vocab)\n294 \n| \nChapter 8: Social Media\n",
      "word_count": 408,
      "char_count": 2486,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Italic (8.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 325,
      "text": "w2v_vocab = set(w2v_model.vocab.keys())\nprint(dataset_vocab - w2v_vocab)\nHere, train_test_X is the combined set of reviews from training and test chunks of\nour corpus. Now, you may ask why this wasn’t true when we worked with the IMDB\nmovie review dataset. The reason is that Google’s Word2vec is trained on Wikipedia\nand news articles. The language and vocabulary used in these articles is similar to the\nlanguage and vocabulary used in the IMDB movie review dataset. This is unlikely to\nbe true with our dataset from social media. So, it’s highly likely that, for our dataset\nfrom social media, the set difference will be pretty high.\nSo, how do we fix this? There are a few ways:\n1. Use pre-trained embeddings from social data, such as the ones from Stanford’s\nNLP group [25]. They trained word embeddings on two billion tweets [26].\n2. Use a better tokenizer. We highly recommend the twokenize tokenizer from\nAllen Ritter’s work [11].\n3. Train your own embeddings. This option should be the last resort and done only\nif you have lots and lots of data (at least 1 to 1.5 million tweets). Even after train‐\ning your own embeddings, you may not get any considerable bump in perfor‐\nmance metrics.\nIn our experience, if you’re going for word-based embeddings, (1)\nand (2) can give you the best return on investment for your efforts.\nEven if you get a considerable boost in the performance metrics, as the time gap\nbetween training data and production data keeps increasing, the performance can\nkeep going down. This is because as the time gap increases, the overlap between the\nvocabulary of the training data and production data keeps reducing. One of the main\nreasons for this is the fact that the vocabulary of social media is always evolving—new\nwords and acronyms are created and used all the time. You might think that new\nwords get added only once in a while, but, surprisingly, this is far from true.\nFigure 8-11 shows how fast the vocabulary on social media can evolve [8]. The plot\non the left shows the percentage of unseen tokens on a month-by-month basis. This\nanalysis was done using approximately 2 million tweets over a span of 27 months.\nThe plot in the middle shows the same statistics as a bar plot of total versus new\ntokens on a monthly basis. The plot on the right is a cumulative bar chart. On aver‐\nage, approximately 20% of the vocabulary for any month are new words.\nNLP for Social Data \n| \n295\n",
      "word_count": 422,
      "char_count": 2421,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 326,
      "text": "Figure 8-11. Plots depicting how fast the vocabulary of social media can evolve [8]\nWhat does this mean for us? No matter how good our word embeddings are, because\nof the ever-evolving vocabulary of social media, within a couple of months, our\nembeddings will become obsolete (i.e., a large portion of our vocabulary won’t be\npresent in our word embeddings). This means that when we query the embedding\nmodel with a word to fetch its embedding, it will return null since the query word\nwas not present in the training data when the embeddings were trained. This is analo‐\ngous to saying that all such words were completely ignored. This, in turn, will dra‐\nmatically reduce the accuracy of our sentiment classifier with time, because with time,\nmore and more words will end up getting ignored.\nWord embeddings are not the best way to represent SMTD, espe‐\ncially when you want to use them for more than four to six months.\nResearchers working in this area identified this problem pretty early and tried various\nways to circumvent it. One of the better ways to deal with this persistent OOV prob‐\nlem with SMTD is to use character n-gram embeddings. We discussed this idea when\nwe covered fastText in Chapters 3 and 4. Each character n-gram in the corpus has an\nembedding for it. Now, if the word is present in the vocabulary of the embeddings,\nthen we use the word embedding directly. If not—i.e., the word is OOV—we break\nthe word into character n-grams and combine all these embeddings to come up with\nthe embedding for the word. fastText has pre-trained character n-gram embeddings\n296 \n| \nChapter 8: Social Media\n",
      "word_count": 280,
      "char_count": 1617,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        },
        {
          "index": 1,
          "width": 960,
          "height": 543,
          "ext": "png",
          "size_bytes": 270520
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 327,
      "text": "but they’re not not Twitter or SMTD specific. Researchers have also tried character\nembeddings. An interested reader can look into various works along these lines [27,\n28, 29, 30].\nCustomer Support on Social Channels\nFrom its inception to present day, social media has evolved as a channel of communi‐\ncation. It started primarily with the objective of helping people across the globe get\nconnected and express themselves. But the widespread adoption of social media has\nforced brands and organizations to take another look at their communication strate‐\ngies. A great example of this is brands providing customer support on social plat‐\nforms like Twitter and Facebook. Brands never intended to do this to begin with.\nEarly in this decade, as the adoption of social platforms grew, brands started to create\nand own properties and assets like Twitter handles and Facebook pages primarily to\nreach out to their customers and users and run branding and marketing campaigns.\nHowever, over time brands saw that users and customers were reaching out to them\nwith complaints and grievances. As the volume of the complaints and issues grew,\nthis prompted brands to create dedicated handles and pages to handle support traffic.\nFigure 8-12 shows the support pages of Apple and Bank of America. Twitter and\nFacebook have launched various features to support brands [31], and most customer\nrelationship management (CRM) tools support customer service on social channels.\nA brand can connect their social channels to the CRM tool and use the tool to\nrespond to inbound messages.\nFigure 8-12. Example of brands’ support pages on Twitter [32]\nOwing to the public nature of conversations, brands are obligated to respond quickly.\nHowever, brands’ support pages receive a lot of traffic. Some of this is genuine ques‐\ntions, grievances, and requests. These are popularly known as “actionable conversa‐\ntions,” as customer support teams should act on them quickly. On the other hand, a\nlarge portion of traffic is simply noise: promos, coupons, offers, opinions, troll\nNLP for Social Data \n| \n297\n",
      "word_count": 334,
      "char_count": 2081,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1394,
          "height": 560,
          "ext": "png",
          "size_bytes": 419103
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 328,
      "text": "messages, etc. This is popularly called “noise.” Customer support teams cannot\nrespond to noise and want to steer clear of all such messages. Ideally, they want only\nactionable messages to be converted into tickets in their CRM tools. Figure 8-13\nshows examples of both actionable messages and noise.\nFigure 8-13. Example of actionable versus noisy messages [8]\nImagine we work at a CRM product organization and are asked to build a model to\nsegregate actionable messages from noise. How can we go about it? The problem of\nidentifying noise versus actionable messages is analogous to the spam classification\nproblem or sentiment classification problem. We can build a model that can look at\nthe inbound messages. The pipeline will be very similar:\n1. Collect a labeled dataset\n2. Clean it\n3. Pre-process it\n4. Tokenize it\n5. Represent it\n6. Train a model\n7. Test model\n8. Put it in production\n298 \n| \nChapter 8: Social Media\n",
      "word_count": 156,
      "char_count": 925,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1280,
          "height": 899,
          "ext": "png",
          "size_bytes": 216899
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 329,
      "text": "We’ve already discussed various aspects of this pipeline in this chapter. Much like sen‐\ntiment analysis on SMTD, the key here, too, is the pre-processing step. With this,\nwe’re now ready to move to the last topic of this chapter: identifying controversial\ncontent on social platforms.\nMemes and Fake News\nUsers on social platforms are known to share various kinds of information and\nthoughts in various ways. These platforms were initially designed to be self-\nregulating. However, over time, users have evolved to behave beyond community\nnorms; this is known as “trolling.” A large portion of posts on social platforms are full\nof controversial content such as trolls, memes, internet slang, and fake news. Some of\nit might be advocacy of propaganda, or it could be just for fun. In any case, this con‐\ntent needs to be monitored and filtered out. In this section, we’ll discuss how to study\nthe trends of such content and the role NLP has to play in it.\nIdentifying Memes\nMemes are one of the most interesting elements that have been curated by social\nmedia users to communicate messages with fun or satire. These memes get reused\nwith minimal changes in form, such as the image of “grumpy cat” (Figure 8-14),\nwhich has been used in many scenarios with different text associated with it. This\nresembles the original concept of “genes” as coined by Richard Dawkins [33]. Lada\nAdamic from Facebook studied information flow via these memes in the Facebook\nnetwork [34]; she says, “…memes propagating via a manual copy and paste mecha‐\nnism can be exact, or they might contain a “mutation,” an accidental or intentional\nmodification.” Figure 8-14 shows examples of two popular memes that you might\nhave come across.\nFigure 8-14. Examples of memes [35]\nMemes and Fake News \n| \n299\n",
      "word_count": 300,
      "char_count": 1779,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 601,
          "ext": "png",
          "size_bytes": 1062540
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 330,
      "text": "Before we cover some key methods for understanding the trends in memes, let’s dis‐\ncuss why it’s important to understand these trends. Misuse of trolling memes in a live\nfeed of a professional network platform like LinkedIn may not be desirable. This is\nsimilar to groups on Facebook or Google that intend to spread awareness or informa‐\ntion related to official processes or group events (e.g., a Facebook page for a fund‐\nraiser event or a Google group for helping students applying to their graduate\nschool). Identifying content that could be a meme that’s heckling others or is other‐\nwise offensive or violating other group or platform rules is important. There are two\nprimary ways in which a meme could be identified:\nContent-based\nContent-based meme identification uses content to match with other memes of\nsimilar patterns that have already been identified. For example, in a community,\nit has been identified that “This is Bill. Be like Bill” (Figure 8-14) has emerged as\na meme. To identify if a new post belongs to the same template, we can extract\nthe text and use a similarity metric like Jaccard distance to identify problematic\ncontent. In this way, it’s possible to identify memes of this pattern: “This is Per‐\nsonX. Be like PersonX.” In our running example, even a regular expression would\nbe able to identify such templates from a new post.\nBehavior-based\nBehavior-based meme identification is done mainly using the activity on the\npost. Studies have shown that the sharing behavior of a meme changes drastically\nfrom its inception to later hours. Usually, viral content can be identified by ana‐\nlyzing the number of shares, comments, likes for a particular post. In general,\nthese numbers often go beyond the average metrics for other non-meme posts.\nThis is more in the realm of anomaly detection. An interested reader can read the\nsurvey of such methods studied extensively on the Facebook network [34].\nNow that we’ve discussed the basic definition of memes in the context of social media\nand briefly touched on how to identify or measure their effects, we’ll now move to\nanother important and pressing issue in social media: fake news.\nFake News\nIn the last few years, fake news on social platforms has become a huge issue. The\nnumber of incidents related to fake news has risen significantly along with the rise in\nusers on social platforms. This consists of users both creating fake content and con‐\nstantly sharing it on social networks to make it viral. In this section, we’ll take a look\nat how we can detect fake news using the NLP techniques we’ve learned so far.\nLet’s look at an example of such fake news: “Lottery Winner Arrested for Dumping\n$200,000 of Manure on Ex-Boss’ Lawn.” [36] This got over 2.3 million Facebook\nshares in 2018 [37].\n300 \n| \nChapter 8: Social Media\n",
      "word_count": 475,
      "char_count": 2809,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 331,
      "text": "Various media houses and content moderators are actively working on detecting and\nweeding out such fake news. There are some principled approaches that can be used\nto tackle this menace:\n1. Fact verification using external data sources: Fact verification deals with validating\nvarious facts in a news article. It can be treated as a language understanding task\nwhere, given a sentence and a set of facts, a system needs to find out if the set of\nfacts supports the claim or not.\nConsider we have access to external data sources, such as Wikipedia, where we\nassume the facts have been entered correctly. Now, given a piece of news text,\nsuch as, “Einstein was born in 2000,” we should be able to verify it using data\nsources consisting of facts. Note that, at the beginning, we don’t know which\npiece of information could be wrong, so this cannot be solved trivially just by\npattern matching.\nAmazon Research at Cambridge created a curated dataset to deal with such cases\nof misinformation present in natural text [38]. The dataset consists of examples\nthat look like:\n{\n    \"id\": 78526,\n    \"label\": \"REFUTES\",\n    \"claim\": \"Lorelai Gilmore's father is named Robert.\",\n    \"attack\": \"Entity replacement\",\n    \"evidence\": [\n        [\n            [<annotation_id>, <evidence_id>, \"Lorelai_Gilmore\", 3]\n        ]\n    ]\n}\nAs you might be able to see, we can develop a model that takes the {claim, evi\ndence} as an input and produces the label REFUTES. This is more of a classifica‐\ntion task with three labels: AGREES, REFUTES, and NONE. The evidence set contains\nthe Wikipedia URL of the related entities of the sentence, and 3 denotes the sen‐\ntence that has the correct fact in the corresponding Wikipedia article.\nA similar dataset could be built by individual media houses to extract knowledge\nfrom existing articles related to their domains. For example, a sports news com‐\npany might build a set primarily containing facts related to sports.\nWe can use BoW-based methods to represent both the claim and the evidence\nand pass them as a pair through a logistic regression to obtain a classification\nlabel. More advanced techniques include using DL methods such as LSTM or\npre-trained BERT to obtain encodings of these inputs. We can then concatenate\nthese embeddings and pass it to a neural network to classify the claim. An inter‐\nested reader can look at [39, 40, 41].\nMemes and Fake News \n| \n301\n",
      "word_count": 395,
      "char_count": 2400,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Regular (8.9pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 332,
      "text": "2. Classifying fake versus real: A simple setup for this problem would be to build a\nparallel data corpus with instances of fake and real news excerpts and classify\nthem as real or fake. While the setup is simple, it could be very hard for a\nmachine to solve this task reasonably well due to the fact that people may use\nvarious linguistic nuances to confuse the machine in flagging fake content.\nResearchers from Harvard recently developed a system [42] to identify which\ntext is written by humans and which text is generated by machines (and therefore\ncould be fake). This system uses statistical methods to understand the facts and\nuses the fact that, when generating text, machines tend to use generic and com‐\nmon words. This is different from humans, who tend to use words that are more\nspecific and adhere to an individual’s writing style. Their methods show that\nthere could often be a clear distinction in the statistical properties of word usage\nthat can be used to distinguish fake text from real text. We encourage readers to\nlook into the work of Sebastian Gehrmann et al. [42, 43] for a complete under‐\nstanding of the method.\nA similar technique was used by the AllenNLP team to develop a tool called\nGrover [44], which uses an ML model to generate text that looks human-written.\nThey exploit the nuances present in the text generated to understand the quirks\nand attributes, which can then be used to build a system that helps in detecting\npotentially fake, machine-generated articles. We encourage you to play with the\ndemo [44] that’s been open sourced by the team to understand its mechanism.\nWe discussed two critical issues in social media—memes and fake news—and pro‐\nvided a quick survey of how to detect them. We also discussed how we can pose these\nproblems as a simple natural language understanding task (such as classification) and\nwhat a potential dataset to solve that task might look like. This section should give\nyou a good starting point to build systems that can identify malicious or fake content\npresent in social media.\nWrapping Up\nIn this chapter, we started with an overview of the various applications of NLP in\nsocial media and discussed some of the unique challenges social media text data poses\nto traditional NLP methods. We then took a detailed look at different NLP applica‐\ntions, such as building word clouds, detecting trending topics on Twitter, under‐\nstanding tweet sentiment, customer support on social media, and detecting memes\nand fake news. We also saw a range of text processing issues we might encounter\nwhile developing these tools and how to solve them. We hope this gave you a good\nunderstanding of how to apply NLP techniques on SMTD and solve an NLP problem\ndealing with social media text data you may encounter in your workplace. Let’s now\nmove on to the next chapter, where we’ll address another vertical where NLP has pro‐\nven to be very useful: e-commerce.\n302 \n| \nChapter 8: Social Media\n",
      "word_count": 507,
      "char_count": 2958,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 333,
      "text": "References\n[1] Twitter. Quarterly results: 2019 Fourth quarter. Last accessed June 15, 2020.\n[2] Internet Live Stats. “Twitter Usage Statistics”. Last accessed June 15, 2020.\n[3] Zephoria Digital Marketing. “The Top 20 Valuable Facebook Statistics–Updated\nMay 2020”.\n[4] Lewis, Lori. “This Is What Happens In An Internet Minute”. March 5, 2019.\n[5] Choudhury, Monojit. “CS60017 - Social Computing, Indian Institute of Technol‐\nogy Kharagpur, Lecture 1: NLP for Social Media: What, Why and How?”. Last\naccessed June 15, 2020.\n[6] Ritter, Alan, Sam Clark, and Oren Etzioni. “Named Entity Recognition in Tweets:\nAn Experimental Study.” Proceedings of the 2011 Conference on Empirical Methods in\nNatural Language Processing (2011): 1524–1534.\n[7] Barman, Utsab, Amitava Das, Joachim Wagner, and Jennifer Foster. “Code Mix‐\ning: A Challenge for Language Identification in the Language of Social Media.” Pro‐\nceedings of the First Workshop on Computational Approaches to Code Switching (2014):\n13–23.\n[8] Gupta, Anuj, Saurabh Arora, Satyam Saxena, and Navaneethan Santhanam. “Con‐\ntinuous Learning Systems: Building ML systems that learn from their mistakes”. Open\nData Science Conference (2019).\n[9] Mueller, Andreas. word_cloud: A little word cloud generator in Python, (GitHub\nrepo). Last accessed June 15, 2020.\n[10] Mueller, Andreas. “Gallery of Examples”. Last accessed June 15, 2020.\n[11] Ritter, Allen. “Twokenize”. Last accessed June 15, 2020.\n[12] Ritter, Allen. “OSU Twitter NLP Tools”. Last accessed June 15, 2020.\n[13] Noah’s ARK lab. “Tweet NLP”. Last accessed June 15, 2020.\n[14] Natural Language Toolkit. TweetTokenizer. Last accessed June 15, 2020.\n[15] Routar de Sousa, J. Guilherme. Twikenizer. Last accessed June 15, 2020.\n[16] Twitter’s Trending Topics. Last accessed June 15, 2020.\n[17] Tweepy, an easy-to-use Python library for accessing the Twitter API. Last\naccessed June 15, 2020.\n[18] Twitter. Enterprise Data: Unleash the Power of Twitter Data. Last accessed June\n15, 2020.\nWrapping Up \n| \n303\n",
      "word_count": 301,
      "char_count": 2016,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 334,
      "text": "[19] Wexler, Steve. “How to Visualize Sentiment and Inclination”. Tableau (blog), Jan‐\nuary 14, 2016.\n[20] Kaggle. UMICH SI650—Sentiment Classification. Last accessed June 15, 2020.\n[21] Sanders Twitter sentiment corpus, (GitHub repo). Last accessed June 15, 2020.\n[22] Loria, Steven. “TextBlob: Simple, Pythonic, text processing––Sentiment analysis,\npart-of-speech tagging, noun phrase extraction, translation, and more”. Last accessed\nJune 15, 2020.\n[23] Beautiful Soup. Last accessed June 15, 2020.\n[24] Solomon, Brad. Demoji. Last accessed June 15, 2020.\n[25] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. “GloVe:\nGlobal Vectors for Word Representation”. Last accessed June 15, 2020.\n[26] The Stanford Natural Language Procesisng Group. “Pre-trained GloVe embed‐\ndings from Tweets”. Last accessed June 15, 2020.\n[27] Dhingra, Bhuwan, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl, and William\nW. Cohen. “Tweet2Vec: Character-Based Distributed Representations for Social\nMedia”. (2016).\n[28] Yang, Zhilin, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W. Cohen, and Rus‐\nlan Salakhutdinov. “Words or Characters? Fine-grained Gating for Reading Compre‐\nhension”. (2016).\n[29] Kuru, Onur, Ozan Arkan Can, and Deniz Yuret. “CharNER: Character-Level\nNamed Entity Recognition.” Proceedings of COLING 2016, the 26th International Con‐\nference on Computational Linguistics: Technical Papers (2016): 911–921.\n[30] Godin, Fredric. “Twitter word embeddings” and \"TwitterEmbeddings”. Last\naccessed June 15, 2020.\n[31] Lull, Travis. “Announcing new customer support features for businesses”. Twitter\n(blog), September 15, 2016; Facebook Help Center. “How does my Facebook Page get\nthe ‘Very responsive to messages’ badge?”; Facebook Help Center. “How are response\nrate and response time defined for my Page?”.\n[32] Apple’s and Bank of America’s support handles on Twitter: https://twitter.com/\nAppleSupport and https://twitter.com/BofA_Help. Last accessed June 15, 2020.\n[33] Rogers, Kara. “Meme: Cultural Concept”. Encyclopedia Britannica. Last modified\nMarch 5, 2020.\n[34] Adamic, Lada A., Thomas M. Lento, Eytan Adar, and Pauline C. Ng. “Informa‐\ntion Evolution in Social Networks.” Proceedings of the Ninth ACM International Con‐\nference on Web Search and Data Mining (2016): 473–482.\n304 \n| \nChapter 8: Social Media\n",
      "word_count": 321,
      "char_count": 2328,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 335,
      "text": "[35] Popsugar Tech. Last accessed June 15, 2020.\n[36] “Lottery Winner Arrested for Dumping $200,000 of Manure on Ex-Boss’ Lawn”.\nWorld News Daily Report. Last accessed June 15, 2020.\n[37] Silverman, Craig. “Publishers Are Switching Domain Names to Try and Stay\nAhead of Facebook’s Algorithm Changes”. BuzzFeed News, March 1, 2018.\n[38] Thorne, James, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mit‐\ntal. “FEVER: a large-scale dataset for Fact Extraction and VERification”, (2018).\n[39] Hassan, Naeemul, Bill Adair, James T. Hamilton, Chengkai Li, Mark Tremayne,\nJun Yang, and Cong Yu. “The Quest to Automate Fact Checking.” Proceedings of the\n2015 Computation+ Journalism Symposium (2015).\n[40] Graves, Lucas. “Understanding the Promise and Limits of Automated Fact-\nChecking.” Reuters Institute, February 28, 2018.\n[41] Karadzhov, Georgi, Preslav Nakov, Lluís Màrquez, Alberto Barrón-Cedeño, and\nIvan Koychev. “Fully Automated Fact Checking Using External Sources.” Proceedings\nof the International Conference Recent Advances in Natural Language Processing\n(2017).\n[42] Strobelt, Hendrik and Sebastian Gehrmann. “Catching a Unicorn with GLTR: A\nTool to Detect Automatically Generated Text”. Last accessed June 15, 2020.\n[43] Gehrmann, Sebastian, Hendrik Strobelt, and Alexander M. Rush. “GLTR: Statis‐\ntical Detection and Visualization of Generated Text”, (2019).\n[44] Allen Institute for AI. “Grover: A State-of-the-Art Defense against Neural Fake\nNews”. Last accessed June 15, 2020.\nWrapping Up \n| \n305\n",
      "word_count": 215,
      "char_count": 1519,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 336,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 337,
      "text": "CHAPTER 9\nE-Commerce and Retail\nToday’s new marketplaces must nurture\nand manage perfect competition to thrive.\n—Jeff Jordan, Andreessen Horowitz\nIn today’s world, e-commerce has become synonymous with shopping. An enriched\ncustomer experience compared to what a physical retail store offers has fueled this\ngrowth of e-commerce. Worldwide retail e-commerce sales in 2019 were $3.5 trillion\nand are projected to reach $6.5 trillion by 2022 [1]. Recent advancements in ML and\nNLP have played a major role in this rapid growth.\nVisit the home page of any e-retailer, and you’ll find a lot of information in the form\nof text and images. A significant portion of this information consists of text in the\nform of product descriptions, reviews, etc. Retailers strive to utilize this information\nintelligently to deliver customer delight and build competitive advantage. An e-\ncommerce portal faces a range of text-related problems that can be solved by NLP\ntechniques. We saw different kinds of NLP problems and solutions in the previous\nsection (Chapters 4 through 7). In this chapter, we’ll give an overview of how NLP\nproblems in the e-commerce domain can be addressed using what we’ve learned in\nthis book so far. We’ll discuss some of the key NLP tasks in this domain, including\nsearch, \nbuilding \na \nproduct \ncatalog, \ncollecting \nreviews, \nand \nproviding\nrecommendations.\nFigure 9-1 shows some of these e-commerce tasks. Let’s start with an overview of\nthem.\n307\n",
      "word_count": 233,
      "char_count": 1464,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (9.3pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 338,
      "text": "Figure 9-1. NLP applications in e-commerce\nE-Commerce Catalog\nAny large e-commerce enterprise needs an easy-to-access product catalog. A product\ncatalog is a database of the products that the enterprise deals or a user can purchase.\nThis contains product description attributes as well as images for each product. Bet‐\nter product descriptions with relevant information help the customer choose the right\nproduct through the catalog. Such information can also help in product search and\nrecommendations. Imagine a recommendation engine that automatically knows that\nyou like the color blue! That’s certainly not possible unless and until the engine noti‐\nces that most of your recent purchases or searches were on apparel of the color blue.\nThe first thing needed to achieve this is identifying that “blue” is associated with the\nproducts as a color attribute. Extracting such information automatically is called\nattribute extraction. Attribute extraction from product descriptions can guarantee\nthat all the relevant product information is properly indexed and displayed for each\nproduct, improving product discoverability.\nReview Analysis\nThe most notable part of an e-commerce platform is the user reviews section for\nevery product. Reviews provide a different perspective of the product that cannot be\nobtained from the product attributes alone, such as quality, usability, comparisons\nwith other products, and delivery feedback. All reviews may not be useful, or they\nmight not come from trusted users. Further, it’s hard to process multiple reviews for a\ngiven product manually. NLP techniques provide an overall perspective for all\nreviews by performing tasks such as sentiment analysis, review summarization, iden‐\ntifying review helpfulness, and so on. We saw one example of NLP for review analysis\nin Chapter 5 when we discussed keyphrase extraction. We’ll see other use cases later\nin this chapter.\n308 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 295,
      "char_count": 1951,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 859,
          "height": 500,
          "ext": "png",
          "size_bytes": 27188
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 339,
      "text": "Product Search\nSearch systems in e-commerce are different compared to general search engines like\nGoogle, Bing, and Yahoo. An e-commerce search engine is closely tied to the prod‐\nucts available and the different kinds of information associated with them. For\ninstance, in a regular search engine, we’re dealing largely with free-form text data\n(like news articles or blogs) as opposed to structured sales and review data for e-\ncommerce. We might search for “red checkered shirt for a wedding,” and the e-\ncommerce search engine should be able to fetch it. Similar forms of focused search\ncan also be seen on travel websites for flight and hotel bookings, such as Airbnb and\nTripAdvisor. The specific nature of the information associated with each type of e-\ncommerce business calls for a customized pipeline of information processing, extrac‐\ntion, and search.\nProduct Recommendations\nWithout a recommendation engine, any e-commerce platform would be incomplete.\nA customer likes when the platform intelligently understands their choices and sug‐\ngests products to buy next. It actually helps the customer organize their thoughts\nabout shopping and helps to achieve better utility. Recommendations of discounted\nitems, same-brand products, or products with favorite attributes can really engage the\ncustomer on the website and make them spend more time. This directly increases the\npossibility of the customers buying those products. In addition to transaction-based\nrecommendation facilities, there is a rich set of algorithms that are developed based\non product content information and reviews that are textual in nature. NLP is used to\nbuild such recommendation systems.\nWith this overview, we’re all set to explore the role of NLP in e-commerce in more\ndetail. Let’s start with how it’s used in building search for e-commerce.\nSearch in E-Commerce\nCustomers visit an e-commerce website to find and purchase their desired products\nquickly. Ideally, a search feature should enable the customer to reach the right prod‐\nuct with the least number of clicks. The search needs to be fast and precise and fetch\nresults that closely match customers’ needs. A good search mechanism positively\nimpacts the conversion rate, which directly impacts the revenue of the e-retailer.\nGlobally, on average, only 4.3% of user search attempts convert to a purchase. By\nsome estimates, 34% of results in search on the top 50 portals do not produce useful\nresults [2], and there’s often a large scope for improvement.\nIn Chapter 7, we discussed how general search engines work and where NLP is use‐\nful. However, for e-commerce, the search engine needs to be more fine-tuned to the\nbusiness needs. Search in e-commerce is closed domain—i.e., the search engine\nSearch in E-Commerce \n| \n309\n",
      "word_count": 438,
      "char_count": 2773,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 340,
      "text": "typically fetches items from within the product information, rather than from a\ngeneric set of documents or content on the open web (like Google or Bing). The\nunderlying product information is built on the product catalog, attributes, and\nreviews. Search works on different facets of this information, like color, style, or cate‐\ngory. This kind of search in e-commerce is generally called “faceted search,” which is\nthe focus of this section.\nFaceted search is a specialized variant of search that allows the customer to navigate\nin a streamlined way with filters. For example, if we’re planning to buy a TV, then we\nmight look for filters like brand, price, TV size, etc. In e-commerce websites, users are\npresented with a set of search filters depending on the product. Figures 9-2 and 9-3\nillustrate search in e-commerce through Amazon and Walmart.\nThe left-most section of both images depicts a set of filters (alternatively, “facets”) that\nallows the customer to guide their search in a way that matches their buying needs. In\nFigure 9-2, we see a search for television models, so the filters show aspects such as\nresolution and display size. Along with such custom filters, there are also some gen‐\neral features that are valid for many such product searches, such as brand, price\nrange, and mode of shipping, as shown in Figure 9-3. These filters are explicit dimen‐\nsions to perceive the product. And this guided search enables the user to arrange the\nsearch results on their own to get more control over shopping, rather than having to\nsift through a lot of results to get what they’re looking for.\nFigure 9-2. Faceted search on Amazon.com\n310 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 284,
      "char_count": 1691,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1258,
          "height": 723,
          "ext": "png",
          "size_bytes": 391534
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 341,
      "text": "Figure 9-3. Faceted search on Walmart.com\nThese filters are the key that defines the faceted search. However, they may not always\nbe readily available for all products. Some reasons for that are:\n• The seller didn’t upload all the required information while listing the product on\nthe e-commerce website. This is typically the case when a new e-commerce busi‐\nness ramps up and aggressively promotes quick onboarding of various sellers. To\nachieve this, they often allow the sellers to list without having quality checks in\nplace for the product metadata.\n• Some of the filters are difficult to obtain, or the seller may not have the complete\ninformation to provide—for example, the caloric value of a food product, which\nis typically derived from the nutrient information provided on the product case.\nE-retailers don’t expect this information to be provided by the seller, but it’s cru‐\ncial because it may capture important customer signals that are directly related to\nthe conversation of that product sale.\nFaceted search can be built with most popular search engine backends like Solr and\nElasticsearch. Besides regular text search, different facet attributes are also added to\nthe search query. Elasticsearch’s DSL also comes with a built-in faceted search\ninterface [3].\nSearch in E-Commerce \n| \n311\n",
      "word_count": 209,
      "char_count": 1308,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1442,
          "height": 839,
          "ext": "png",
          "size_bytes": 580196
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 342,
      "text": "In an e-commerce setting, we also need to account for business\nneeds other than relevance in terms of facets and text. For instance,\nproducts that are part of a promotion or sale may be bumped up in\nresults. This can be built by utilizing features like Elasticsearch\nboosting.\nApart from search algorithms, there are many nuances associated with faceted search,\nand we’ll focus on these for the rest of this chapter. The issues mentioned above relate\nto the problem we’ll discuss in the next section: building an e-commerce catalog.\nBuilding an E-Commerce Catalog\nAs we saw earlier in this chapter, building an informative catalog is one of the pri‐\nmary problems in e-commerce. It can be split into several subproblems:\n• Attribute extraction\n• Product categorization and taxonomy creation\n• Product enrichment\n• Product deduplication and matching\nLet’s take a look at each of these in this section.\nAttribute Extraction\nAttributes are properties that define a product. For example, in Figure 9-2, we saw\nbrand, resolution, TV size, etc., as relevant attributes. An accurate display of these\nattributes will provide a complete overview of the product on the e-commerce web‐\nsite so that the customer can make an informed choice. A rich set of attributes relates\ndirectly to the improvement of clicks and click-through rates, which influence the\nproduct’s sale. Figure 9-4 shows an example of a product description obtained by a set\nof filters or attributes.\nAs you can see, attributes like {clothing, color, size} are basically what defines this\nproduct to a customer. Each of these attributes can have multiple values, as shown in\nthe figure. In this example, color takes seven values. However, directly obtaining\nattributes from the sellers for all products is difficult. Moreover, the quality of the\nattributes should be consistent enough to allow a customer to have the correct and\nrelevant information about a product.\n312 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 318,
      "char_count": 1966,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (9.6pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 343,
      "text": "Figure 9-4. Product obtained by a set of filters or attributes\nTraditionally, e-commerce websites employed manual labeling or crowdsourcing\ntechniques to obtain the attributes. This is typically done by third-party companies or\ncrowdsourcing platforms (e.g., Mechanical Turk), where specific questions about\neach product are asked and the crowd workers are expected to answer them. Some‐\ntimes, the questions are framed in a multiple-choice way to restrict the answer into a\nset of values. But generally, it’s expensive and not scalable with the increase in the vol‐\nume of products. That’s where techniques from machine learning step in. This is a\nchallenging task because it requires an understanding of the context of the informa‐\ntion present in the product. For example, look at the two product descriptions shown\nin Figure 9-5.\nPink is a popular brand with younger women. Similarly, pink is a very common color\nof apparel. Hence, in the first case, Pink is a brand name attribute, whereas in the\nother case, pink is just a color. In Figure 9-5, we see that the backpack is from the\nbrand “Pink” with a color of neon red, whereas the sweatshirt is of the color pink.\nCases like these and many more are prevalent and pose a challenging task for a com‐\nputer to solve.\nBuilding an E-Commerce Catalog \n| \n313\n",
      "word_count": 220,
      "char_count": 1311,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 420,
          "height": 423,
          "ext": "png",
          "size_bytes": 104075
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 344,
      "text": "Figure 9-5. Cases where “pink” is the attribute value for two different attributes\nIf we can obtain a set of attributes in some structured data format, then the search\nmechanism can accurately utilize them to retrieve results according to customer\nneeds. The algorithms that extract the attribute information from various product\ndescriptions are generally called attribute extraction algorithms. These algorithms take\na collection of textual data as input and produce the attribute-value pairs as output.\nThere are two types of attribute extraction algorithms: direct and derived.\nDirect attribute extraction algorithms assume the presence of the attribute value in the\ninput text. For example, “Sony XBR49X900E 49-Inch 4K Ultra HD Smart LED TV\n(2017 Model)” contains the brand “Sony.” A brand is typically an attribute that’s\n314 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 132,
      "char_count": 869,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1442,
          "height": 1585,
          "ext": "png",
          "size_bytes": 1876710
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 345,
      "text": "expected to be present in the product title in most cases. On the other hand, derived\nattribute extraction algorithms do not assume that the attribute of interest is present in\nthe input text. They derive that information from the context. Gender is one such\nattribute that is usually not present in the product title, but from the input text, the\nalgorithm can identify if the product is specifically for men or women. Consider the\nproduct description: “YunJey Short Sleeve Round Neck Triple Color Block Stripe T-\nShirt Casual Blouse.” The product is for women, but the gender “female” is not explic‐\nitly mentioned in the product description or title. In this case, the gender has to be\ninferred from the text (for instance, from the product description).\nDirect attribute extraction\nTypically, the direct attribute extraction is modeled as a sequence-to-sequence label‐\ning problem. A sequence labeling model takes a sequence (e.g., of words) as input and\noutputs another sequence of the same length. In Chapter 5, we briefly touched on this\nkind of problem in the notebook on training a named entity recognizer. Following a\nsimilar approach, let’s take a look at how direct attribute extraction algorithms work.\nOur training data will be of the form shown in Figure 9-6, for an example product\ntitled, “The Green Pet Shop Self Cooling Dog Pad.”\nFigure 9-6. Training data format for direct attribute extraction\nHere, what we have to extract is “The Green Pet Shop,” which is indicated by the -\nattribute tags, whereas the rest of it is indicated by an O (Other) tag. Getting labeled\ndata in BIO is crucial for any direct attribute extraction process. We should also have\ndata that represents various categories (e.g., B-Attribute1, B-Attribute2, etc.).\nThere are two broad ways to collect this data. A simpler one would be to use regular\nexpressions on existing text descriptions with brands and attributes and use that data‐\nset. This is akin to weak supervision. We can also get a subset of the data labeled by\nhuman annotators. With such labeled data, a rich set of features needs to be extracted\nto train an ML model. Ideally, the input features should capture the attribute charac‐\nteristics and locational and contextual information. Here’s a list of some of the fea‐\ntures that can capture all three of these aspects. We can develop more complex\nfeatures along similar lines and perform analysis to understand if they’re significant\nin improving performance. Some common features for this task are:\nCharacteristic features\nThese are typically token-based features, such as the letter case of the token,\nlength of the token, and its character composition.\nBuilding an E-Commerce Catalog \n| \n315\n",
      "word_count": 445,
      "char_count": 2704,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1133,
          "height": 122,
          "ext": "png",
          "size_bytes": 12525
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 346,
      "text": "Locational features\nThese features capture the positional aspect of the token in the input sequence,\nsuch as the number of tokens before the given token or the ratio of the token\nposition and the total length of the sequence.\nContextual features\nThese features mostly encode information about the neighboring tokens, such as\nthe identity of the preceding/succeeding token, POS tag of the token, whether the\npreceding token is a conjunction, etc.\nOnce the features are generated and output tags are encoded properly, we get the\nsequence pairs for training the model. At this point, the training process is similar to\nthat of an NER system. Even though the pipeline looks simple and similar to NER\nsystems, there are challenges with these feature-generation schemes and modeling\ntechniques because of domain-specific knowledge. Further, it’s a challenge to obtain\nlarge enough datasets that cover a range of attributes.\nTo deal with such data sparsity and other feature incompleteness issues, some\napproaches suggest the use of a sequence of word embeddings in the input. The input\nsequence will be passed to the model as is, and it’s supposed to predict the output\nsequence. Recent efforts include deep recurrent structures like RNN or LSTM-CRF to\nperform the seq2seq labeling task [4]. We saw how word embeddings and RNNs are\nuseful in NLP in Chapters 3 and 4. This is another example of where such representa‐\ntions can be useful. Figure 9-7 shows an example of how one such DL model [5] per‐\nforms better than the typical ML models.\nFigure 9-7. Characteristic performance improvement in the LSTM framework for\nattribute extraction [5]\nIndirect attribute extraction\nIndirect attributes are attributes that are not directly mentioned in the description.\nThese attributes, however, can be inferred from other direct attributes or the overall\ndescription. For instance, gender- or age-specific words can be inferred from the text.\nA phrase like “Suit for your baby aged 1–5 years” implies that the product is for tod‐\ndlers. Due to the absence of explicit mentions, a sequence labeling approach won’t\nwork.\n316 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 346,
      "char_count": 2146,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 968,
          "height": 172,
          "ext": "png",
          "size_bytes": 23174
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 347,
      "text": "For indirect attribute classification, we use text classification, since instead of extract‐\ning information, we can infer high-level classes (i.e., indirect attributes) from the\noverall input. Recall the example of “YunJae Short Sleeve Round Neck Triple Color\nBlock Stripe T-Shirt Casual Blouse.” For this case, we represent the whole input string\nusing any of the sentence representation methods from Chapter 3. We can also create\nfeatures, such as the presence of class-specific words, character n-grams, and word n-\ngrams. Then, we can train a model to classify the input to an indirect attribute label.\nIn this example here, for the “gender” attribute, we should use men, women, unisex,\nand child as different class labels.\nFor the models that use deep recurrent structures, the amount of\ndata needed is typically much more than what’s needed when less-\ncomplex ML models such as CRF and HMM are used. The more\ndata there is, the better the deep models learn. This is common to\nall DL models, as we saw in earlier chapters, but for e-commerce,\ngetting a large set of well-sampled, annotated data is very expen‐\nsive. Hence, it needs to be taken care of before we to build any\nsophisticated models.\nSo far, we’ve discussed attribute extraction from textual data and the various recent\napproaches that extend this to multimodal attribute extraction, incorporating various\nmodalities such as title, description, image, reviews, etc., about the product [6].\nIn the next sections, we’ll talk about expanding techniques similar to the ones we\napplied to product attributes to other facets of e-commerce and retail.\nProduct Categorization and Taxonomy\nProduct categorization is a process of dividing products into groups. These groups\ncan be defined based on similarity—e.g., products of the same brand or products of\nthe same type can be grouped together. Generally, e-commerce has pre-defined broad\ncategories of products, such as electronics, personal care products, and foods. When a\nnew product arrives, it should be categorized into the taxonomy before it’s put in the\ncatalog. Figure 9-8 shows a taxonomy for the electronics category with a hierarchy of\ngranular subcategories.\nWe can further define successively smaller groups with stricter definitions of prod‐\nucts, such as laptops and tablets inside the computer category. For a more contextual\nexample, this book will have a level category of technical books, while it’s subcatego‐\nries will be related to AI or natural language processing. This task is a lot like the text\nclassification we covered in Chapter 4.\nBuilding an E-Commerce Catalog \n| \n317\n",
      "word_count": 416,
      "char_count": 2613,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 348,
      "text": "Figure 9-8. A typical category hierarchy—taxonomy of a product\nA good taxonomy and properly linked products can be critical because it allows an e-\ncommerce site to:\n• Show products similar to the product searched\n• Provide better recommendations\n• Select appropriate bundles of products for better deals for the customer\n• Replace old products with new ones\n• Show price comparisons of different products in the same category\nThis categorization process is typically manual to start at small scale, but as the vari‐\nety of products increases, it gets harder and harder to process them manually. At\nscale, this categorization is typically posed as a classification task where the algorithm\ntakes information from a variety of sources and applies the classification technique to\nsolve it [7, 8].\nSpecifically, there are cases where algorithms take input as the title or description and\nclassify the product into a suitable category when all the categories are known. This\nagain falls into the typical case of text classification. In this way, the categorization\nprocess can be automated. Once the category is determined, it’s extended directly to\n318 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 190,
      "char_count": 1187,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1260,
          "height": 1043,
          "ext": "png",
          "size_bytes": 82267
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 349,
      "text": "the relevant attribute extraction process that we discussed earlier. It’s logical that a\nproduct will be passed to the attribute extraction process only when its category is\ndiscovered.\nThe accuracy of the algorithm can be improved when both images and text can be\nused to solve the problem. Images can be passed to a convolutional neural network\nfor generating image embedding, and the text sequence can be encoded via LSTM,\nboth of which, in turn, can be concatenated and passed to any classifier for the final\noutput [6].\nBuilding a taxonomy tree is an extensive process. Placing the products at the right\nlevel in the taxonomy can be done via a hierarchical text classification. A hierarchical\ntext classification in context is nothing more than applying classification models in\nhierarchy according to levels in a taxonomy.\nGenerally simple rule-based classification methods are used mainly for the high-level\ncategories. They can use a dictionary-based matching as a start. Subcategories that are\ncomplex and require deeper context to determine the right taxonomic level are dealt\nwith by ML classification techniques such as SVM or decision tree [9]. Figure 9-9\nshows various taxonomy levels for a specific product example.\nFigure 9-9. Taxonomy tree with different levels [9]\nBuilding an E-Commerce Catalog \n| \n319\n",
      "word_count": 209,
      "char_count": 1322,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1080,
          "height": 956,
          "ext": "png",
          "size_bytes": 52649
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 350,
      "text": "For a new e-commerce platform, creating a product taxonomy via product categori‐\nzation can be an insurmountable task. Building rich content requires a huge amount\nof relevant data, manual interventions, and category experts’ domain knowledge. All\nthese can be expensive for a nascent e-commerce platform. However, there are some\nAPIs offered by Semantics3, eBay, and Lucidworks that can help with the process.\nThese APIs typically build on large catalog content of various big retailers and pro‐\nvide the intelligence inside to categorize a product by scanning its unique product\ncode. Small-scale e-commerce can use the power of such cloud APIs for bootstrap‐\nping taxonomy creation and categorization. Figure 9-10 shows a snapshot of one such\nAPI from Semantics3 [10]. Their API helps categorize a product from its name.\nFigure 9-10. Semantics3 terminal snapshot\n320 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 140,
      "char_count": 907,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 720,
          "height": 762,
          "ext": "png",
          "size_bytes": 65958
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 351,
      "text": "Once a significant amount of product information has been gathered, it’s advisable to\nuse custom rule-based systems. Some of these APIs also support user-defined rules,\nas well as product enrichment and deduplication, which we’ll cover in the next\nsections.\nProduct Enrichment\nFor better search and recommendations, it’s important to gather richer product infor‐\nmation. Some potential sources of this information are short and long titles, product\nimages, and product descriptions. But this information is often either incorrect or\nincomplete. For example, a misleading title can hamper the faceted search in an e-\ncommerce platform. Improving a product title will not only improve the click-\nthrough rate in search, but also the conversion rate in terms of product purchase.\nIn the example shown in Figure 9-11, the product title is too long and contains words\nlike iPad, iPhone, and Samsung, which can easily mislead the search. The full title is\n“Stylus Pen LIBERRWAY 10 Pack of Pink Purple Black Green Silver Stylus Universal\nTouch Screen Capacitive Stylus for Kindle Touch ipad iphone 6/6s 6Plus 6s Plus Sam‐\nsung S5 S6 S7 Edge S8 Plus Note.” This text is too complicated even for a human to\nparse and make sense of, let alone a machine. Such cases are ideal for product\nenrichment.\nFigure 9-11. Example of a clumsy product title and an ideal case for product enrichment\nFirst, we’ll go through the problem scenario shown in Figure 9-11. When different\ntaxonomic and enrichment levels are filled, at least to an acceptable threshold (typi‐\ncally defined by the retail platform itself), then we can attempt to make the product\ntitle more expressive and accurate.\nBuilding an E-Commerce Catalog \n| \n321\n",
      "word_count": 277,
      "char_count": 1707,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1442,
          "height": 642,
          "ext": "png",
          "size_bytes": 450246
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 352,
      "text": "The process can start with direct string matching. It’s also necessary to filter out\ntokens that are not part of the product’s attribute values. In the example, the product\nis a stylus, and iPad and iPhone are not part of its attribute values. These tokens are\nmisleading and can affect the quality of faceted search. Hence, such tokens should be\nremoved from the product title, unless they’re important to indicate domain-specific\ncontext for the product.\nIdeally, a pre-defined template for the product titles helps maintain consistency\nacross products. A good approach is to build a template composed of attributes from\nthe taxonomy tree. The product category or type could be the first token in the prod‐\nuct title—e.g., “iPad” or “Macbook.” That will follow lower-level or granular attributes\nfrom the taxonomy tree, such as brand, size, color, etc. So, the combined title would\nbe: “iPad 64GB - Space Grey.” Attributes from the leaf of the taxonomy can be omit‐\nted to keep the product title simple.\nProduct enrichment is typically seen as a larger and more continuous process than\njust improving product titles in any online retail setup. Apart from taxonomic levels,\nthere are other ways to define the enrichment levels. Most of them are based on the\nimportance of the attribute information. [9] has defined these taxonomies, shown in\nFigure 9-12. Mandatory attributes are part of every product, while nice-to-have\nattributes provide a high level of detail that can be missing.\nFigure 9-12. Table showing the categorization of various enrichment levels [9]\nNext, we’ll turn our attention to product duplication and matching.\n322 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 270,
      "char_count": 1674,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 764,
          "height": 301,
          "ext": "png",
          "size_bytes": 24687
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 353,
      "text": "Product Deduplication and Matching\nProducts are often added to the platform by third-party sellers, and different sellers\ncan refer to the same product by different names. They seldom follow the same ter‐\nminology, which can result in the same product getting listed with multiple titles and\nproduct images. For example, “Garmin nuvi 2699LMTHD GPS Device” and “nuvi\n2699LMTHD Automobile Portable GPS Navigator” refer to the same product.\nIn addition to product categorization and attribute extraction, product deduplication\nis also an important aspect of e-commerce. Identifying duplicate products is also a\nchallenging task, and we’ll discuss ways to handle this problem via attribute match,\ntitle match, and image match.\nAttribute match\nIf two products are the same, then the values of various attributes must be the same.\nHence, once the attributes are extracted, we compare values for attributes for both of\nthe products in question. Ideally, maximum overlap of the attributes will indicate\nstrong product matching. In order to match the attribute values, we can use string\nmatching [11]. Two strings can be matched via exact character match or using string\nsimilarity metrics. String similarity metrics are typically built to take care of slight\nspelling mistakes, abbreviations, etc.\nAbbreviations are a big problem in product-related data. The same word can be rep‐\nresented in multiple accepted abbreviations. They should be mapped to a consistent\nform (discussed in “Product Enrichment” on page 321) or form agnostic rules formu‐\nlated to tackle the problem. An intuitive rule to tackle abbreviations while matching\ntwo words could be matching the first and last characters and checking whether those\ncharacters belong to the shorter or longer word.\nTitle match\nOne product can often have multiple title variants. Below are some title variants for\nthe same GPS navigator, sold by different sellers:\n• Garmin nuvi 2699LMTHD GPS Device\n• nuvi 2699LMTHD Automobile Portable GPS Navigator\n• Garmin nuvi 2699LMTHD — GPS navigator — automotive 6.1 in\n• Garmin Nuvi 2699lmthd Gps Device\n• Garmin nuvi 2699LMT HD 6” GPS with Lifetime Maps and HD Traffic\n(010–01188–00)\nTo retrieve all such instances, a matching mechanism is needed to identify them as\nthe same. A simple method could be to compare bigrams and trigrams among these\nBuilding an E-Commerce Catalog \n| \n323\n",
      "word_count": 373,
      "char_count": 2371,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 354,
      "text": "titles. It’s also possible to generate title-level features (such as counts of common\nbigrams and trigrams) and then calculate the Euclidean distance between them. We\ncould use sentence-level embedding and a pair of textual phrases simultaneously to\nlearn a distance metric that improves matching accuracy [12]. This can also be done\nwith a neural network architecture called the Siamese network [13]. The Siamese\nnetwork takes two sequences simultaneously and learns to generate the embeddings\nin such a way that, if the sequences are similar, they appear closer to each other in the\nembedding space, else farther.\nImage match\nFinally, there could still be irregularities (e.g., abbreviations or domain-specific word\nusage) in attributes and titles, which are difficult to align with one another. In those\ncases, product images can serve as rich source information for product matching and\ndeduplication. For image matching, pixel-to-pixel match, feature map matching, or\neven advanced image-matching techniques like Siamese networks are popular [14],\nand when applied in this setting can reduce the amount of product duplication. Most\nof the algorithms are based on the principles of computer vision approaches and\ndepend on image quality and other size-related particulars.\nA/B testing is a good method of measuring the results and effec‐\ntiveness of different algorithms in the e-commerce world. For pro‐\ncedures like attribute extraction, product enrichment and A/B\ntesting different models will lead to an impact on business metrics.\nThese metrics can be direct or indirect sales, click-through rates,\ntime spent on one web page, etc., and an improvement in relevant\nmetrics shows that a model works better.\nIn a practical setting, all these algorithms are used in conjunction, and their results\nare combined to deduplicate the products. In the next few sections, we’ll discuss NLP\nfor analyzing product reviews, which are a fundamental part of any online shopping\nexperience.\nReview Analysis\nReviews are an integral part of any e-commerce portal. They capture direct feedback\nfrom customers about products. It’s important to leverage this abundant information\nand create important signals to send feedback to the e-commerce system so that it can\nuse them to further improve the customer experience. Moreover, reviews can be\nviewed by all customers, and they directly affect the sales of the products. In this sec‐\ntion, we’ll delve deeper into the different facets of review sentiment analysis.\n324 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 392,
      "char_count": 2543,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (9.6pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 355,
      "text": "Sentiment Analysis\nWe covered generic sentiment analysis as a classification task in Chapter 4. But there\nare various nuances when it comes to sentiment analysis for e-commerce reviews.\nFigure 9-13 shows a screenshot of customer reviews of iPhone X on Amazon. Most of\nus are familiar with seeing such aspect-level reviews on e-commerce websites—this is\nwhere you can slice and dice reviews based on aspects and attributes.\nFigure 9-13. Analysis of customer reviews: ratings, keywords, and sentiments\nAs you can see, 67% of the reviews have a rating of five stars (i.e., the highest), and\n22% of the reviews have the lowest rating of one star. It’s important for an e-\ncommerce company to know what leads customers to give bad ratings. To illustrate\nthis point, Figure 9-14 shows two examples of extreme reviews of the same product.\nReview Analysis \n| \n325\n",
      "word_count": 142,
      "char_count": 856,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1118,
          "height": 832,
          "ext": "png",
          "size_bytes": 64916
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 356,
      "text": "Figure 9-14. A positive and a negative review\nCertainly, both of these reviews contain some information about the product, which\ngives the retailer cues about what customers are thinking. Specifically, negative\nreviews are more important to understand. In Figure 9-14, look at the first review\nwhere the customer states that there are issues with phones that are being shipped. It’s\nmostly related to the defective screen, which the retailer should take care of. In\ncontrast, the positive review expresses generic positive sentiment rather than explic‐\nitly pointing out what aspects the user really liked. Hence, it’s crucial to have a full\nunderstanding of the reviews. By nature, they’re in the text and mostly in an unstruc‐\ntured format, full of unforced errors such as spelling mistakes, incorrect sentence\nconstructions, incomplete words, and abbreviations. This makes review analysis even\nmore challenging.\nTypically, a review contains more than one sentence. It’s advisable\nto break a review into sentences and pass each sentence as one data\npoint. This is also relevant for sentence-wise aspect tagging, aspect-\nwise sentiment analysis, etc.\nRatings are considered to be directly proportional to the overall sentiment of the\nreviews. There are cases where the user mistakenly rates the product poorly but gives\na positive review. Understanding emotions directly from the text will help retailers\nrectify these anomalies during analysis. But in most cases, a review doesn’t talk about\njust one aspect of the product but tries to cover most aspects of it, ultimately reflect‐\ning everything in the review rating.\nTake another look at the iPhone X review screenshot in Figure 9-13. Look at the sec‐\ntion where it reads: “Read reviews that mention.” These are nothing but the important\nkeywords Amazon has found may help customers navigate better when skimming\nthrough the reviews. This clearly indicates that there are certain aspects customers are\ntalking about. It could be user experience, manufacturing aspects, price, or something\nelse. How can we know what the customer’s emotions or feedback are? So far, we’ve\nprovided only a high-level index of emotion for the entire review, but that won’t allow\nus to dig down deeper to understand it better. This necessitates an aspect-level under‐\nstanding of the reviews. These aspects could be pre-defined or extracted from the\n326 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 382,
      "char_count": 2424,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 394,
          "height": 514,
          "ext": "png",
          "size_bytes": 7986
        },
        {
          "index": 1,
          "width": 1442,
          "height": 316,
          "ext": "png",
          "size_bytes": 107095
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 357,
      "text": "review data itself. Based on that, the approaches will be supervised or unsupervised\naccordingly.\nAspect-Level Sentiment Analysis\nBefore we start the discussion of various techniques for aspect-level sentiment analy‐\nsis, we need to understand what an aspect is. An aspect is a semantically rich,\nconcept-centric collection of words that indicates certain properties or characteristics\nof the product. For example, in Figure 9-15, we’ll see the kind of aspects a travel web‐\nsite might have: location, value, and cleanliness.\nThis isn’t constrained only to the inherent attributes of the product, but also to any‐\nthing and everything related to the supply, presentation, delivery, return, quality, etc.,\naround the product. Typically, a clear distinguishing of these aspects is difficult unless\nalready assumed.\nIf the retailer has a clear understanding of the product’s aspects, then finding aspects\nfalls under the supervised category of algorithms. There’s a common technique for\nusing seed words or seed lexicons, which essentially hints at the crucial tokens that\ncould be present under a particular aspect. For example, regarding user experience as\nan aspect for iPhone X, seed words could be screen resolution, touch, response time,\netc. Again, it’s up to the retailer at what level of granularity they’d like to operate. For\nexample, screen quality alone could be a more granular aspect. In the next sections,\nwe’ll look at supervised and unsupervised techniques of aspect-level sentiment\nanalysis.\nSupervised approach\nA supervised approach depends mainly on seed words. It tries to identify the pres‐\nence of these seed words in a sentence. If it identifies a particular seed word in a sen‐\ntence, it tags the sentence with the corresponding aspect. Once all the sentences are\ntagged to any of the aspects, the sentiment analysis has to be done at a sentence level.\nNow, since we already have an additional tag for each sentence, sentences having one\ntag can be filtered, and sentiments for them can be aggregated to understand the cus‐\ntomer’s feedback for that aspect. For example, all review sentences related to screen\nquality, touch, and response time can be grouped together.\nFor a change, let’s look at an example from a travel website in Figure 9-15, where the\naspect-level sentiment analysis is apparent. As you see, there are specific ratings for\nlocation, check-in, value, and cleanliness, which are semantic concepts rightfully\nextracted from the data to present a more detailed view of the reviews.\nReview Analysis \n| \n327\n",
      "word_count": 402,
      "char_count": 2546,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 358,
      "text": "Figure 9-15. Aspect-level ratings on reviews given on a travel website\nUnsupervised approach\nAs it’s understood, arranging a good-quality seed lexicon is difficult, so there are\nunsupervised ways of detecting aspects. Topic modeling is a useful technique in iden‐\ntifying latent topics present in a document. We can think of these topics as aspects in\nour case. Imagine if we can group sentences that are talking about the same aspect.\nThat’s exactly what a topic modeling algorithm does. One of the most popular topic\nmodeling approaches is the latent Dirichlet algorithm (LDA). We covered LDA in\nmore detail in Chapter 7.\n328 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 108,
      "char_count": 665,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1344,
          "height": 1554,
          "ext": "png",
          "size_bytes": 199842
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 359,
      "text": "In a similar fashion, we can pre-define the number of aspects we expect out of the set\nof sentences. The topic modeling algorithm also outputs the probability of each word\nto be in all the topics (here, aspects). Hence, it’s also possible to group words that have\na high chance of belonging to a certain aspect and call them characteristic words for\nthat particular aspect. This will ultimately help annotate the unannotated aspects.\nFurther, a more unsupervised approach can be performed by creating sentence repre‐\nsentation and then performing clustering as opposed to LDA. In our experience, the\nlatter sometimes gives better results when there are fewer review sentences. In the\nnext section, we’ll see how we can predict ratings for all of these aspects and provide a\nmore granular view of user preferences.\nConnecting Overall Ratings to Aspects\nWe’ve already seen how we can detect the sentiment for each aspect. Typically, users\nalso give an overall rating. The idea here is to connect that rating to individual aspect-\nlevel sentiment. For this, we use a technique called latent rating regression analysis\n(LARA) [15]. Details of LARA implementation are outside the scope of this book, but\nhere’s an example of the system generating aspect-level ratings for a hotel review. The\ntable shown in Figure 9-16 from [15] gives some details on these aspect-based ratings.\nFigure 9-16. Aspect-wise sentiment prediction using LARA\nReview Analysis \n| \n329\n",
      "word_count": 235,
      "char_count": 1455,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1359,
          "height": 1053,
          "ext": "png",
          "size_bytes": 86772
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 360,
      "text": "We can assume that the final rating is nothing but a weighted combination of individ‐\nual aspect-level sentiments. The objective will be estimating the weights and the\naspect-level sentiment together. It’s also possible to perform these two operations\nsequentially—i.e., first determining the aspect-level sentiment and then the weights.\nThese weights on top of various sentiments present for each aspect will ultimately\nindicate how much importance a reviewer places on that specific topic. It’s possible\nthat a customer is extremely unhappy with some aspect, but maybe that aspect isn’t\ntheir priority. This information is crucial for e-retailers to have before they take any\naction. More details of this implementation are covered in [15].\nUser information is also key in handling reviews. Imagine a sce‐\nnario where a popular user, as opposed to a less-popular user,\nwrites a good review. The user matters! While performing the\nreview analysis, a “user weight” can be defined for all users based\non their ratings (generally given by other peers) and can be used in\nall calculations to discount the reviewer bias.\nWe’ll now go deeper into an example algorithm to understand aspects.\nUnderstanding Aspects\nIt’s a business objective for retailers to analyze a particular aspect of a product and\nhow various sentiments and opinions have been reflected in reviews. Similarly, a user\nmight be interested in a specific aspect of a product and may want to scan through all\nthe reviews on it. Hence, once we derive all the aspects and tag each sentence with\nthem, it’s possible to group the sentences by aspects. But given the huge volume of\nreviews an e-commerce website encounters, there will still be a lot of sentences under\nan aspect. Here, a summarization algorithm may save the day. Think about a situation\nwhere we need to take an action regarding an aspect but we don’t have the capacity to\ngo through all the sentences regarding that particular aspect. We’d need an automatic\nalgorithm that can pick and choose the best representative sentences for that aspect.\nLexRank [16] is an algorithm, similar to PageRank, that assumes each sentence is a\nnode and connects via sentence similarity. Once done, it picks the most central sen‐\ntences out of it and presents an extractive summary of the sentences under an aspect.\nAn example pipeline for review analysis, covering overall and aspect-level sentiments,\nis shown in Figure 9-17.\nIn this pipeline, we start with a set of reviews. After applying review-level aspect\ndetection, we can run sentiment analysis for every aspect as well as aggregate them\nbased on aspects. After aggregation, summarization algorithms such as LexRank can\nbe used to summarize them. In the end, we can take away the overall sentiment for an\naspect of a product as well as get a summary of opinions explaining the sentiment.\n330 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 473,
      "char_count": 2893,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 429,
          "height": 573,
          "ext": "png",
          "size_bytes": 13997
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 361,
      "text": "Figure 9-17. The complete flowchart of review analysis: overall sentiments, aspect-level\nsentiments, and aspect-wise significant reviews\nA complete understanding of a product can only be achieved by\nboth user reviews and editorial reviews. Editorial reviews are gen‐\nerally provided by expert users or domain experts. These reviews\nare more reliable and can be shown at the top of the review section.\nBut on the other hand, general user reviews reveal the true picture\nof the product experience from all users’ perspectives. Hence,\nmelding editorial reviews with general user reviews is important.\nThat may be achieved by mixing both kinds of reviews in the top\nsection and ranking them accordingly.\nWe’ve seen how review analysis can be done from the perspective of aspects, senti‐\nment, and ratings. In the next sections, we’ll briefly cover the nuances of personaliza‐\ntion for e-commerce.\nReview Analysis \n| \n331\n",
      "word_count": 144,
      "char_count": 917,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 394,
          "height": 514,
          "ext": "png",
          "size_bytes": 7986
        },
        {
          "index": 1,
          "width": 978,
          "height": 1220,
          "ext": "png",
          "size_bytes": 59826
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 362,
      "text": "Recommendations for E-Commerce\nIn Chapter 7, we discussed various techniques for recommendations using textual\ndata. Along with product search and review analysis, product recommendation is\nanother main pillar in e-commerce. In Figure 9-18, we show a comprehensive study\non the different algorithms used as well as the data utilization required for recom‐\nmendations in various scenarios [17].\nFigure 9-18. Comprehensive study of techniques for various e-commerce recommenda‐\ntion scenarios\nIn e-commerce, products are recommended based on a user’s purchase profile: fash‐\nionista, book lover, enjoyer of popular products, etc. These purchase profiles can be\ninferred from the user’s behavior on the platform. Imagine a user has interacted with\na set of products in the platform via viewing or clicking or purchasing them. These\n332 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 130,
      "char_count": 870,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1029,
          "height": 1418,
          "ext": "png",
          "size_bytes": 97072
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 363,
      "text": "interactions contain information that can help decide the set of products the user will\nbe interested in next. This can be achieved by neighborhood-based methods where\nwe look for similar products (in terms of attributes, purchase history, customers who\npurchased them, etc.) and provide them in the form of recommendations.\nClicks, purchase history, etc., are mainly numerical data, whereas e-commerce also\nhas a huge amount of textual data that can be utilized in product recommendations.\nAlong with numerical sources, the recommendation algorithm can include product\ndescriptions in text to induce better understanding about those products and provide\nmore similar products that match with even more granular attributes. For example,\nthe clothing material (e.g., 52% cotton, 48% polyester) mentioned in a product\ndescription could be important textual information to consider while looking for\nsimilar apparel.\nRecommendation engines deal with information from various\nsources. Proper matching of various data tables and consistency of\nthe information across various data sources is important to main‐\ntain. For example, while collating the information about product\nattributes and product transaction history, the consistency of the\ninformation should be checked carefully. Complementary and sub‐\nstitute data can give indications about data quality. One should\ncheck for anomalous behavior while working with multifarious\ndata sources, as in the case of e-commerce recommendation.\nReviews contain a lot of nuanced information and user opinions about products,\nwhich can guide product recommendations. Imagine a user providing feedback\nregarding the screen size of a mobile device (e.g., “I would have preferred a smaller\nscreen”). The specific feedback from the user for a specific attribute of the product\ncan provide a strong signal to filter the set of related products to make the recom‐\nmendation more useful to the user. We’ll look at a detailed case study relating to this\nand see how we can potentially build a recommendation system for e-commerce lev‐\neraging product reviews. Reviews are not only useful for finding better products for\nrecommendation but can also reveal the interrelationships between various products\nvia nuanced feedback from customers.\nA Case Study: Substitutes and Complements\nRecommender systems are built on the idea of “similar” products. This similarity can\nbe defined as content based or user profile based. There’s another way of identifying\nitem interrelationships specifically in an e-commerce setting.\nComplements are products that are typically bought together. On the other hand,\nthere are pairs that are bought in lieu of the other, and they’re known as substitute\npairs. Even though the economic definition is much more rigorous, these lines of\nRecommendations for E-Commerce \n| \n333\n",
      "word_count": 424,
      "char_count": 2833,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-Regular (9.6pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 394,
          "height": 514,
          "ext": "png",
          "size_bytes": 7986
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 364,
      "text": "thought typically capture the behavioral aspect of product purchase. Sometimes, due\nto huge disparities in individual user behavior, it’s difficult to infer the interrelation‐\nships between products from them. But in aggregation, these user interactions can\nreveal interesting properties about substitution and complementarity between prod‐\nucts. There are several ways [18] we can identify substitutes and complements using\nuser interaction data, but here, we’ll focus on an approach that relies primarily on the\nreviews as a form of textual information present in the products.\nJulian McAuley has presented [19] a comprehensive way of understanding product\ninterrelationships in a framework where the query product is given and the frame‐\nwork returns the ranked products, both substitutes and complements, (see\nFigure 9-19). We’ll discuss this application as a case study in the context of\ne-commerce.\nFigure 9-19. Substitutes and complements based on product reviews [19]\n334 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 150,
      "char_count": 1017,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1376,
          "height": 1351,
          "ext": "png",
          "size_bytes": 694360
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 365,
      "text": "Latent attribute extraction from reviews\nTypically, as we’ve discussed, reviews contain specific information about product\nattributes. Explicit extraction of attributes from reviews may have limitations in rep‐\nresentation, as we need to define an explicit ontology, so instead, we learn them via a\nlatent vector representation. The details of latent factor models are outside the scope\nof this book, but an interested reader can find the relevant material at [20].\nEach product is associated with a review. One review can discuss or mention various\nopinions regarding aspects related to the product. While these topics are latent and\ncan’t be identified distinctly, we can obtain a distribution of the share of discussion on\nvarious attributes as they’re discussed in the review. This distribution can be modeled\non all the reviews related to that product using popular topic models like LDA [21].\nThis provides a vectorial representation, or “topic vector,” which tells us how a partic‐\nular product has been discussed in reviews. This representation can be thought of as a\nfeature representation (from the usual ML terminology) of the product itself.\nProduct linking\nThe next task is to understand how the two products are linked. We already obtained\ntopic vectors, which capture the intrinsic properties of the product in a latent\nattribute space. Now, given a pair of products, we want to create a combined feature\nvector out of the respective topic vectors for the products and then predict if there’s\nany relationship between them. This can be viewed as a binary classification problem\nwhere the features have to be obtained from the respective topic vectors for the prod‐\nuct pair. We call this process “link prediction,” similar to [22].\nTo ensure that the topic vector is expressive enough to predict a link or relationship\nbetween a product pair, the objectives of obtaining topic vectors and link prediction\ncan be solved jointly rather than one after the other—i.e., we learn topic vectors for\neach product as well as the function to combine them for a product pair.\nFigure 9-20 depicts the interpretation of a topic vector after it’s learned, which is cov‐\nered in detail in [19]. It shows how a topic vector becomes expressive enough to cap‐\nture the intrinsic attributes of the product. Hierarchical dependence also emerges\nfrom such a representation, which in a way depicts the taxonomy that the product\nbelongs to.\nThis case study shows that reviews contain useful information that reveals various\ninterrelationships between products. Such latent representation, which has more\nexpressivity than exact extraction of attributes from reviews, has shown to be efficient\nnot only for the link prediction task, but also for revealing meaningful notions about\nthe product taxonomy. Such representation can be useful for making better product\nrecommendations via better product linking and obtaining more similar products.\nRecommendations for E-Commerce \n| \n335\n",
      "word_count": 469,
      "char_count": 2973,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 366,
      "text": "Figure 9-20. Topic vector and topic hierarchy express how different taxonomic identities\nand relations are captured in reviews [19]\nWrapping Up\nA primary driver behind the e-commerce industry’s immense success has been mas‐\nsive data collection and adaptation of data-driven decisions. NLP techniques have\nplayed a significant role in improving user experience and driving more revenue in e-\ncommerce and retail industries.\nIn this chapter, we covered different aspects of NLP in e-commerce. We started with\nan introduction on faceted search, then delved deep into product attributes. These\nareas are closely linked to product enrichment and categorization. We then covered\nreview analysis and product recommendations for e-commerce. Most of the examples\nand the setting in this chapter are product commerce, but the same techniques can be\nused in other areas as well, such as travel and food. We hope this chapter will be a\ngood starting point for baking NLP and intelligence into your domain.\nReferences\n[1] Clement, J. “Global Retail E-commerce Sales 2014–2023”. Statista, March 19,\n2010.\n[2] Fletcher, Iain. “How to Increase E-commerce Conversion with Site Search”. Search\nand Content Analytics (blog). Last accessed June 15, 2020.\n[3] Elasticsearch DSL. Faceted Search. Last accessed June 15, 2020.\n[4] Huang, Zhiheng, Wei Xu, and Kai Yu. “Bidirectional LSTM-CRF Models for\nSequence Tagging”. 2015.\n[5] Majumder, B. P., Aditya Subramanian, Abhinandan Krishnan, Shreyansh Gandhi,\nand Ajinkya More. “Deep Recurrent Neural Networks for Product Attribute Extrac‐\ntion in eCommerce”. 2018.\n[6] Logan IV, Robert L., Samuel Humeau, and Sameer Singh. “Multimodal Attribute\nExtraction”. 2017.\n336 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 260,
      "char_count": 1730,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1420,
          "height": 271,
          "ext": "png",
          "size_bytes": 67511
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 367,
      "text": "[7] Popescu, Ana-Maria, and Oren Etzioni. “Extracting Product Features and Opin‐\nion from Reviews.” Proceedings of the Conference on Human Language Technology and\nEmpirical Methods in Natural Language Processing (2005): 339–346.\n[8] Wang, Tao, Yi Cai, Ho-fung Leung, Raymond YK Lau, Qing Li, and Huaqing Min.\n“Product Aspect Extraction Supervised with Online Domain Knowledge.” Knowledge-\nBased Systems 71 (2014): 86–100.\n[9] Trietsch, R. C. “Product Attribute Value Classification from Unstructured Text in\nE-Commerce.” (master’s thesis, Eindhoven University of Technology, 2016).\n[10] “Product Classification with AI: How Machine Learning Sped Up Logistics for\nAeropost”. Semantics3 (blog), June 25, 2018.\n[11] Cheatham, Michelle, and Pascal Hitzler. “String Similarity Metrics For Ontology\nAlignment.” International Semantic Web Conference. Berlin: Springer, 2013: 294–309\n[12] Bilenko, Mikhail and Raymond J. Mooney. “Adaptive Duplicate Detection Using\nLearnable String Similarity Measures.” Proceedings of the Ninth ACM SIGKDD Inter‐\nnational Conference on Knowledge Discovery and Data Mining (2003): 39–48.\n[13] Neculoiu, Paul, Maarten Versteegh, and Mihai Rotaru. “Learning Text Similarity\nwith Siamese Recurrent Networks.” Proceedings of the First Workshop on Representa‐\ntion Learning for NLP (2016): 148–157.\n[14] Zagoruyko, Sergey and Nikos Komodakis. “Learning to Compare Image Patches\nvia Convolutional Neural Networks.” Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (2015): 4353–4361.\n[15] Wang, Hongning, Yue Lu, and Chengxiang Zhai. “Latent Aspect Rating Analysis\non Review Text Data: A Rating Regressions Approach.” Proceedings of the 16th ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining (2010):\n783–792.\n[16] Erkan, Günes and Dragomir R. Radev. “LexRank: Graph-Based Lexical Central‐\nity as Salience in Text Summarization.” Journal of Artificial Intelligence Research 22\n(2004): 457–479.\n[17] Sarwar, Badrul, George Karypis, Joseph Konstan, and John Riedl. “Analysis of\nRecommendation Algorithms for E-Commerce.” Proceedings of the 2nd ACM Confer‐\nence on Electronic Commerce (2000): 158–167.\n[18] Misra, Subhasish, Arunita Das, Bodhisattwa Majumder, and Amlan Das. “System\nfor calculating competitive interrelationships in item-pairs.” US Patent Application\n15/834,054, filed April 25, 2019.\n[19] McAuley, Julian, Rahul Pandey, and Jure Leskovec. “Inferring Networks of Sub‐\nstitutable and Complementary Products.” Proceedings of the 21th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining (2015): 785–794.\nWrapping Up \n| \n337\n",
      "word_count": 356,
      "char_count": 2629,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 368,
      "text": "[20] McAuley, Julian and Jure Leskovec. “Hidden Factor and Hidden Topics: Under‐\nstanding Rating Dimensions with Review Text.” Proceedings of the 7th ACM Confer‐\nence on Recommender Systems (2013): 165–172.\n[21] Blei, David M., Andrew Y. Ng, and Michael I. Jordan. “Latent Dirichlet Alloca‐\ntion.” Journal of Machine Learning Research 3 (2003): 993–1022.\n[22] Menon, Aditya Krishna and Charles Elkan. “Link Prediction via Matrix Factori‐\nzation.” Joint European Conference on Machine Learning and Knowledge Discovery in\nDatabases. Berlin: Springer, 2011: 437–452\n338 \n| \nChapter 9: E-Commerce and Retail\n",
      "word_count": 88,
      "char_count": 604,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 369,
      "text": "CHAPTER 10\nHealthcare, Finance, and Law\nSoftware is eating the world,\nbut AI is going to eat software.\n—Jensen Huang, Nvidia CEO\nNLP is affecting and improving all major industries and sectors. In the last two chap‐\nters, we covered how NLP is being utilized in the e-commerce, retail, and social\nmedia sectors. In this chapter, we’ll cover three major industries where the impact of\nNLP is rapidly increasing to have a substantial influence on the global economy:\nhealthcare, finance, and law. We’ve chosen these areas to demonstrate a wide range of\nproblems, solutions, and challenges you might face in your organization.\nThe term healthcare encompasses all goods and services for maintenance and\nimprovement of health and well-being. It’s estimated to be worth over 10 trillion dol‐\nlars as a market globally and accounts for tens of millions of people in the workforce\n[1]. The financial industry is one of the bedrocks of modern civilization and is esti‐\nmated to be worth over 26.5 trillion dollars. The legal services industry is estimated to\nbe worth over 850 billion dollars annually and is projected to cross a trillion dollars\nby 2021.\nIn this first section, we’ll start with an overview of the healthcare industry. Then we’ll\ncover broad applications in the healthcare landscape, along with a detailed discussion\nof specific use cases.\nHealthcare\nHealthcare as an industry encompasses both goods (i.e., medicines and equipment)\nand services (consultation or diagnostic testing) for curative, preventive, palliative,\nand rehabilitative care.\n339\n",
      "word_count": 247,
      "char_count": 1557,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (9.3pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 370,
      "text": "Curative care is provided to cure a patient suffering from a curable\ndisease, and preventative care is meant to prevent one from falling\nsick. Rehabilitative care helps patients recuperate from illness and\nincludes activities like physical therapy. Palliative care focuses on\nimproving the quality of life for patients suffering from terminal\nconditions.\nFor most advanced economies, healthcare accounts for a substantial part of the gross\ndomestic product, often exceeding 10%. Being such a large segment, there are mas‐\nsive benefits to automating and optimizing these processes and systems, and that’s\nwhere NLP comes in. Figure 10-1 from Chilmark Research [2] shows a range of\napplications where NLP helps. Each column shows the broad area, like clinical\nresearch or revenue cycle management. The blue cells show the applications that are\nused currently, the purple cells are applications that are emerging and being tested,\nand the red cells are more next generation and will be practically applicable in a\nlonger time horizon.\nFigure 10-1. NLP in healthcare use cases by Chilmark Research [2]\nHealthcare deals with large amounts of unstructured text, and NLP can be used in\nsuch places to improve health outcomes. Broad areas where NLP can help include but\nare not limited to analyzing medical records, billing, and ensuring drug safety. In the\nnext sections we’ll briefly cover some of these applications.\n340 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 230,
      "char_count": 1462,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 394,
          "height": 514,
          "ext": "png",
          "size_bytes": 7986
        },
        {
          "index": 1,
          "width": 1403,
          "height": 805,
          "ext": "png",
          "size_bytes": 68192
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 371,
      "text": "Health and Medical Records\nA large proportion of health and medical data is often collected and stored in\nunstructured text formats. This includes medical notes, prescriptions, and audio tran‐\nscripts, as well as pathology and radiology reports. An example of such a record is\nshown in Figure 10-2.\nFigure 10-2. An example of an electronic medical record [3]\nThis makes the data hard to search, organize, study, and understand in its raw form.\nThis is exacerbated by a lack of standardization in how the data is stored. NLP can\nhelp doctors search and analyze this data better and even automate some of the work‐\nflows, such as by building automated question-answering systems to decrease time to\nlook up relevant patient information. We’ll cover some of these in detail later in the\nchapter.\nHealthcare \n| \n341\n",
      "word_count": 135,
      "char_count": 812,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1023,
          "height": 737,
          "ext": "png",
          "size_bytes": 698629
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 372,
      "text": "Patient Prioritization and Billing\nNLP techniques can be used on physician notes to understand their state and urgency\nto prioritize various health procedures and checkups. This can minimize delays and\nadministrative errors and automate processes. Similarly, parsing and extracting infor‐\nmation from unstructured notes to identify medical codes can facilitate billing.\nPharmacovigilance\nPharmacovigilance entails all activities that are needed to ensure that a drug is safe.\nThis involves collection and detection and monitoring of adverse drug or medication\nreactions. A medical procedure or drug can have unintended or noxious effects, and\nmonitoring and preventing these effects is essential to making sure the drug acts as\nintended. With increasing use of social media, more of such side effects are being\nmentioned in social media messages; monitoring and identifying these is part of the\nsolution. We covered some of these techniques in Chapter 8, which focused on\ngeneric social media analysis. We’ll also cover some social media–specific cases later\nin this chapter. Besides social media, NLP techniques applied to medical records also\nfacilitate pharmacovigilance.\nClinical Decision Support Systems\nDecision support systems assist medical workers in making healthcare-related deci‐\nsions. These include screening, diagnosis, treatments, and monitoring. Various text\ndata can be used as an input to these systems, including electronic health records,\ncolumn-tabulated laboratory results, and operative notes. NLP is utilized on all of\nthese to improve the decision support systems.\nHealth Assistants\nHealth assistants and chatbots can improve the patient and caregiver experiences by\nusing various aspects of expert systems and NLP. For instance, services like Woebot\n[4] (Figure 10-3) can keep the spirits of patients suffering from mental illness and\ndepression high. Woebot combines NLP with cognitive therapy to do this by asking\nrelevant questions reinforcing positive thoughts.\n342 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 296,
      "char_count": 2042,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 373,
      "text": "Figure 10-3. A Woebot conversation\nSimilarly, assistants can assess patients’ symptoms to diagnose potential medical\nissues. Depending on the urgency and critical nature of the diagnoses, chatbots can\nbook appointments with relevant doctors. One example of such a system is Buoy [5].\nThese systems can also be built based on the user’s specific needs by utilizing existing\ndiagnostic frameworks. One example of such a framework is Infermedica [6]\n(Figure 10-4), where a chat interface can elicit symptoms from the user as well as give\na list of possible ailments with their probability.\nHealthcare \n| \n343\n",
      "word_count": 95,
      "char_count": 606,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 350,
          "height": 623,
          "ext": "png",
          "size_bytes": 90478
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 374,
      "text": "Figure 10-4. Diagnosis chatbot made by Infermedica API\nIn the next sections, we’ll cover some of these applications in more detail.\nElectronic Health Records\nIncreased adoption of storing clinical and healthcare data electronically has led to an\nexplosion of medical data and overwhelmingly large personal records. With this\nincreasing adoption and larger document size and history, it’s getting harder for doc‐\ntors and clinical staff to access this data, leading to an information overload. This, in\nturn, leads to more errors, omissions, and delays and affects patient safety.\nIn the next few sections, we’ll broadly cover how NLP can help manage this overload\nand improve patient outcomes. In this section, we’ll deal with electronic health\nrecords (EHRs).\nHARVEST: Longitudinal report understanding\nVarious tools have been built to overcome the informational overload we mentioned\nearlier. A notable effort is called HARVEST [7] from Columbia University. The tool\nhas been used extensively across hospitals in New York City. To start with, however,\nwe need to cover how a standard clinical information system works.\nFigure 10-5 shows a screenshot of a standard clinical information review system that’s\nused at New York Presbyterian Hospital (iNYP). iNYP delivers text-heavy, dense,\ntime-consuming, and generally unwieldy reports. There’s an option for basic text\n344 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 214,
      "char_count": 1418,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1441,
          "height": 862,
          "ext": "png",
          "size_bytes": 373282
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 375,
      "text": "search, but the text-heavy information lends itself to being skimmed over, which is an\nimpediment in the context of a busy, minute-to-minute hospital environment.\nFigure 10-5. Screenshot of the standard clinical information review system at New York\nPresbyterian Hospital\nIn contrast, HARVEST parses all of the medical data to make it easy to analyze and\ncan sit on top of any medical system. Figure 10-6 demonstrates how HARVEST is\nused on the iNYP system, showing the revamped and evolved visual depiction of the\nformerly text-heavy reporting format.\nHealthcare \n| \n345\n",
      "word_count": 90,
      "char_count": 572,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 1210,
          "ext": "png",
          "size_bytes": 942759
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 376,
      "text": "Figure 10-6. HARVEST system [7] for the same patient from Figure 10-5\nWe can see a timeline of each visit to the clinic or hospital. It’s accompanied by a\nword cloud of important medical conditions for the patient in the given time range.\nThe user can drill down to detailed notes and history if needed as well. All of this is\nalso supported by summaries of each report so a user can get the gist of a patient’s\nmedical history quickly. HARVEST is much more than a reformatted novelty—it’s\nextremely useful for giving not just doctors, but also general medical staff and care‐\ngivers, a near real-time, informative snapshot of what’s going on with a patient.\n346 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 124,
      "char_count": 708,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1439,
          "height": 1300,
          "ext": "png",
          "size_bytes": 1143964
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 377,
      "text": "All historical observations (from doctors, nurses, nutritionists, etc.) related to that\npatient are run through a named entity recognizer called HealthTermFinder. This\nfinds all healthcare-related terms, which are then mapped to the Unified Medical\nLanguage System (UMLS) semantic group. These terms are visualized in the word\ncloud. Word cloud weights are determined by TF-IDF, which we covered in detail in\nChapter 7. Also, the larger to smaller font sizes indicate the degree and frequency of\nthe various issues a patient has been carrying. This visual pattern can also drive the\nidentification and exploration of issues that otherwise might not have been\nconsidered.\nHARVEST is able to depict a patient’s medical history across a period of time, how‐\never long that might be, in a much more effective and easy-to-comprehend fashion.\nWhat becomes more valuable in such instances is that it helps with the analytical\ncapability of the medical professional to home in on root issues and not get caught up\nin merely treating symptoms or biased misdiagnoses. A study was conducted where\nthe HARVEST system was tested by medical practitioners at New York Presbyterian\nHospital. In this test, more than 75% of participants said they would definitely use\nHARVEST regularly in the future, despite it being a completely new user interface,\nwhile the rest also showed some leaning toward using the system. Figure 10-7 shows\na snapshot of some of the feedback provided by these practitioners at the time.\nHARVEST delivers understandable summaries and conclusions by collating a\npatient’s history of healthcare issues across their lifetime. Its unique selling point is\nthat it can mine, extract, and visually present content at a macro level—based on\ndetailed micro-level observations—irrespective of where and by whom in the hospital\na patient might have been seen. Such systems can be built to visualize and analyze a\nlarge amount of information. When the underlying knowledge base is unstructured\ntext, as it is in the case of EHRs, NLP techniques play a key role in such analytics and\ninformation visualization tools.\nHealthcare \n| \n347\n",
      "word_count": 338,
      "char_count": 2132,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 378,
      "text": "Figure 10-7. Clinical feedback on HARVEST at New York Presbyterian Hospital [7]\n348 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 20,
      "char_count": 129,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1318,
          "height": 1648,
          "ext": "png",
          "size_bytes": 177720
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 379,
      "text": "Question answering for health\nIn the last section, we looked at how basic NLP techniques like NER can be used to\nimprove the user’s experience with handling records and information at scale. But to\ntake the user experience to the next level, we can consider building a question-\nanswering (QA) system on top of these records.\nWe’ve covered question-answering systems in Chapter 7, but our focus here is on the\nnuances of questions that arise specifically in healthcare scenarios. For example, these\nquestions can include:\n• What dosage of a particular medicine is a patient required to take?\n• For what ailment is a particular medication taken?\n• What were the results of a medical test?\n• By how much was the result of a medical test out of range for a given test date?\n• What lab test confirmed a particular disease?\nAs we’ve discussed throughout the book, building the right dataset for a particular\ntask is often the key to solving any NLP problem. For the particular problem of the\nQA system in the healthcare domain, we’ll focus on a dataset known as emrQA,\nwhich was created by a joint collaboration between IBM Research Center, MIT, and\nUIUC [8, 9]. Figure 10-8 shows an example of what such a dataset entails. For\ninstance, for the question, “Has the patient ever had an abnormal BMI?”, a correct\nanswer is extracted from past health records.\nFigure 10-8. Example of a question-answer pair in emrQA\nHealthcare \n| \n349\n",
      "word_count": 247,
      "char_count": 1427,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 968,
          "height": 622,
          "ext": "png",
          "size_bytes": 80589
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 380,
      "text": "To create such datasets of questions and answers and build a QA system on them, a\ngeneral question-answering dataset creation framework consists of:\n1. Collecting domain-specific questions and then normalizing them. For instance, a\npatient’s treatment can be asked about in multiple ways, like, “How was the prob‐\nlem managed?” or “What was done to correct the patient’s problem?” These all\nhave to be normalized in the same logical form.\n2. Question templates are mapped with expert domain knowledge and logical forms\nare assigned to them. The question template is an abstract question. For example,\nfor a certain type of question, we expect a number or a medication type as a\nresponse. More concretely, a question template is “What is the dosage of medica‐\ntion?”, which then maps to an exact question, like, “What is the dosage of Nitro‐\nglycerin?” This question is of a logical form that expects a dosage as response.\nWe’ll see this in more detail in Figure 10-9.\n3. Existing annotations and the information collected in (1) and (2) are used to cre‐\nate a range of question-and-answer pairs. Here, already available information like\nNE tags as well as answer types linked to the logical form are used to bootstrap\ndata. This step is especially relevant, as it reduces the manual effort needed in the\ncreation of the QA dataset.\nMore specifically for emrQA, this process involved polling physicians at the Veterans\nAdministration to gather prototypical questions, which led to over 2,000 noisy tem‐\nplates that were normalized to around 600. These prototypical questions were then\nlogically mapped to an i2b2 dataset [10]. i2b2 datasets are already expertly annotated\nwith a range of fine-grained information like medication concepts, relations, asser‐\ntions, coreference resolution, etc. Although they’re not made explicitly for QA pur‐\nposes, by using logical mapping and existing annotations, questions and answers are\ngenerated out of them. A high-level overview of this process is shown in Figure 10-9.\nThis process is closely supervised by a set of physicians to ensure the quality of the\ndataset.\nTo build a baseline QA system, neural seq-to-seq models and heuristic-based models\nwere used. These models are covered in more detail in the emrQA team’s work. To\nevaluate these models, they divided the dataset into two sets: emrQL-1 and emrQL-2.\nemrQL-1 had more diversity in vocabulary in test and training data. Heuristic models\nperformed better than neural models for emrQL-1, while neural models did better for\nemrQL-2.\nMore broadly, this is an interesting use case on how to build complex datasets using\nheuristics, mapping, and other simpler annotated datasets. These learnings can be\napplied to a range of other problems, beyond processing health records, that require\ngeneration of a QA-like dataset. Now, we’ll cover how health records can be used to\npredict health outcomes.\n350 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 472,
      "char_count": 2942,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 381,
      "text": "Figure 10-9. QA dataset generation using existing annotations\nOutcome prediction and best practices\nWe’ve seen how NLP can aid exploration and how doctors can ask questions from\npatient health records. Here, we’ll cover a more cutting-edge application using health\nrecords: predicting health outcomes. Health outcomes are a set of attributes that\nexplain the consequences of a disease for a patient. They include how fast and how\ncompletely a patient recovers. They’re also important in measuring efficacies of dif‐\nferent treatments. This work is a joint collaboration between Google AI, Stanford\nMedicine, and UCSF [11].\nBesides predicting health outcomes, another focus of scalable and accurate deep\nlearning with electronic health records is to ensure that we can build models and sys‐\ntems that can be both scalable as well as highly accurate. Scalability is necessary, as\nhealthcare has a diverse set of inputs—data collected from one hospital or depart‐\nment can be different from another. So it should be simple to train the system for a\ndifferent outcome or different hospital. It’s necessary to be accurate in order not to\nraise too many false alarms; the need for accuracy is obvious in the healthcare indus‐\ntry, where people’s lives are on the line.\nAs simple as EHRs might sound, they are far from it; there are a lot of nuances and\ncomplexity attached to them. Even something as simple as body temperature can\nhave a range of diagnoses depending on whether it was taken via tongue, forehead, or\nother body parts. To handle all these cases, an open Fast Healthcare Interoperability\nResources (FHIR) standard was created, which used a standardized format with\nunique locators for consistency and reliability.\nOnce the data is in a consistent format, it’s fed into a model based on RNNs. All his‐\ntorical data is fed from the start of the record to its end. The output variable is the\noutcome we’re looking to predict.\nHealthcare \n| \n351\n",
      "word_count": 321,
      "char_count": 1950,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1431,
          "height": 624,
          "ext": "png",
          "size_bytes": 80005
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 382,
      "text": "The model was evaluated on a range of health outcomes. It achieved an AUC score\n(or area under the curve) of 0.86 on whether the patients would stay longer in the\nhospital, 0.77 on unexpected readmissions, and 0.95 on predicting patient mortality.\nAn AUC score [12] is a measure used often in such cases because AUC is a summary\nmeasure of performance across all potential diagnostic thresholds for positivity,\nrather than performance at any specific threshold [13]. A score of 1.0 indicates per‐\nfect accuracy, while 0.5 is the same as a random chance.\nIt’s important in healthcare that models are interpretable. In other words, they should\npinpoint why they suggested a particular outcome. Without interpretability, it’s hard\nfor doctors to accommodate the results in their diagnosis. To achieve this, attention, a\nconcept in deep learning, is used to understand what data points and incidents are\nmost important for an outcome. An example of this attention map can be seen in\nFigure 10-10.\nFigure 10-10. An example of attention applied to a health record\nThis Google AI team also came up with some of the best practices one should keep in\nmind while building ML models for healthcare, outlining ideas in all parts of the\nmachine learning life cycle, from defining the problem and collecting data to validat‐\ning the results. These suggestions are relevant to NLP and computer vision as well as\nstructured data problems. The reader can peruse them in detail in [14].\n352 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 253,
      "char_count": 1518,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1441,
          "height": 1035,
          "ext": "png",
          "size_bytes": 718480
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 383,
      "text": "These techniques focus mostly on managing the physical well-being of humans,\nwhich is relatively easy to quantify because there is a variety of numerical measures\navailable, but there are no obvious quantifiable measures for a person’s mental well-\nbeing. Let’s look at some techniques for monitoring a person’s mental health.\nThe following section includes discussions of mental health issues\nand suicide.\nMental Healthcare Monitoring\nGiven the fast-moving pace of economic and technological change and the fast pace\nof life in today’s world, it’s no surprise that most people, particularly in generations X,\nY, and Z, tend to experience some form of mental health issue in their lifetimes. By\nsome estimates, over 790 million people are affected by mental health–related issues\nglobally, which translates to more than 1 in every 10 people [15]. A study by the\nNational Institutes of Health estimated that one in four Americans are likely to be\naffected by one or more mental health conditions in a given year. Over 47,000\nAmericans committed suicide in 2017, and this number has been increasing at a\nrapid pace [16].\nWith social media usage at an all-time high, it’s increasingly possible to use signals\nfrom social media to track the emotional state and mental balance of both particular\nindividuals and across groups of individuals. It should also be possible to gain\ninsights into these aspects across various demographic groups, including age and\ngender. In this section, we’ll briefly cover an exploratory analysis [17] on public data\nfrom Twitter users and how techniques learned in Chapter 9 can be applied to this\nproblem.\nThere are innumerable aspects to evaluating an individual’s mental well-being. The\nstudy by Glen Coppersmith et al. focuses, as an illustrative example, on utilizing\nsocial media in identifying individuals who are at risk for suicide. The goal of the\nstudy was to develop an early warning system along with identifying the root causes\nof the issues.\nIn this study, 554 users were identified and evaluated who stated that they attempted\nto take their lives. 312 of these users gave an explicit indication of their latest suicide\nattempt. Profiles that were marked as private were not included in this study. They\nonly examined public data, which does not include any direct messages or deleted\nposts.\nHealthcare \n| \n353\n",
      "word_count": 379,
      "char_count": 2352,
      "fonts": [
        "MinionPro-Regular (9.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 503,
          "height": 479,
          "ext": "png",
          "size_bytes": 10854
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 384,
      "text": "Each user’s tweets were analyzed with the following perspectives:\n• Is the user’s statement of attempting to take their life apparently genuine?\n• Is the user is speaking about their own suicide attempt?\n• Is the suicide attempt localizable in time?\nSee Figure 10-11 for a few example tweets.\nFigure 10-11. Nuances of building a social dataset\nThe first two tweets refer to genuine suicide attempts, while the bottom two are sar‐\ncastic or false statements. The middle two are examples where an explicit suicide\nattempt date is mentioned.\nIn order to analyze the data, the following steps were followed:\n1. Pre-processing: Because Twitter data is often noisy, it was normalized and cleaned\nfirst. URLs and usernames are represented with homogenous tokens. We covered\nvarious aspects of cleaning social media data in detail in Chapter 9.\n2. Character models: Character n-gram–based models followed by logistic regres‐\nsion were used to classify various tweets. Performance was measured with 10-\nfold cross validation.\n3. Emotional states: To estimate emotional content in tweets, a dataset was boot‐\nstrapped using hashtags. For instance, all tweets containing #anger but not con‐\ntaining #sarcasm and #jk were put into an emotional label. Tweets with no\nemotional content were also classified as No Emotion.\nThese models were then tested on how well they could flag potential suicide risks.\nThey were able to identify 70% of people who were very likely to attempt suicide,\nwith only 10% false alarms. Figure 10-12 shows a confusion matrix detailing misclas‐\nsification of various emotions that were modeled.\nIdentifying potential mental health issues can be used to intervene in flagged cases.\nWith accurate monitoring and alerting, NLP bots like Woebot can also be used to ele‐\nvate the moods of folks at higher risk. In the next section, we’ll dig deeper into\nextracting entities from medical data.\n354 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 313,
      "char_count": 1950,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1012,
          "height": 222,
          "ext": "png",
          "size_bytes": 48960
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 385,
      "text": "Figure 10-12. Confusion matrix for emotion classification\nMedical Information Extraction and Analysis\nWe’ve seen a range of applications built on health records and information. If we were\nto start building applications using health records, one of the first steps would be to\nextract medical entities and relations from it. Medical information extraction (IE)\nhelps to identify clinical syndromes, medical conditions, medication, dosage,\nstrength, and common biomedical concepts from health records, radiology reports,\nand discharge summaries, as well as nursing documentation and medical education\ndocuments. We can use both cloud APIs and pre-built models for it.\nFirst, we’ll start with understanding Amazon Comprehend Medical [18]. It’s a part of\na larger suite by AWS, Amazon Comprehend, that allows us to do popular NLP tasks\nlike keyphrase extraction, and sentiment and syntax analysis, as well as language and\nentity recognition in the cloud. Amazon Comprehend Medical helps process medical\ndata, including medical named entity and relationship extraction and medical ontol‐\nogy linking.\nHealthcare \n| \n355\n",
      "word_count": 163,
      "char_count": 1116,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1353,
          "height": 1121,
          "ext": "png",
          "size_bytes": 56973
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 386,
      "text": "We can use Amazon Comprehend Medical as a cloud API on our medical text. We\ncover the cloud API in detail in this chapter’s notebooks, but here, we’ll give a short\noverview of how they function. To start, we’ll take health records from FHIR as an\ninput [19]. As a reminder, FHIR is a standard that describes how healthcare informa‐\ntion is documented and shared across the United States. We’ll take a sample elec‐\ntronic health record from a hypothetical Good Health Clinic [20]. To robustly test\nComprehend Medical, we’ll also remove all formatting and line breaks from it to see\nhow well the system can do on this. As a starting input, let’s consider a small sequence\nof this medical record:\nGood Health Clinic Consultation Note Robert Dolin MD Robert Dolin MD Good Health\nClinic Henry Levin the 7th Robert Dolin MD History of Present Illness Henry \nLevin, the 7th is a 67 year old male referred for further asthma management.\nOnset of asthma in his twenties teens. He was hospitalized twice last year, and\nalready twice this year. He has not been able to be weaned off steroids for the \npast several months. Past Medical History Asthma Hypertension (see HTN.cda for \ndetails) Osteoarthritis, right knee Medications Theodur 200mg BID Proventil \ninhaler 2puffs QID PRN Prednisone 20mg qd HCTZ 25mg qd Theodur 200mg BID\nProventil inhaler 2puffs QID PRN Prednisone 20mg qd HCTZ 25mg qd\nWhen we provide this as an input to Comprehend Medical, we get the output shown\nin Figure 10-13.\nFigure 10-13. Comprehend Medical output for the FHIR record example\n356 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 272,
      "char_count": 1599,
      "fonts": [
        "UbuntuMono-Regular (8.5pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1438,
          "height": 779,
          "ext": "png",
          "size_bytes": 247310
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 387,
      "text": "As we can see, we were able to extract everything, from clinic and doctor details to\ndiagnosis and medications, as well as their frequency, dosage, and route. If we need\nto, we can also link extracted information to standard medical ontologies such as\nICD-10-CM or RxNorm. Access to all Comprehend Medical features is through an\nAWS boto library, which we cover in more detail in the notebooks for Chapter 10.\nCloud APIs and libraries can be a good starting point for building medical informa‐\ntion extraction, but if we have specific requirements and prefer to build our own sys‐\ntem, we recommend BioBERT as a starting point. We’ve covered BERT, Bidirectional\nEncoder Representations, throughout the book. However, the default BERT model is\ntrained on regular web text, which is very different from medical text and records.\nFor instance, the different word distributions vary substantially between regular\nEnglish and medical records. This affects the performance of BERT in medical tasks.\nIn order to build better models for biomedical data, BERT for Biomedical Text (Bio‐\nBERT) was created [21]. It adapts BERT to biomedical texts to get better perfor‐\nmance. In the domain adaptation phase, we initialize the model weights with a\nstandard BERT model and pre-trained biomedical texts, including texts from\nPubMed, a search engine for medical results. Figure 10-14 shows the process of pre-\ntraining and fine-tuning BioBERT.\nThis model and weights were open sourced and can be found on GitHub [22, 23].\nBioBERT can be fine-tuned on a range of specific medical problems like medical\nnamed entity recognition and relation extraction. It has also been applied to\nquestion-answering on healthcare texts. BioBERT obtains significantly higher perfor‐\nmance than BERT and other state-of-the-art techniques. It can also be adapted\ndepending on the medical task and dataset.\nWe’ve discussed a range of healthcare applications where NLP can help. We covered\ndifferent facets of applications that can be built on health records and learned how\nsocial media monitoring can be applied to mental health issues. At the end, we saw\nhow to lay the foundations of our healthcare application. Now, we’ll delve into the\nworld of finance and law and see how NLP helps.\nHealthcare \n| \n357\n",
      "word_count": 365,
      "char_count": 2271,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 388,
      "text": "Figure 10-14. BioBERT pre-training and fine-tuning\nFinance and Law\nFinance is a diverse area that encompasses a wide spectrum, from public company\nmonitoring to investment banking deal flow. Globally, the financial services industry\nis expected to grow to 26 trillion USD by 2022 [24]. As finance and law are more\ninterrelated, we’ll cover them in the same section. In the context of integrating and\nutilizing NLP in the context of finance frameworks, operations, reporting, and evalu‐\nation, we can look at finance from the following three angles:\nOrganization perspectives\nDifferent organization types have different requirements and perspectives that\nneed to be taken into account. These perspectives include:\n• Private companies\n• Public companies\n358 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 121,
      "char_count": 801,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1068,
          "height": 1270,
          "ext": "png",
          "size_bytes": 170646
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 389,
      "text": "• Non-profit enterprises\n• Governmental organizations\nActions\nThere are different actions that an organization can take, including:\n• Allocating and reallocating funds\n• Accounting and auditing, which includes identifying anomalies and outliers\nto investigate for both value and risk\n• Prioritization and resource planning\n• Compliance with legal and policy norms\nFinancial context\nThese actions can have various contexts, including:\n• Forecasting and budgeting\n• Retail banking\n• Investment banking\n• Stock market operations\n• Cryptocurrency operations\nTo make real-time, thoughtful, planned decisions around structuring, viewing, man‐\naging, and reporting financial flows, there must be a constant focus on the changing\nnature of the company, and the financial infrastructure must be built and designed\naccordingly. ML and NLP can help design such a system. Figure 10-15 [25] shows\nhow UK bankers think ML and NLP can improve their operations and in what areas.\nFigure 10-15. Estimated ML benefits survey in the UK [25]\nFinance and Law \n| \n359\n",
      "word_count": 158,
      "char_count": 1046,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1397,
          "height": 599,
          "ext": "png",
          "size_bytes": 45594
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 390,
      "text": "They estimate large improvements in operational efficiency as well as analytics\ninsights. With the application of ML and NLP, anti-fraud and anti–money laundering\nefforts are also expected to yield better benefits.\nNLP Applications in Finance\nIn this section, we’ll cover some specific applications of NLP in finance, including\nloan risk assessments, auditing and accounting problems, and financial sentiment\nanalysis.\nFinancial sentiment\nStock market trading relies on a set of information about specific companies. This\nknowledge helps create a set of actions that determine whether to buy, hold, or sell off\nstock. This analysis can be based on companies’ quarterly financial reports or on what\nanalysts are commenting about the companies in their reports. This can also come\nfrom social media.\nSocial media analysis, which we covered in detail in Chapter 8, helps in monitoring\nsocial media posts and pointing out potential opportunities for trading. For instance,\nif a CEO is resigning, that sentiment is often negative, which can negatively affect the\ncompany’s stock price. On the other hand, if the CEO is not performing well and\nmarkets welcome their resignation, that could lead to an increase in stock price.\nExamples of companies that provide this information for trading include DataMinr\nand Bloomberg. Figure 10-16 shows the DataMinr terminal, where alerts and\nmarketing-affecting news related to Dell is surfaced to the user.\nFigure 10-16. Dataminr social terminal\n360 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 234,
      "char_count": 1529,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 629,
          "height": 321,
          "ext": "png",
          "size_bytes": 113379
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 391,
      "text": "Financial sentiment analysis is different from regular sentiment analysis. It’s not just\ndifferent in domain, but also in purpose. Generally, the purpose is to guess how the\nmarkets will react to a piece of news, as opposed to whether the news is inherently\npositive or not. Just like we saw earlier in BioBERT for healthcare, there have been\nefforts to adapt BERT to the financial domain. One of these is FinBERT [26].\nFinBERT uses a subset of financial news from Reuters. For sentiment classification, it\nuses Financial PhraseBank, which has over 4,000 sentences labeled by people with\nbackgrounds in business and finance. Unlike regular sentiment analysis, where posi‐\ntive means that something is of positive emotion, in Financial PhraseBank, a positive\nsentiment indicates that the stock price of the company will increase based on the\nnews in the sentence. FinBERT led to an accuracy of 0.97 and an F1 of 0.95—a sub‐\nstantial improvement over other general state-of-the-art methods. FinBERT is a\nlibrary that’s available on GitHub, along with its data [26]. We can build on this\nlibrary for custom problems and use the pre-trained models for financial sentiment\nclassification.\nRisk assessments\nCredit risk is a way to quantify the chances of a successful loan repayment. It’s gener‐\nally calculated by an individual’s past spending and loan repayment history. However,\nthis information is limited in many scenarios, especially in underprivileged commun‐\nities. It’s estimated that more than half of the world’s population is excluded from\nfinancial services [27]. NLP can help alleviate this problem. NLP techniques can add\na lot more data points that can be used to assess credit risk. For example, in business\nloans, entrepreneurial ability and attitude can be measured using NLP. This approach\nis used by Capital Float and Microbnk. Similarly, incoherencies in data provided by\nthe borrower can also be surfaced for more scrutiny. Other more nuanced aspects,\nsuch as lenders’ and borrowers’ emotions while applying for a loan, can also be incor‐\nporated. This is covered in more detail in [27].\nOften in personal loan agreements, various information has to be captured from loan\ndocuments, which are then fed to credit risk models. The information captured helps\nin identifying credit risk, and erroneous data extraction from these documents can\nlead to flawed assessments. Named entity recognition (NER), which we covered in\ndetail in Chapter 5, can improve this. An example of such a loan agreement is shown\nin Figure 10-17, where we see a loan agreement and different relevant entities extrac‐\nted from it. This example is taken from a work [28] on domain adaptation of NER for\nthe finance domain. We’ll cover such entity extraction in more detail in “NLP and the\nLegal Landscape” on page 363.\nFinance and Law \n| \n361\n",
      "word_count": 459,
      "char_count": 2831,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 392,
      "text": "Figure 10-17. Loan agreement with annotated entities\nAccounting and auditing\nThe global firms Deloitte, Ernst & Young, and PwC now have a significant focus on\ndelivering more meaningful, actionable, and relevant audit conclusions and observa‐\ntions on a company’s annual performance. While applying aspects of NLP and ML to\nareas like contract document reviews and long-term procurement agreements,\nDeloitte, for example, has evolved its Audit Command Language into a more efficient\nNLP application. This is covered in more detail in their report on government data\n[29].\nIn addition, after decades of long, drawn-out ticking and tying of reams of endless,\ntypical day-to-day transactions and other pieces of paper like invoices, companies\nhave finally realized that NLP and ML has a significant advantage in the audit pro‐\ncess. This advantage manifests in the direct identification, focus, visualization, and\ntrend analysis of outliers in transaction types. Time and effort are spent on the inves‐\ntigation of these outliers and their causes. This results in early identification of poten‐\ntially significant risks and possible fraudulent activity like money laundering along\n362 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 185,
      "char_count": 1227,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 1227,
          "ext": "png",
          "size_bytes": 877423
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 393,
      "text": "with potentially value-generating activities that can be emulated and extrapolated\nacross a company and customized for various business processes.\nNext, we’ll turn our attention to the use of NLP in legal matters.\nNLP and the Legal Landscape\nThe integration and utilization of technological tools in the law profession has been\nin progress for decades. Given the amount of research, case referencing, brief prepa‐\nration, document review, contract design, background analysis, and opinion drafting,\nthose in the legal profession, including law offices and court systems, have long\nlooked for a multitude of ways, means, and tools to slash their hours of manual effort.\nWe won’t cover legal NLP in as much detail, as research work in the domain is pro‐\ntected by patents instead of open or partially open. So, we’ll discuss the ideas in gen‐\neral terms.\nSome core tasks where NLP helps legal services include:\nLegal research\nThis involves finding relevant information for a specific case, including searching\nboth legislatures and case law and regulations. One such service is ROSS Intelli‐\ngence [30]. It allows matching of facts and relevant cases and also analyzes legal\ndocuments. We can see it in action in Figure 10-18.\nFigure 10-18. ROSS match for relevant passages\nContract review\nThis refers to reviewing a contract and making sure it follows a set of norms and\nregulations. It involves making comments and suggesting edits for different clau‐\nses. One example is SpotDraft [31], which focuses on GPDR-based regulations.\nContract generation\nThis refers to generating contracts based on a question-and-answer setup. Simple\ncases may just require a simple form, whereas for more complex cases, an inter‐\nactive chatbot system may be more suitable. After taking in all the responses, a\nslot-filling algorithm generates the contract.\nFinance and Law \n| \n363\n",
      "word_count": 294,
      "char_count": 1862,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 991,
          "height": 278,
          "ext": "png",
          "size_bytes": 37213
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 394,
      "text": "Legal Discovery\nThis refers to finding anomalies and patterns in electronically stored information\nthat can be used for the case. In some cases, this discovery is completely unsuper‐\nvised. In other cases, it can involve more active learning (i.e., providing an initial\nset of tagged documents). One such product is siren.io [32], which aids discovery\nfor intelligence, law enforcement, cyber security, and financial crime domains.\nLegal entity extraction with LexNLP\nIn any kind of contract, there are a bunch of legal terms and entities we need to\nextract before building any kind of intelligent application. LexNLP [33] helps with\nthat because it has legal word segmentation and tokenization. This is important\nbecause of legal abbreviations like LLC or F.3d, which regular parsers can’t handle.\nSimilarly, LexNLP helps us segment documents into sections and extract facts like\nrecurring contract dates or regulations. Moreover, it plugs into the ContraxSuite,\nwhich has a range of other legal features that we’ll cover later.\nNow, let’s see how this works in action:\nimport lexnlp.extract.en.acts\nimport lexnlp.extract.en.definitions\nprint(\"List of acts in the document\")\ndata_contract = list(lexnlp.extract.en.acts.get_acts(text))\ndf = pd.DataFrame(data=data_contract,columns=data_contract[0].keys())\ndf['Act_annotations'] = list(lexnlp.extract.en.acts.get_acts_annotations(text))\ndf.head(10)\nprint(\"Different ACT definitions in the contract\")\ndata_acts = list(lexnlp.extract.en.definitions.get_definitions(text))\ndf = pd.DataFrame(data=data_acts,columns=[\"Acts\"])\ndf.head(20)\nFigure 10-19 shows the list of acts in the document extracted using LexNLP.\nAs shown in the code, we extracted information from a SAFE (simple agreement for\nfuture equity), a common document for investments. We extracted all the acts and\ntheir definitions that were present in the document. Similarly, this can be extended to\nextract companies, citations, constraints, legal durations, regulations, etc. We cover\nsome of these in the notebook for Chapter 10.\n364 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 281,
      "char_count": 2090,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "MinionPro-It (10.5pt)",
        "UbuntuMono-Regular (8.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 395,
      "text": "Figure 10-19. Output from LexNLP\nBesides legal entity extraction, LexNLP also provides legal dictionaries [34] and\nknowledge sets for multiple countries for accounting, financial information, regula‐\ntors, and legal and medical areas. It also integrates with ContraxSuite [35], which\nallows us to deduplicate documents, cluster legal entities according to how they’re\nmentioned (as seen in Figure 10-20), and so on. When building custom applications,\nwe can also inject code to build on the baseline platform.\nFinance and Law \n| \n365\n",
      "word_count": 80,
      "char_count": 534,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 1254,
          "ext": "png",
          "size_bytes": 249375
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 396,
      "text": "Figure 10-20. Clustering of legal entities from a set of documents\nWrapping Up\nIn this chapter, we learned about how NLP is utilized in healthcare, finance, and law,\ncovering everything from model building, using online APIs, and dataset creation.\nThese areas offer a diverse set of issues and solutions, so even if the domain you’re\nworking in is unrelated to these areas, the techniques learned here may be applicable\nin solving any unconventional problem. In the next chapter, we’ll see how all of this\ncomes together in building a complete NLP solution.\nReferences\n[1] Business Wire. “The $11.9 Trillion Global Healthcare Market: Key Opportunities\n& Strategies (2014–2022)”. June 25, 2019.\n[2] Chilmark Research. “NLP Use Cases for Healthcare Providers”. July 17, 2019.\n[3] Wikipedia. “Electronic health record”. Last modified April 17, 2020.\n[4] Woebot. Last accessed June 15, 2020.\n[5] Buoy: a healthcare chatbot. Last accessed June 15, 2020.\n[6] Infermedica. Last accessed June 15, 2020.\n[7] Hirsch, Jamie S., Jessica S. Tanenbaum, Sharon Lipsky Gorman, Connie Liu, Eric\nSchmitz, Dritan Hashorva, Artem Ervits, David Vawdrey, Marc Sturm, and Noémie\nElhadad. “HARVEST, a longitudinal patient record summarizer.” Journal of the Amer‐\nican Medical Informatics Association 22.2 (2015): 263–274.\n366 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 206,
      "char_count": 1347,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1190,
          "height": 619,
          "ext": "png",
          "size_bytes": 293359
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 397,
      "text": "[8] Raghavan, Preethi and Siddharth Patwardhan. “Question Answering on Elec‐\ntronic Medical Records.” Proceedings of the 2016 Summit on Clinical Research Infor‐\nmatics (2016).\n[9] Raghavan, Preethi, Siddharth Patwardhan, Jennifer J. Liang, and Murthy V.\nDevarakonda. “Annotating Electronic Medical Records for Question Answering”,\n(2018).\n[10] i2b2. “NLP Research Datasets”. Last accessed June 15, 2020.\n[11] Rajkumar, Alvin and Oren, Eyal. “Deep Learning for Electronic Health Records”.\nGoogle AI Blog, May 8, 2018.\n[12] Google Machine Learning Crash Course. “Classification: ROC Curve and AUC”.\nLast accessed June 15, 2020.\n[13] Hilden, Jørgen. “The Area Under the Roc Curve and Its Competitors.” Medical\nDecision Making 11.2 (1991): 95–101.\n[14] Liu, Yun and Po-Hsuan Cameron Chen. “Lessons Learned from Developing ML\nfor Healthcare”. Google AI Blog, December 10, 2019.\n[15] Ritchie, Hanna and Max Roser. “Mental Health”. Our World In Data, April 2018.\n[16] National Institute of Mental Health (NIMH). “Suicide”. Last accessed June 15,\n2020.\n[17] Coppersmith, Glen, Kim Ngo, Ryan Leary, and Anthony Wood. “Exploratory\nAnalysis of Social Media Prior to a Suicide Attempt.” Proceedings of the Third Work‐\nshop on Computational Linguistics and Clinical Psychology (2016): 106–117.\n[18] Amazon Comprehend Medical. Last accessed June 15, 2020.\n[19] Fast Healthcare Interoperability Resources (FHIR) specification. Last accessed\nJune 15, 2020.\n[20] FHIR sample healthcare record, (download).\n[21] Lee, Jinhyuk, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,\nChan Ho So, and Jaewoo Kang. “BioBERT: A Pre-Trained Biomedical Language Rep‐\nresentation Model for Biomedical Text Mining.” Bioinformatics 36.4 (2020): 1234–\n1240.\n[22] DMIS Laboratory - Korea University. BioBERT: a pre-trained biomedical lan‐\nguage representation model, (GitHub repo). Last accessed June 15, 2020.\n[23] NAVER. BioBERT: a pre-trained biomedical language representation model for\nbiomedical text mining, (GitHub repo). Last accessed June 15, 2020.\n[24] Ross, Sean. “What Percentage of the Global Economy Is the Financial Services\nSector?” Investopedia, February 6, 2020.\nWrapping Up \n| \n367\n",
      "word_count": 311,
      "char_count": 2170,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 398,
      "text": "[25] Bank of England. “Machine Learning in UK Financial Services”. October 2019.\n[26] Araci, Dogu. “FinBERT: Financial Sentiment Analysis with Pre-trained Lan‐\nguage Models”, (2019).\n[27] Crouspeyre, Charles, Eleonore Alesi, and Karine Lespinasse. “From Creditwor‐\nthiness to Trustworthiness with Alternative NLP/NLU Approaches.” Proceedings of\nthe First Workshop on Financial Technology and Natural Language Processing (2019):\n96–98.\n[28] Alvarado, Julio Cesar Salinas, Karin Verspoor, and Timothy Baldwin. “Domain\nAdaption of Named Entity Recognition to Support Credit Risk Assessment.” Proceed‐\nings of the Australasian Language Technology Association Workshop (2015): 84–90.\n[29] Eggers, William D., Neha Malik, and Matt Gracie. “Using AI to Unleash the\nPower of Unstructured Government Data.” Deloitte Insights (2019).\n[30] Ross Intelligence. Last accessed June 15, 2020.\n[31] SpotDraft. Last accessed June 15, 2020.\n[32] Siren: Investigative Intelligence Platform. Last accessed June 15, 2020.\n[33] LexPredict. LexNLP by LexPredict, (GitHub repo). Last accessed June 15, 2020.\n[34] LexPredict. LexPredict Legal Dictionaries, (GitHub repo). Last accessed June 15,\n2020.\n[35] ContraxSuite. Last accessed June 15, 2020.\n368 \n| \nChapter 10: Healthcare, Finance, and Law\n",
      "word_count": 174,
      "char_count": 1272,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 399,
      "text": "PART IV\nBringing It All Together\n",
      "word_count": 6,
      "char_count": 33,
      "fonts": [
        "MyriadPro-SemiboldCond (28.4pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 400,
      "text": "",
      "word_count": 0,
      "char_count": 0,
      "fonts": [],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 401,
      "text": "CHAPTER 11\nThe End-to-End NLP Process\nThe process is more important than the goal. The person you become\nis infinitely more valuable than whatever the result is.\n—Anthony Moore\nSo far in the book, we’ve addressed a range of NLP problems, starting from what an\nNLP pipeline looks like to how NLP is applied in different domains. Efficiently apply‐\ning what we’ve learned to build end-to-end software products involving NLP takes\nmore than just stitching together various steps in an NLP pipeline—there are several\ndecision points during the process. While a lot of this knowledge comes only with\nexperience, we’ve distilled some of our knowledge about the end-to-end NLP process\nin this chapter to help you hit the ground running faster and better.\nIn Chapter 2, we already saw what a typical pipeline for an NLP system looks like.\nHow is this chapter then any different from that? In Chapter 2, we focused primarily\non the technical aspects of the pipeline—for example, how do we represent text?\nWhat pre-processing steps should we do? How do we build a model, and then how do\nwe evaluate it? In the subsequent chapters in Parts I and II of the book, we delved\ndeeper into different algorithms to perform various NLP tasks. We also saw how NLP\nis used in various industry domains, such as healthcare, e-commerce, and social\nmedia. However, in all these chapters, we spent little time on the issues related to\ndeploying and maintaining such systems and on the processes to follow when manag‐\ning such projects. These are the focus of this chapter. Most of the points discussed\nhere are broadly applicable not just to NLP, but also to other concepts, such as data\nscience (DS), machine learning, artificial intelligence (AI), etc. Throughout this chap‐\nter, we use these terms interchangeably; where the focus is specifically on NLP tasks,\nwe mention that explicitly.\n371\n",
      "word_count": 315,
      "char_count": 1870,
      "fonts": [
        "MyriadPro-SemiboldCond (16.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (9.3pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MinionPro-Regular (9.3pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 402,
      "text": "We’ll start the discussion by revisiting the NLP pipeline we introduced in Chapter 2\nand take a look at the last two steps: deployment, followed by monitoring and updat‐\ning the model, which we didn’t cover in earlier chapters. We’ll also see what it takes to\nbuild and maintain a mature NLP system. This is followed by a discussion on the data\nscience processes followed in various AI teams, especially with respect to building\nNLP software in particular. We’ll conclude the chapter with a lot of recommenda‐\ntions, best practices, and do’s and don’ts to successfully deliver NLP projects. Let’s\nstart by looking at how to deploy NLP software.\nRevisiting the NLP Pipeline: Deploying NLP Software\nIn Chapter 2, we saw that a typical production pipeline for NLP projects consists of\nthe following stages: data acquisition, text cleaning, text pre-processing, text repre‐\nsentation and feature engineering, modeling, evaluation, deployment, monitoring,\nand model updating. When we encounter a new problem scenario involving NLP in\nour organization, we have to first start thinking about creating an NLP pipeline cov‐\nering these stages. Some of the questions we should ask ourselves in this process are:\n• What kind of data do we need for training the NLP system? Where do we get this\ndata from? These questions are important at the start and also later as the model\nmatures.\n• How much data is available? If it’s not enough, what data augmentation techni‐\nques can we try?\n• How will we label the data, if necessary?\n• How will we quantify the performance of our model? What metrics will we use to\ndo that?\n• How will we deploy the system? Using API calls over the cloud, or a monolith\nsystem, or an embedded module on an edge device?\n• How will the predictions be served: streaming or batch process?\n• Would we need to update the model? If yes, what will the update frequency be:\ndaily, weekly, monthly?\n• Do we need a monitoring and alerting mechanism for model performance? If\nyes, what kind of mechanism do we need and how will we put it in place?\n372 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 363,
      "char_count": 2098,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 403,
      "text": "Once we’ve thought through these key decision points, a broad design of our pipeline\nis ready! We can then start to focus on building version 1 of the model with strong\nbaselines, implementing the pipeline, deploying the model, and from there, iteratively\nimproving our solution. In Chapter 2, we saw how different stages of the NLP pipe‐\nline before deployment are implemented for various NLP tasks. Let’s now take a look\nat the final stages of the pipeline: deployment, monitoring, and model updating.\nWhat does deployment mean? Any NLP model we build is typically a part of some\nlarger software system. Once our model is working well in isolation, we plug it into a\nlarger system and ensure that everything is working well. The set of all of the tasks\nrelated to integrating the model with the rest of the software and making it\nproduction-ready is called deployment. Typical steps in deployment of a model\ninclude:\n1. Model packaging: If the model is large, it might need to be saved in persistent\ncloud storage, such as AWS S3, Azure Blob Storage, or Google Cloud Storage, for\neasy access. It might also be serialized and wrapped up in a library call for easy\naccess. There are also open formats like ONNX [1] that provide interoperability\nacross different frameworks.\n2. Model serving: The model can be made available as a web service for other serv‐\nices to consume. In cases where a more tightly coupled system and batch process\nis more applicable, the model could be part of a task flow system like Airflow [2],\nOozie [3], or Chef [4], instead of a web service. Microsoft has also released refer‐\nence pipelines for MLOps [5] and MLOps in Python [6].\n3. Model scaling: Models that are hosted as web services should be able to scale with\nrespect to request traffic. Models that are running as part of a batch service\nshould also be able to scale with respect to the input batch size. Public cloud plat‐\nforms as well as on-premise cloud systems have technologies that enable that.\nFigure 11-1 shows one such pipeline for text classification on AWS. More details\non the engineering of this pipeline can be found in the AWS post [7].\nRevisiting the NLP Pipeline: Deploying NLP Software \n| \n373\n",
      "word_count": 382,
      "char_count": 2200,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 404,
      "text": "Figure 11-1. AWS Cloud and SageMaker to serve text classification [8]\nLet’s look at an example to understand the deployment of an NLP model into a larger\nsystem.\nAn Example Scenario\nLet’s say we work for a social media platform and are asked to build a classifier to\nidentify abusive user comments. The goal of this classifier is to prevent abusive con‐\ntent from appearing on the platform by flagging any content that’s potentially abusive\nand sending it for human moderation. We worked hard on collecting the data rele‐\nvant to this task, designing a set of features, and testing a range of algorithms, and we\nbuilt a predictive model that takes a new comment as input and classifies it as abusive\nor safe. What next?\n374 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 134,
      "char_count": 767,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1409,
          "height": 1344,
          "ext": "png",
          "size_bytes": 122048
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 405,
      "text": "Our model is just a small part of the larger social media platform. There are several\ncomponents: content is being rendered dynamically, and there are various modules to\ninteract with users, components responsible for storage and retrieval of data, and so\non. It’s possible that different subsystems of the platform are written in different pro‐\ngramming languages. Our classifier is just a small component of the product, and we\nneed to integrate it into the larger setup. How do we go about doing this? A common\nway to address this scenario is to create a web service where the model sits behind the\nweb service. The rest of the product interacts with the model via this web service. It\nqueries the service with the new comment(s) and gets back the prediction(s). The call\nto this web service is integrated into the product wherever necessary. Popular web\napplication frameworks such as Flask [9], Falcon [10], and Django [11] are typically\nused to create such web services.\nDeveloping various NLP solutions involves relying on a range of pre-existing libra‐\nries. Setting up a web service and hosting what we built in the cloud or some server\nrequires us to ensure that there are no compatibility issues. To address this, there is a\nrange of options available. The most common option is to package various libraries\ninto a container like Docker [12] or Kubernetes [13]. Operationalizing a web service\nfor production requires addressing many other issues, such as tech stack, load balanc‐\ning, latency, throughput, availability, and reliability. Building and making a model\nproduction ready includes a whole lot of engineering tasks, which can often be time\nconsuming. Cloud services such as AWS SageMaker [14] and Azure Cognitive Serv‐\nices [15] try to make these engineering tasks easy. Sometimes, the whole process, to\nthe last detail, is automated to such an extent that it’s as simple as one-click-get-done\nto set up the service. The idea is to let the AI teams focus on the most important part:\nmodel building.\nAnother important issue to address is model size. Modern NLP models can be quite\nlarge. For example, Google’s Word2vec model is 4.8 GB in size and takes over 100 sec‐\nonds just to load into memory (refer back to Ch3/Pre_Trained_Word_Embed‐\ndings.ipynb). Likewise, a fastText classification model is typically over 2 GB in size.\nDL models like BERT are known to be even bulkier. Hosting such large models in the\ncloud can be both challenging and expensive. There’s a lot of work happening in the\narea of model compression to address such scenarios. Some of them are listed below:\n• “Compressing BERT for Faster Prediction,” a blog post by a team at Rasa NLP\n[16]\n• “A Survey of Model Compression and Acceleration for Deep Neural Networks,” a\nreport by a team at Microsoft Research and Tsinghua University [17]\n• “FastText.zip: Compressing text classification models,” a report by a team at Face‐\nbook AI Research [18]\nRevisiting the NLP Pipeline: Deploying NLP Software \n| \n375\n",
      "word_count": 500,
      "char_count": 2996,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 406,
      "text": "• “Awesome ML Model Compression,” a GitHub repository by Cedric Chee that\nincludes relevant papers, videos, libraries, and tools [19]\nThis is just a brief overview of various steps that go into deploying our NLP model.\nThere are books and other materials that cover this in complete detail. As a start,\ninterested readers can look at the later chapters of the book Machine Learning Engi‐\nneering [20].\nFor most industry use cases, model building is seldom a one-time activity. As the\ndeployed system gets used more, the models built need to adapt to new scenarios and\nnew data points. Hence, the models should be updated regularly. Let’s discuss the\nissues to consider while building and maintaining mature NLP software.\nBuilding and Maintaining a Mature System\nIn most real-world settings, the underlying patterns in data change over a period of\ntime. This means that the models that were trained long before can become stale—\ni.e., the data used to train the model is very different from the data in the production\nenvironment that’s being fed to the model for predictions. This is called covariate\nshift, and it results in a performance drop of the model. Model update is a common\napproach to deal with such scenarios. On a similar note, in most industrial settings,\nonce the first version of a model is consumed, improving the model becomes inevita‐\nble. Updating and improving an existing NLP model could just mean retraining with\nnewer or additional training data, or it sometimes involves adding new features.\nWhen updating such models, the goal is to ensure that the deployed system performs\nat least as well as the existing system. Most model updates and improvements lead to\nmore complex models. As the models grow in complexity, we need to ensure that the\nsystem doesn’t crumble under increasing complexity. We need to manage the com‐\nplexity of a mature NLP model while making sure it’s also maintainable. Some of the\nissues we need to consider in this process are:\n• Finding better features\n• Iterating existing models\n• Code and model reproducibility\n• Troubleshooting and testing\n• Minimizing technical debt\n• Automating the ML process\nIn this section, let’s take a look at these issues one by one, starting with a discussion\nabout how to find better features.\n376 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 389,
      "char_count": 2323,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 407,
      "text": "Finding Better Features\nThroughout this book, we’ve repeatedly stressed the importance of building a simple\nmodel first. This version 1 model is seldom an end in itself. We may keep on adding\nnew features and periodically retraining the model beyond V1. Our goal is to find the\nfeatures that are most expressive to capture the regularities in the data that are useful\nfor making predictions. How can we develop such features? We saw different ways to\ngenerate textual feature representations in Chapter 3. We can start with one of those\nthat doesn’t require prior knowledge about the problem domain (e.g., basic vectoriza‐\ntion, distributed representations, and universal representations) or use our prior\nknowledge about the problem and domain to develop specific features for our prob‐\nlem (i.e., handcrafted features) or use a combination of both.\nDesigning specific features for a given problem (or feature engineering) can be both\ndifficult and expensive. This is why problem-agnostic text representations are com‐\nmonly used as a starting point. However, domain-specific features have their own\nvalue. For example, in a task of sentiment classification, more than vector representa‐\ntions of raw text, domain-specific indicators, such as count of negative words, count\nof positive words, and other word- and phrase-level features, are useful to extract the\nsentiment in a more robust manner.\nLet’s say we implemented a bunch of features to build our NLP models. Does the best\nmodel need each one of these features? How do we choose the most informative fea‐\ntures among the several we implemented? For example, if we use two features where\none can be derived from the other, we’re not adding any extra information to the\nmodel. Feature selection is a great technique to handle such cases and make informed\ndecisions. There are plenty of statistical methods that can be used to fine-tune our\nfeature sets by removing redundant or irrelevant features. This broad area is called\nfeature selection.\nTwo popular techniques for feature selection are wrapper methods and filter meth‐\nods. Wrapper methods use an ML model to score feature subsets. Each new subset is\nused to train a model, which is tested on a hold-out set and then used to identify the\nbest features based on the error rate of the model. Wrapper methods are computa‐\ntionally expensive, but they often provide the best set of features. Filter methods use\nsome sort of proxy measure instead of the error rate to rank and score features (e.g.,\ncorrelation among the features and correlation with the output predictions). Such\nmeasures are fast to compute while still capturing the usefulness of the feature set.\nFilter methods are usually less computationally expensive than wrappers, but they\nproduce a feature set that’s not as well optimized to a specific type of predictive\nmodel. In DL-based approaches, while feature engineering and feature selection is\nautomated, we have to experiment with various model architectures.\nBuilding and Maintaining a Mature System \n| \n377\n",
      "word_count": 488,
      "char_count": 3040,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 408,
      "text": "Since feature selection methods are usually task specific (i.e., methods for classifica‐\ntion tasks are different from methods for, say, machine translation), interested readers\ncan look into resources such as sparse features, dense features, and feature interac‐\ntions from Wide and Deep Learning from Google AI [21]. The book Feature Engi‐\nneering for Machine Learning [22] would also be useful. However, we hope this\noverview convinced you of feature selection’s role in building mature, production-\nquality NLP systems. Assuming we’re going through this process of adding new fea‐\ntures and evaluating them, how should we incorporate them into our training process\nand update our NLP models? Let’s take a look at this question now.\nIterating Existing Models\nAs we mentioned earlier, any NLP model is seldom a static entity. We’re often\nrequired to update our models even in production systems. There are several reasons\nfor this. We may get more (and newer) data that differs from previous training data.\nIf we don’t update our model to reflect this change, it will soon become stale and\nchurn out poor predictions. We may get some user feedback on where the model pre‐\ndictions are going wrong. This will then require us to reflect on the model and its fea‐\ntures and make amendments accordingly. In both cases, we need to set up a process\nto periodically retrain and update the existing model and deploy the new model in\nproduction.\nWhen we develop a new model, intuitively, it’s always good to compare the results\nwith our previous best models to understand the incremental value addition. How do\nwe know this new model is better than the existing one? The analysis of model per‐\nformance can be based on comparing raw predictions from both models, or it could\nbe from a perspective of a derived performance based on the predictions. Let’s explain\nthese two cases by revisiting the abusive comments detection example from earlier in\nthis chapter.\nLet’s say we have a gold standard test set of abusive versus non-abusive comments.\nWe can always use this to compare an old model with the new one in terms of, say,\nclassification accuracy. We can also follow an external validation approach and look\nfor other aspects, such as how many model decisions were contested by users every\nday. It would be practical to set up a dashboard to monitor these metrics periodically\nand display them for each model so that we can choose the one that’s the best\nimprovement over the current model among the various models we may build. We\ncan also A/B test a new model with an old model (or any baseline system) and meas‐\nure business KPIs to see how well the new model performs. When onboarding a new\nmodel, it might also be a good practice to first roll it out to a small fraction of users,\nmonitor its performance, and then progressively expand it to the entire user base.\n378 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 497,
      "char_count": 2912,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 409,
      "text": "Code and Model Reproducibility\nMaking sure your NLP models continue working in the same fashion in different\nenvironments can be critical for the long-term success of any project. A model or\nresult that’s reproducible is generally considered more robust. There is a range of best\npractices you can use to achieve this while building systems.\nMaintaining separation between code, data, and model(s) is always a good strategy.\nSeparating code and data is generally a best practice in software engineering, and it\nbecomes even more critical for AI systems. While there are established version con‐\ntrol systems for code, such as Git, versioning of models and datasets can be different.\nAs of recently, there are tools like Data Version Control [23] that address this issue.\nIt’s always a good practice to name model and data versions appropriately so that we\ncan revert back easily, if needed. While storing the models, you should try to have all\nyour model parameters, along with other variables, in a separate file. Similarly, try to\navoid hardcoded parameter values in your model. If you have to use arbitrary num‐\nbers in your training process (e.g., a seed value somewhere), explain it in the code as\ncomments.\nAnother good practice is creating checkpoints in your code and model often. You\nshould store your learned model in a repository both periodically and at milestones.\nWhen training a model, it’s also a good idea to use the same seed wherever random\ninitialization is used. This ensures that the model creates similar results (and internal\nrepresentation) every time the same parameters and data are used.\nA keystone for improving reproducibility is to make sure to note all steps explicitly.\nThis is especially necessary in the exploratory phase of data analysis. On the same\nnote, it helps to record as many intermediate steps and data outputs as possible. This\nhelps in transforming your experimental model to an in-production model without\nany loss of information. To read further, we would suggest a report on AI reproduci‐\nbility state of the art [24] and an interview of a reproducibility researcher at Face‐\nbook, Joelle Pineau [25]. This brings us to the next topic in this section. While\nmaking all these iterations and building multiple models, how do we ensure there are\nno errors and bugs in the training process and that our data isn’t noisy? How do we\ntroubleshoot and test our code and models?\nTroubleshooting and Interpretability\nTo maintain the quality of software, testing is a key step in any software development\nprocess. However, considering the probabilistic nature of ML models, how to test ML\nmodels is not obvious. Figures 11-2 and 11-3 illustrate some of the good practices for\ntesting out AI systems. We already saw how to use Lime (Figure 11-3) in Chapter 4.\nBuilding and Maintaining a Mature System \n| \n379\n",
      "word_count": 472,
      "char_count": 2847,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 410,
      "text": "Figure 11-2. TensorFlow Model Analysis (TFMA) [26]\nFigure 11-3. Lime for NLP model analysis\n380 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 22,
      "char_count": 139,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1111,
          "height": 668,
          "ext": "png",
          "size_bytes": 56192
        },
        {
          "index": 1,
          "width": 1307,
          "height": 935,
          "ext": "png",
          "size_bytes": 47189
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 411,
      "text": "As we discussed earlier in the chapter, a model is just a small component of any AI\nsystem. When it comes to testing the entire system, barring the model, most techni‐\nques for testing of software engineering are applicable and work well. When it comes\nto testing the model, the following steps are helpful:\n• Run the model on train, validation, and test datasets used during the model-\nbuilding phase. There should not be any major deviation in the results for any of\nthe metrics. K-fold cross validation is often used to verify model performance.\n• Test the model for edge cases. For example, for sentiment classification, test with\nsentences with double or triple negation.\n• Analyze the mistakes the model is making. The findings from the analysis should\nbe similar to the findings from the analysis of the mistakes it was making during\nthe development phase. For NLP, packages and techniques like TensorFlow\nModel Analysis [26], Lime [27], Shap [28], and attention networks [5] can give a\ndeeper understanding of what the model is doing deep down. You can see this in\naction in Figures 11-2 and 11-3. The insights from these during development and\nproduction should not change much.\n• Another good practice is to build a subsystem that keeps track of key statistics of\nthe features. Since all features are numerical, we can maintain statistics like mean,\nmedian, standard deviation, distribution plots, etc. Any deviation in these statis‐\ntics is a red flag, and we’re likely to see the system churning out wrong predic‐\ntions. The reason could be as simple as a bug in the pipeline or as complex as a\ncovariate shift in the underlying data. Packages like TensorFlow Model Analysis\n[26] can track these metrics. Figure 11-4 shows distributions for metrics of vari‐\nous features for a dataset that can be tracked to find covariate shift or bugs.\nFigure 11-4. Feature statistics in TensorFlow Extended [29]\nBuilding and Maintaining a Mature System \n| \n381\n",
      "word_count": 330,
      "char_count": 1959,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1124,
          "height": 531,
          "ext": "png",
          "size_bytes": 48454
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 412,
      "text": "• Create dashboards for tracking model metrics and create an alerting mechanism\non them in case there are any deviations in the metrics. We’ll discuss this point in\ndetail in the next section.\n• It’s always good to know what a model is doing inside. This goes a long way\ntoward understanding why a model is behaving in a certain way. A key question\nin AI has been how to create intelligent systems where we can explain why the\nmodel is doing what it is doing. This is called interpretability. It’s the degree to\nwhich a human can understand the cause of a decision [30]. While many algo‐\nrithms in machine learning (such as decision trees, random forest, XGboost, etc.)\nand computer vision have been very interpretable, this is not true for NLP, espe‐\ncially DL algorithms. With recent techniques such as attention networks, Lime,\nand Shapley, we have greater interpretability in NLP models. Interested readers\ncan look at Interpretable Machine Learning by Christoph Molnar [31] for further\ndiscussion on this topic.\nMonitoring\nOnce an ML system has been deployed and is in production, we need to make sure\nthe model continues working well. As an example deployment, if the model is being\ntrained automatically every day with new data points, certain bugs can creep in, or\nthe model can malfunction. To ensure that this doesn’t happen, we need to monitor\nthe model for a range of things and trigger alerts at the right points:\n• Model performance has to be monitored regularly. For a web service–based\nmodel, it can be the mean and various percentiles—50th (median), 90th, 95th,\nand 99th (or deeper)—for response time. If the model is deployed as a batch ser‐\nvice, statistics on the batch processing and task times have to be monitored.\n• Similarly, it helps to store monitor model parameters, behavior, and KPIs. Model\nKPIs for the abusive comments example would be the percentage of comments\nthat were reported by users but not flagged by the model. For a text classification\nservice, it could be the distribution of classes that are classified each day.\n• For all the metrics we’re monitoring, we need to periodically run them through\nan anomaly detection system that can alert changes in normal behavior. This\ncould be a sudden spike in the response rate of a web service or a sudden drop in\nretraining times. In the worst case, when the performance drops substantially, we\nmay also want to hit circuit breakers (i.e., move to a more stable model or a\ndefault approach).\n• If our overall engineering pipeline is using a logging framework, there’s a good\nchance it also has support for monitoring anomalies over time for any metric.\nFor instance, ELK stack by Elastic offers built-in anomaly detection [7].\n382 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 470,
      "char_count": 2758,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 413,
      "text": "Sumo Logic also flags outliers that can be queried as needed [32]. Microsoft also\noffers anomaly detection as a service [33].\nMonitoring our ML models and their deployments can save substantial time as the\nproject scales. As the system matures and the model stabilizes, proper monitoring\nallows MLOps teams to largely manage it, so data scientists can solve other harder\nproblems. Although, as systems mature, we also start accumulating more technical\ndebt, which we’ll cover in the next section.\nMinimizing Technical Debt\nThroughout this book, and especially in this chapter, we’ve seen various aspects of\ntraining NLP models, deploying them as a part of a larger system, and iteratively\nimproving from there on. As we start iterating from the first version of the system,\nthe system and various components, including the model, can easily become com‐\nplex. This brings the challenges of maintaining the system. We may have scenarios\nwhere we don’t know if the incremental improvements justify the complexity. Such\nscenarios can create a technical debt. Let’s take a brief look at addressing technical\ndebt in building AI software.\nIt’s important to plan and build for the future when working with any software sys‐\ntem. We have to ensure that our system continues being both performant and easy to\nmaintain after all these continuous iterations and testing. Unused and poorly imple‐\nmented improvements can create technical debt. If we’re not using a feature or any of\nits combinations with other features, it’s important to drop it out of the pipeline. A\nfeature or part of the code that doesn’t work just clogs our infrastructure, hinders fast\niteration, and brings down clarity.\nA good rule of thumb is to look at the coverage a feature provides. If a feature is\npresent in only a few data points, say, 1%, then maybe it’s not worth keeping. But even\nsomething like this can’t be applied blindly. For example, if the same feature covers\njust 1% of the data but gives 95% classification accuracy just based on that feature,\nthen it’s really effective and most certainly worth continuing to use. From our experi‐\nence, an important tip (that we’ve also reiterated several times in the book) is: opt for\na simpler model that has performance comparable to a much more complex model if you\nwant to minimize technical debt. Complex models may become necessary if there’s no\nequivalent simple model though.\nBuilding and Maintaining a Mature System \n| \n383\n",
      "word_count": 406,
      "char_count": 2454,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 414,
      "text": "Besides these recommendations, we’d also like to share some landmark work on\nbuilding mature ML systems:\n• “A Few Useful Things to Know About Machine Learning” by Pedro Domingoes\nof the University of Washington [34]\n• “Machine Learning: The High-Interest Credit Card of Technical Debt” by a team\nat Google AI [35]\n• “Hidden Technical Debt in Machine Learning Systems” by a team at Google\nAI [36]\n• Feature Engineering for Machine Learning, a book written by Alice Zheng and\nAmanda Casari [22]\n• “Ad Click Prediction: A View from the Trenches,” a work by a Google Search\nteam on the issues faced by a large online ML system [37]\n• “Rules of Machine Learning,” an online guide created by Martin Zenkovich of\nGoogle [38]\n• “The Unreasonable Effectiveness of Data,” a report by renowned UC Berkeley\nresearcher Peter Norvig and a Google AI team [39]\n• “Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,” another\nmodern look at the previous report by a team from Carnegie Mellon\nUniversity [40]\nSo far, we’ve discussed various best practices used in building mature AI systems.\nFrom finding better features to version control of datasets, these practices are manual\nand effort intensive. Driven by the ultimate goal of building intelligent machines and\nreducing manual effort, an interesting recent work has been to automate some\naspects of building AI systems. Let’s look at some key efforts in this direction.\nAutomating Machine Learning\nOne of the holy grails of machine learning is to automate more and more of the fea‐\nture engineering process. This has led to the creation of a subarea called AutoML\n(automated machine learning), which aims to make machine learning more accessi‐\nble. In most cases, it generates a data analysis pipeline that can include data pre-\nprocessing, feature selection, and feature engineering methods. This pipeline\nessentially selects ML methods and parameter settings that are optimized for a spe‐\ncific problem and data. As all of these steps can be time consuming for the ML expert\nand may be intractable for a beginner, AutoML can be a much-needed bridge for a\ngap in the world of machine learning. AutoML is itself essentially “doing machine\nlearning using machine learning,” making this powerful and complex technology\nmore widely accessible for those hoping to make use of massive amounts of data.\n384 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 395,
      "char_count": 2398,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 415,
      "text": "As an example, one research group at Google has used AutoML techniques [41] for\nlanguage modeling with the Penn Treebank dataset. Penn Treebank is a benchmark\ndataset for linguistic structure [42]. The research group at Google found that their\nAutoML approach can design models that achieve accuracies on par with state-of-\nthe-art models designed by world-class machine learning experts. Figure 11-5 shows\nan example of a neural network generated by AutoML.\nFigure 11-5. AutoML-generated network [41]\nOn the left side of the figure is a neural network that Google experts created to parse\ntext. On the right side is another network that was created automatically by Google’s\nAutoML. AutoML that explores various neural network architectures automatically\nperformed as well as the handcrafted model. It’s fascinating to see that their system\ndid almost as well as humans even for designing ML models.\nAutoML is the cutting edge of machine learning. One should only build it from the\nbottom up when more traditional methods for improving performance are exhaus‐\nted. It often requires a high amount of computing and GPU resources and a higher\nlevel of technical skill when doing it from scratch.\nauto-sklearn\nAs we mentioned previously, it’s generally a good idea to work on automating\nmachine learning only after most other options have been exhausted. In cases where\nthe need for AutoML [43] is more clear, one of the best libraries for applying it is\nauto-sklearn. It uses recent advancements in Bayesian optimization and meta-\nlearning to search in a huge hyperparameter space to figure out a reasonably good\nML model on its own. As it’s integrated with sklearn, which is one of the more popu‐\nlar ML libraries, using it is quite simple:\nBuilding and Maintaining a Mature System \n| \n385\n",
      "word_count": 292,
      "char_count": 1790,
      "fonts": [
        "MyriadPro-SemiboldCond (11.6pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 822,
          "height": 383,
          "ext": "png",
          "size_bytes": 62379
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 416,
      "text": "  import autosklearn.classification\n  import sklearn.model_selection\n  import sklearn.datasets\n  import sklearn.metrics\n  X, y = sklearn.datasets.load_digits(return_X_y=True)\n  X_train, X_test, y_train, y_test = \\\n        sklearn.model_selection.train_test_split(X, y, random_state=1)\n  automl = autosklearn.classification.AutoSklearnClassifier()\n  automl.fit(X_train, y_train)\n  y_hat = automl.predict(X_test)\n  print(\"Accuracy\", sklearn.metrics.accuracy_score(y_test, y_hat))\nThis code builds an autosklearn classifier for the MNIST digits dataset [44]. It splits\nthe dataset into training and test sets. While running for about an hour, this will auto‐\nmatically yield accuracy of over 98%.\nWhen we peek through what’s happening internally, we see different stages of\nAutoML, as shown in the snippet below:\n[(0.080000, SimpleClassificationPipeline({'balancing:strategy': 'none',\n'categorical_encoding:__choice__': 'one_hot_encoding', 'classifier:__choice__': \n'lda',\n'imputation:strategy': 'mean', 'preprocessor:__choice__': 'polynomial',\n'rescaling:__choice__': 'minmax',\n'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n'classifier:lda:n_components': 151,\n'classifier:lda:shrinkage': 'auto', 'classifier:lda:tol': \n0.02939556179271624,\n'preprocessor:polynomial:degree': 2, 'preprocessor:polynomial:include_bias': \n'True',\n'preprocessor:polynomial:interaction_only': 'True',\n'categorical_encoding:one_hot_encoding:minimum_fraction': 0.0729529152649298},\ndataset_properties={\n  'task': 2,\n  'sparse': False,\n  'multilabel': False,\n  'multiclass': True,\n  'target_type': 'classification',\n  'signed': False})),\n...\n...\n...\n...\n(0.020000, SimpleClassificationPipeline({'balancing:strategy': 'none', \n'categorical_encoding:__choice__':\n'one_hot_encoding', 'classifier:__choice__': 'passive_aggressive', \n'imputation:strategy': 'mean',\n'preprocessor:__choice__': 'polynomial', 'rescaling:__choice__': 'minmax',\n'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', \n'classifier:passive_aggressive:C':\n0.03485276894122253, 'classifier:passive_aggressive:average': 'True',\n'classifier:passive_aggressive:fit_intercept': 'True', \n386 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 162,
      "char_count": 2210,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "UbuntuMono-Bold (8.5pt)",
        "UbuntuMono-Regular (8.5pt)",
        "UbuntuMono-Regular (10.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 417,
      "text": "'classifier:passive_aggressive:loss': 'hinge',\n'classifier:passive_aggressive:tol': 4.6384320611389e-05, \n'preprocessor:polynomial:degree': 3,\n'preprocessor:polynomial:include_bias': 'True', \n'preprocessor:polynomial:interaction_only': 'True',\n'categorical_encoding:one_hot_encoding:minimum_fraction': 0.11994577706637469},\ndataset_properties={\n  'task': 2,\n  'sparse': False,\n  'multilabel': False,\n  'multiclass': True,\n  'target_type': 'classification',\n  'signed': False})),\n]\nauto-sklearn results:\n  Dataset name: d74860caaa557f473ce23908ff7ba369\n  Metric: accuracy\n  Best validation score: 0.991011\n  Number of target algorithm runs: 240\n  Number of successful target algorithm runs: 226\n  Number of crashed target algorithm runs: 1\n  Number of target algorithms that exceeded the time limit: 2\n  Number of target algorithms that exceeded the memory limit: 11\nNext, let’s take a look at Google Cloud services, as well as a few other approaches to\nNLP problems.\nGoogle Cloud AutoML and other techniques\nGoogle Cloud Services has also recently released AutoML as a service. This doesn’t\nrequire any technical knowledge beyond providing training data in the expected for‐\nmat. They’ve specifically built Cloud AutoML services for different parts of AI,\nincluding computer vision and structured tabular data, as well as for NLP.\nFor NLP, their Cloud AutoML is applied automatically when training custom models\nfor:\n• Text classification\n• Entity extraction\n• Sentiment analysis\n• Machine translation\nFor all these tasks, Google Cloud has defined a specific format that the AutoML mod‐\nels expect the data to be in. More information on these can be found in their docu‐\nmentation [45, 46]. Microsoft also has tooling for AutoML in their Azure Machine\nLearning [47].\nBuilding and Maintaining a Mature System \n| \n387\n",
      "word_count": 231,
      "char_count": 1816,
      "fonts": [
        "UbuntuMono-Regular (8.5pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (11.6pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 418,
      "text": "Another interesting approach to tackling an NLP problem in a more automated way\nis to use the AutoCompete framework created by Abhishek Thakur [48], a top-\nranked Kaggle Competitions Grandmaster. Even though his initial approach was to\nfocus on any data science problem specifically targeted to competitions, it has now\nevolved to a general framework to solve such problems. He has also released a\ndetailed notebook titled “Approaching (Almost) Any NLP Problem on Kaggle” [49]\nthat creates a general modeling framework for NLP problems with a well-defined\ndataset and goals. While this may not completely solve the specific NLP task you’re\nworking at, it’s a good start to look at creating baseline models.\nSo far, we’ve addressed a range of issues that might come up when trying to build,\ndeploy, and maintain NLP software. However, an equally important component of\nsuch an endeavor is to follow standard product development processes. While the\nfield of software development processes and life cycle is well established, there are\nsome important things to consider while working on projects that involve predictive\nmodels like the ones we’ve discussed throughout the book. Let’s now take a look at\nthat aspect.\nThe Data Science Process\nData science is a broad term describing the algorithms and processes used to extract\nmeaningful information and actionable insights from all forms of data. Thus, all NLP\nwork in the industry can be categorized under the data science umbrella. While data\nscience as a term is relatively new, it’s been around in some form or another for the\npast few decades. Over the years, people have formulated and formalized the best\nprocesses and practices of working with data. Two popular processes in the industry\nare the KDD process and the Microsoft Team Data Science Process.\nThe KDD Process\nThe ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) is\none of the oldest and most reputed data mining conferences in the world. Some of\nthe founders of the conference also created the KDD process in 1996. The KDD pro‐\ncess [50], depicted in Figure 11-6, consists of a series of steps that should be applied\nto a data science or data mining problem to get better results.\n388 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 374,
      "char_count": 2262,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 419,
      "text": "Figure 11-6. The KDD process [50]\nThese steps are ordered as follows:\n1. Understanding the domain: This includes learning about the application and\nunderstanding the goals of a problem. It also involves getting deeper into the\nproblem domain and extracting relevant domain knowledge.\n2. Target dataset creation: This includes selecting a subset of data and variables the\nproblem will focus on. We may have a plethora of data sources at our disposal,\nbut we focus on the subset we need to work on.\n3. Data pre-processing: This encompasses all activities needed so that the data can be\ntreated coherently. This includes filling missing values, noise reduction, and\nremoving outliers.\n4. Data reduction: If the data has a lot of dimensions, this step can be used to make\nit easier to work with. This includes steps like dimensionality reduction and pro‐\njecting the data into another space. This step is optional depending on the data.\n5. Choosing the data mining task: Various classes of algorithms can be applied to a\nproblem. They may be regression, classification, or clustering. It’s important to\nselect the right task based on our understanding from Step 1.\n6. Choosing the data mining algorithm: Based on the selected data mining task, we\nneed to select the right algorithm. For instance, for classification, we can choose\nalgorithms such as SVM, random forests, CNNs, etc., as we saw in Chapter 4.\n7. Data mining: This is a core step of applying the selection algorithm from Step 6\nto the given dataset and creating predictive models. Tuning with respect to\nparameters and hyperparameters also happens here.\nThe Data Science Process \n| \n389\n",
      "word_count": 272,
      "char_count": 1646,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1386,
          "height": 718,
          "ext": "png",
          "size_bytes": 73188
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 420,
      "text": "8. Interpretation: Once the algorithm is applied, the user has to interpret the results.\nThis can be done partially by visualizing various components of results.\n9. Consolidation: This is the final step where we deploy the built model into an\nexisting system, document the approach, and generate reports.\nAs seen in the figure, the KDD process is highly iterative. There can be any number of\nloops between various steps. At each step, we can and may need to go back to earlier\nsteps and refine the information there before moving ahead. This process is a good\nreference when working on a specific data science problem. While not exactly the\nsame, the pipelines we’ve discussed throughout the book deal with the same idea of\nbringing structure to building NLP systems. Now, let’s take a look at the second\nprocess.\nMicrosoft Team Data Science Process\nThe KDD process was introduced in the late ’90s. As the fields of machine learning\nand data science grew, bigger teams working exclusively on such data science projects\nbegan to emerge. Further, in the fast-moving world of data-driven development,\nmore flexible and iteration-based frameworks were needed, so other data science pro‐\ncesses began to emerge. The Microsoft Team Data Science Process (TDSP) addresses\nthis. It was released by the Microsoft Azure team in 2017 and is one of the modern\nprocesses for applying machine learning and working in data science [51].\nTDSP is an agile, iterative data science process for executing and delivering advanced\nanalytics solutions. It’s designed to improve the collaboration and efficiency of data\nscience teams in enterprise organizations. The main features of TDSP are:\n• A data science life cycle definition\n• A standardized project structure, which includes project documentation and\nreporting templates\n• An infrastructure for project execution\n• Tools for data science, like version control, data exploration, and modeling\nThe TDSP documentation [52] provides detailed insight into all of these aspects, so\nwe’ll just take a brief look in this section. The TDSP data science life cycle, showing\ndifferent phases of a data project, is shown in Figure 11-7.\n390 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 355,
      "char_count": 2206,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 421,
      "text": "Figure 11-7. The Microsoft TDSP life cycle [51]\nWhile TDSP shares some similarities with the KDD process, an interesting aspect of\nTDSP is that it defines a life cycle of a data science project from a business and team\nmanagement perspective. This includes the following stages:\n• Business understanding\n• Data acquisition and understanding\n• Modeling\n• Deployment\n• Customer acceptance\nAt a high level, the data science life cycle showcases how various components of an\neffective and agile data science team should operate. The “Charter” and “Exit Report”\ndocuments in the TDSP documentation are particularly important to consider. They\nhelp define the project at the start of an engagement and provide a final report to the\ncustomer or client.\nOverall, these processes can be useful for taking the problems and solutions we’ve\ndiscussed so far in this book from prototyping to deployment in a production system.\nThese processes are of course not specific to NLP and are more generic recommenda‐\ntions for any data-driven projects involving ML approaches. While there are other\nThe Data Science Process \n| \n391\n",
      "word_count": 179,
      "char_count": 1112,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1440,
          "height": 1001,
          "ext": "png",
          "size_bytes": 122856
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 422,
      "text": "similar project management processes for data science that are emerging as the field\ngrows, we hope this gives you an overview of what to look out for in managing your\nown NLP projects in a software development setting.\nMaking AI Succeed at Your Organization\nSo far, this book has focused on successfully building and deploying solutions for var‐\nious AI problems. Success of any AI project is dependent not just on the technical\nsuperiority of the solution—there are many other factors involved, too. It’s a known\nfact that a large number of AI projects in industry fail because the model doesn’t get\ndeployed or, if deployed, fails to achieve its objectives. According to a recent study by\nGartner [53], more than 85% of AI projects fail. Here, we discuss some key points\nand rules of thumb to make AI projects succeed. Many of these points come from our\nown experience of working in various domains of AI across various organizations.\nTeam\nIt’s important to have the right team to solve the AI problems at hand. In understand‐\ning the problem statement, prioritizing, developing, deploying, and consuming, a lot\ndepends on the skills of the team. While there’s no fixed recipe, in our experience, the\nright blend comes with having (1) scientists who build models, (2) engineers who\noperationalize and maintain models, and (3) leaders who manage AI teams and strat‐\negize. It’s good to have (4) scientists who have worked in industry after graduate\nschool, (5) engineers who understand scale and data pipelines, and 6) leaders who\nhave also been individual contributor scientists in the past. While (5) is pretty self-\nexplanatory, (4) and (6) warrant some explanation.\nLet’s look at (4) first. It’s important that scientists understand the fundamentals of\nmachine learning and are able to think of novel solutions. Graduate school (especially\na PhD) prepares you well for that. But, in industry, solving an AI problem is not just\napplying novel algorithms. It’s also about collecting and cleaning the data, making the\ndata consumption-ready, and applying known techniques. This is very different from\nacademia, where most work happens on known public datasets that are both readily\navailable and clean. Most researchers in academia work on devising novel approaches\nto beat the state-of-the-art results. In many cases, scientists fresh from academia end\nup applying sophisticated approaches that prove counterproductive. One is building\nAI for products—AI is just a means, and not the end. That’s why it’s important that\nsenior scientists on the team have built and deployed models in industrial settings.\nMoving on to (6): AI leadership is very different from software engineering leader‐\nship. Even though what runs in production in any AI system is code, AI is fundamen‐\ntally different from software engineering. Many leaders and organizations are not\naware of this nuance. They believe that, because it’s code, all the principles of software\nengineering apply to it. From defining the problem statement to planning project\n392 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 497,
      "char_count": 3078,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MyriadPro-SemiboldCond (18.9pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 423,
      "text": "timelines, developing an AI system is different from developing a traditional IT sys‐\ntem. This is why it’s recommended that AI leaders in your organization have the\nexperience of having been individual contributors (ICs) in the AI field.\nRight Problem and Right Expectations\nIn many cases, either the problem at hand is ill defined or AI teams set the wrong\nexpectations. Let’s understand this better with some examples. Consider a scenario\nwhere we’re given a dump of what customers say about a particular product or brand,\nand we’re asked to bring out “interesting” insights. This is a very common scenario in\nindustry; we discussed similar scenarios in “Topic Modeling” on page 250. Now, can\nwe apply topic modeling to this particular scenario? It depends on what “interesting”\nmeans in this context. It could be what the majority of customers are saying, or it\ncould be what a small subset of customers belonging to a particular region are saying,\nor it could be what customers are saying about a specific product feature. The possi‐\nbilities are many. It’s important to work with the stakeholders first to clearly define\nthe task. A great way to do this is to take a set of diverse example inputs that include\nedge cases and ask the stakeholders to write down the desired output. An important\nthing to keep in mind is that the ready availability of a lot of data does not make\nsomething an AI problem by default; many problems can be solved using engineering\nand rule-based and human-in-the-loop approaches.\nAnother common problem is stakeholders having wrong expectations of AI technol‐\nogy. This often happens because of articles in popular media that tend to compare AI\nto the human brain. While that’s correct as a motivation behind the area of AI, it’s far\nfrom the truth. For example, consider a scenario where we built a sentiment analysis\nsystem and, for a given input sentence, our system predicts a wrong output. It gives a\nvery high accuracy, but not 100%. Most stakeholders coming from the world of soft‐\nware engineering treat this as a bug and are not willing to accept anything that’s not\n100% correct. They are not aware of the fact that any AI system (as of today) will give\nwrong output for a subset of inputs. Another expectation of AI is that it will replace\nhuman effort completely, thus saving money. This is seldom the case. It’s better to\ntreat AI as augmented intelligence to support human efforts rather than artificial\nintelligence to replace human efforts. Also, beyond a point, model performance stag‐\nnates and doesn’t continue rising with time. We see this in Figure 11-8, where reality\nbehaves more like an S curve while the expectation continues rising.\nEven a very mature and advanced AI system requires human supervision. In many\ncases, we can reduce human efforts, but that happens over a long period of time. In\nthe same vein, stakeholders coming from software engineering may not understand\nthe importance of building responsible AI. Responsible AI ensures trustworthy solu‐\ntions that are fair, transparent, and accountable. Google [54] and Microsoft [55] have\npublished best practices for building responsible AI systems.\nMaking AI Succeed at Your Organization \n| \n393\n",
      "word_count": 537,
      "char_count": 3215,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 424,
      "text": "Figure 11-8. Expectation versus reality in AI performance\nData and Timing\nData is at the heart of any AI system. We’ve discussed various aspects of data in detail\nin previous chapters. Let’s look at one more: in many cases, just because an organiza‐\ntion has gigabytes or even petabytes of data, it doesn’t mean they’re ready for AI and\ncan quickly reap its benefits. There’s a difference between having data and having the\nright data. Let’s understand this:\nQuality of data\nTo perform well, any AI system needs a high quality of data for both training and\nprediction. What does high quality mean? Data that is structured, homogenous,\ncleaned, and free of noise and outliers. Going from a dump of noisy data to high-\nquality data is often a long process. The best way to think of it is the following\nanalogy: raw data is crude oil and AI models are fighter jets. Fighter jets need avi‐\nation fuel to fly; they cannot fly on crude oil. So, to enable fighter jets, someone\nmust set up the petroleum refinery to systematically extract the aviation fuel\nfrom the crude oil. And setting up this refinery is a long and expensive process.\nAnother important point is to have the right representative data: data that allows\nus to solve the problem at hand. For example, there’s no way we can improve our\nsearch feature if we don’t already have the metadata about what we want to\nsearch. So, if we don’t have “Adidas Shoes Size 10 Tennis Shoes” but only have\n“Adidas Shoes Size 10,” there’s no way we can easily make our search help find\ntennis shoes.\nQuantity of data\nMost AI models are a compressed representation of the dataset used to train\nthem. Not having enough data that’s a true representation of the data the model\nwill see in production is a big reason for models not performing well. How much\n394 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 328,
      "char_count": 1842,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 777,
          "height": 610,
          "ext": "png",
          "size_bytes": 21002
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 425,
      "text": "data is enough? This is a hard question to answer, but there are some rules of\nthumb. For instance, for sentence classification using baseline algorithms such as\nNaive Bayes or random forest, we’ve observed that having at least two to three\nthousand data points per class is a must to be able to build an acceptable\nclassifier.\nData labeling\nAs of today, most success stories of AI in industry have come from supervised AI.\nAs we discussed in initial chapters, it’s the subarea where, for each data point, we\nhave the ground truth. For many problems, the ground truth comes from human\nannotators. This is often a time-consuming and expensive process. In many\nindustrial settings, stakeholders aren’t aware of the importance of this step.\nData labeling is often a continuous process. While we do get data labeled in bulk\nas a one-time effort to build the first versions of our model, once the model is put\nin production and stabilizes, getting the production data annotated is a continu‐\nous process from there on. Further, we need to define processes for labeling and\nenforce quality checks to improve the accuracy and consistency of human anno‐\ntators. This is done using metrics like kappa to measure inter-annotator\nagreement [56].\nCurrently, AI talent comes at a high cost. Without the right data, it will be futile to\nhire AI talent; having the right data is a prerequisite for AI teams to deliver well and\nfast. By this, we don’t mean that we must have all of the prerequisites in place before\nbringing in AI talent. What we mean is that we must be fully aware of other prerequi‐\nsites, such as the right data, and have realistic expectations in the absence of it.\nA Good Process\nAnother important factor that often leads to the failure of AI projects is not following\nthe right process. In this chapter, we’ve already discussed both the KDD and Micro‐\nsoft processes. Both of them are great starting points. Here are some other important\npoints to consider when getting started:\nSet up the right metrics\nMost AI projects in industry aim to solve a business problem. In many cases,\nteams set up AI metrics like precision, recall, etc., as success metrics. But we must\nalso set up the right business metrics along with AI metrics. For example, let’s say\nwe’re building a text classifier to automatically assign customer complaints to the\nright customer care teams. The right metric for this is the number of times a\ncomplaint is reassigned to another team. A classifier that has a 95% F1 score but\nleads to many complaints being reassigned multiple times is of no use. Another\nexample of this is a chatbot system that correctly detects intent but has high user\ndrop-off rates. User interaction and drop-off rates provide a complete picture\nthat’s missed by using only AI-specific metrics.\nMaking AI Succeed at Your Organization \n| \n395\n",
      "word_count": 487,
      "char_count": 2841,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 426,
      "text": "Start simple, establish strong baselines\nAI scientists are often influenced by the latest techniques and recent state-of-the-\nart (SOTA) models and apply those in their work straight away. Most SOTA tech‐\nniques are both compute- and data-intensive, which leads to cost and time\noverruns. The best way is to start with simple approaches and build strong base‐\nlines. Many times, a SOTA technique might only give us marginal improvement\nover a rule-based system! Try multiple simple approaches first before pondering\nover complex approaches.\nMake it work, make it better\nBuilding a model is often only 5–10% of most AI projects; the remaining 90% is\nmade up of various steps, ranging from data collection to deployment, testing,\nmaintenance, monitoring, integration, pilot testing, etc. It’s always good to build\nan acceptable model quickly and complete one full project cycle instead of\nspending a huge amount of time building an amazing model. This helps all stake‐\nholders realize the value proposition of the project.\nKeep shorter turnaround cycles\nEven when solving a standard problem with well-known approaches, we must\nstill apply them to our dataset to see if they work or not. For example, if we’re\nbuilding a sentiment analysis system, it’s a well-known fact that Naive Bayes gives\nvery strong baselines. Yet it’s very much possible that for our dataset, Naive Bayes\nmight not give good numbers. Building AI systems involves a lot of experiments\nto figure what works and what doesn’t. Hence, it’s important to build models\nquickly and present the results to stakeholders frequently. This helps raise any\nred flags early and get early feedback.\nThere are a few other important things to consider, which we’ll cover next.\nOther Aspects\nIn addition to the various points we’ve discussed so far, there are some more key\npoints to consider, including compute costs and return on investment. Let’s discuss\nthose now:\nCost of compute\nMany AI models (especially DL-based models) are compute-intensive. Over\ntime, GPUs on the cloud or physical hardware prove to be considerably expen‐\nsive. Many organizations are known to spend huge amounts on GPU and other\ncloud services—so much that they have to create parallel projects to reduce these\ncosts.\nBlindly following SOTA\nPractitioners are often keen to apply SOTA models in their work. This often\nproves to be disastrous. For example, Meena [57], a SOTA chatbot system from\n396 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 398,
      "char_count": 2471,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 427,
      "text": "Google that gave amazing results, took over 2,048 TPU for 30 days for training.\nThat compute time is worth $1.4M. While Meena has shown some very impres‐\nsive results, imagine using Meena techniques to build a chatbot for automating\ncustomer support that saves $1,000 a day. We would need to run the chatbot for\nover four years just to break even on the training cost.\nROI\nAI projects are expensive; various stages, such as data collection, labeling, hiring\nAI talent, and compute all involve costs. For this reason, it’s important to estimate\nthe gains at the start of the project itself. We must establish the process and clear\nmetrics to measure the returns early on in the project.\nFull automation is hard\nWe can never achieve complete automation, at least for any moderately complex\nAI project—it will continue to require some manual effort. Figure 11-9 repre‐\nsents this in the same S curve we discussed earlier. Levels for complete automa‐\ntion and acceptable performance might change depending on the project, but the\nbroad point will hold true.\nFigure 11-9. Complete automation can be hard\nWe’ve covered some key points in this section, but making AI succeed in business is a\nvast topic. We suggest a few articles for further reading. While some of them bring\nout the distinctions between software engineering and AI, others discuss rules of\nthumb for building AI systems:\n• “Why Is Machine Learning ‘Hard'?,” a blog post by S. Zayd Enam, a Stanford\nresearcher [58]\n• “Software 2.0,” a blog post on AI as a different way of writing software by Andrej\nKarpathy, a well-known researcher, educator, and scientist at Tesla [59]\nMaking AI Succeed at Your Organization \n| \n397\n",
      "word_count": 284,
      "char_count": 1680,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 880,
          "height": 611,
          "ext": "png",
          "size_bytes": 25699
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 428,
      "text": "• “NLP’s Clever Hans Moment Has Arrived,” an article by Benjamin Heinzerling\nthat argues the validity of SOTA results obtained on certain popular datasets [60]\n• “Closing the AI Accountability Gap,” a report by a team at Google AI and the\nnonprofit Partnership on AI [61]\n• “The Twelve Truths of Machine Learning for the Real World,” a blog post by\nDelip Rao, researcher and O’Reilly author [62]\n• “What I’ve Learned Working with 12 Machine Learning Startups,” an article by\nDaniel Shenfeld, a startup veteran and ML consultant [63]\nThese will give you a more holistic picture. Figure 11-10 demonstrates what we’ve\ncovered in this section and the chapter.\nFigure 11-10. Life cycle of an AI project\nMany of these suggestions are not hard rules set in stone; their application will\ndepend on the context of your project, problem, data, and organization. We hope the\ndiscussion in this section will help in making your AI endeavors succeed.\nPeeking over the Horizon\nWe’d like to end this chapter and the book with various perspectives of how machine\nlearning is evolving. ML will continue improving on the cutting edge, and its applica‐\ntions will be more relevant to business in the coming years. One way to look at this is\nthe influential lecture by renowned scientist C.P. Snow in 1959, titled The Two Cul‐\ntures and the Scientific Revolution [64]. In this lecture, Snow states that the intellectual\n398 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 245,
      "char_count": 1447,
      "fonts": [
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1439,
          "height": 850,
          "ext": "png",
          "size_bytes": 69491
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 429,
      "text": "world can be seen from two distinct perspectives, which seem to be getting more divi‐\nded over time. One perspective is of science and technology and the other is con‐\ncerned with arts and humanities. He argues why it’s important for these two\nperspectives to have a common core for better advancements of the entire area. This\nis true for AI as well.\nAnalogously, in the world of AI, we see a similar set of two distinct perspectives\nemerging. On one hand, we have the advances made by researchers and scientists\nworking on the forefront. On the other hand, we have businesses trying to leverage\nAI. This includes everyone from Fortune 500 companies to early stage startups. The\nworld increasingly believes that the successful adoption of AI in industry will stem\nfrom an intersection of both.\nFrom the perspective of researchers and scientists, we see two macro trends: building\ntruly intelligent machines and applying AI for social good. For instance, François Chol‐\nlet of Google has stressed the importance of building better metrics to measure intel‐\nligence in “On the Measure of Intelligence” [65]. Most evaluation of AI models at\npresent is inherently narrow in nature and measures specific skills as opposed to\nbroad abilities and general intelligence. Chollet proposes certain measures inspired\nby testing of human intelligence, including efficiencies in new skill acquisition. They\nintroduce a dataset called Abstraction and Reasoning Corpus (ARC) that’s inspired by\na classic IQ test: the Raven’s Progressive Matrices [66]. One such example is presented\nin Figure 11-11, where the task is for the computer to infer the missing area by look‐\ning at the overall input matrix pattern. Work on improving measures of AI is neces‐\nsary for developing better and more robust AI in the future.\nAI and technology in general can be a force for social good. And there are now vari‐\nous initiatives that are working on AI for social good. Wadhwani AI is working on\nimproving maternal and early childhood health with AI [67]. Google AI for Social\nGood has a range of initiatives, including applying AI to predict and better manage\nfloods [68]. Similarly, Microsoft is using AI to solve global climate issues, improve\naccessibility, and preserve cultural heritage [69]. Allen AI has been improving\ncommon-sense reasoning in NLP through the WinoGrande dataset [70]. Such work\nby foundations and research labs is helping to incorporate the forefront of ML and\nNLP to improve human well-being.\nPeeking over the Horizon \n| \n399\n",
      "word_count": 413,
      "char_count": 2523,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 430,
      "text": "Figure 11-11. Example of an ARC task for general intelligence from [66]\n400 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 20,
      "char_count": 119,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 1062,
          "height": 2096,
          "ext": "png",
          "size_bytes": 89718
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 431,
      "text": "A completely different perspective comes from the world of business. This is more\npractical and is concerned with business impact and business models. For instance,\nseveral consulting firms have conducted surveys across organizations on use cases\nand effectiveness of AI across industry verticals. McKinsey & Company’s Global AI\nsurvey is one such example [71]. They discuss how AI has helped different verticals\nsave money by reducing inefficiencies and make more money by expanding the mar‐\nket. They also assess the impact of AI on the workforce and on which parts of organi‐\nzations it’s most impactful. Another such study is a report by MIT Sloan and BCG\n[72]. This is immensely useful for business leaders to learn how to onboard and grow\nAI inside their organizations.\nVenture capital (VC) firms have been investing heavily in startups building AI-\npowered businesses. Based on their understanding of how new AI businesses are\nformed and how they can succeed, they’re compiling reports and debriefs. Andressen\nHorowitz, a major VC firm, has published a report, “The New Business of AI,” based\non their learnings in many AI investments [73]. The report addresses business issues\nthat AI startups are facing despite the hype, like lower gross margins and product-\nscaling challenges. They’ve provided practical advice on building AI businesses that\ncan scale better and be more competitive.\nThis range of perspectives will be applicable depending on where your organization is\nin their AI journey. First, when starting a new AI business, lessons from VCs will help\nyou decide what to build. Second, to formulate an AI strategy in a large organization,\nsurveys and reports from industry will align you better. Last but not least, as your\norganization matures, incorporating SOTA techniques can lead to a step change in\nyour products.\nFinal Words\nAnd here we come to the end of Practical Natural Language Processing! We hope\nyou’ve learned a few things about NLP tasks and pipelines and their applications in\nvarious domains and that these will help you in your day-to-day work. The advances\nin NLP are just starting to bear big fruits. Some of the most fundamental questions in\nNLP, like context and common sense, have probably yet to even be asked properly.\nTrue mastery of any skill requires a lifetime of learning, and we hope that our refer‐\nences, research papers, and industry reports will help you continue the journey.\nReferences\n[1] ONNX: An open format built to represent machine learning models. Last\naccessed June 15, 2020.\n[2] Apache Airflow. Last accessed June 15, 2020.\nFinal Words \n| \n401\n",
      "word_count": 426,
      "char_count": 2609,
      "fonts": [
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (18.9pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 432,
      "text": "[3] Apache Oozie. Last accessed June 15, 2020.\n[4] Chef. Last accessed June 15, 2020.\n[5] Microsoft. “MLOps examples”. Last accessed June 15, 2020.\n[6] Microsoft. MLOps using Azure ML Services and Azure DevOps, (GitHub repo).\nLast accessed June 15, 2020.\n[7] Elastic. “Anomaly Detection”.\n[8] Krzus, Matt and and Jason Berkowitz. “Text Classification with Gluon on Amazon\nSageMaker and AWS Batch”. AWS Machine Learning Blog, March 20, 2018.\n[9] The Pallets Projects. “Flask”. Last accessed June 15, 2020.\n[10] The Falcon Web Framework. Last accessed June 15, 2020.\n[11] Django: The web framework for perfectionists with deadlines. Last accessed June\n15, 2020.\n[12] Docker. Last accessed June 15, 2020.\n[13] Kubernetes: Production-Grade Container Orchestration. Last accessed June 15,\n2020.\n[14] Amazon. AWS SageMaker. Last accessed June 15, 2020.\n[15] Microsoft. Azure Cognitive Services. Last accessed June 15, 2020.\n[16] Sucik, Sam. “Compressing BERT for Faster Prediction”. Rasa (blog), August 8,\n2019.\n[17] Cheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. “A Survey of Model Compres‐\nsion and Acceleration for Deep Neural Networks.” 2017.\n[18] Joulin, Armand, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve\nJégou, and Tomas Mikolov. “FastText.zip: Compressing Text Classification Models”,\n2016.\n[19] Chee, Cedric. Awesome machine learning model compression research papers,\ntools, and learning material, (GitHub repo). Last accessed June 15, 2020.\n[20] Burkov, Andriy. Machine Learning Engineering (Draft). 2019.\n[21] Cheng, Heng-Tze. “Wide & Deep Learning: Better Together with TensorFlow.”\nGoogle AI Blog, June 29, 2016.\n[22] Zheng, Alice and Amanda Casari. Feature Engineering for Machine Learning.\nBoston: O’Reilly, 2018. ISBN: 978-9-35213-711-4\n[23] DVC: Open source version control system for machine learning projects. Last\naccessed June 15, 2020.\n402 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 281,
      "char_count": 1911,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 433,
      "text": "[24] Gundersen, Odd Erik and Sigbjørn Kjensmo. “State of the Art: Reproducibility in\nArtificial Intelligence.” The Thirty-Second AAAI Conference on Artificial Intelligence\n(2018).\n[25] Gibney, E. “This AI Researcher Is Trying to Ward Off a Reproducibility Crisis.”\nNature 577.7788 (2020): 14.\n[26] TensorFlow. “Getting Started with TensorFlow Model Analysis”. Last accessed\nJune 15, 2020.\n[27] Marco Tulio Correia Ribeiro. Lime: Explaining the predictions of any machine\nlearning classifier, (GitHub repo). Last accessed June 15, 2020.\n[28] Lundberg, Scott. Shap: A game theoretic approach to explain the output of any\nmachine learning model, (GitHub repo). Last accessed June 15, 2020.\n[29] TensorFlow. “Get started with TensorFlow Data Validation”. Last accessed June\n15, 2020.\n[30] Miller, Tim. “Explanation in Artificial Intelligence: Insights from the Social Sci‐\nences”, (2017).\n[31] Molnar, Christoph. Interpretable Machine Learning: A Guide for Making Black\nBox Models Explainable. 2019.\n[32] Sumo Logic. “Outlier”. Last accessed June 15, 2020.\n[33] Microsoft. “Anomaly Detector API Documentation”. Last accessed June 15, 2020.\n[34] Domingos, Pedro. “A Few Useful Things to Know about Machine Learning.”\nCommunications of the ACM 55.10(2012): 78–87.\n[35] Sculley, D., Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar\nEbner, Vinay Chaudhary, and Michael Young. “Machine Learning: The High Interest\nCredit Card of Technical Debt.” SE4ML: Software Engineering for Machine Learning\n(NIPS 2014 Workshop).\n[36] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar\nEbner, VinayChaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison.\n“Hidden Technical Debt in Machine Learning Systems.” Proceedings of the 28th Inter‐\nnational Conference on Neural Information Processing Systems 2 (2015): 2503–2511.\n[37] McMahan, H. Brendan, Gary Holt, David Sculley, Michael Young, Dietmar\nEbner, Julian Grady, Lan Nie et al. “Ad Click Prediction: A View from the Trenches.”\nProceedings of the 19th ACM SIGKDD International Conference on Knowledge Discov‐\nery and Data Mining (2013): 1222–1230.\n[38] Zinkevich, Martin. “Rules of Machine Learning: Best Practices for ML Engineer‐\ning”. Google Machine Learning. Last accessed June 15, 2020.\nFinal Words \n| \n403\n",
      "word_count": 327,
      "char_count": 2296,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 434,
      "text": "[39] Halevy, Alon, Peter Norvig, and Fernando Pereira. “The Unreasonable Effective‐\nness of Data.” IEEE Intelligent Systems 24.2 (2009): 8–12.\n[40] Sun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. “Revisiting\nUnreasonable Effectiveness of Data in Deep Learning Era.” Proceedings of the IEEE\nInternational Conference on Computer Vision (2017): 843–852.\n[41] Petrov, Slav. “Announcing SyntaxNet: The World’s Most Accurate Parser Goes\nOpen Source”. Google AI Blog, May 12, 2016.\n[42] Marcus, Mitchell, Beatrice Santorini, and Mary Ann Marcinkiewicz. “Building a\nLarge Annotated Corpus of English: The Penn Treebank”. Computational Linguistics\n19, Number 2, Special Issue on Using Large Corpora: II (June 1993).\n[43] Feurer, Matthias, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Man‐\nuel Blum, and Frank Hutter. “Efficient and Robust Automated Machine Learning.”\nAdvances in Neural Information Processing Systems 28 (2015): 2962–2970.\n[44] Le Cun, Yann, Corinna Cortes and Christopher J.C. Burges. “The MNIST data‐\nbase of handwritten digits”. Last accessed June 15, 2020.\n[45] Google Cloud. “Features and capabilities of AutoML Natural Language”. Last\naccessed June 15, 2020.\n[46] Google Cloud. “AutoML Translation”. Last accessed June 15, 2020.\n[47] Microsoft Azure. “What is automated machine learning (AutoML)?”, February\n28, 2020.\n[48] Thakur, Abhishek and Artus Krohn-Grimberghe. “AutoCompete: A Framework\nfor Machine Learning Competition”, (2015).\n[49] Thakur, Abhishek. “Approaching (Almost) Any NLP Problem on Kaggle”. Last\naccessed June 15, 2020.\n[50] Fayyad, Usama, Gregory Piatetsky-Shapiro, and Padhraic Smyth. “The KDD\nProcess for Extracting Useful Knowledge from Volumes of Data.” Communications of\nthe ACM 39.11 (1996): 27–34.\n[51] Microsoft Azure. “What is the Team Data Science Process?”, January 10, 2020.\n[52] Microsoft. “Team Data Science Process Documentation”. Last accessed June 15,\n2020.\n[53] Kidd, Chrissy. “Why Does Gartner Predict up to 85% of AI Projects Will ‘Not\nDeliver’ for CIOs?”, BMC Machine Learning & Big Data Blog, December 18, 2018.\n[54] Google AI. “Responsible AI Practices”. Last accessed June 15, 2020.\n[55] Microsoft. “Microsoft AI principles”. Last accessed June 15, 2020.\n404 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 328,
      "char_count": 2289,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 435,
      "text": "[56] Artstein, Ron and Massimo Poesio. “Inter-Coder Agreement for Computational\nLinguistics.” Computational Linguistics 34.4 (2008): 555–596.\n[57] Adiwardana, Daniel and Thang Luong. “Towards a Conversational Agent that\nCan Chat About…Anything”. Google AI Blog, January 28, 2020.\n[58] Enam, S. Zayd. “Why is Machine Learning ‘Hard’?”, Zayd’s Blog, November 10,\n2016.\n[59] Karpathy, Andrej. “Software 2.0”. Medium Programming, November 11, 2017.\n[60] Heinzerling, Benjamin. “NLP’s Clever Hans Moment has Arrived”. The Gradient,\nAugust 26, 2019.\n[61] Raji, Inioluwa Deborah, Andrew Smart, Rebecca N. White, Margaret Mitchell,\nTimnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker\nBarnes. “Closing the AI Accountability Gap: Defining an End-to-End Framework for\nInternal Algorithmic Auditing”, (2020).\n[62] Rao, Delip. “The Twelve Truths of Machine Learning for the Real World”. Delip\nRao (blog), December 25, 2019.\n[63] Shenfeld, David. “What I’ve Learned Working with 12 Machine Learning Start‐\nups”. Towards Data Science (blog), May 6, 2019.\n[64] Snow, Charles Percy. The Two Cultures and the Scientific Revolution. Connecticut:\nMartino Fine Books, 2013.\n[65] Chollet, François. “On The Measure of Intelligence”, (2019).\n[66] John, Raven J. “Raven Progressive Matrices,” in Handbook of Nonverbal Assess‐\nment, Boston: Springer, 2003.\n[67] Wadhwani AI. “Maternal, Newborn, and Child Health”. Last accessed June 15,\n2020.\n[68] Matias, Yossi. “Keeping People Safe with AI-Enabled Flood Forecasting”. Google\nThe Keyword (blog), September 24, 2018.\n[69] Microsoft. “AI for Good”. Last accessed June 15, 2020.\n[70] Sakaguchi, Keisuke, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.\n“WinoGrande: An Adversarial Winograd Schema Challenge at Scale”, (2019).\n[71] Cam, Arif, Michael Chui, and Bryce Hall. “Global AI Survey: AI Proves Its\nWorth, but Few Scale Impact”. McKinsey & Company Featured Insights, November\n2019.\n[72] Ransbotham, Sam, Philipp Gerbert, Martin Reeves, David Kiron, and Michael\nSpira. “Artificial Intelligence in Business Gets Real.” MIT Sloan Management Review\n(September 2018).\nFinal Words \n| \n405\n",
      "word_count": 304,
      "char_count": 2138,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 436,
      "text": "[73] Casado, Martin and Matt Bornstein. “The New Business of AI (and How It’s Dif‐\nferent From Traditional Software)”. Andreesen Horowitz, February 16, 2020.\n406 \n| \nChapter 11: The End-to-End NLP Process\n",
      "word_count": 32,
      "char_count": 205,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MyriadPro-SemiboldCond (9.0pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 437,
      "text": "Index\nA\nA/B testing, 263, 324, 378\nAbstraction and Reasoning Corpus (ARC)\ndataset, 399, 400\nabstractive summarization, 256, 259\naccounting and auditing, 362\naccuracy, 69\nACM SIGKDD Conference on Knowledge Dis‐\ncovery and Data Mining (KDD), 388\nactive learning, 42\nNER using, 176-177\nwith Prodigy, 153\ntext classification, 150-152, 153\nAdamic, Lada, 299\nadapting to new domains, 149-152\nadult content filtering, 277\nadvanced processing, 49, 57-60\nAI (see artificial intelligence)\nAirbnb, 262, 309\nAirflow (Apache), 373\nAlgolia, 250\nAllen AI, 399\nAllenNLP, 175, 184\nDeepQA library, 269\nGrover, 302\nAmazon, 5, 122, 240\nfaceted search on, 310\n“Reviews that mention” filter, 166\nsentiment analysis APIs, 126\nAmazon Alexa, 3, 5, 7, 31, 197\nAmazon Comprehend, 65, 190\nAmazon Comprehend Medical, 356-357\nAmazon Mechanical Turk, 150\nAmazon Research, 301\nAmazon Translate, 5\nAmazon Web Services (AWS), 72\nAWS Cloud, 374\nAWS S3, 373\nSageMaker, 374, 375\nambiguity , 12-13\nanomaly detection, 382\nanswer extraction, 268\nanswering questions (see question answering)\nApache Airflow, 373\nApache Nutch, 244\nApache Oozie, 373\nApache Solr software, 245\nAPIs\nintegration of, 229\nmodeling with, 65\nMT, 264-265\nproduct categorization and taxonomy, 320,\n321\ntext classification with, 126, 153\nApple, 297\nApple Siri, 3, 5, 7, 31\nArabic, 74\nARC (see Abstraction and Reasoning Corpus)\nArria, 5\nartificial intelligence (AI), 14, 197, 371\nexpectations vs reality, 393\nfurther reading, 397-398\nkey points and rules of thumb, 392-398\nperspectives on, 399\nfor social good, 399\naspect-based sentiment analysis, 122-123,\n327-329\naspects, 327\n407\n",
      "word_count": 245,
      "char_count": 1612,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (25.2pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 438,
      "text": "connecting overall ratings to, 329-330\nunderstanding, 330-331\nASR (automatic speech recognition), 48\nassessment tools, 6\nATIS (Airline Travel Information Systems)\ndataset, 220\nattention, 352\nattention networks, 381\nattribute extraction, 308, 312-317\nalgorithms, 314\nderived, 315\ndirect, 314, 315-316\nindirect, 316-317\nlatent, 335\nattribute match, 323\nAUC, 69\nAudit Command Language (Deloitte), 362\nauditing, 362\nauthorship attribution, 123\nAutoCompete framework, 388\nautoencoders, 27-28\nautomated machine learning (AutoML),\n384-388\nautomated scoring, 6, 113\nAutomatic Content Extraction Program, 162\nautomatic response generation, 219\nautomatic speech recognition (ASR), 48\nautomation, full, 397\nautosklearn, 385-387\nautotldr bot (Reddit), 256\nAWS (see Amazon Web Services)\nB\nback translation, 41, 41, 266\nbag of n-grams (BoN), 89-90\nbag of words (BoW), 87-89, 294, 301\nbaselines, 396\nBayes’ theorem, 20\nBBC, 190\nBCG, 401\nBeautiful Soup library, 44, 291\nBender, Emily, 12\nBERT (Bidirectional Encoder Representations\nfrom Transformers), 26, 107, 221\nfact-verification with, 301\nsearch with, 246\nsize, 375\ntext classification with, 145-146\nBERT for Biomedical Text (BioBERT), 357, 358\nBERT for financial text (FinBERT), 361\nbest practices, 352\nbias, 108, 156\nbigram flipping, 41\nbilling, 342\nbinary classification, 120\nBing (search engine), 5, 266\nBing Answer Search API, 269\nBing Microsoft Translator, 5\nBing Translate API, 264\nBioBERT (BERT for Biomedical Text), 357, 358\nbiographical information extraction, 242\nBLEU (Bilingual Evaluation Understudy), 69,\n70\nBloomberg, 360\nBloomberg Terminal, 187\nBoN (bag of n-grams), 89-90\nbootstrapping, 149\nBoW (bag of words), 87-89, 294, 301\nBuoy system, 343\nBurmese, 74\nbusiness issues, 401\nC\nCapital Float and Microbnk, 361\nCaptcha tests, 150\nCarnegie Mellon University, 384\nCasari, Amanda, 384\ncatalogs, 308-309, 312-324\ncategorization, product, 317-321\nCBOW (continuous bag of words), 98-100, 102,\n104\nCDQA-Suite library, 269\nCFG (context-free grammar), 18\ncharacter embeddings, 297\ncharacter models, 354\ncharacter n-gram embeddings, 296\ncharacter n-gram–based models, 354\ncharacteristic features, 315\nchatbots, 163, 197-235, 396\napplications, 198-200\nbuild walkthrough, 206-216\ncase study, 230-234\nchitchat types, 202\nexact answer, 200\nexamples, 197\nexisting frameworks, 231-232\nFAQ bot, 199-200, 201\nflow-based, 201\n408 \n| \nIndex\n",
      "word_count": 330,
      "char_count": 2376,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 439,
      "text": "goal-oriented dialog, 202\nhealthcare, 342-344\nopen, 201\nopen-ended, 201, 233-234\npipeline for, 203-204\ntaxonomy of, 200-203\nterminology, 205\ntesting, 214-216\ntext-based, 204\nvoice-based, 204\nChee, Cedric, 376\nChef, 373\nChinese, 74\nchitchats, 202\nChollet, François, 399\nChomsky, Noam, 3, 18\nChronic, 187\nCJK languages, 74\nclassification, 119\n(see also text classification)\nbinary, 120\nmulticlass, 120\nproduct categorization and taxonomy,\n317-321\ntopic, 120\nclassification tasks, 19, 70\nclinical decision support systems, 342\nclinical information systems, 344-345\ncloud storage, 373\nclpsych.org, 123\nCMU Book Summaries dataset, 247\nCNNs (convolutional neural networks), 24,\n140, 143-144, 221\ncode mixing, 56\ncode reproducibility, 379\ncoherence, 255\ncollaborative filtering, 260\nColumbia University, 344\ncommon knowledge, 13-14\ncommon sense, 30\ncomplements, 333, 334\ncompute costs, 396\nconditional random fields (CRFs), 22, 172\nconfusion matrix, 70\ncongratsbot, 187\nCONLL-03 dataset, 175\nconnotation, 93\ncontent acquisition, 246\ncontent classification and organization, 121\ncontent discovery\nchatbots in, 198\nwith emotional content, 354\nlegal, 364\ncontext, 8, 12\ncontext window, 103\ncontext-based conversations, 228\ncontext-free grammar (CFG), 18\ncontextual features, 316\ncontextual word representations, 107\ncontinuous bag of words (CBOW), 98-100, 102,\n104\ncontract generation, 363\ncontract review, 363\nContraxSuite, 364, 365\nconversational agents, 7\ncase study, 31-33\ntypical flow, 32\nconversations\nactionable, 297\ncontext-based, 228\nconvolutional neural networks (CNNs), 140,\n143-144, 221\nCoppersmith, Glen, 353\ncoreference resolution, 32, 58\ncorporate ticketing (case study), 152-155\ncost of compute, 396\ncosts, 30\nCOTA (Customer Obsession Ticketing Assis‐\ntant), 74-76\ncovariate shift, 376\ncrawlers, 244\ncrawling/content acquisition, 246\ncreativity, 14\ncredit risk, 361-362\nCRFs (conditional random fields), 22, 172\nCRM (customer relationship management) ,\n297\ncrowdsourcing, 150\nCustomer Obsession Ticketing Assistant\n(COTA), 74-76\ncustomer relationship management (CRM),\n297\ncustomer support\nchatbots in, 199\non social channels, 121, 278, 297-299\nD\nDARPA, 198\nIndex \n| \n409\n",
      "word_count": 292,
      "char_count": 2178,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 440,
      "text": "Dartmouth College, 14\nDas, Vir, 275\ndata acquisition, 39-42\ndata annotation, 228-229\ndata augmentation, 40-42\ndata extraction (see information extraction)\ndata labeling, 395\ndata mining, 389\ndata pre-processing, 389\n(see also pre-processing)\ndata reduction, 389\ndata science (DS), 371, 388-392\nData Version Control (DVC), 379\ndatabases, in-memory, 108\nDataminr, 360\ndatasets\ncomplex, 350\ncrowdsourcing for, 150\nfor dialog systems, 220, 235\nfact-verification with, 301\ngoal-oriented, 220\ni2b2, 350\npublic, 40, 153, 289\ntarget, 389\ntext classification with, 126-127, 153\nversion control , 379\nDawkins, Richard, 299\nDBOW (distributed bag of words), 106\nDBpedia dataset, 136\nDBpedia Spotlight, 179\ndecision support systems, 342\ndeep learning (DL)\nattribute extraction with, 316\nfor dialogue generation, 225-226\nfact-verification with, 301\nfeature engineering, 61, 62\nlimitations, 28-31\nfor NLP, 23-28\noverview, 14-16\nquestion answering with, 269\nreinforcement learning, 225-226\nfor text classification, 140-147\nwith EHRs, 351\ndeep neural networks, 269\nDeepQA library, 269\nDeloitte, 362\nDemoji, 292\ndenotation, 93\ndeployment, 72, 372-374\ndevice, 30\nexample scenario, 374-376\ntypical steps, 373\nderived attribute extraction, 315\ndeterministic matches, 18\ndevice deployment, 30\ndiagnosis chatbots, 343, 344\ndialog act classification, 217\ndialog act or intent, 205\ndialog act prediction, 220-222\ndialog management, 32\ndialog managers, 204\ndialog state or context, 205\ndialog systems\ncomponents, 216-224\ndatasets for, 220, 235\ndeep reinforcement learning for, 225-226\nin detail, 204-216\nend-to-end approach, 225\nexamples with code walkthrough, 219-224\nhuman-in-the-loop, 226-227\nother pipelines, 224-227\npipeline for, 203-204\nterminology, 205\nDialogflow (Google), 206, 232\nchatbot build with, 206-216\ncreating agents with, 207\ndigits: removing, 53\ndimensionality, 142\ndirect attribute extraction, 314, 315-316\ndiscriminative classifier, 131\ndisplaCy visualizer, 171\ndistant supervision, 183\ndistributed bag of words (DBOW), 106\ndistributed memory (DM), 106\ndistributed representations, 92-106\ndistributional hypothesis, 93\ndistributional similarity, 93\ndiversity across languages, 14\nDjango Project, 375\nDL (see deep learning)\nDM (distributed memory), 106\nDoc2vec model, 106, 140\nserving recommendations with, 261\ntext classification with, 138-140\ntraining, 139\nDocker, 375\ndocument categorization, 120\n(see also text classification)\n410 \n| \nIndex\n",
      "word_count": 328,
      "char_count": 2439,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 441,
      "text": "document embeddings\ntext classification with, 138-140\nvisualizing, 110-110\nDocument Understanding Conference series,\n256\ndomain adaptation, 151-152\nDomingoes, Pedro, 384\nDS (data science), 371, 388-392\nDSTC dataset, 220\nDuckling library, 186\nDuolingo, 6\nE\ne-commerce and retail, 307-336\ncatalogs, 308-309, 312-324\nchatbots, 198\nNLP applications, 5, 308\nproduct categorization and taxonomy,\n317-321\nproduct deduplication and matching,\n323-324\nproduct enrichment, 321-322\nrecommendations for, 332-335\nreview analysis, 324-331\nsearch, 309-312\ntext classification, 122-123\nEarley parser, 18\nEasy Data Augmentation (EDA), 42\neBay, 122, 320\nEconomic News Article Tone and Relevance\ndataset (Figure Eight), 126, 148\nEDA (Easy Data Augmentation), 42\neditorial reviews, 331\nEducational Testing Service (ETS), 112, 113\nEHRs (see electronic health records)\nElastic on Azure, 249\nElasticsearch, 247-248\nDSL, 311\nELK stack, 382\nPython API, 247\nElasticsearch Learning to Rank, 249\nelectronic health records (EHRs), 344-353\nexample, 341\nEliza chatbot, 197\nELK stack (Elastic), 382\nELMo, 107\nemail platforms, 5\nemails: IE from, 186, 191\nembedding, 93\ncharacter, 297\ncharacter n-gram, 296\npre-trained, 108, 295\nsubword, 136-138\nvisualizing, 108-110\nword, 94-103\nword-based, 295\nbeyond words, 103-105\nemojis, 292\nemotion classification, 354, 355\nemr, 349\nemrQA dataset, 349, 350\nEnam, S. Zayd, 397\nEnglish, 9, 23, 74, 266\nensembling, 156\nenterprise search engines, 243, 246-247\nentity linking, 178\nEntityRuler (spaCy), 171, 176\nErnst & Young, 362\nerror correction, 47-49\nETS (Educational Testing Service), 112, 113\nevaluation, 68-72, 399\nextrinsic, 68, 71-72\nintrinsic, 68-71\nvisual methods, 70\nevent extraction, 164, 187-188\nextractive summarization, 256, 259\nF\nF1 score, 69\nFacebook, 275\ncustomer support on, 297\ndata acquisition, 40\nfastText embeddings, 95, 105, 136\nfastText library, 136-138\nmemes on, 299\nFacebook AI Research, 375\nFacebook Messenger, 186, 197\nfaceted search, 310-312\nfacets, 310\nfact-verification, 301\nfake news, 123, 277, 299, 300-302\nFalcon Web Framework, 375\nfalse propaganda, 277\nFAQ bots, 199-200, 201\nFast Healthcare Interoperability Resources\n(FHIR) standard, 351, 356\nfastText classification models, 375\nfastText embeddings (Facebook), 95, 105, 136\nIndex \n| \n411\n",
      "word_count": 319,
      "char_count": 2275,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 442,
      "text": "fastText library (Facebook), 136-138\nfat-finger problem, 42, 46\nfeature engineering, 60-62, 66, 377\nfeature extraction, 60, 81\n(see also feature engineering)\nfeature representation, 82\nhandcrafted, 112-113\nfeature selection, 377-378\nfeatures, 15\nfeedback, 245, 247\nfrom customers (see review analysis)\nexplicit, 154\nimplicit, 154\nlearning from, 154\nfew-shot learning, 29\nFHIR (Fast Healthcare Interoperability Resour‐\nces) standard, 351, 356\nfigurative language, 12\nFigure Eight, 126, 138, 148, 150\nfiltering adult content, 277\nfiltering, collaborative, 260\nfinancial industry, 339\nFinancial PhraseBank, 361\nfinancial sentiment, 360-361\nfinancial services, 5, 358-360\nestimated ML benefits, 359\nNLP applications, 360-363\nFinBERT (BERT for financial text), 361\nFinnish, 74\nfixed responses, 218\nFlask, 375\nformalism, 283\nforms: data extraction from, 163\nFreebase, 183\nFrench, 74\nG\nGATE (General Architecture for Text Engineer‐\ning), 18\ngated recurrent units (GRUs), 23\nGehrmann, Sebastian, 302\ngenerative chatbots, open-ended, 233-234\ngenerative classifier, 131\ngensim library, 96, 103, 135, 258\nKPE with, 168\ntext summarization with, 258\ntutorial on LDA, 255\nGlobal AI survey (McKinsey), 401\nGloVe , 95, 135, 142, 295\nGmail, 5, 186\nGnip, 288\ngoal-oriented dialogs, 202, 220\nGoldberg, Yoav, 81\ngood, social, 399\nGoogle\nCaptcha tests, 150\ndata acquisition, 40\ndataset search system, 126\nsearch engine, 5, 241, 244, 246, 266\nWord2vec model, 94-97, 102, 103, 105, 108,\n134, 295, 375\nGoogle AI, 351, 352, 384, 398\nGoogle AI for Social Good, 399\nGoogle Analytics, 263\nGoogle APIs, 126, 153, 264\nGoogle Assistant, 5, 197\nGoogle Cloud\nAutoML, 385, 387\nDialogflow, 206-216, 232\nNatural Language, 65, 126\nNLP, 190\ntask queues, 72\nGoogle Cloud Storage, 373\nGoogle Docs, 6\nGoogle Home, 3\nGoogle Knowledge Graph, 6, 179\nGoogle News, 163\nGoogle Search, 7, 242, 384\nGoogle Translate, 5, 7, 13, 240, 263, 264\nlanguage identification, 123\ngrammar, 280\ngrammar-correction tools, 6, 113\nGrammarly, 6, 113\nground truth, 15, 68\nGrover (AllenNLP), 302\nGRUs (gated recurrent units), 23\nH\nhandcrafted feature representations, 112-113\nHarvard University, 302\nHARVEST system, 344-347\nhealth assistants, 342-344\nhealth outcomes, 351\nhealth records, 341\nhealthcare, 5, 339-357\nchatbots in, 199\nGoogle APIs, 153\nNLP applications, 340\n412 \n| \nIndex\n",
      "word_count": 344,
      "char_count": 2318,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 443,
      "text": "patient prioritization and billing, 342\nquestion answering for, 349-350\nHealthTermFinder, 347\nHeinzerling, Benjamin, 398\nheuristics, 16-18, 104\ncombining with ML, 64\nreapplying, 67\nsimple, 63\nHidden Markov Model (HMM), 21-22\nHindi, 74\nHMM (Hidden Markov Model), 21-22\nHorowitz, Andreessen, 307\nHTML parsing and cleanup, 44-45\nHuang, Jensen, 339\nHubbard, Elbert Green, 197\nhuman-in-the-loop, 226-227\nI\ni2b2 datasets, 350\nIBM Research, 113, 178, 349\nIBM Watson, 6, 65, 179, 269\nRE with, 184-185\nICD-10-CM, 357\nIDF (inverse document frequency), 91\nIE (see information extraction)\nimage match, 324\nimage representation, 82\nimages: text extraction from, 47\nImbalanced-Learn, 130\nIMDB dataset, 289\nin-memory databases, 108\nindexers, 245\nindexing, 241, 245, 246, 247\nindirect attribute extraction, 316-317\nInfermedica API, 343, 344\ninformation, 161\ninformation extraction (IE), 7, 161-195\nadvanced tasks, 185-190\ncase study, 190-193\ngeneral pipeline, 165-166\nhistorical background, 162\nlegal, 364-365\nfrom loan agreements, 362\nmedical, 355-357\nopen, 183, 184\nreal-world applications, 162-163\ntasks, 164-165\ntemporal, 164, 186-187\ntypical pipeline, 166\ninformation retrieval, 7, 241-250\nInstagram, 275\nintelligent machines, 399\nintelligent tutoring systems, 6\ninteractive learning, 228\ninterpretability, 352, 382\ninverted indexes, 245\niNYP system, 344-345\nJ\nJapanese, 74\nJAPE (Java Annotation Patterns Engine), 18\nJeopardy!, 6, 269\nJordan, Jeff, 307\nK\nKaggle, 289\nKarpathy, Andrej, 397\nKDD (Knowledge Discovery and Data Mining)\nprocess, 388-390\nkey performance indicators (KPIs), 124, 378,\n382\nkey terms, 284\nkey words, 251\n(see also topics)\nkeyphrase extraction (KPE), 164, 166-169, 177\nkeyword extraction, 164\nknowledge bases, 6\nKnowledge Discovery and Data Mining (KDD)\nprocess, 388-390\nknowledge-based question answering, 269\nKoffka, Kurt, 37\nKorean, 74\nKPE (see keyphase extraction)\nKPIs (key performance indicators), 124, 378,\n382\nktrain, 145\nKubernetes, 375\nL\nlabeled data, 39\nlabeling, 395\nlabels, 15, 68\nlanguage\nambiguity in, 12-13\nbuilding blocks of, 8-12\ncharacteristics that make NLP challenging,\n12-14\nIndex \n| \n413\n",
      "word_count": 298,
      "char_count": 2122,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 444,
      "text": "diversity across, 14\nfigurative, 12\nnon-English languages, 56, 73-74\nlanguage detection, 56\nlanguage identification, 123\nlanguage learning apps, 6\nlanguage modeling, 6, 107\nlanguage models, large, pre-trained, 145-147\nLARA (latent rating regression analysis), 329\nlarge, pre-trained language models, 145-147\nlatent attribute extraction, 335\nlatent Dirichlet allocation (LDA), 251-253, 328,\n335\nlatent rating regression analysis (LARA), 329\nlatent semantic analysis (LSA), 251\nLatin, 74\nlaw (see legal services)\nLDA (latent Dirichlet allocation), 251-253, 328,\n335\nleadership, 392\nlearning\ninteractive, 228\nwith no or less data, 149-152\nlearning and assessment tools, 6\nlegal discovery, 364\nlegal entity extraction, 364-365\nlegal research, 363\nlegal services, 339, 358, 363-365\nchatbots, 199\nlemmatization, 54\nlength of text, 283\nlexemes, 10\nlexicon-based sentiment analysis, 125\nLexNLP, 364-365\nLexRank, 330\nLime, 147, 380, 381\nlinguistics, 8-12\nlink prediction, 335\nLinkedIn, 300\nLipton, Zachary, 31\nloan agreements: entity extraction from, 362\nlocational features, 316\nlogistic regression, 131\nlong short-term memory networks (LSTMs),\n23, 140, 144-145, 301\nLSTM-CRF, 316\nlowercasing, 53\nLSA (latent semantic analysis), 251\nLSI (Latent Semantic Indexing), 75\nLSTM-CRF, 316\nLSTMs (long short-term memory networks),\n23, 140, 144-145, 301\nLucidworks, 320\nM\nmachine intelligence, 14\n(see also artificial intelligence)\nmachine learning (ML), 371\nautomated, 384-388\nclassical, 62\nestimated benefits, 359\nfeature engineering, 62\nfuture directions, 398-401\nlandmark work, 384\nfor NLP, 19-22\noverview, 14-16\nmachine translation (MT), 7, 240, 263-266\nwith APIs, 264-265\npractical advice, 265-266\nuse cases, 264\nmachine translation services, 5\nMalayalam, 74\nMAP (Mean Average Precision), 69, 70\nMAPE (Mean Absolute Percentage Error), 69\nmarkup elements: removing, 291\nMcAuley, Julian, 334\nMcKinsey &amp; Company, 401\nMechanical Turk, 220, 313\nmedical care (see healthcare)\nmedical information extraction and analysis,\n355-357\nmedical records, 341\nMeena system, 396\nmemes, 299-300\nmental healthcare monitoring, 123, 353-354\nmental health–related issues, 353\nMessage Understanding Conferences (US\nNavy), 162\nmetadata, 249, 394\nMETEOR, 69\nmetrics, 395\nMicrosoft, 58\nAI for social good, 399\nanomaly detection, 383\ndata acquisition, 40\nreference pipelines for MLOps, 373\nsentiment analysis APIs, 126\ntranslation API, 264\n414 \n| \nIndex\n",
      "word_count": 334,
      "char_count": 2420,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 445,
      "text": "Microsoft Azure, 390\nAutoML, 387\nBlob Storage, 373\nCognitive Services, 65, 375\nMachine Learning, 387\nNEL with, 179-181\nText Analytics API, 179-181\nMicrosoft Cortana, 5\nMicrosoft Outlook, 5\nMicrosoft Research, 375\nMicrosoft REST API, 46\nMicrosoft Team Data Science Process (TDSP),\n388, 390-392\nMicrosoft Word, 6, 113\nMilne, A.A., 119\nMinimum Viable Product (MVP) approach,\nxviii\nmisinformation, 301\n(see also fake news)\nMIT, 349\nMIT Sloan, 401\nMITIE, 175\nML (see machine learning)\nMLOps, 373\nmodel compression, 375-402\nmodel ensembling, 65, 66\nmodel packaging, 373\nmodel scaling, 373\nmodel serving, 373\nmodel size, 375\nmodel stacking, 65, 66\nmodeling, 62-68, 376\nwith APIs, 65\napproaches to, 65-67\ncombining heuristics with, 64\ncustomization, 230\ndeployment, 72\nevaluation of, 399\ngoodness of fit, 68\ninterpretability, 352\niterating existing models, 378\nmonitoring, 72, 382-383\nrecommendations for, 384\nreproducibility, 379\nsequence-to-sequence (seq2seq) models,\n225\nslot identification with, 222-224\nstate-of-the-art (SOTA) models, 396\nstrategies for, 67\ntesting models, 379, 381-382\nupdating, 73, 376\nweb service–based, 382\nMolnar, Christoph, 382\nmonitoring, 72, 382-383\nMoore, Anthony, 371\nmorphemes, 10\nmorphological analysis, 10, 74\nMRR (Mean Reciprocal Rank), 69, 70\nMT (see machine translation)\nmulti-document summarization, 256\nmulticlass classification, 120\nmultilingual writing, 280\nMultiWoZ dataset, 220\nMVP approach (see Minimum Viable Product\napproach)\nN\nn-grams, 89\nNadella, Satya, 58\nNaive Bayes, 20, 395\nNaive Bayes classifier, 127-130\nnamed entity disambiguation (NED), 179\nnamed entity disambiguation and linking, 164\nnamed entity linking (NEL), 179-181\nnamed entity recognition (NER), 32, 58, 164,\n169-178\nwith active learning, 176-177\nbuilding, 171-175\nexamples, 169\nwith existing libraries, 175\nin finance, 361\nHealthTermFinder, 347\npractical advice, 177-178\nrule-based, 171\ntypical training data, 174\nNatty, 187\nnatural language generation, 204\nNatural Language Processing (NLP), xvii\napproaches to, 16-31\ncase studies, 31-33, 74-76\nclassical, 61, 62\ncore applications, 5\ndeployment, 372-374\nDL-based, 61\nend-to-end process, 371-398\nheuristics-based, 16-18\nkey decision points, 373\noverview, 3-33\nIndex \n| \n415\n",
      "word_count": 311,
      "char_count": 2232,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 446,
      "text": "pipeline (see pipeline)\nproject life cycle, 398\nfor social data, 284-299\nsoftware applications, 5-6\ntasks and applications, 4, 6-8\ntasks organized according to relative diffi‐\nculty, 8\nNatural Language Tool Kit (NLTK)\nlemmatization with, 54\nPOS tagging, 57\nsentence segmentation with, 50\nstemming with, 53\nstop words, 52\ntokenizer, 286\nword tokenization with, 51-51\nnatural language understanding (NLU), 203,\n217\ncase study, 32\nRasa, 227-230\nNavajo, 266\nNCRF++, 175\nNED (named entity disambiguation), 179\nNEL (named entity linking), 179-181\nNER (see named entity recognition)\nNetflix, 40, 240, 260\nneural embeddings, 134-140\nneural text representation, 108\nNew York Presbyterian Hospital, 347\niNYP, 344-345\nnews and content discovery, 198\nnews classification, 123, 302\nnews: tagging, 162\nNIST Text Analysis Conference, 162\nNLP (see Natural Language Processing)\nNLPAug, 42\nNLTK (see Natural Language Tool Kit)\nnltk.tokenize.TweetTokenizer, 286\nNLU (see natural language understanding)\nnoise, 298\nadding to data, 41\nexamples, 298\non social media, 283\nnon-English language detection, 56\nnon-English language processing, 73-74\nnon-text data, 291\nnormalization\ntemporal IE and normalization, 186\ntext, 56, 246\nUnicode, 45\nNorvig, Peter, 384\nnumbers: removing, 53\nO\nOccam’s razor, 29\nOCR (optical character recognition), 47, 163\nOlah, Christopher, 23\none-hot encoding, 85-87\nonline support, 123\nONNX, 373\nOntoNotes, 175\nOOV (out of vocabulary) problem, 87, 105,\n136, 281-282, 296\nOozie (Apache), 373\nopen formats, 373\nopen IE, 183, 184\nOpen Mind Common Sense, 17\nopen-ended generative chatbots, 233-234\nopinion mining, 277\noptical character recognition (OCR), 47, 163\nout of vocabulary (OOV) problem, 87, 105,\n136, 281-282, 296\noutcome prediction, 351-353\nP\npackaging, 373\nparse trees, 11\nParsedatetime, 187\nParsey McParseface Tagger, 57\npart-of-speech (POS) tagging, 21, 57, 60\nPartnership on AI, 398\nPDF documents: text extraction from, 47\nPDFMiner, 47\nPenn Treebank dataset, 385\nperplexity, 70\npersonnel, 392-393\npharmacovigilance, 342\nphonemes, 8, 9\nphonetic typing, 280\nPineau, Joelle, 379\npipeline, 37-76\ngeneric components, 38\nkey stages, 37-38\nplagiarism detection, 6\nPLSA (probabilistic latent semantic analysis),\n251\npolarity, 290\nPolyglot Python Library, 56\nPorter Stemmer, 53\n416 \n| \nIndex\n",
      "word_count": 331,
      "char_count": 2296,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 447,
      "text": "POS (part-of-speech) tagging, 21, 57, 60\npost-modeling phases, 72-73\npragmatics, 12\npre-processing, 49-60, 65, 354, 389\nadvanced processing, 60\ncommon steps, 55\nfrequent steps, 52-55\nother steps, 55-57\nSMTD, 290-294\npre-trained language models, large, 145-147\npre-training, 26, 108\nprecision, 69\npredicting health outcomes, 351-353\npregex, 18\npreliminaries, 50-52\nprobabilistic latent semantic analysis (PLSA),\n251\nProdigy, 151, 153, 176\nproduct catalogs (see catalogs)\nproduct categorization and taxonomy, 317-321\nproduct deduplication and matching, 323-324\nproduct enrichment, 321-322\nproduct intervention, 40\nproduct linking, 335\nproduct recommendations, 309, 332\n(see also recommendations)\nproduct search, 309\npropaganda, false, 277\npublic datasets, 40, 153, 289\nPubMed, 357\npunctuation: removing, 53\nPwC, 362\nPyPDF, 47\nPython, 48\nQ\nquality of data, 394\nquantity of data, 394\nquery-focused summarization, 256\nquestion answering (QA), 7, 240\nwith deep neural networks, 269\nknowledge-based, 269\nquestion-answering (QA) systems, 266-269\ndataset creation framework , 350\ndataset generation with existing annota‐\ntions, 351\ndeveloping, 268\nDL-based, 269\nfor health, 349-350\nR\nrandom forest, 395\nranking, 70, 245, 247\nRao, Delip, 398\nRasa, 175, 228-230, 233\nRasa NLP, 375\nRasa NLU, 227-230\nrecall, 69\nRecall at rank K, 70\nRecall-Oriented Understudy for Gisting Evalu‐\nation (ROUGE), 70, 259\nreceipts: data extraction from, 163\nrecommendation engines, 333\n(see also recommender systems)\nrecommendations, 240\ncase study, 333-335\nfor e-commerce, 332-335\nproduct, 309\ntechniques for, 332\nrecommender systems, 260-263, 333\ncreating, 261-262\ne-commerce, 309, 333\nexamples, 261-262\npractical advice, 262-263\nrecurrent neural networks (RNNs), 23, 107,\n140, 316\nReddit autotldr bot, 256\nRedis database, 108\nRegexNER (Stanford NLP), 171, 176\nregression techniques, 19\nregression, logistic, 131\nregular expressions (regex), 17\nreinforcement learning, 15\ndeep, 225-226\nfor dialogue generation, 225-226\nrelated queries, 242\nrelation extraction (see relationship extraction)\nrelationship extraction (RE), 58, 164, 181-185\napproaches to, 182-184\nexample, 182\nunsupervised, 183\nwith Watson API, 184-185\nreplacing entities, 41\nrepresentative data, 394\nreproducibility, 379\nresearch, legal, 363\nresponse generation, 218\nautomatic, 219\ncase study, 33\nIndex \n| \n417\n",
      "word_count": 319,
      "char_count": 2344,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 448,
      "text": "fixed responses, 218\ntemplates-based, 219\nretail (see e-commerce and retail)\nReuters, 361\nreview analysis, 308, 324-331\nconnecting overall ratings to aspects,\n329-330\nexample pipeline, 331\nlatent attribute extraction from, 335\nsubstitutes and complements based on, 334\nrisk assessments, 361-362\nRitter, Allen, 295\nRMSE (Root Mean Squared Error), 69\nRNNs (recurrent neural networks), 23, 107,\n140, 316\nROI (return on investment), 397\nRosette Text Analytics, 181\nROSS Intelligence service, 363\nROUGE (Recall-Oriented Understudy for Gist‐\ning Evaluation), 70, 259\nRuder, Sebastian, 102\nrule-based systems, 18\nrules, 18\nrumor/fake news detection, 277\nRxNorm, 357\nS\nSAFE (simple agreement for future equity), 364\nSageMaker (AWS), 374, 375\nSanders, Niek, 289\nsarcasm, 108\nscaling, 373\nscanned documents: text extraction from, 47\nscraping data, 40\nScrapy, 44, 244\nsearch, 239, 241-250\nfaceted, 310-312\nfocused, 309\nproduct, 309\nsearch engines, 5\ncase study, 249-250\ncomponents, 243-245\nin e-commerce, 309-312\nenterprise, 243\nexample build, 247-248\nfeatures that use NLP, 241\nfeedback, 247\ngeneric, 243\nmanaged services, 250\nquery processing and execution, 246\nranking, 247\nsearch results classification, 242\nsearchers, 245\nseed words or seed lexicons, 327\nself-attention, 25, 27\nsemantics, 12, 83\nSemantics3, 320\nsemi-supervised learning, 16\nsentence segmentation, 50\nsentiment analysis, 32, 122, 325-327\naspect-based, 122-123\naspect-level, 327-329\nfinancial, 360-361\nlexicon-based, 125\nwith social media data, 277, 288-290\nsupervised approach, 327\ntesting, 381\ntracking changes over time, 289\nunsupervised , 328-329\nsentiment analysis APIs, 126\nSentiment Analysis: Emotion in Text dataset\n(Figure Eight), 138\nsequence classification, 172\nsequence labeling, 172\nsequence-to-sequence models (seq2seq) , 225\nShakespeare, William, 161\nShap, 381\nShenfeld, Daniel, 398\nshopping (see e-commerce and retail)\nshort-term memory, long, 23\nshorthand, 46\nSiamese networks, 324\nsimple agreement for future equity (SAFE), 364\nsingle-document summarization, 256\nSinglish, 56\nsiren.io platform, 364\nSkipGram, 101-103\nslots or entities, 205, 217-218, 222-224\nsmall datasets, 29\nSMTD (see social media text data)\nsnippet extraction, 242\nSNIPS dataset, 220, 222\nSnips platform, 222\nSnorkel software, 42, 159\nSnow, C.P., 398\nsocial datasets, 354\nsocial good, 399\nsocial media, 5, 121, 275-302\n418 \n| \nIndex\n",
      "word_count": 334,
      "char_count": 2380,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 449,
      "text": "data generated in, 276\nexample posts, 279, 281\ninformation extraction from, 163\nsocial media text data (SMTD), 278\napplications that use, 277-278\nNLP for, 284-299\nnormalization of, 56\npre-processing, 290-294\nsentiment analysis with, 288-290\nspecial characters, 281\ntext representation for, 294-297\ntokenizers for, 286\nunique challenges, 278-284\nsoftware applications, 5-6\nsoftware deployment, 372-374\nSOTA (state-of-the-art) models, 396\nspaCy library, 175\nEntityRuler, 171, 176\nlemmatization with, 54\nNER with, 175\nPOS tagging, 57\npre-processing with, 57\nrule-based matcher, 63, 64\nsource code, 56\ntokenization with, 51\nSpanish, 74\nspecial characters, 281\nspeech recognition, 32, 203\nspeech representation, 82\nspeech synthesis, 32, 204\nspell checking (see spelling correction)\nspelling correction, 46-47, 242\nspelling, nonstandard, 74, 280, 293-294\nspelling- and grammar-correction tools, 6, 113\nspelling-correction libraries, 294\nsplit-joined words, 292\nSpoke corporate ticketing system (case study),\n152-155\nSpotDraft, 363\nSQuAD dataset, 269\nStack Overflow web pages: text extraction from,\n44\nstacking, 65, 66\nStanford Medicine, 351\nStanford Named Entity Recognizer (NER), 175\nStanford Natural Language Processing Group\nCoreNLP output, 58\nGloVe, 95, 295\nRegexNER, 171, 176\nSUTime, 186\nTokensRegex, 18, 63\nstate-of-the-art (SOTA) models, 396\nSteinhardt, Jacob, 31\nstemming, 53\nstop words, 52, 250\nstorage, 373\nsubjectivity, 290\nsubstitutes, 333, 334\nsubword embeddings, 136-138\nsuicide, 353\nsummarization (see text summarization)\nsummarization algorithms, 330\nSumo Logic, 383\nSumy library, 257\nsupervised learning, 15\nsupervised machine learning techniques, 19\nsupport vector machines (SVMs), 20, 21,\n132-133\nSUTime (Stanford NLP), 186\nSVMs (support vector machines), 20, 21,\n132-133\nSwahili, 74\nSwiftype, 250\nsynonym replacement, 40\nSynsets in Wordnet, 40\nsyntax, 11\nsynthetic data generation, 29\nT\nt-distributed Stochastic Neighboring Embed‐\nding (t-SNE), 108-110\nTaggedDocument class, 139\ntarget datasets, 389\ntask managers, 204\ntaxonomy, product, 317-321\nTeam Data Science Process (TDSP), 388,\n390-392\nteam personnel, 392-393\ntechnical debt minimization, 383-384\ntemplate filling, 165, 189-190\ntemplates-based response generation, 219\ntemporal IE and normalization, 186\ntemporal information extraction, 164, 186-187\nTensorBoard, 110, 111\nTensorFlow Extended, 381\nTensorFlow model analysis (TFMA), 380, 381\nTerm Frequency–Inverse Document Frequency\n(see TF-IDF)\nIndex \n| \n419\n",
      "word_count": 332,
      "char_count": 2480,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 450,
      "text": "term vector models, 84\n(see also vector space models)\nTesseract, 47\ntesting, 379, 381-382\nText Analysis Conference (NIST), 162\ntext categorization, 120\n(see also text classification)\ntext classification, 7, 119-156, 246\napplications, 121-123\nAWS pipeline, 374\ncase study, 152-155\nCNNs for, 143-144\ndefinition, 119\nDL for, 140-147\nwith document embeddings, 138-140\nexample scenario, 374-376\nwith existing APIs or libraries, 126, 153\ninterpreting models, 147-148\nKPIs for, 382\nwith large, pre-trained language models,\n145-147\nlogistic regression, 131\nLSTMs for, 144-145\nNaive Bayes classifier, 127-130\nwith neural embeddings, 134-140\nwith no or less data, 155\npipeline for building systems, 123-126\npractical advice, 155-156\nwith public datasets, 126-127, 153\nreasons for poor performance, 129-129\nsimple, 125-125\nwith SVMs, 132-133\ntext data, 283\ntext encoding, 45\ntext extraction and cleanup, 42-49\ntext normalization, 56, 246\ntext recommendations (see recommendations)\ntext representation, 60, 81-113\nbasic vectorization approaches, 85-92\ndistributed representations, 92-106\ndistributional, 93\nneural, 108\nfor SMTD, 294-297\nuniversal representations, 107-108\ntext summarization, 7, 240, 256-260\nabstractive, 256, 259\nexample setup, 257-258\nextractive, 256, 259\nmulti-document, 256\npractical advice, 258-260\nquery-focused, 256\nquery-independent, 256\nsingle-document, 256\nuse cases, 256-257\ntext-based chatbots, 204\ntext-generation tasks, 70-71\ntextacy, 167\nTextBlob toolkit, 290, 294\nTextEvaluator software (ETS), 112\nTextRank, 168, 258, 259\ntextual data, 260-263\nTF (term frequency), 90\nTF-IDF (Term Frequency–Inverse Document\nFrequency), 90-92, 241, 245, 246, 250, 294,\n347\nTF-IDF–based word replacement, 41\nTFMA (TensorFlow model analysis), 380, 381\nThakur, Abhishek, 388\ntitle match, 323-324\ntokenization\nlanguage-specific exceptions, 52\nsentence (see sentence segmentation)\nfor SMTD, 286, 295\nspecialized tokenizers, 286\ntweet, 51, 138\nword, 51-52\nTokensRegex (Stanford NLP), 18, 63\ntopic classification, 120\n(see also text classification)\ntopic detection, 120\ntrending topics, 277, 286-288\ntopic hierarchy, 335\ntopic modeling, 7, 15, 239, 250-256, 328\nalgorithms, 251, 328\nexample visualization, 251\ntraining models, 254-255\nuse cases, 255-256\ntopic vectors, 335\ntopics, 251\ntraining data, 15\ncrowdsourcing, 150\nno data, 149-150, 155\ntransfer learning, 26, 66, 107, 151-152\ntransformers, 25-26, 27\ntranslation (see machine translation)\ntranslation APIs, 264-265\ntranslation cache, 265\ntranslation memory, 265\n420 \n| \nIndex\n",
      "word_count": 339,
      "char_count": 2529,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 451,
      "text": "transliteration, 57, 280-281\ntrending topic detection, 277\ntrending topics, 277, 286-288\nTripAdvisor, 309\ntrolling, 299\ntroubleshooting, 379-382\ntruly intelligent machines, 399\nTsinghua University, 375\nTuring, Alan, 197\nTurkish, 74\nturnaround cycles, 396\nTurnitin, 6\nTweepy, 287-288\ntweet tokenization, 51\nTweetTokenizer, 138\n20 Newsgroups dataset, 153\nTwikenizer, 286\nTwitter, 41, 138, 275, 283\ncustomer support on, 297\nevent extraction from, 188\nexample posts, 279\nrate limits, 288\nsentiment analysis with, 288-290\nspecialized tokenizers for data from, 286\nTwitter Sentiment Corpus (Sanders), 289\ntwokenize, 286, 294, 295\nTwokenizer, 286\nU\nUber, 74-76\nUCI Machine Learning Repository, 126\nUCSF, 351\nUIUC, 349\nULMFit, 151\nUMLS (Unified Medical Language System) ,\n347\nUnicode characters, 45\nUnified Medical Language System (UMLS) ,\n347\nuniversal text representations, 107-108\nUniversity of Michigan Sentiment Analysis\ncompetition, 289\nunsupervised learning, 15\nupdating, 73, 376\nuppercasing, 53\nURLs: removal of, 293\nUS Navy Message Understanding Conferences,\n162\nuser reviews, 331\nuser weights, 330\nUzbek, 74\nV\nvector semantics, 94\nvector space models (VSMs), 84\nventure capital (VC) firms, 401\nversion control, 379\nVeterans Administration, 350\nvideo representation, 82\nvisual evaluation methods, 70\nvisualizing embeddings, 108-110\nvocabulary\never-evolving, 281-282, 296\nnew words, 281-282, 296\nsplit-joined words, 292\nvocabulary variation, 74\nvoice-based assistants, 5\ncase study, 31-33\nvoice-based chatbots, 204\n(see also chatbots)\nVSMs (vector space models), 84\nW\nWadhwani AI, 399\nWalmart, 113, 311\nweak supervision, 149, 153\nweather forecasting, 5\nweb services, 373, 375\nweb service–based models, 382\nWeizenbaum, Joseph, 197\nWhatsApp, 275\nWikipedia, 95, 179, 183, 301\nWinograd Schema Challenge, 13\nWinoGrande dataset, 399\nWittgenstein, Ludwig, 239\nWoebot, 342, 343, 354\nword clouds, 250, 284-285\nword embeddings, 94-103\npre-trained, 95-97\nfor SMTD, 296\ntext classification with, 134\ntraining, 98-103\nvisualizing, 109\nword tokenization, 51-52\nword vectors, 103\nWord2vec model (Google), 94-97, 102, 105,\n108, 295\npre-trained, 134\nIndex \n| \n421\n",
      "word_count": 301,
      "char_count": 2147,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 452,
      "text": "size, 375\ntraining, 103\nWordnet, 16\nWordNetLemmatizer, 54\nworld knowledge, 30\nY\nYouTube, 240\nZ\nZenkovich, Martin, 384\nZheng, Alice, 384\n422 \n| \nIndex\n",
      "word_count": 24,
      "char_count": 150,
      "fonts": [
        "MinionPro-Regular (9.0pt)",
        "MyriadPro-SemiboldCond (14.0pt)",
        "MyriadPro-SemiboldCond (9.0pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 453,
      "text": "About the Authors\nSowmya Vajjala has a PhD in computational linguistics from the University of\nTubingen, Germany. She currently works as a research officer at the National\nResearch Council, Canada’s largest federal research and development organization.\nHer past work experience spans both academia, as faculty at Iowa State University,\nUSA, and industry at Microsoft Research and The Globe and Mail.\nBodhisattwa Majumder is a doctoral candidate in NLP and ML at UC San Diego.\nEarlier he studied at IIT Kharagpur where he graduated summa cum laude. Previ‐\nously, he conducted ML research and built large-scale NLP systems, at Google AI\nResearch and Microsoft Research that went into products serving millions of users.\nCurrently, he is leading his university team in the Amazon Alexa Prize for 2019–\n2020.\nAnuj Gupta has built NLP and ML systems as a senior leader at Fortune 100 compa‐\nnies as well as startups. He has incubated and led multiple ML teams in his career. He\nstudied computer science at IIT Delhi and IIIT Hyderabad. He is currently the head\nof machine learning and data science at Vahan Inc. Above all, he is a father and a\nhusband.\nHarshit Surana is the CTO at DeepFlux Inc. He has built and scaled ML systems and\nengineering pipelines at several Silicon Valley startups as a founder and an advisor.\nHe studied computer science at Carnegie Mellon University where he worked with\nthe MIT Media Lab on common sense AI. His research in NLP has received over 200\ncitations.\nColophon\nThe animal on the cover of Practical Natural Language Processing is an eclectus parrot\n(Eclectus roratus). Native to the lowland rainforests of Oceania, they can be found\nanywhere from northeastern Australia to the islands that make up the Moluccas. For\ncenturies they have been domesticated in Indonesia and New Guinea, where their\nfeathers are used in elaborate headdresses used to communicate one’s standing or kin‐\nship to the birds.\nThe male’s plumage is bright green with touches of red and blue under the wings,\nwhile the female has a red crown and a purplish-blue chest. These birds are the most\nsexually dimorphic species in the parrot family, which led early biologists to classify\nthem as separate species. Another aspect that distinguishes the eclectus from other\nparrot species is their polygynandry. This allows the females to safely nest for up to\n11 months without often leaving, as they can depend on more than one male to for‐\nage for them.\n",
      "word_count": 410,
      "char_count": 2455,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)",
        "MyriadPro-SemiboldCond (15.8pt)",
        "MinionPro-Bold (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 454,
      "text": "Large populations of the eclectus remain. Many of the animals on O’Reilly covers are\nendangered; all of them are important to the world.\nThe cover illustration is by Karen Montgomery, based on a black and white engraving\nfrom Shaw’s Zoology. The cover fonts are Gilroy Semibold and Guardian Sans. The\ntext font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the\ncode font is Dalton Maag’s Ubuntu Mono.\n",
      "word_count": 72,
      "char_count": 424,
      "fonts": [
        "MinionPro-Regular (10.5pt)",
        "MinionPro-It (10.5pt)"
      ],
      "images": [],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    },
    {
      "page_number": 455,
      "text": "There’s much more  \nwhere this came from.\nExperience books, videos, live online  \ntraining courses, and more from O’Reilly  \nand our 200+ partners—all in one place.\nLearn more at oreilly.com/online-learning\n©2019 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175\n",
      "word_count": 44,
      "char_count": 299,
      "fonts": [
        "Gilroy-Light (17.0pt)",
        "Gilroy-SemiBold (28.0pt)",
        "Gilroy-Light (15.5pt)",
        "GuardianSans-Light (4.5pt)"
      ],
      "images": [
        {
          "index": 0,
          "width": 2144,
          "height": 1812,
          "ext": "jpeg",
          "size_bytes": 1212294
        }
      ],
      "bbox": [
        0.0,
        0.0,
        504.0,
        661.5
      ]
    }
  ],
  "extraction_time": 37.25847411155701,
  "file_size_mb": 30.570831298828125,
  "basic_extraction": {
    "page_count": 455,
    "text_stats": {
      "total_words": 135768,
      "total_chars": 874799
    },
    "fonts_used": [
      "ArialUnicodeMS (10.5pt)",
      "ArialUnicodeMS (9.0pt)",
      "Gilroy-Light (15.5pt)",
      "Gilroy-Light (17.0pt)",
      "Gilroy-Medium (22.0pt)",
      "Gilroy-SemiBold (28.0pt)",
      "Gilroy-SemiBold (52.0pt)",
      "GuardianSans-Light (4.5pt)",
      "GuardianSansNarrow-Regul (24.0pt)",
      "MinionPro-Bold (10.0pt)",
      "MinionPro-Bold (10.5pt)",
      "MinionPro-It (10.0pt)",
      "MinionPro-It (10.5pt)",
      "MinionPro-It (6.3pt)",
      "MinionPro-It (8.0pt)",
      "MinionPro-It (8.5pt)",
      "MinionPro-It (9.0pt)",
      "MinionPro-It (9.3pt)",
      "MinionPro-It (9.6pt)",
      "MinionPro-Regular (10.0pt)",
      "MinionPro-Regular (10.5pt)",
      "MinionPro-Regular (14.1pt)",
      "MinionPro-Regular (6.3pt)",
      "MinionPro-Regular (8.0pt)",
      "MinionPro-Regular (8.5pt)",
      "MinionPro-Regular (9.0pt)",
      "MinionPro-Regular (9.3pt)",
      "MinionPro-Regular (9.6pt)",
      "MinionPro-SemiboldIt (16.0pt)",
      "MinionPro-SemiboldIt (18.0pt)",
      "MyriadPro-Cond (12.3pt)",
      "MyriadPro-Cond (6.3pt)",
      "MyriadPro-Cond (9.0pt)",
      "MyriadPro-SemiboldCond (10.0pt)",
      "MyriadPro-SemiboldCond (11.5pt)",
      "MyriadPro-SemiboldCond (11.6pt)",
      "MyriadPro-SemiboldCond (12.0pt)",
      "MyriadPro-SemiboldCond (14.0pt)",
      "MyriadPro-SemiboldCond (15.8pt)",
      "MyriadPro-SemiboldCond (16.8pt)",
      "MyriadPro-SemiboldCond (18.9pt)",
      "MyriadPro-SemiboldCond (20.0pt)",
      "MyriadPro-SemiboldCond (25.2pt)",
      "MyriadPro-SemiboldCond (28.4pt)",
      "MyriadPro-SemiboldCond (31.5pt)",
      "MyriadPro-SemiboldCond (9.0pt)",
      "MyriadPro-SemiboldCondIt (20.0pt)",
      "Symbola (10.5pt)",
      "Symbola (8.5pt)",
      "UbuntuMono-Bold (10.0pt)",
      "UbuntuMono-Bold (10.5pt)",
      "UbuntuMono-Bold (8.5pt)",
      "UbuntuMono-Italic (10.0pt)",
      "UbuntuMono-Italic (8.5pt)",
      "UbuntuMono-Regular (10.0pt)",
      "UbuntuMono-Regular (8.5pt)",
      "UbuntuMono-Regular (8.9pt)"
    ],
    "images_count": 243,
    "metadata": {
      "format": "PDF 1.6",
      "title": "Practical Natural Language Processing",
      "author": "Sowmya  Vajjala;Bodhisattwa  Majumder;Anuj  Gupta;Harshit  Surana;",
      "creator": "AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)",
      "producer": "Antenna House PDF Output Library 6.2.609 (Linux64)",
      "creationDate": "D:20200617144731Z",
      "modDate": "D:20200618124937+05'30'"
    },
    "pages": [
      {
        "page_number": 1,
        "text": "Sowmya Vajjala, \nBodhisattwa Majumder, \nAnuj Gupta & Harshit Surana\nPractical \nNatural Language \nProcessing\nA Comprehensive Guide to Building \nReal-World NLP Systems\n",
        "word_count": 21,
        "char_count": 166,
        "fonts": [
          "Gilroy-SemiBold (52.0pt)",
          "Gilroy-Medium (22.0pt)",
          "GuardianSansNarrow-Regul (24.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1967,
            "height": 1651,
            "ext": "jpeg",
            "size_bytes": 2426876
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.4639892578125
        ]
      },
      {
        "page_number": 2,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [
          {
            "index": 0,
            "width": 2100,
            "height": 2756,
            "ext": "jpeg",
            "size_bytes": 1112530
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.4400024414062
        ]
      },
      {
        "page_number": 3,
        "text": "Praise for Practical Natural Language Processing\nPractical NLP focuses squarely on an overlooked demographic: the practitioners and\nbusiness leaders in industry! While many great books focus on ML’s algorithmic\nfundamentals, this book exposes the anatomy of real-world systems: from e-commerce\napplications to virtual assistants. Painting a realistic picture of modern production\nsystems, the book teaches not only deep learning, but also the heuristics and patchwork\npipelines that define the (actual) state of the art for deployed NLP systems. The authors\nzoom out, teaching problem formulation, and aren’t afraid to zoom in on the grimy\ndetails, including handling messy data and sustaining live systems. This book will prove\ninvaluable to industry professionals keen to build and deploy NLP in the wild.\n—Zachary Lipton, Assistant Professor, Carnegie Mellon\nUniversity, Scientist at Amazon AI, Author of Dive into Deep Learning\nThis book does a great job bridging the gap between natural language processing (NLP)\nresearch and practical applications. From healthcare to e-commerce and finance, it\ncovers many of the most sought-after domains where NLP is being put to use and\nwalks through core tasks in a clear and understandable manner. Overall, the book\nis a great manual on how to get the most out of current NLP in your industry.\n—Sebastian Ruder, Research Scientist, Google DeepMind\nThere are two kinds of computer science books on the market: academic textbooks\nthat give you a deep understanding of a domain but can be difficult to access for a\nnon-academic, and “cookbooks” that outline solutions to very specific problems\nwithout providing the technical foundations that would allow the reader to generalize\nthe recipes. This book offers the best of both worlds: it is thorough yet accessible. It\nprovides the reader with a solid foundation in natural-language processing. . . . If\nyou would like to go from zero to one in NLP, this book is for you!\n—Marc Najork, Research Engineering Director, Google AI,\nACM & IEEE Fellow\n",
        "word_count": 322,
        "char_count": 2038,
        "fonts": [
          "MinionPro-Regular (10.0pt)",
          "MyriadPro-SemiboldCondIt (20.0pt)",
          "MinionPro-It (10.0pt)",
          "MyriadPro-SemiboldCond (20.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 4,
        "text": "There are text books or research papers or books on programming tips, but not a book\nthat tells us how to build an end-to-end NLP system from scratch. I am happy to see\nthis book on practical NLP, which fills this much needed gap. The authors have\nmeticulously, thoughtfully and lucidly covered each and every aspect of NLP\nthat one has to be aware of while building large scale practical systems; at the same\ntime, this book has also managed to cover a large number of examples and varied\napplication areas and verticals. This book is a must for all aspiring NLP engineers,\nentrepreneurs who want to build companies around language technologies, and also\nacademic researchers who would like to see their inventions reach the real users.\n—Monojit, Principal Researcher, Microsoft Research India,\nAdjunct Faculty at IIIT Hyderabad, Ashoka University, IIT Kharagpur\nThis book bridges the gap between theory and practice by explaining the underlying\nconcepts while keeping in mind varied real-world deployments across different\nbusiness verticals. There is much hard-fought practical advice from the trenches\nwhether it is about tweaking parameters of open source libraries, setting up\ndata pipelines for building models, or optimizing for fast inference.\nA must-read for engineers building NLP applications.\n—Vinayak Hegde, CTO-in-Residence, Microsoft For Startups\nThis book shows how to put NLP to practice. It bridges the gap between NLP theory and\npractical engineering. The authors achieved a rare feat by simplifying the esoteric art\nof design and architecture of production quality machine learning systems.\nI wish I had access to this book early on in my professional career and evaded\nthe mistakes I made along the way. . . . I am deeply convinced that this\nbook is an essential read for anybody aiming to develop involved\nin developing a robust, high-performing NLP system.\n—Siddharth Sharma, ML Engineer, Facebook\nI feel this is not only an essential book for NLP practitioners, it is also a valuable reference\nfor the research community to understand the problem spaces in real-world\napplications. I very much appreciate this book and wish this could be a\nlong-term project with up-to-date NLP application trending!\n—Mengting Wan, Data Scientist (ML&NLP) at Airbnb,\nMicrosoft Research Fellow\n",
        "word_count": 365,
        "char_count": 2301,
        "fonts": [
          "MinionPro-Regular (10.0pt)",
          "MinionPro-It (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 5,
        "text": "Sowmya Vajjala, Bodhisattwa Majumder,\nAnuj Gupta, and Harshit Surana\nPractical Natural Language\nProcessing\nA Comprehensive Guide to Building\nReal-World NLP Systems\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n",
        "word_count": 31,
        "char_count": 244,
        "fonts": [
          "MinionPro-SemiboldIt (16.0pt)",
          "MyriadPro-Cond (12.3pt)",
          "MinionPro-SemiboldIt (18.0pt)",
          "MyriadPro-SemiboldCond (31.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 6,
        "text": "978-1-492-05405-4\n[LSI]\nPractical Natural Language Processing\nby Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, and Harshit Surana\nCopyright © 2020 Anuj Gupta, Bodhisattwa Prasad Majumder, Sowmya Vajjala, and Harshit Surana. All\nrights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com.\nAcquistions Editor: Jonathan Hassell\nDevelopmental Editor: Melissa Potter\nProduction Editor: Beth Kelly\nCopyeditor: Holly Forsyth\nProofreader: Charles Roumeliotis\nIndexer: nSight Inc.\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nJune 2020:\n First Edition\nRevision History for the First Edition\n2020-06-17: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492054054 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Practical Natural Language Processing,\nthe cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\n",
        "word_count": 287,
        "char_count": 2074,
        "fonts": [
          "MinionPro-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (10.0pt)",
          "MinionPro-It (8.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 7,
        "text": "This book is dedicated to our respective advisors: Detmar Meurers, Julian McAuley,\nKannan Srinathan, and Luis von Ahn.\n",
        "word_count": 18,
        "char_count": 119,
        "fonts": [
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 8,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 9,
        "text": "Table of Contents\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xv\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xvii\nPart I. \nFoundations\n1. NLP: A Primer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\nNLP in the Real World                                                                                                    5\nNLP Tasks                                                                                                                      6\nWhat Is Language?                                                                                                           8\nBuilding Blocks of Language                                                                                      9\nWhy Is NLP Challenging?                                                                                         12\nMachine Learning, Deep Learning, and NLP: An Overview                                  14\nApproaches to NLP                                                                                                       16\nHeuristics-Based NLP                                                                                                16\nMachine Learning for NLP                                                                                       19\nDeep Learning for NLP                                                                                             22\nWhy Deep Learning Is Not Yet the Silver Bullet for NLP                                    28\nAn NLP Walkthrough: Conversational Agents                                                         31\nWrapping Up                                                                                                                  33\n2. NLP Pipeline. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\nData Acquisition                                                                                                            39\nText Extraction and Cleanup                                                                                        42\nHTML Parsing and Cleanup                                                                                    44\nUnicode Normalization                                                                                             45\nSpelling Correction                                                                                                    46\nvii\n",
        "word_count": 368,
        "char_count": 2720,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (12.0pt)",
          "MyriadPro-SemiboldCond (25.2pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 10,
        "text": "System-Specific Error Correction                                                                            47\nPre-Processing                                                                                                                49\nPreliminaries                                                                                                               50\nFrequent Steps                                                                                                            52\nOther Pre-Processing Steps                                                                                      55\nAdvanced Processing                                                                                                 57\nFeature Engineering                                                                                                      60\nClassical NLP/ML Pipeline                                                                                       62\nDL Pipeline                                                                                                                  62\nModeling                                                                                                                         62\nStart with Simple Heuristics                                                                                     63\nBuilding Your Model                                                                                                 64\nBuilding THE Model                                                                                                 65\nEvaluation                                                                                                                       68\nIntrinsic Evaluation                                                                                                    68\nExtrinsic Evaluation                                                                                                   71\nPost-Modeling Phases                                                                                                   72\nDeployment                                                                                                                 72\nMonitoring                                                                                                                  72\nModel Updating                                                                                                          73\nWorking with Other Languages                                                                                   73\nCase Study                                                                                                                       74\nWrapping Up                                                                                                                  76\n3. Text Representation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81\nVector Space Models                                                                                                     84\nBasic Vectorization Approaches                                                                                  85\nOne-Hot Encoding                                                                                                     85\nBag of Words                                                                                                               87\nBag of N-Grams                                                                                                          89\nTF-IDF                                                                                                                         90\nDistributed Representations                                                                                         92\nWord Embeddings                                                                                                     94\nGoing Beyond Words                                                                                              103\nDistributed Representations Beyond Words and Characters                               105\nUniversal Text Representations                                                                                 107\nVisualizing Embeddings                                                                                             108\nHandcrafted Feature Representations                                                                      112\nWrapping Up                                                                                                                113\nviii \n| \nTable of Contents\n",
        "word_count": 187,
        "char_count": 4641,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (12.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 11,
        "text": "Part II. \nEssentials\n4. Text Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  119\nApplications                                                                                                                  121\nA Pipeline for Building Text Classification Systems                                               123\nA Simple Classifier Without the Text Classification Pipeline                            125\nUsing Existing Text Classification APIs                                                                126\nOne Pipeline, Many Classifiers                                                                                  126\nNaive Bayes Classifier                                                                                              127\nLogistic Regression                                                                                                  131\nSupport Vector Machine                                                                                         132\nUsing Neural Embeddings in Text Classification                                                    134\nWord Embeddings                                                                                                   134\nSubword Embeddings and fastText                                                                       136\nDocument Embeddings                                                                                          138\nDeep Learning for Text Classification                                                                      140\nCNNs for Text Classification                                                                                  143\nLSTMs for Text Classification                                                                                144\nText Classification with Large, Pre-Trained Language Models                         145\nInterpreting Text Classification Models                                                                   147\nExplaining Classifier Predictions with Lime                                                        148\nLearning with No or Less Data and Adapting to New Domains                          149\nNo Training Data                                                                                                     149\nLess Training Data: Active Learning and Domain Adaptation                         150\nCase Study: Corporate Ticketing                                                                               152\nPractical Advice                                                                                                           155\nWrapping Up                                                                                                                157\n5. Information Extraction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  161\nIE Applications                                                                                                             162\nIE Tasks                                                                                                                         164\nThe General Pipeline for IE                                                                                        165\nKeyphrase Extraction                                                                                                  166\nImplementing KPE                                                                                                  167\nPractical Advice                                                                                                        168\nNamed Entity Recognition                                                                                         169\nBuilding an NER System                                                                                         171\nNER Using an Existing Library                                                                              175\nNER Using Active Learning                                                                                    176\nPractical Advice                                                                                                        177\nNamed Entity Disambiguation and Linking                                                            178\nNEL Using Azure API                                                                                             179\nTable of Contents \n| \nix\n",
        "word_count": 307,
        "char_count": 4505,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (12.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 12,
        "text": "Relationship Extraction                                                                                              181\nApproaches to RE                                                                                                     182\nRE with the Watson API                                                                                         184\nOther Advanced IE Tasks                                                                                           185\nTemporal Information Extraction                                                                         186\nEvent Extraction                                                                                                       187\nTemplate Filling                                                                                                        189\nCase Study                                                                                                                     190\nWrapping Up                                                                                                                193\n6. Chatbots. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  197\nApplications                                                                                                                  198\nA Simple FAQ Bot                                                                                                    199\nA Taxonomy of Chatbots                                                                                            200\nGoal-Oriented Dialog                                                                                              202\nChitchats                                                                                                                    202\nA Pipeline for Building Dialog Systems                                                                   203\nDialog Systems in Detail                                                                                             204\nPizzaStop Chatbot                                                                                                    206\nDeep Dive into Components of a Dialog System                                                    216\nDialog Act Classification                                                                                         217\nIdentifying Slots                                                                                                        217\nResponse Generation                                                                                               218\nDialog Examples with Code Walkthrough                                                           219\nOther Dialog Pipelines                                                                                                224\nEnd-to-End Approach                                                                                             225\nDeep Reinforcement Learning for Dialogue Generation                                   225\nHuman-in-the-Loop                                                                                                226\nRasa NLU                                                                                                                      227\nA Case Study: Recipe Recommendations                                                                230\nUtilizing Existing Frameworks                                                                               231\nOpen-Ended Generative Chatbots                                                                        233\nWrapping Up                                                                                                                234\n7. Topics in Brief. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  239\nSearch and Information Retrieval                                                                             241\nComponents of a Search Engine                                                                            243\nA Typical Enterprise Search Pipeline                                                                    246\nSetting Up a Search Engine: An Example                                                             247\nA Case Study: Book Store Search                                                                          249\nTopic Modeling                                                                                                            250\nTraining a Topic Model: An Example                                                                   254\nx \n| \nTable of Contents\n",
        "word_count": 303,
        "char_count": 4721,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (12.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 13,
        "text": "What’s Next?                                                                                                             255\nText Summarization                                                                                                    256\nSummarization Use Cases                                                                                       256\nSetting Up a Summarizer: An Example                                                                257\nPractical Advice                                                                                                        258\nRecommender Systems for Textual Data                                                                 260\nCreating a Book Recommender System: An Example                                       261\nPractical Advice                                                                                                        262\nMachine Translation                                                                                                   263\nUsing a Machine Translation API: An Example                                                  264\nPractical Advice                                                                                                        265\nQuestion-Answering Systems                                                                                    266\nDeveloping a Custom Question-Answering System                                          268\nLooking for Deeper Answers                                                                                  268\nWrapping Up                                                                                                                269\nPart III. \nApplied\n8. Social Media. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  275\nApplications                                                                                                                  277\nUnique Challenges                                                                                                       278\nNLP for Social Data                                                                                                     284\nWord Cloud                                                                                                              284\nTokenizer for SMTD                                                                                                286\nTrending Topics                                                                                                        286\nUnderstanding Twitter Sentiment                                                                         288\nPre-Processing SMTD                                                                                             290\nText Representation for SMTD                                                                              294\nCustomer Support on Social Channels                                                                 297\nMemes and Fake News                                                                                                299\nIdentifying Memes                                                                                                   299\nFake News                                                                                                                  300\nWrapping Up                                                                                                                302\n9. E-Commerce and Retail. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  307\nE-Commerce Catalog                                                                                                  308\nReview Analysis                                                                                                        308\nProduct Search                                                                                                         309\nProduct Recommendations                                                                                    309\nSearch in E-Commerce                                                                                               309\nBuilding an E-Commerce Catalog                                                                            312\nTable of Contents \n| \nxi\n",
        "word_count": 268,
        "char_count": 4390,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (12.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 14,
        "text": "Attribute Extraction                                                                                                 312\nProduct Categorization and Taxonomy                                                                317\nProduct Enrichment                                                                                                321\nProduct Deduplication and Matching                                                                  323\nReview Analysis                                                                                                           324\nSentiment Analysis                                                                                                   325\nAspect-Level Sentiment Analysis                                                                           327\nConnecting Overall Ratings to Aspects                                                                329\nUnderstanding Aspects                                                                                           330\nRecommendations for E-Commerce                                                                        332\nA Case Study: Substitutes and Complements                                                      333\nWrapping Up                                                                                                                336\n10. Healthcare, Finance, and Law. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  339\nHealthcare                                                                                                                     339\nHealth and Medical Records                                                                                  341\nPatient Prioritization and Billing                                                                           342\nPharmacovigilance                                                                                                   342\nClinical Decision Support Systems                                                                        342\nHealth Assistants                                                                                                      342\nElectronic Health Records                                                                                      344\nMental Healthcare Monitoring                                                                              353\nMedical Information Extraction and Analysis                                                    355\nFinance and Law                                                                                                          358\nNLP Applications in Finance                                                                                 360\nNLP and the Legal Landscape                                                                                363\nWrapping Up                                                                                                                366\nPart IV. \nBringing It All Together\n11. The End-to-End NLP Process. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  371\nRevisiting the NLP Pipeline: Deploying NLP Software                                         372\nAn Example Scenario                                                                                              374\nBuilding and Maintaining a Mature System                                                            376\nFinding Better Features                                                                                           377\nIterating Existing Models                                                                                        378\nCode and Model Reproducibility                                                                          379\nTroubleshooting and Interpretability                                                                    379\nMonitoring                                                                                                                382\nMinimizing Technical Debt                                                                                    383\nAutomating Machine Learning                                                                              384\nxii \n| \nTable of Contents\n",
        "word_count": 263,
        "char_count": 4298,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (12.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 15,
        "text": "The Data Science Process                                                                                           388\nThe KDD Process                                                                                                     388\nMicrosoft Team Data Science Process                                                                   390\nMaking AI Succeed at Your Organization                                                                392\nTeam                                                                                                                           392\nRight Problem and Right Expectations                                                                 393\nData and Timing                                                                                                      394\nA Good Process                                                                                                        395\nOther Aspects                                                                                                           396\nPeeking over the Horizon                                                                                           398\nFinal Words                                                                                                                  401\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  407\nTable of Contents \n| \nxiii\n",
        "word_count": 126,
        "char_count": 1481,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (12.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 16,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 17,
        "text": "Foreword\nThe field of natural language processing (NLP) has undergone a dramatic shift in\nrecent years, both in terms of methodology and in terms of the applications sup‐\nported. Methodological advances have ranged from new ways of representing docu‐\nments to new techniques for language synthesis. With these have come new\napplications ranging from open-ended conversational systems to techniques that use\nnatural language for model interpretability. Finally, these advances have seen NLP\ngain a foothold in related areas, such as computer vision and recommender systems,\nsome of which my lab is working on with support from Amazon, Samsung, and the\nNational Science Foundation.\nAs NLP is expanding into these exciting new areas, so too has the audience of practi‐\ntioners wanting to make use of NLP techniques. In the Data Science course (CSE 258)\nthat I take at the University of California–San Diego, which is often the most attended\nin the computer science department, I see that more and more students are doing\ntheir projects on NLP-based topics. NLP is rapidly becoming a necessary skill\nrequired by engineers, product managers, scientists, students, and enthusiasts wish‐\ning to build applications on top of natural language data. On one hand, new tools and\nlibraries for NLP and machine learning have made natural language modeling more\naccessible than ever. But on the other hand, resources for learning NLP must target\nthis ever-growing and diverse audience. This is especially true for organizations that\nhave recently adopted NLP or for students working with natural language data for the\nfirst time.\nIt has been my pleasure over the last few years to collaborate with Bodhisattwa\nMajumder on exciting new applications in NLP and dialog, so I was thrilled to hear\nabout his efforts (along with Sowmya Vajjala, Anuj Gupta, and Harshit Surana) to\nwrite a book on NLP. They have a wide experience in scaling NLP including at early-\nstage startups, the MIT Media Lab, Microsoft Research, and Google AI.\nI am excited by the end-to-end approach taken in their book, which will make it use‐\nful for a range of scenarios and will help readers to work with the labyrinth of\nxv\n",
        "word_count": 357,
        "char_count": 2182,
        "fonts": [
          "MyriadPro-SemiboldCond (25.2pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 18,
        "text": "possible options while building NLP applications. I am especially thrilled about the\nemphasis on modern NLP applications such as chatbots, as well as the focus on inter‐\ndisciplinary topics such as ecommerce and retail. These topics will be especially use‐\nful for industry leaders and researchers, and are critical subjects that have been given\nonly limited coverage in existing textbooks. This book is ideal both as a first resource\nto discover the field of natural language processing and a guide for seasoned practi‐\ntioners looking to discover the latest developments in this exciting area.\n— Julian McAuley\nProfessor of Computer Science and Engineering\nUniversity of California, San Diego\nxvi \n| \nForeword\n",
        "word_count": 111,
        "char_count": 712,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 19,
        "text": "Preface\nNatural language processing (NLP) is a field at the intersection of computer science,\nartificial intelligence, and linguistics. It concerns building systems that can process\nand understand human language. Since its inception in the 1950s and until very\nrecently, NLP has primarily been the domain of academia and research labs, requir‐\ning long formal education and training. The past decade’s breakthroughs have resul‐\nted in NLP being increasingly used in a range of diverse domains such as retail,\nhealthcare, finance, law, marketing, human resources, and many more. There are a\nrange of driving forces for these developments:\n• Widely available and easy-to-use NLP tools, techniques, and APIs are now all-\npervading in the industry. There has never been a better time to build quick NLP\nsolutions.\n• Development of more interpretable and generalized approaches has improved\nthe baseline performance for even complex NLP tasks, such as open-domain\nconversational tasks and question answering, which were not practically feasible\nbefore.\n• More and more organizations, including Google, Microsoft, and Amazon, are\ninvesting heavily in more interactive consumer products, where language is used\nas the primary medium of communication.\n• Increased availability of useful open source datasets, along with standard bench‐\nmarks on them, has acted as a catalyst in this revolution, as opposed to being\nimpeded by proprietary datasets only available to limited organizations and\nindividuals.\n• The viability of NLP has moved beyond English or other major languages. Data‐\nsets and language-specific models are being created for the less-frequently digi‐\ntized languages too. A fruitful product that came out this effort was a near-\nperfect automatic machine translation tool available to all individuals with a\nsmartphone.\nxvii\n",
        "word_count": 276,
        "char_count": 1832,
        "fonts": [
          "MyriadPro-SemiboldCond (25.2pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 20,
        "text": "With this rapidly expanding usage, a growing proportion of the workforce that builds\nthese NLP systems is grappling with limited experience and theoretical knowledge\nabout the topic. This book addresses this need from an applied perspective. Our book\naims to guide the readers to build, iterate, and scale NLP systems in a business set‐\nting, and to tailor them for various industry verticals.\nWhy We Wrote This Book\nThere are many popular books on NLP available. While some of these serve as text‐\nbooks, focusing on theoretical aspects, some others aim to introduce NLP concepts\nthrough a lot of code examples. There are a few others that focus on specific NLP or\nmachine learning libraries and provide “how-to” guides on solving different NLP\nproblems using the libraries. So, why do we need another book on NLP?\nWe have been building and scaling NLP solutions for over a decade at leading univer‐\nsities and technology companies. While mentoring colleagues and other engineers,\nwe noticed a gap between NLP practice in the industry and the NLP skill sets of new\nengineers and those who are just starting with NLP in particular. We started under‐\nstanding these gaps even better during NLP workshops we were conducting for\nindustry professionals, where we noticed that business and engineering leaders also\nhave these gaps.\nMost online courses and books tackle NLP problems using toy use cases and popular\n(often large, clean, and well-defined) datasets. While this imparts the general meth‐\nods of NLP, we believe it does not provide enough of a foundation to tackle new\nproblems and develop specific solutions in the real world. Commonly encountered\nproblems while building real-world applications, such as data collection, working\nwith noisy data and signals, incremental development of solutions, and issues\ninvolved in deploying the solutions as a part of a larger application, are not dealt with\nby existing resources, to the best of our knowledge. We also saw that best practices to\ndevelop NLP systems were missing in most scenarios. We felt a book was needed to\nbridge this gap, and that is how this book was born!\nThe Philosophy\nWe want to provide a holistic and practical perspective that enables the reader to suc‐\ncessfully build real-world NLP solutions embedded in larger product setups. Thus,\nmost chapters are accompanied by code walkthroughs in the associated Git reposi‐\ntory. The book is also supplemented with extensive references for readers who want\nto delve deeper. Throughout the book, we start with a simple solution and incremen‐\ntally build more complex solutions by taking a minimum viable product (MVP)\napproach, as commonly found in industry practice. We also give tips based on our\nexperience and learnings. Where possible, each chapter is accompanied by a\nxviii \n| \nPreface\n",
        "word_count": 457,
        "char_count": 2810,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 21,
        "text": "discussion on the state-of-the-art in that topic. Most chapters conclude with a case\nstudy of real-world use cases.\nConsider the task of building a chatbot or text classification system at your organiza‐\ntion. In the beginning there may be little or no data to work with. At this point, a\nbasic solution using rule-based systems or traditional machine learning will be apt.\nAs you accumulate more data, more sophisticated NLP techniques (which are often\ndata intensive) can be used, including deep learning. At each step of this journey\nthere are dozens of alternative approaches one can take. This book will help you navi‐\ngate this maze of options.\nScope\nThis book provides a comprehensive view on building real-world NLP applications.\nWe will cover the complete lifecycle of a typical NLP project—from data collection to\ndeploying and monitoring the model. Some of these steps are applicable to any ML\npipeline, while some are very specific to NLP. We also introduce task-specific case\nstudies and domain-specific guides to build an NLP system from scratch. We specifi‐\ncally cover a gamut of tasks ranging from text classification to question answering to\ninformation extraction and dialog systems. Similarly, we provide recipes to apply\nthese tasks in domains ranging from e-commerce to healthcare, social media, and\nfinance. Owing to the depth and breadth of the topics and scenarios we cover, we will\nnot go step by step explaining the code and all the concepts. For details of the imple‐\nmentation, we have provided detailed source code notebooks. The code snippets in\nthis book cover the core logic and often skip introductory steps like setting up a\nlibrary or importing a package as they are covered in the associated notebooks. To\ncover the wide range of concepts we have given more than 450 extensive references to\ndelve deeper into these topics. This book will be a day-to-day cookbook giving you a\npragmatic view while building any NLP system, as well as be a stepping stone to\nbroaden the application of NLP into your domain.\nWho Should Read This Book\nThis book is for anyone involved in building NLP applications for real-world use\ncases. This includes software developers and testers, machine learning engineers, data\nengineers, MLOps engineers, NLP engineers, data scientists, product managers, peo‐\nple managers, VPs, CXOs, and startup founders. This also includes those involved in\ndata creation and annotation processes—in short anyone and everyone who is\ninvolved in any way in building NLP systems in industry. While not all chapters are\nuseful for people with all roles, we tried to give lucid explanations using less technical\njargon and more intuitive understanding wherever possible. We believe there is\nsomething in every chapter for all potential readers interested in getting a holistic\nperspective about building NLP applications.\nPreface \n| \nxix\n",
        "word_count": 463,
        "char_count": 2879,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 22,
        "text": "Some chapters or sections can be understood without much coding experience and\ncode bits can be skipped as needed. For example, the first two sections in Chapter 1\nand Chapter 9, or the sections “The Data Science Process” and “Making AI succeed in\nyour organization” in Chapter 11 can be understood without any coding experience\nby all groups of readers. As you progress through the book, you will find more such\nsections in all chapters. However, to extract maximum benefit from this book, its\nnotebooks, and references, we expect the reader to have the following background:\n• Intermediate proficiency in Python programming. For example, understanding\nPython features such as list comprehension, writing functions and classes, and\nusing existing libraries.\n• Familiarity with various aspects of the software development life cycle (SDLC)\nsuch as design, development, testing, DevOps, etc.\n• Basics of machine learning, including familiarity with commonly used machine\nlearning algorithms such as logistic regression and decision trees and the ability\nto use them in Python with existing libraries such as scikit-learn.\n• Basic knowledge of NLP is useful but not mandatory. Having an idea of tasks\nsuch as text classification and named entity recognition is also helpful.\nWhat You Will Learn\nOur primary audience is comprised of engineers and scientists involved in building\nreal-world NLP systems for different verticals. Some of the common job titles are:\nSoftware Engineer, NLP Engineer, ML Engineer, and Data Scientist. The book may\nalso be helpful for product managers and engineering leaders. However, it may not be\nas helpful for those pursuing cutting-edge research in NLP because we do not cover\nin-depth theoretical and technical details related to NLP concepts. With this book,\nyou will:\n• Understand the wide spectrum of problem statements, tasks, and solution\napproaches within NLP.\n• Gain experience in implementing and evaluating different NLP applications and\napplying machine learning and deep learning methods for this process.\n• Fine-tune an NLP solution based on the business problem and industry vertical.\n• Evaluate various algorithms and approaches for the given task, dataset, and stage\nof the NLP product.\n• Plan the lifecycle of the NLP product and produce software solutions following\nbest practices around release, deployment, and DevOps for NLP systems.\nxx \n| \nPreface\n",
        "word_count": 372,
        "char_count": 2399,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 23,
        "text": "• Understand best practices, opportunities, and the roadmap for NLP from a busi‐\nness and product leader’s perspective.\nYou will also learn to adapt your solutions for different industry verticals like health‐\ncare, finance, and retail. Moreover, you will learn about specific caveats you will\nencounter in each.\nStructure of the Book\nThe book is divided into four sections. Figure P-1 illustrates the chapter organization.\nIsolated chapters that are not directly connected to other chapters are easiest to skip\nwhile moving forward.\nFigure P-1. How the book’s sections are structured\nPart I, Foundations acts as a bedrock for the rest of the book, by giving an overview of\nNLP (Chapter 1), discussing typical data processing and modeling pipelines used in\nbuilding NLP systems (Chapter 2), and introducing different ways of representing\ntextual data in NLP (Chapter 3).\nPreface \n| \nxxi\n",
        "word_count": 140,
        "char_count": 887,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1213,
            "height": 1197,
            "ext": "png",
            "size_bytes": 71384
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 24,
        "text": "Part II, Essentials, focuses on the most common NLP applications, with an emphasis\non real-world use cases. Where possible, we show multiple solutions to the problem\nat hand to demonstrate how to choose among different options. Some applications\ninclude text classification (Chapter 4), information extraction (Chapter 5), and the\nbuilding of chat bots (Chapter 6). We also introduce other applications, such as\nsearch, topic modeling, text summarization, and machine translation, along with a\ndiscussion about practical use cases (Chapter 7).\nPart III, Applied (Chapters 8–10) specifically focuses on three industry verticals where\nNLP is heavily used, with a detailed discussion on those domains’ specific problems\nand how NLP is useful in addressing them.\nFinally, Part IV (Chapter 11) brings all the learning together by dealing with the\nissues involved in end-to-end deployment of NLP systems in practice.\nHow to Read This Book\nHow one reads the book depends on their role and objective. For a data scientist or\nan engineer delving into NLP, we recommend reading Chapters 1–6 and then focus‐\ning on the particular domain or subproblem of interest. For someone in a leadership\nrole, we recommend focusing on Chapters 1, 2, and 11. They might want to give extra\nfocus to case studies for Chapters 3–7, which provide more ideas on the process of\nbuilding NLP applications from scratch. A product leader might want to delve deep\ninto the references provided for relevant chapters, as well as Chapter 11.\nNLP applications for various domains can be different from the general problems\ncovered in Chapters 3–7. That is why we have focused more on certain domains such\nas e-commerce, social media, healthcare, finance, and law. If your interest or work\ntakes you to these areas, you can dig deeper into those chapters and corresponding\nreferences.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program ele‐\nments such as variable or function names, databases, data types, environment\nvariables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nxxii \n| \nPreface\n",
        "word_count": 371,
        "char_count": 2342,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "UbuntuMono-Bold (10.0pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 25,
        "text": "Constant width italic\nShows text that should be replaced with user-supplied values or by values deter‐\nmined by context.\nThis element signifies a tip or suggestion.\nThis element signifies a general note.\nThis element indicates a warning or caution.\nUsing Code Examples\nSupplemental material (code examples, exercises, etc.) is available for download at\nhttps://oreil.ly/PracticalNLP.\nIf you have a technical question or a problem using the code examples, please send\nemail to bookquestions@oreilly.com.\nThis book is here to help you get your job done. In general, if example code is offered\nwith this book, you may use it in your programs and documentation. You do not\nneed to contact us for permission unless you’re reproducing a significant portion of\nthe code. For example, writing a program that uses several chunks of code from this\nbook does not require permission. Selling or distributing examples from O’Reilly\nbooks does require permission. Answering a question by citing this book and quoting\nexample code does not require permission. Incorporating a significant amount of\nexample code from this book into your product’s documentation does require\npermission.\nWe appreciate, but generally do not require, attribution. An attribution usually\nincludes the title, author, publisher, and ISBN. For example: “Practical Natural Lan‐\nguage Processing by Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, and Har‐\nshit Surana (O’Reilly). Copyright 2020 Anuj Gupta, Bodhisattwa Prasad Majumder,\nSowmya Vajjala, and Harshit Surana, 978-1-492-05405-4.”\nPreface \n| \nxxiii\n",
        "word_count": 233,
        "char_count": 1570,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (9.6pt)",
          "UbuntuMono-Italic (10.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          },
          {
            "index": 1,
            "width": 394,
            "height": 514,
            "ext": "png",
            "size_bytes": 7986
          },
          {
            "index": 2,
            "width": 503,
            "height": 479,
            "ext": "png",
            "size_bytes": 10854
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 26,
        "text": "If you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com.\nO’Reilly Online Learning\nFor more than 40 years, O’Reilly Media has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nOur unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at https://oreil.ly/PNLP.\nEmail bookquestions@oreilly.com to comment or ask technical questions about this\nbook.\nFor news and information about our books and courses, visit http://oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://youtube.com/oreillymedia\nFurther Information\nIn the GitHub repo you will find notebooks explaining various concepts covered in\nthe book. The notebooks have been organized by chapter. We also provide additional\nnotebooks, not necessarily covered in the book.\nxxiv \n| \nPreface\n",
        "word_count": 242,
        "char_count": 1716,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1456,
            "height": 488,
            "ext": "png",
            "size_bytes": 16533
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 27,
        "text": "Book website: http://www.practicalnlp.ai\nThe world of NLP is always evolving. To stay updated on how concept mentioned in\nthe book fit into the broader context one, two, and five years from now, follow our\nblog. We keep it updated with relevant writeups and articles, and tag every post with\nthe book’s corresponding chapter title.\nContact the authors:\nEmail: authors@practicalnlp.ai\nLinkedin: https://linkedin.com/company/practical-nlp\nTwitter: https://twitter.com/PracticalNLProc\nFacebook: https://oreil.ly/facebookPNLP\nAcknowledgments\nA book like this is a compendium of knowledge; hence, it cannot exist in isolation.\nWhile writing this book, we drew a lot of inspiration and information from several\nbooks, research papers, software projects and numerous other resources on the inter‐\nnet. We thank the NLP and Machine Learning community for all their efforts. We\nhave merely stood on the shoulders on these giants. We also thank various people\nwho attended some of the authors’ talks and workshops and participated in discus‐\nsions that lead to the idea of writing this book and shaping its premise. This book is\nthe result of a long collaborative effort and several people supported us in different\nways in our endeavor.\nWe thank the O’Reilly reviewers Will Scott, Darren Cook, Ramya Balasubramaniam,\nPriyanka Raghavan, and Siddharth Narayanan for their meticulous, invaluable, and\ndetailed comments which helped us improve earlier drafts. Detailed feedback from\nSiddharth Sharma, Sumod Mohan, Vinayak Hegde, Aasish Pappu, Taranjeet Singh,\nKartikay Bagla, and Varun Purushotham were instrumental in improving the quality\nof the content.\nWe are also very thankful to Rui Shu, Shreyans Dhankhar, Jitin Kapila, Kumarjit\nPathak, Ernest Kirubakaran Selvaraj, Robin Singh, Ayush Datta, Vishal Gupta, and\nNachiketh for helping us prepare the early versions of code notebooks. We would\nespecially like to thank Varun Purushotham, who spent several weeks reading and\nrereading our drafts and preparing and cross-checking the code notebooks. This\nbook would not be the same without his contribution.\nWe would like to thank the O’Reilly Media team, without whom this would not have\nbeen possible: Jonathan Hassell, for giving us this opportunity; Melissa Potter, for\nregularly following up with us throughout this journey and patiently answering all\nPreface \n| \nxxv\n",
        "word_count": 350,
        "char_count": 2362,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 28,
        "text": "our questions! Beth Kelly and Holly Forsyth, for all the help and support in shaping it\ninto a book from the chapter drafts.\nFinally, the following are personal thank you notes by each author:\nSowmya: My first and biggest thank you goes to my daughter, Sahasra Malathi, whose\nbirth and first year of life coincided with the writing of this book. It is not easy to\nwrite a book, and not easy at all to write it with a newborn. And yet, here we are.\nThank you, Sahasra! My mom, Geethamani, and my husband, Sriram, supported my\nwriting by taking up baby care and household duties at different phases of writing.\nMy friends, Purnima and Visala, were always available to listen to my excited updates\nas well as rants about the book. My boss, Cyril Goutte, encouraged me and checked\non my writing progress throughout. Discussions with my former colleagues, Chris\nCardinal and Eric Le Fort, taught me a lot about developing NLP solutions for indus‐\ntry problems, without which I perhaps would never have thought of being a part of\nthis kind of book. I thank all of them for their support.\nBodhisattwa: I would like to take this opportunity to thank my parents, their unques‐\ntionable sacrifice, and the constant encouragement that made me the person I am\ntoday. Their efforts have instilled in me the love and dedication to learning in my life.\nI am eternally grateful to my advisors, Prof. Animesh Mukherjee and Pawan Goyal,\nwho introduced me to this world of NLP; and Prof. Julian McAuley, who is nothing\nless than fundamental to my technical, academic, and personal development in my\nPhD career. The courses taken by my other professors—Taylor Berg-Kirkpatrick,\nLawrence Saul, David Kriegman, Debasis Sengupta, Sudeshna Sarkar, and Sourav Sen\nGupta—have significantly shaped my learning on the subject. In the early days of the\nbook, my colleagues from Walmart Labs—especially Subhasish Misra, Arunita Das,\nSmaranya Dey, Sumanth Prabhu, and Rajesh Bhat—gave me the motivation for this\ncrazy idea. To my mentors at Google AI, Microsoft Research, Amazon Alexa, and my\nlabmates from the UCSD NLP Group, thank you for being supportive and helpful in\nthis entire journey. Also, my friends Sanchaita Hazra, Sujoy Paul, and Digbalay Bose,\nwho stood by me through thick and thin in this mammoth project, deserve a special\nmention. At last, none of this would have been possible without my coauthors, who\nbelieved in this project and stayed together till the last bit of it!\nAnuj: First and foremost, I would like to express my sincere gratitude to my wife,\nAnu, and my son, Nirvaan. Without their unwavering support, I would not have been\nable to devote the last three years to this endeavor. Thank you so much! I would also\nlike to thank my parents and family for their encouragement. A big shout out goes to\nSaurabh Arora, for introducing me to the world of NLP. Many thanks to my friends,\nthe late Vivek Jain and Mayur Hemani; they have always encouraged me to keep\ngoing, especially in hard times. I would also like to thank all of the amazing people\ninvolved in machine learning communities in Bangalore; especially: Sumod Mohan,\nVijay Gabale, Nishant Sinha, Ashwin Kumar, Mukundhan Srinivasan, Zainab Bawa,\nxxvi \n| \nPreface\n",
        "word_count": 546,
        "char_count": 3218,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 29,
        "text": "and Naresh Jain for all the wonderful and thought-provoking discussions. I would\nlike to thank my colleagues—former and present—at CSTAR, Airwoot, FreshWorks,\nHuawei Research, Intuit, and Vahan, Inc., for everything they taught me. To my pro‐\nfessors, Kannan Srinathan, P.R.K Rao, and B. Yegnanarayana, whose teachings have\nhad a profound impact on me.\nHarshit: I want to thank my parents, who have supported and encouraged me to pur‐\nsue every crazy idea I have had. I cannot thank my dear friends Preeti Shrimal and\nDev Chandan enough. They have been with me throughout the book’s entire journey.\nTo my cofounders, Abhimanyu Vyas and Aviral Mathur, thank you for adjusting our\nstartup endeavor to help me complete the book. I want to thank all my former collea‐\ngues at Quipio and Notify.io who helped crystalize my thinking, especially Zubin\nWadia, Amit Kumar, and Naveen Koorakula. None of this would have been possible\nwithout my professors and everything they taught me—thank you, Prof. Luis von\nAhn, Anil Kumar Singh, Alan W Black, William Cohen, Lori Levin, and Carlos\nGuestrin. I also want to acknowledge Kaustuv DeBiswas, Siddharth Narayanan, Sid‐\ndharth Sharma, Alok Parlikar, Nathan Schneider, Aasish Pappu, Manish Jawa, Sumit\nPandey, and Mohit Ranka, who have supported me at various junctures of this\njourney.\nPreface \n| \nxxvii\n",
        "word_count": 215,
        "char_count": 1342,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 30,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 31,
        "text": "PART I\nFoundations\n",
        "word_count": 3,
        "char_count": 19,
        "fonts": [
          "MyriadPro-SemiboldCond (28.4pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 32,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 33,
        "text": "CHAPTER 1\nNLP: A Primer\nA language is not just words. It’s a culture, a tradition,\na unification of a community,\na whole history that creates what a community is.\nIt’s all embodied in a language.\n—Noam Chomsky\nImagine a hypothetical person, John Doe. He’s the CTO of a fast-growing technology\nstartup. On a busy day, John wakes up and has this conversation with his digital\nassistant:\nJohn: “How is the weather today?”\nDigital assistant: “It is 37 degrees centigrade outside with no rain today.”\nJohn: “What does my schedule look like?”\nDigital assistant: “You have a strategy meeting at 4 p.m. and an all-hands at 5:30 p.m.\nBased on today’s traffic situation, it is recommended you leave for the office by\n8:15 a.m.”\nWhile he’s getting dressed, John probes the assistant on his fashion choices:\nJohn: “What should I wear today?”\nDigital assistant: “White seems like a good choice.”\nYou might have used smart assistants such as Amazon Alexa, Google Home, or Apple\nSiri to do similar things. We talk to these assistants not in a programming language,\nbut in our natural language—the language we all communicate in. This natural lan‐\nguage has been the primary medium of communication between humans since time\nimmemorial. But computers can only process data in binary, i.e., 0s and 1s. While we\ncan represent language data in binary, how do we make machines understand the\n3\n",
        "word_count": 233,
        "char_count": 1374,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (9.3pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 34,
        "text": "language? This is where natural language processing (NLP) comes in. It is an area of\ncomputer science that deals with methods to analyze, model, and understand human\nlanguage. Every intelligent application involving human language has some NLP\nbehind it. In this book, we’ll explain what NLP is as well as how to use NLP to build\nand scale intelligent applications. Due to the open-ended nature of NLP problems,\nthere are dozens of alternative approaches one can take to solve a given problem. This\nbook will help you navigate this maze of options and suggests how to choose the best\noption based on your problem.\nThis chapter aims to give a quick primer of what NLP is before we start delving\ndeeper into how to implement NLP-based solutions for different application scenar‐\nios. We’ll start with an overview of numerous applications of NLP in real-world sce‐\nnarios, then cover the various tasks that form the basis of building different NLP\napplications. This will be followed by an understanding of language from an NLP\nperspective and of why NLP is difficult. After that, we’ll give an overview of heuris‐\ntics, machine learning, and deep learning, then introduce a few commonly used algo‐\nrithms in NLP. This will be followed by a walkthrough of an NLP application. Finally,\nwe’ll conclude the chapter with an overview of the rest of the topics in the book.\nFigure 1-1 shows a preview of the organization of the chapters in terms of various\nNLP tasks and applications.\nFigure 1-1. NLP tasks and applications\nLet’s start by taking a look at some popular applications you use in everyday life that\nhave some form of NLP as a major component.\n4 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 288,
        "char_count": 1678,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1431,
            "height": 730,
            "ext": "png",
            "size_bytes": 109911
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 35,
        "text": "NLP in the Real World\nNLP is an important component in a wide range of software applications that we use\nin our daily lives. In this section, we’ll introduce some key applications and also take a\nlook at some common tasks that you’ll see across different NLP applications. This\nsection reinforces the applications we showed you in Figure 1-1, which you’ll see in\nmore detail throughout the book.\nCore applications:\n• Email platforms, such as Gmail, Outlook, etc., use NLP extensively to provide a\nrange of product features, such as spam classification, priority inbox, calendar\nevent extraction, auto-complete, etc. We’ll discuss some of these in detail in\nChapters 4 and 5.\n• Voice-based assistants, such as Apple Siri, Google Assistant, Microsoft Cortana,\nand Amazon Alexa rely on a range of NLP techniques to interact with the user,\nunderstand user commands, and respond accordingly. We’ll cover key aspects of\nsuch systems in Chapter 6, where we discuss chatbots.\n• Modern search engines, such as Google and Bing, which are the cornerstone of\ntoday’s internet, use NLP heavily for various subtasks, such as query understand‐\ning, query expansion, question answering, information retrieval, and ranking\nand grouping of the results, to name a few. We’ll discuss some of these subtasks\nin Chapter 7.\n• Machine translation services, such as Google Translate, Bing Microsoft Transla‐\ntor, and Amazon Translate are increasingly used in today’s world to solve a wide\nrange of scenarios and business use cases. These services are direct applications\nof NLP. We’ll touch on machine translation in Chapter 7.\nOther applications:\n• Organizations across verticals analyze their social media feeds to build a better\nand deeper understanding of the voice of their customers. We’ll cover this in\nChapter 8.\n• NLP is widely used to solve diverse sets of use cases on e-commerce platforms\nlike Amazon. These vary from extracting relevant information from product\ndescriptions to understanding user reviews. Chapter 9 covers these in detail.\n• Advances in NLP are being applied to solve use cases in domains such as health‐\ncare, finance, and law. Chapter 10 addresses these.\n• Companies such as Arria [1] are working to use NLP techniques to automatically\ngenerate reports for various domains, from weather forecasting to financial\nservices.\nNLP in the Real World \n| \n5\n",
        "word_count": 379,
        "char_count": 2357,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 36,
        "text": "• NLP forms the backbone of spelling- and grammar-correction tools, such as \nGrammarly and spell check in Microsoft Word and Google Docs.\n• Jeopardy! is a popular quiz show on TV. In the show, contestants are presented\nwith clues in the form of answers, and the contestants must phrase their respon‐\nses in the form of questions. IBM built the Watson AI to compete with the show’s\ntop players. Watson won the first prize with a million dollars, more than the\nworld champions. Watson AI was built using NLP techniques and is one of the\nexamples of NLP bots winning a world competition.\n• NLP is used in a range of learning and assessment tools and technologies, such as\nautomated scoring in exams like the Graduate Record Examination (GRE), plagi‐\narism detection (e.g., Turnitin), intelligent tutoring systems, and language learn‐\ning apps like Duolingo.\n• NLP is used to build large knowledge bases, such as the Google Knowledge\nGraph, which are useful in a range of applications like search and question\nanswering.\nThis list is by no means exhaustive. NLP is increasingly being used across several\nother applications, and newer applications of NLP are coming up as we speak. Our\nmain focus is to introduce you to the ideas behind building these applications. We do\nso by discussing different kinds of NLP problems and how to solve them. To get a\nperspective on what you are about to learn in this book, and to appreciate the nuan‐\nces that go into building these NLP applications, let’s take a look at some key NLP\ntasks that form the bedrock of many NLP applications and industry use cases.\nNLP Tasks\nThere is a collection of fundamental tasks that appear frequently across various NLP\nprojects. Owing to their repetitive and fundamental nature, these tasks have been\nstudied extensively. Having a good grip on them will make you ready to build various\nNLP applications across verticals. (We also saw some of these tasks earlier in\nFigure 1-1.) Let’s briefly introduce them:\nLanguage modeling\nThis is the task of predicting what the next word in a sentence will be based on\nthe history of previous words. The goal of this task is to learn the probability of a\nsequence of words appearing in a given language. Language modeling is useful\nfor building solutions for a wide variety of problems, such as speech recognition,\noptical character recognition, handwriting recognition, machine translation, and\nspelling correction.\n6 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 415,
        "char_count": 2456,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 37,
        "text": "Text classification\nThis is the task of bucketing the text into a known set of categories based on its\ncontent. Text classification is by far the most popular task in NLP and is used in a\nvariety of tools, from email spam identification to sentiment analysis.\nInformation extraction\nAs the name indicates, this is the task of extracting relevant information from\ntext, such as calendar events from emails or the names of people mentioned in a\nsocial media post.\nInformation retrieval\nThis is the task of finding documents relevant to a user query from a large collec‐\ntion. Applications like Google Search are well-known use cases of information\nretrieval.\nConversational agent\nThis is the task of building dialogue systems that can converse in human lan‐\nguages. Alexa, Siri, etc., are some common applications of this task.\nText summarization\nThis task aims to create short summaries of longer documents while retaining the\ncore content and preserving the overall meaning of the text.\nQuestion answering\nThis is the task of building a system that can automatically answer questions\nposed in natural language.\nMachine translation\nThis is the task of converting a piece of text from one language to another. Tools\nlike Google Translate are common applications of this task.\nTopic modeling\nThis is the task of uncovering the topical structure of a large collection of docu‐\nments. Topic modeling is a common text-mining tool and is used in a wide range\nof domains, from literature to bioinformatics.\nFigure 1-2 shows a depiction of these tasks based on their relative difficulty in terms\nof developing comprehensive solutions.\nNLP in the Real World \n| \n7\n",
        "word_count": 271,
        "char_count": 1654,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 38,
        "text": "Figure 1-2. NLP tasks organized according to their relative difficulty\nIn the rest of the chapters in this book, we’ll see these tasks’ challenges and learn how\nto develop solutions that work for certain use cases (even the hard tasks shown in the\nfigure). To get there, it is useful to have an understanding of the nature of human lan‐\nguage and the challenges in automating language processing. The next two sections\nprovide a basic overview.\nWhat Is Language?\nLanguage is a structured system of communication that involves complex combina‐\ntions of its constituent components, such as characters, words, sentences, etc. Lin‐\nguistics is the systematic study of language. In order to study NLP, it is important to\nunderstand some concepts from linguistics about how language is structured. In this\nsection, we’ll introduce them and cover how they relate to some of the NLP tasks we\nlisted earlier.\nWe can think of human language as composed of four major building blocks: pho‐\nnemes, morphemes and lexemes, syntax, and context. NLP applications need knowl‐\nedge of different levels of these building blocks, starting from the basic sounds\nof language (phonemes) to texts with some meaningful expressions (context).\n8 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 203,
        "char_count": 1248,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1346,
            "height": 1078,
            "ext": "png",
            "size_bytes": 50275
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 39,
        "text": "Figure 1-3 shows these building blocks of language, what they encompass, and a few\nNLP applications we introduced earlier that require this knowledge. Some of the\nterms listed here that were not introduced earlier in this chapter (e.g., parsing, word\nembeddings, etc.) will be introduced later in these first three chapters.\nFigure 1-3. Building blocks of language and their applications\nBuilding Blocks of Language\nLet’s first introduce what these blocks of language are to give context for the chal‐\nlenges involved in NLP.\nPhonemes\nPhonemes are the smallest units of sound in a language. They may not have any\nmeaning by themselves but can induce meanings when uttered in combination with\nother phonemes. For example, standard English has 44 phonemes, which are either\nsingle letters or a combination of letters [2]. Figure 1-4 shows these phonemes along\nwith sample words. Phonemes are particularly important in applications involving\nspeech understanding, such as speech recognition, speech-to-text transcription, and\ntext-to-speech conversion.\nWhat Is Language? \n| \n9\n",
        "word_count": 163,
        "char_count": 1074,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 968,
            "height": 726,
            "ext": "png",
            "size_bytes": 50386
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 40,
        "text": "Figure 1-4. Phonemes and examples\nMorphemes and lexemes\nA morpheme is the smallest unit of language that has a meaning. It is formed by a\ncombination of phonemes. Not all morphemes are words, but all prefixes and suffixes\nare morphemes. For example, in the word “multimedia,” “multi-” is not a word but a\nprefix that changes the meaning when put together with “media.” “Multi-” is a mor‐\npheme. Figure 1-5 illustrates some words and their morphemes. For words like “cats”\nand “unbreakable,” their morphemes are just constituents of the full word, whereas\nfor words like “tumbling” and “unreliability,” there is some variation when breaking\nthe words down into their morphemes.\nFigure 1-5. Morpheme examples\nLexemes are the structural variations of morphemes related to one another by mean‐\ning. For example, “run” and “running” belong to the same lexeme form. Morphologi‐\ncal analysis, which analyzes the structure of words by studying its morphemes and\nlexemes, is a foundational block for many NLP tasks, such as tokenization, stemming,\n10 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 173,
        "char_count": 1071,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1302,
            "height": 843,
            "ext": "png",
            "size_bytes": 62957
          },
          {
            "index": 1,
            "width": 598,
            "height": 264,
            "ext": "png",
            "size_bytes": 9160
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 41,
        "text": "learning word embeddings, and part-of-speech tagging, which we’ll introduce in the\nnext chapter.\nSyntax\nSyntax is a set of rules to construct grammatically correct sentences out of words and\nphrases in a language. Syntactic structure in linguistics is represented in many differ‐\nent ways. A common approach to representing sentences is a parse tree. Figure 1-6\nshows an example parse tree for two English sentences.\nFigure 1-6. Syntactic structure of two syntactically similar sentences\nThis has a hierarchical structure of language, with words at the lowest level, followed\nby part-of-speech tags, followed by phrases, and ending with a sentence at the highest\nlevel. In Figure 1-6, both sentences have a similar structure and hence a similar syn‐\ntactic parse tree. In this representation, N stands for noun, V for verb, and P for prep‐\nosition. Noun phrase is denoted by NP and verb phrase by VP. The two noun phrases\nare “The girl” and “The boat,” while the two verb phrases are “laughed at the monkey”\nand “sailed up the river.” The syntactic structure is guided by a set of grammar rules\nfor the language (e.g., the sentence comprises an NP and a VP), and this in turn\nguides some of the fundamental tasks of language processing, such as parsing. Pars‐\ning is the NLP task of constructing such trees automatically. Entity extraction and\nrelation extraction are some of the NLP tasks that build on this knowledge of parsing,\nwhich we’ll discuss in more detail in Chapter 5. Note that the parse structure\ndescribed above is specific to English. The syntax of one language can be very differ‐\nent from that of another language, and the language-processing approaches needed\nfor that language will change accordingly.\nWhat Is Language? \n| \n11\n",
        "word_count": 290,
        "char_count": 1746,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 739,
            "height": 514,
            "ext": "png",
            "size_bytes": 22899
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 42,
        "text": "Context\nContext is how various parts in a language come together to convey a particular\nmeaning. Context includes long-term references, world knowledge, and common\nsense along with the literal meaning of words and phrases. The meaning of a sentence\ncan change based on the context, as words and phrases can sometimes have multiple\nmeanings. Generally, context is composed from semantics and pragmatics. Semantics\nis the direct meaning of the words and sentences without external context. Pragmat‐\nics adds world knowledge and external context of the conversation to enable us to\ninfer implied meaning. Complex NLP tasks such as sarcasm detection, summariza‐\ntion, and topic modeling are some of tasks that use context heavily.\nLinguistics is the study of language and hence is a vast area in itself, and we only\nintroduced some basic ideas to illustrate the role of linguistic knowledge in NLP. Dif‐\nferent tasks in NLP require varying degrees of knowledge about these building blocks\nof language. An interested reader can refer to the books written by Emily Bender [3,\n4] on the linguistic fundamentals for NLP for further study. Now that we have some\nidea of what the building blocks of language are, let’s see why language can be hard\nfor computers to understand and what makes NLP challenging.\nWhy Is NLP Challenging?\nWhat makes NLP a challenging problem domain? The ambiguity and creativity of\nhuman language are just two of the characteristics that make NLP a demanding area\nto work in. This section explores each characteristic in more detail, starting with\nambiguity of language.\nAmbiguity\nAmbiguity means uncertainty of meaning. Most human languages are inherently\nambiguous. Consider the following sentence: “I made her duck.” This sentence has\nmultiple meanings. The first one is: I cooked a duck for her. The second meaning is: I\nmade her bend down to avoid an object. (There are other possible meanings, too;\nwe’ll leave them for the reader to think of.) Here, the ambiguity comes from the use\nof the word “made.” Which of the two meanings applies depends on the context in\nwhich the sentence appears. If the sentence appears in a story about a mother and a\nchild, then the first meaning will probably apply. But if the sentence appears in a book\nabout sports, then the second meaning will likely apply. The example we saw is a\ndirect sentence.\nWhen it comes to figurative language—i.e., idioms—the ambiguity only increases.\nFor example, “He is as good as John Doe.” Try to answer, “How good is he?” The\nanswer depends on how good John Doe is. Figure 1-7 shows some examples illustrat‐\ning ambiguity in language.\n12 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 444,
        "char_count": 2658,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 43,
        "text": "Figure 1-7. Examples of ambiguity in language from the Winograd Schema Challenge\nThe examples come from the Winograd Schema Challenge [5], named after Professor\nTerry Winograd of Stanford University. This schema has pairs of sentences that differ\nby only a few words, but the meaning of the sentences is often flipped because of this\nminor change. These examples are easily disambiguated by a human but are not solv‐\nable using most NLP techniques. Consider the pairs of sentences in the figure and the\nquestions associated with them. With some thought, how the answer changes should\nbe apparent based on a single word variation. As another experiment, consider taking\nan off-the-shelf NLP system like Google Translate and try various examples to see\nhow such ambiguities affect (or don’t affect) the output of the system.\nCommon knowledge\nA key aspect of any human language is “common knowledge.” It is the set of all facts\nthat most humans are aware of. In any conversation, it is assumed that these facts are\nknown, hence they’re not explicitly mentioned, but they do have a bearing on the\nmeaning of the sentence. For example, consider two sentences: “man bit dog” and\n“dog bit man.” We all know that the first sentence is unlikely to happen, while the sec‐\nond one is very possible. Why do we say so? Because we all “know” that it is very\nunlikely that a human will bite a dog. Further, dogs are known to bite humans. This\nknowledge is required for us to say that the first sentence is unlikely to happen while\nthe second one is possible. Note that this common knowledge was not mentioned in\nWhat Is Language? \n| \n13\n",
        "word_count": 281,
        "char_count": 1622,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 960,
            "height": 982,
            "ext": "png",
            "size_bytes": 60029
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 44,
        "text": "either sentence. Humans use common knowledge all the time to understand and\nprocess any language. In the above example, the two sentences are syntactically very\nsimilar, but a computer would find it very difficult to differentiate between the two, as\nit lacks the common knowledge humans have. One of the key challenges in NLP is\nhow to encode all the things that are common knowledge to humans in a computa‐\ntional model.\nCreativity\nLanguage is not just rule driven; there is also a creative aspect to it. Various styles,\ndialects, genres, and variations are used in any language. Poems are a great example\nof creativity in language. Making machines understand creativity is a hard problem\nnot just in NLP, but in AI in general.\nDiversity across languages\nFor most languages in the world, there is no direct mapping between the vocabularies\nof any two languages. This makes porting an NLP solution from one language to\nanother hard. A solution that works for one language might not work at all for\nanother language. This means that one either builds a solution that is language agnos‐\ntic or that one needs to build separate solutions for each language. While the first one\nis conceptually very hard, the other is laborious and time intensive.\nAll these issues make NLP a challenging—yet rewarding—domain to work in. Before\nlooking into how some of these challenges are tackled in NLP, we should know the\ncommon approaches to solving NLP problems. Let’s start with an overview of how\nmachine learning and deep learning are connected to NLP before delving deeper into\ndifferent approaches to NLP.\nMachine Learning, Deep Learning, and NLP: An Overview\nLoosely speaking, artificial intelligence (AI) is a branch of computer science that aims\nto build systems that can perform tasks that require human intelligence. This is some‐\ntimes also called “machine intelligence.” The foundations of AI were laid in the 1950s\nat a workshop organized at Dartmouth College [6]. Initial AI was largely built out of\nlogic-, heuristics-, and rule-based systems. Machine learning (ML) is a branch of AI\nthat deals with the development of algorithms that can learn to perform tasks auto‐\nmatically based on a large number of examples, without requiring handcrafted rules.\nDeep learning (DL) refers to the branch of machine learning that is based on artificial\nneural network architectures. ML, DL, and NLP are all subfields within AI, and the\nrelationship between them is depicted in Figure 1-8.\nWhile there is some overlap between NLP, ML, and DL, they are also quite different\nareas of study, as the figure illustrates. Like other early work in AI, early NLP applica‐\ntions were also based on rules and heuristics. In the past few decades, though, NLP\n14 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 461,
        "char_count": 2767,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 45,
        "text": "application development has been heavily influenced by methods from ML. More\nrecently, DL has also been frequently used to build NLP applications. Considering\nthis, let’s do a short overview of ML and DL in this section.\nFigure 1-8. How NLP, ML, and DL are related\nThe goal of ML is to “learn” to perform tasks based on examples (called “training\ndata”) without explicit instruction. This is typically done by creating a numeric repre‐\nsentation (called “features”) of the training data and using this representation to learn\nthe patterns in those examples. Machine learning algorithms can be grouped into\nthree primary paradigms: supervised learning, unsupervised learning, and reinforce‐\nment learning. In supervised learning, the goal is to learn the mapping function from\ninput to output given a large number of examples in the form of input-output pairs.\nThe input-output pairs are known as training data, and the outputs are specifically\nknown as labels or ground truth. An example of a supervised learning problem related\nto language is learning to classify email messages as spam or non-spam given thou‐\nsands of examples in both categories. This is a common scenario in NLP, and we’ll see\nexamples of supervised learning throughout the book, especially in Chapter 4.\nUnsupervised learning refers to a set of machine learning methods that aim to find\nhidden patterns in given input data without any reference output. That is, in contrast\nto supervised learning, unsupervised learning works with large collections of unla‐\nbeled data. In NLP, an example of such a task is to identify latent topics in a large col‐\nlection of textual data without any knowledge of these topics. This is known as topic\nmodeling, and we’ll discuss it in Chapter 7.\nMachine Learning, Deep Learning, and NLP: An Overview \n| \n15\n",
        "word_count": 295,
        "char_count": 1813,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1388,
            "height": 864,
            "ext": "png",
            "size_bytes": 83542
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 46,
        "text": "Common in real-world NLP projects is a case of semi-supervised learning, where we\nhave a small labeled dataset and a large unlabeled dataset. Semi-supervised techni‐\nques involve using both datasets to learn the task at hand. Last but not least, rein‐\nforcement learning deals with methods to learn tasks via trial and error and is\ncharacterized by the absence of either labeled or unlabeled data in large quantities.\nThe learning is done in a self-contained environment and improves via feedback\n(reward or punishment) facilitated by the environment. This form of learning is not\ncommon in applied NLP (yet). It is more common in applications such as machine-\nplaying games like go or chess, in the design of autonomous vehicles, and in robotics.\nDeep learning refers to the branch of machine learning that is based on artificial neu‐\nral network architectures. The ideas behind neural networks are inspired by neurons\nin the human brain and how they interact with one another. In the past decade, deep\nlearning–based neural architectures have been used to successfully improve the per‐\nformance of various intelligent applications, such as image and speech recognition\nand machine translation. This has resulted in a proliferation of deep learning–based\nsolutions in industry, including in NLP applications.\nThroughout this book, we’ll discuss how all these approaches are used for developing\nvarious NLP applications. Let’s now discuss the different approaches to solve any\ngiven NLP problem.\nApproaches to NLP\nThe different approaches used to solve NLP problems commonly fall into three cate‐\ngories: heuristics, machine learning, and deep learning. This section is simply an\nintroduction to each approach—don’t worry if you can’t quite grasp the concepts yet,\nas they’ll be discussed in detail throughout the rest of the book. Let’s jump in by dis‐\ncussing heuristics-based NLP.\nHeuristics-Based NLP\nSimilar to other early AI systems, early attempts at designing NLP systems were based\non building rules for the task at hand. This required that the developers had some\nexpertise in the domain to formulate rules that could be incorporated into a program.\nSuch systems also required resources like dictionaries and thesauruses, typically com‐\npiled and digitized over a period of time. An example of designing rules to solve an\nNLP problem using such resources is lexicon-based sentiment analysis. It uses counts\nof positive and negative words in the text to deduce the sentiment of the text. We’ll\ncover this briefly in Chapter 4.\nBesides dictionaries and thesauruses, more elaborate knowledge bases have been built\nto aid NLP in general and rule-based NLP in particular. One example is Wordnet [7],\nwhich is a database of words and the semantic relationships between them. Some\n16 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 447,
        "char_count": 2816,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 47,
        "text": "examples of such relationships are synonyms, hyponyms, and meronyms. Synonyms\nrefer to words with similar meanings. Hyponyms capture is-type-of relationships. For\nexample, baseball, sumo wrestling, and tennis are all hyponyms of sports. Meronyms\ncapture is-part-of relationships. For example, hands and legs are meronyms of the\nbody. All this information becomes useful when building rule-based systems around\nlanguage. Figure 1-9 shows an example depiction of such relationships between\nwords using Wordnet.\nFigure 1-9. Wordnet graph for the word “sport” [8]\nMore recently, common sense world knowledge has also been incorporated into\nknowledge bases like Open Mind Common Sense [9], which also aids such rule-based\nsystems. While what we’ve seen so far are largely lexical resources based on word-\nlevel information, rule-based systems go beyond words and can incorporate other\nforms of information, too. Some of them are introduced below.\nRegular expressions (regex) are a great tool for text analysis and building rule-based\nsystems. A regex is a set of characters or a pattern that is used to match and find\nApproaches to NLP \n| \n17\n",
        "word_count": 174,
        "char_count": 1138,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1417,
            "height": 1267,
            "ext": "png",
            "size_bytes": 113646
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 48,
        "text": "substrings in text. For example, a regex like ‘^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]\n+)\\.([a-zA-Z]{2,5})$’ is used to find all email IDs in a piece of text. Regexes are a great\nway to incorporate domain knowledge in your NLP system. For example, given a cus‐\ntomer complaint that comes via chat or email, we want to build a system to automati‐\ncally identify the product the complaint is about. There is a range of product codes\nthat map to certain brand names. We can use regexes to match these easily.\nRegexes are a very popular paradigm for building rule-based systems. NLP software\nlike StanfordCoreNLP includes TokensRegex [10], which is a framework for defining\nregular expressions. It is used to identify patterns in text and use matched text to cre‐\nate rules. Regexes are used for deterministic matches—meaning it’s either a match or\nit’s not. Probabilistic regexes is a sub-branch that addresses this limitation by includ‐\ning a probability of a match. Interested readers can look at software libraries such as\npregex [11]. Last accessed June 15, 2020.\nContext-free grammar (CFG) is a type of formal grammar that is used to model natu‐\nral languages. CFG was invented by Professor Noam Chomsky, a renowned linguist\nand scientist. CFGs can be used to capture more complex and hierarchical informa‐\ntion that a regex might not. The Earley parser [12] allows parsing of all kinds of\nCFGs. To model more complex rules, grammar languages like JAPE (Java Annotation\nPatterns Engine) can be used [13]. JAPE has features from both regexes as well as\nCFGs and can be used for rule-based NLP systems like GATE (General Architecture\nfor Text Engineering) [14]. GATE is used for building text extraction for closed and\nwell-defined domains where accuracy and completeness of coverage is more impor‐\ntant. As an example, JAPE and GATE were used to extract information on pacemaker\nimplantation procedures from clinical reports [15]. Figure 1-10 shows the GATE\ninterface along with several types of information highlighted in the text as an example\nof a rule-based system.\nRules and heuristics play a role across the entire life cycle of NLP projects even now.\nAt one end, they’re a great way to build first versions of NLP systems. Put simply, \nrules and heuristics help you quickly build the first version of the model and get a\nbetter understanding of the problem at hand. We’ll discuss this point in depth in\nChapters 4 and 11. Rules and heuristics can also be useful as features for machine\nlearning–based NLP systems. At the other end of the spectrum of the project life\ncycle, rules and heuristics are used to plug the gaps in the system. Any NLP system\nbuilt using statistical, machine learning, or deep learning techniques will make mis‐\ntakes. Some mistakes can be too expensive—for example, a healthcare system that\nlooks into all the medical records of a patient and wrongly decides to not advise a\ncritical test. This mistake could even cost a life. Rules and heuristics are a great way to\nplug such gaps in production systems. Now let’s turn our attention to machine learn‐\ning techniques used for NLP.\n18 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 526,
        "char_count": 3146,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 49,
        "text": "Figure 1-10. GATE tool\nMachine Learning for NLP\nMachine learning techniques are applied to textual data just as they’re used on other\nforms of data, such as images, speech, and structured data. Supervised machine learn‐\ning techniques such as classification and regression methods are heavily used for vari‐\nous NLP tasks. As an example, an NLP classification task would be to classify news\narticles into a set of news topics like sports or politics. On the other hand, regression\ntechniques, which give a numeric prediction, can be used to estimate the price of a\nstock based on processing the social media discussion about that stock. Similarly,\nunsupervised clustering algorithms can be used to club together text documents.\nAny machine learning approach for NLP, supervised or unsupervised, can be\ndescribed as consisting of three common steps: extracting features from text, using\nthe feature representation to learn a model, and evaluating and improving the model.\nWe’ll learn more about feature representations for text specifically in Chapter 3 and\nevaluation in Chapter 2. We’ll now briefly outline some of the commonly used super‐\nvised ML methods in NLP for the second step (using the feature representation to\nlearn a model). Having a basic idea of these methods will help you understand the\nconcepts discussed in later chapters.\nApproaches to NLP \n| \n19\n",
        "word_count": 219,
        "char_count": 1367,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 775,
            "height": 541,
            "ext": "png",
            "size_bytes": 26704
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 50,
        "text": "Naive Bayes\nNaive Bayes is a classic algorithm for classification tasks [16] that mainly relies on\nBayes’ theorem (as is evident from the name). Using Bayes’ theorem, it calculates the\nprobability of observing a class label given the set of features for the input data. A\ncharacteristic of this algorithm is that it assumes each feature is independent of all\nother features. For the news classification example mentioned earlier in this chapter,\none way to represent the text numerically is by using the count of domain-specific\nwords, such as sport-specific or politics-specific words, present in the text. We\nassume that these word counts are not correlated to one another. If the assumption\nholds, we can use Naive Bayes to classify news articles. While this is a strong assump‐\ntion to make in many cases, Naive Bayes is commonly used as a starting algorithm for\ntext classification. This is primarily because it is simple to understand and very fast to\ntrain and run.\nSupport vector machine\nThe support vector machine (SVM) is another popular classification [17] algorithm.\nThe goal in any classification approach is to learn a decision boundary that acts as a\nseparation between different categories of text (e.g., politics versus sports in our news\nclassification example). This decision boundary can be linear or nonlinear (e.g., a cir‐\ncle). An SVM can learn both a linear and nonlinear decision boundary to separate\ndata points belonging to different classes. A linear decision boundary learns to repre‐\nsent the data in a way that the class differences become apparent. For two-\ndimensional feature representations, an illustrative example is given in Figure 1-11,\nwhere the black and white points belong to different classes (e.g., sports and politics\nnews groups). An SVM learns an optimal decision boundary so that the distance\nbetween points across classes is at its maximum. The biggest strength of SVMs are\ntheir robustness to variation and noise in the data. A major weakness is the time\ntaken to train and the inability to scale when there are large amounts of training data.\n20 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 349,
        "char_count": 2127,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 51,
        "text": "Figure 1-11. A two-dimensional feature representation of an SVM\nHidden Markov Model\nThe hidden Markov model (HMM) is a statistical model [18] that assumes there is an\nunderlying, unobservable process with hidden states that generates the data—i.e., we\ncan only observe the data once it is generated. An HMM then tries to model the hid‐\nden states from this data. For example, consider the NLP task of part-of-speech (POS)\ntagging, which deals with assigning part-of-speech tags to sentences. HMMs are used\nfor POS tagging of text data. Here, we assume that the text is generated according to\nan underlying grammar, which is hidden underneath the text. The hidden states are\nparts of speech that inherently define the structure of the sentence following the lan‐\nguage grammar, but we only observe the words that are governed by these latent\nstates. Along with this, HMMs also make the Markov assumption, which means that\neach hidden state is dependent on the previous state(s). Human language is sequential\nin nature, and the current word in a sentence depends on what occurred before it.\nHence, HMMs with these two assumptions are a powerful tool for modeling textual\nApproaches to NLP \n| \n21\n",
        "word_count": 197,
        "char_count": 1194,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1289,
            "height": 1286,
            "ext": "png",
            "size_bytes": 98602
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 52,
        "text": "data. In Figure 1-12, we can see an example of an HMM that learns parts of speech\nfrom a given sentence. Parts of speech like JJ (adjective) and NN (noun) are hidden\nstates, while the sentence “natural language processing ( nlp )…” is directly observed.\nFigure 1-12. A graphical representation of a hidden Markov model\nFor a detailed discussion on HMMs for NLP, refer to Chapter 8 in the book Speech\nand Language Processing by Professor Jurafsky [19].\nConditional random fields\nThe conditional random field (CRF) is another algorithm that is used for sequential\ndata. Conceptually, a CRF essentially performs a classification task on each element in\nthe sequence [20]. Imagine the same example of POS tagging, where a CRF can tag\nword by word by classifying them to one of the parts of speech from the pool of all\nPOS tags. Since it takes the sequential input and the context of tags into considera‐\ntion, it becomes more expressive than the usual classification methods and generally\nperforms better. CRFs outperform HMMs for tasks such as POS tagging, which rely\non the sequential nature of language. We discuss CRFs and their variants along with\napplications in Chapters 5, 6, and 9.\nThese are some of the popular ML algorithms that are used heavily across NLP tasks.\nHaving some understanding of these ML methods helps to understand various solu‐\ntions discussed in the book. Apart from that, it is also important to understand when\nto use which algorithm, which we’ll discuss in the upcoming chapters. To learn more\nabout other steps and further theoretical details of the machine learning process, we\nrecommend the textbook Pattern Recognition and Machine Learning by Christopher\nBishop [21]. For a more applied machine learning perspective, Aurélien Géron’s book\n[22] is a great resource to start with. Let’s now take a look at deep learning approaches\nto NLP.\nDeep Learning for NLP\nWe briefly touched on a couple of popular machine learning methods that are used\nheavily in various NLP tasks. In the last few years, we have seen a huge surge in using\nneural networks to deal with complex, unstructured data. Language is inherently\ncomplex and unstructured. Therefore, we need models with better representation and\n22 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 376,
        "char_count": 2254,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1387,
            "height": 297,
            "ext": "png",
            "size_bytes": 41046
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 53,
        "text": "learning capability to understand and solve language tasks. Here are a few popular \ndeep neural network architectures that have become the status quo in NLP.\nRecurrent neural networks\nAs we mentioned earlier, language is inherently sequential. A sentence in any lan‐\nguage flows from one direction to another (e.g., English reads from left to right).\nThus, a model that can progressively read an input text from one end to another can\nbe very useful for language understanding. Recurrent neural networks (RNNs) are\nspecially designed to keep such sequential processing and learning in mind. RNNs\nhave neural units that are capable of remembering what they have processed so far.\nThis memory is temporal, and the information is stored and updated with every time\nstep as the RNN reads the next word in the input. Figure 1-13 shows an unrolled\nRNN and how it keeps track of the input at different time steps.\nFigure 1-13. An unrolled recurrent neural network [23]\nRNNs are powerful and work very well for solving a variety of NLP tasks, such as text\nclassification, named entity recognition, machine translation, etc. One can also use\nRNNs to generate text where the goal is to read the preceding text and predict the\nnext word or the next character. Refer to “The Unreasonable Effectiveness of Recur‐\nrent Neural Networks” [24] for a detailed discussion on the versatility of RNNs and\nthe range of applications within and outside NLP for which they are useful.\nLong short-term memory\nDespite their capability and versatility, RNNs suffer from the problem of forgetful\nmemory—they cannot remember longer contexts and therefore do not perform well\nwhen the input text is long, which is typically the case with text inputs. Long short-\nterm memory networks (LSTMs), a type of RNN, were invented to mitigate this\nshortcoming of the RNNs. LSTMs circumvent this problem by letting go of the irrele‐\nvant context and only remembering the part of the context that is needed to solve the\ntask at hand. This relieves the load of remembering very long context in one vector\nrepresentation. LSTMs have replaced RNNs in most applications because of this\nworkaround. Gated recurrent units (GRUs) are another variant of RNNs that are used\nmostly in language generation. (The article written by Christopher Olah [23] covers\nApproaches to NLP \n| \n23\n",
        "word_count": 384,
        "char_count": 2332,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1132,
            "height": 369,
            "ext": "png",
            "size_bytes": 40794
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 54,
        "text": "the family of RNN models in great detail.) Figure 1-14 illustrates the architecture of a\nsingle LSTM cell. We’ll discuss specific uses of LSTMs in various NLP applications in\nChapters 4, 5, 6, and 9.\nFigure 1-14. Architecture of an LSTM cell [23]\nConvolutional neural networks\nConvolutional neural networks (CNNs) are very popular and used heavily in com‐\nputer vision tasks like image classification, video recognition, etc. CNNs have also\nseen success in NLP, especially in text-classification tasks. One can replace each word\nin a sentence with its corresponding word vector, and all vectors are of the same size\n(d) (refer to “Word Embeddings” in Chapter 3). Thus, they can be stacked one over\nanother to form a matrix or 2D array of dimension n ✕ d, where n is the number of\nwords in the sentence and d is the size of the word vectors. This matrix can now be\ntreated similar to an image and can be modeled by a CNN. The main advantage\nCNNs have is their ability to look at a group of words together using a context win‐\ndow. For example, we are doing sentiment classification, and we get a sentence like, “I\nlike this movie very much!” In order to make sense of this sentence, it is better to look\nat words and different sets of contiguous words. CNNs can do exactly this by defini‐\ntion of their architecture. We’ll touch on this in more detail in later chapters.\nFigure 1-15 shows a CNN in action on a piece of text to extract useful phrases to ulti‐\nmately arrive at a binary number indicating the sentiment of the sentence from a\ngiven piece of text.\nAs shown in the figure, CNN uses a collection of convolution and pooling layers to\nachieve this condensed representation of the text, which is then fed as input to a fully\nconnected layer to learn some NLP tasks like text classification. More details on the\nusage CNNs for NLP can be found in [25] and [26]. We also cover them in Chapter 4.\n24 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 351,
        "char_count": 1933,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "Symbola (10.5pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1388,
            "height": 548,
            "ext": "png",
            "size_bytes": 60504
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 55,
        "text": "Figure 1-15. CNN model in action [27]\nTransformers\nTransformers [28] are the latest entry in the league of deep learning models for NLP.\nTransformer models have achieved state of the art in almost all major NLP tasks in\nthe past two years. They model the textual context but not in a sequential manner.\nGiven a word in the input, it prefers to look at all the words around it (known as self-\nattention) and represent each word with respect to its context. For example, the word\n“bank” can have different meanings depending on the context in which it appears. If\nthe context talks about finance, then “bank” probably denotes a financial institution.\nOn the other hand, if the context mentions a river, then it probably indicates a bank\nof the river. Transformers can model such context and hence have been used heavily\nApproaches to NLP \n| \n25\n",
        "word_count": 147,
        "char_count": 843,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1410,
            "height": 1460,
            "ext": "png",
            "size_bytes": 111943
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 56,
        "text": "in NLP tasks due to this higher representation capacity as compared to other deep\nnetworks.\nRecently, large transformers have been used for transfer learning with smaller down‐\nstream tasks. Transfer learning is a technique in AI where the knowledge gained\nwhile solving one problem is applied to a different but related problem. With trans‐\nformers, the idea is to train a very large transformer mode in an unsupervised man‐\nner (known as pre-training) to predict a part of a sentence given the rest of the\ncontent so that it can encode the high-level nuances of the language in it. These mod‐\nels are trained on more than 40 GB of textual data, scraped from the whole internet.\nAn example of a large transformer is BERT (Bidirectional Encoder Representations\nfrom Transformers) [29], shown in Figure 1-16, which is pre-trained on massive data\nand open sourced by Google.\nFigure 1-16. BERT architecture: pre-trained model and fine-tuned, task-specific models\nThe pre-trained model is shown on the left side of Figure 1-16. This model is then\nfine-tuned on downstream NLP tasks, such as text classification, entity extraction,\nquestion answering, etc., as shown on the right of Figure 1-16. Due to the sheer\namount of pre-trained knowledge, BERT works efficiently in transferring the knowl‐\nedge for downstream tasks and achieves state of the art for many of these tasks.\nThroughout the book, we have covered various examples of using BERT for various\ntasks. Figure 1-17 illustrates the workings of a self-attention mechanism, which is a\nkey component of a transformer. Interested readers can look at [30] for more details\non self-attention mechanisms and transformer architecture. We cover BERT and its\napplications in Chapters 4, 6, and 10.\n26 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 289,
        "char_count": 1775,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1428,
            "height": 597,
            "ext": "png",
            "size_bytes": 100494
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 57,
        "text": "Figure 1-17. Self-attention mechanism in a transformer [30]\nAutoencoders\nAn autoencoder is a different kind of network that is used mainly for learning com‐\npressed vector representation of the input. For example, if we want to represent a text\nby a vector, what is a good way to do it? We can learn a mapping function from input\ntext to the vector. To make this mapping function useful, we “reconstruct” the input\nback from the vector representation. This is a form of unsupervised learning since\nyou don’t need human-annotated labels for it. After the training, we collect the vector\nrepresentation, which serves as an encoding of the input text as a dense vector.\nAutoencoders are typically used to create feature representations needed for any\ndownstream tasks. Figure 1-18 depicts the architecture of an autoencoder.\nApproaches to NLP \n| \n27\n",
        "word_count": 139,
        "char_count": 847,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1166,
            "height": 1156,
            "ext": "png",
            "size_bytes": 82294
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 58,
        "text": "Figure 1-18. Architecture of an autoencoder\nIn this scheme, the hidden layer gives a compressed representation of input data, cap‐\nturing the essence, and the output layer (decoder) reconstructs the input representa‐\ntion from the compressed representation. While the architecture of the autoencoder\nshown in Figure 1-18 cannot handle specific properties of sequential data like text,\nvariations of autoencoders, such as LSTM autoencoders, address these well. More\ninformation about autoencoders can be found in [31].\nWe briefly introduced some of the popular DL architectures for NLP here. For a more\ndetailed study of deep learning architectures in general, refer to [31], and specifically\nfor NLP, refer to [25]. We hope this introduction gives you enough background to\nunderstand the use of DL in the rest of this book.\nGoing by all the recent achievements of DL models, one might think that DL should\nbe the go-to way to build NLP systems. However, that is far from the truth for most\nindustry use cases. Let’s look at why this is the case.\nWhy Deep Learning Is Not Yet the Silver Bullet for NLP\nOver the last few years, DL has made amazing advances in NLP. For example, in text\nclassification, LSTM- and CNN-based models have surpassed the performance of\nstandard machine learning techniques such as Naive Bayes and SVM for many classi‐\nfication tasks. Similarly, LSTMs have performed better in sequence-labeling tasks like\nentity extraction as compared to CRF models. Recently, powerful transformer models\nhave become state of the art in most of these NLP tasks, ranging from classification to\nsequence labeling. A huge trend right now is to leverage large (in terms of number of\nparameters) transformer models, train them on huge datasets for generic NLP tasks\nlike language models, then adapt them to smaller downstream tasks. This approach\n28 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 306,
        "char_count": 1882,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1356,
            "height": 671,
            "ext": "png",
            "size_bytes": 75616
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 59,
        "text": "(known as transfer learning) has also been successful in other domains, such as com‐\nputer vision and speech.\nDespite such tremendous success, DL is still not the silver bullet for all NLP tasks\nwhen it comes to industrial applications. Some of the key reasons for this are as\nfollows:\nOverfitting on small datasets\nDL models tend to have more parameters than traditional ML models, which\nmeans they possess more expressivity. This also comes with a curse. Occam’s\nrazor [32] suggests that a simpler solution is always preferable given that all other\nconditions are equal. Many times, in the development phase, sufficient training\ndata is not available to train a complex network. In such cases, a simpler model\nshould be preferred over a DL model. DL models overfit on small datasets and\nsubsequently lead to poor generalization capability, which in turn leads to poor\nperformance in production.\nFew-shot learning and synthetic data generation\nIn disciplines like computer vision, DL has made significant strides in few-shot\nlearning (i.e., learning from very few training examples) [33] and in models that\ncan generate superior-quality images [34]. Both of these advances have made\ntraining DL-based vision models on small amounts of data feasible. Therefore,\nDL has achieved much wider adoption for solving problems in industrial set‐\ntings. We have not yet seen similar DL techniques be successfully developed for\nNLP.\nDomain adaptation\nIf we utilize a large DL model that is trained on datasets originating from some\ncommon domains (e.g., news articles) and apply the trained model to a newer\ndomain that is different from the common domains (e.g., social media posts), it\nmay yield poor performance. This loss in generalization performance indicates\nthat DL models are not always useful. For example, models trained on internet\ntexts and product reviews will not work well when applied to domains such as\nlaw, social media, or healthcare, where both the syntactic and semantic structure\nof the language is specific to the domain. We need specialized models to encode\nthe domain knowledge, which could be as simple as domain-specific, rule-based\nmodels.\nInterpretable models\nApart from efficient domain adaptation, controllability and interpretability is\nhard for DL models because, most of the time, they work like a black box. Busi‐\nnesses often demand more interpretable results that can be explained to the cus‐\ntomer or end user. In those cases, traditional techniques might be more useful.\nFor example, a Naive Bayes model for sentiment classification may explain the\neffect of strong positive and negative words on the final prediction of sentiment.\nApproaches to NLP \n| \n29\n",
        "word_count": 426,
        "char_count": 2687,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 60,
        "text": "As of today, obtaining such insights from an LSTM-based classification model is\ndifficult. This is in contrast to computer vision, where DL models are not black\nboxes. There are plenty of techniques [35] in computer vision that are used to\ngain insight into why a model is making a particular prediction. Such approaches\nfor NLP are not as common.\nCommon sense and world knowledge\nEven though we have achieved good performance on benchmark NLP tasks\nusing ML and DL models, language remains a bigger enigma to scientists.\nBeyond syntax and semantics, language encompasses knowledge of the world\naround us. Language for communication relies on logical reasoning and com‐\nmon sense regarding events from the world. For example, “I like pizza” implies “I\nfeel happy when I eat pizza.” A more complex reasoning example would be, “If\nJohn walks out of the bedroom and goes to the garden, then John is not in the\nbedroom anymore, and his current location is the garden.” This might seem triv‐\nial to us humans, but it requires multistep reasoning for a machine to identify\nevents and understand their consequences. Since this world knowledge and com‐\nmon sense are inherent in language, understanding them is crucial for any DL\nmodel to perform well on various language tasks. Current DL models may per‐\nform well on standard benchmarks but are still not capable of common sense\nunderstanding and logical reasoning. There are some efforts to collect common\nsense events and logical rules (such as if-them reasoning), but they are not well\nintegrated yet with ML or DL models.\nCost\nBuilding DL-based solutions for NLP tasks can be pretty expensive. The cost, in\nterms of both money and time, stems from multiple sources. DL models are\nknown to be data guzzlers. Collecting a large dataset and getting it labeled can be\nvery expensive. Owing to the size of DL models, training them to achieve desired\nperformance can not only increase your development cycles but also result in a\nheavy bill for the specialized hardware (GPUs). Further, deploying and maintain‐\ning DL models can be expensive in terms of both hardware requirements and\neffort. Last but not least, because they’re bulky, these models may cause latency\nissues during inference time and may not be useful in cases where low latency is a\nmust. To this list, one can also add technical debt arising from building and\nmaintaining a heavy model. Loosely speaking, technical debt is the cost of rework\nthat arises from prioritizing speedy delivery over good design and implementa‐\ntion choices.\nOn-device deployment\nFor many use cases, the NLP solution needs to be deployed on an embedded\ndevice rather than in the cloud—for example, a machine-translation system that\nhelps tourists speak the translated text even without the internet. In such cases,\nowing to limitations of the device, the solution must work with limited memory\n30 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 484,
        "char_count": 2912,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 61,
        "text": "and power. Most DL solutions do not fit such constraints. There are some efforts\nin this direction [36, 37, 38] where one can deploy DL models on edge devices,\nbut we’re still quite far from generic solutions.\nIn most industry projects, one or more of the points mentioned above plays out. This\nleads to longer project cycles and higher costs (hardware, manpower), and yet the\nperformance is either comparable or sometimes even lower than ML models. This\nresults in a poor return on investment and often causes the NLP project to fail.\nBased on this discussion, it may be apparent that DL is not always the go-to solution\nfor all industrial NLP applications. So, this book starts with fundamental aspects of\nvarious NLP tasks and how we can solve them using techniques ranging from rule-\nbased systems to DL models. We emphasize the data requirements and model-\nbuilding pipeline, not just the technical details of individual models. Given the rapid\nadvances in this area, we anticipate that newer DL models will come in the future to\nadvance the state of the art but that the fundamentals of NLP tasks will not change\nsubstantially. This is why we’ll discuss the basics of NLP and build on them to develop\nmodels of increasing complexity wherever possible, rather than directly jumping to\nthe cutting edge.\nEchoing Professor Zachary Lipton from Carnegie Mellon University and Professor\nJacob Steinhardt from UC Berkeley [39], we also want to provide a word of caution\nabout consuming a lot of scientific articles, research papers, and blogs on ML and\nNLP without context and proper training. Following a large volume of cutting-edge\nwork may cause confusion and not-so-precise understanding. Many recent DL mod‐\nels are not interpretable enough to indicate the sources of empirical gains. Lipton and\nSteinhardt also recognize the possible conflation of technical terms and misuse of\nlanguage in ML-related scientific articles, which often fail to provide any clear path to\nsolving the problem at hand. Therefore, in this book, we carefully describe various\ntechnical concepts in the application of ML in NLP tasks via examples, code, and tips\nthroughout the chapters.\nSo far, we’ve covered some foundational concepts related to language, NLP, ML, and\nDL. Before we wrap up Chapter 1, let’s look at a case study to help get a better under‐\nstanding of the various components of an NLP application.\nAn NLP Walkthrough: Conversational Agents\nVoice-based conversational agents like Amazon Alexa and Apple Siri are some of the\nmost ubiquitous applications of NLP, and they’re the ones most people are already\nfamiliar with. Figure 1-19 shows the typical interaction model of a conversational\nagent.\nAn NLP Walkthrough: Conversational Agents \n| \n31\n",
        "word_count": 447,
        "char_count": 2744,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 62,
        "text": "Figure 1-19. Flow of conversation agents\nHere, we’ll walk through all the major NLP components used in this flow:\n1. Speech recognition and synthesis: These are the main components of any voice-\nbased conversational agent. Speech recognition involves converting speech sig‐\nnals to their phonemes, which are then transcribed as words. Speech synthesis\nachieves the reverse process by transforming textual results into spoken language\nto the user. Both of these techniques have advanced considerably in the last dec‐\nade, and we recommend using cloud APIs for most standard cases.\n2. Natural language understanding: This is the next component in the conversational\nagent pipeline, where the user response received (transcribed as text) is analyzed\nusing a natural language understanding system. This can be broken into many\nsmall NLP subtasks, such as:\n• Sentiment analysis: Here, we analyze the sentiment of the user response. This\nwill be covered in Chapter 4.\n• Named entity recognition: Here, we identify all the important entities the user\nmentioned in their response. This will be covered in Chapter 5.\n• Coreference resolution: Here, we find out the references of the extracted entities\nfrom the conversation history. For example, a user may say “Avengers Endgame\nwas awesome” and later refer back to the movie, saying “The movie’s special\neffects were great.” In this case, we would want to link that “movie” is referring\nto Avengers Endgame. This is covered briefly in Chapter 5.\n3. Dialog management: Once we’ve extracted the useful information from the user’s\nresponse, we may want to understand the user’s intent—i.e., if they’re asking a\nfactual question like “What is the weather today?” or giving a command like\n“Play Mozart songs.” We can use a text-classification system to classify the user\nresponse as one of the pre-defined intents. This helps the conversational agent\nknow what’s being asked. Intent classification will be covered in Chapters 4 and 6.\nDuring this process, the system may ask a few clarifying questions to elicit fur‐\n32 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 334,
        "char_count": 2086,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1349,
            "height": 519,
            "ext": "png",
            "size_bytes": 47838
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 63,
        "text": "ther information from the user. Once we’ve figured out the user’s intent, we want\nto figure out which suitable action the conversational agent should take to fulfill\nthe user’s request. This is done based on the information and intent extracted\nfrom the user’s response. Examples of suitable actions could be generating an\nanswer from the internet, playing music, dimming lights, or asking a clarifying\nquestion. We’ll cover this in Chapter 6.\n4. Response generation: Finally, the conversational agent generates a suitable action\nto perform based on a semantic interpretation of the user’s intent and additional\ninputs from the dialogue with the user. As mentioned earlier, the agent can\nretrieve information from the knowledge base and generate responses using a\npre-defined template. For example, it might respond by saying, “Now playing\nSymphony No. 25” or “The lights have been dimmed.” In certain scenarios, it can\nalso generate a completely new response.\nWe hope this brief case study provided an overview of how different NLP compo‐\nnents we’ll be discussing throughout this book will come together to build one appli‐\ncation: a conversational agent. We’ll see more details about these components as we\nprogress through the book, and we’ll discuss conversational agents specifically in\nChapter 6.\nWrapping Up\nFrom the broader contours of what a language is to a concrete case study of a real-\nworld NLP application, we’ve covered a range of NLP topics in this chapter. We also\ndiscussed how NLP is applied in the real world, some of its challenges and different\ntasks, and the role of ML and DL in NLP. This chapter was meant to give you a base‐\nline of knowledge that we’ll build on throughout the book. The next two chapters\n(Chapters 2 and 3) will introduce you to some of the foundational steps necessary for\nbuilding NLP applications. Chapters 4–7 focus on core NLP tasks along with indus‐\ntrial use cases that can be solved with them. In Chapters 8–10, we discuss how NLP is\nused across different industry verticals such as e-commerce, healthcare, finance, etc.\nChapter 11 brings everything together and discusses what it takes to build end-to-end\nNLP applications in terms of design, development, testing, and deployment. With this\nbroad overview in place, let’s start delving deeper into the world of NLP.\nReferences\n[1] Arria.com. “NLG for Your Industry”. Last accessed June 15, 2020.\n[2] UCL. Phonetic symbols for English. Last accessed June 15, 2020.\nWrapping Up \n| \n33\n",
        "word_count": 407,
        "char_count": 2488,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 64,
        "text": "[3] Bender, Emily M. “Linguistic Fundamentals for Natural Language Processing: 100\nEssentials From Morphology and Syntax.” Synthesis Lectures on Human Language\nTechnologies 6.3 (2013): 1–184.\n[4] Bender, Emily M. and Alex Lascarides. “Linguistic Fundamentals for Natural Lan‐\nguage Processing II: 100 Essentials from Semantics and Pragmatics.” Synthesis Lec‐\ntures on Human Language Technologies 12.3 (2019): 1–268.\n[5] Levesque, Hector, Ernest Davis, and Leora Morgenstern. “The Winograd Schema\nChallenge.” The Thirteenth International Conference on the Principles of Knowledge\nRepresentation and Reasoning (2012).\n[6] Wikipedia. “Dartmouth workshop”. Last modified March 30, 2020.\n[7] Miller, George A. “WordNet: A Lexical Database for English.” Communications of\nthe ACM 38.11 (1995): 39–41.\n[8] Visual Thesaurus of English Collocations. “Visual Wordnet with D3.js”. Last\naccessed June 15, 2020.\n[9] Singh, Push, Thomas Lin, Erik T. Mueller, Grace Lim, Travell Perkins, and Wan Li\nZhu. “Open Mind Common Sense: Knowledge Acquisition from the General Public,”\nMeersman R. and Tari Z. (eds), On the Move to Meaningful Internet Systems 2002:\nCoopIS, DOA, and ODBASE. OTM 2002. Lecture Notes in Computer Science, vol.\n2519. Berlin, Heidelberg: Springer.\n[10] The Stanford Natural Language Processing Group. Stanford TokensRegex, (soft‐\nware). Last accessed June 15, 2020.\n[11] Hewitt, Luke. Probabilistic regular expressions, (GitHub repo).\n[12] Earley, Jay. “An Efficient Context-Free Parsing Algorithm.” Communications of\nthe ACM 13.2 (1970): 94–102.\n[13] “Java Annotation Patterns Engine: Regular Expressions over Annotations”.\nDeveloping Language Processing Components with GATE Version 9 (a User Guide),\nChapter 8. Last accessed June 15, 2020.\n[14] General Architecture for Text Engineering (GATE). Last accessed June 15, 2020.\n[15] Rosier, Arnaud, Anita Burgun, and Philippe Mabo. “Using Regular Expressions\nto Extract Information on Pacemaker Implantation Procedures from Clinical\nReports.” AMIA Annual Symposium Proceedings v.2008 (2008): 81–85.\n[16] Zhang, Haiyi and Di Li. “Naïve Bayes Text Classifier.” 2007 IEEE International\nConference on Granular Computing (GRC 2007): 708.\n[17] Joachims, Thorsten. Learning to Classify Text Using Support Vector Machines, Vol.\n668. New York: Springer Science & Business Media, 2002. ISBN: 978-1-4615-0907-3\n34 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 334,
        "char_count": 2386,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 65,
        "text": "[18] Baum, Leonard E. and Ted Petrie. “Statistical Inference for Probabilistic Func‐\ntions of Finite State Markov Chains.” The Annals of Mathematical Statistics 37.6\n(1966): 1554–1563.\n[19] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft), 2018.\n[20] Settles, Burr. “Biomedical Named Entity Recognition Using Conditional Ran‐\ndom Fields and Rich Feature Sets.” Proceedings of the International Joint Workshop on\nNatural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP)\n(2004): 107–110.\n[21] Bishop, Christopher M. Pattern Recognition and Machine Learning. New York:\nSpringer, 2006. ISBN: 978-0-3873-1073-2\n[22] Géron, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and\nTensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. Boston:\nO’Reilly, 2019. ISBN: 978-1-492-03264-9\n[23] Olah, Christopher. “Understanding LSTM Networks”. August 27, 2015.\n[24] Karpathy, Andrej. “The Unreasonable Effectiveness of Recurrent Neural Net‐\nworks”. May 21, 2015.\n[25] Goldberg, Yoav. “Neural Network Methods for Natural Language Processing.”\nSynthesis Lectures on Human Language Technologies 10.1 (2017): 1–309.\n[26] Britz, Denny. “Understanding Convolutional Neural Networks for NLP”.\nNovember 7, 2015.\n[27] Le, Hoa T., Christophe Cerisara, and Alexandre Denis. “Do Convolutional Net‐\nworks need to be Deep for Text Classification?” Workshops at the Thirty-Second AAAI\nConference on Artificial Intelligence, 2018.\n[28] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.”\nAdvances in Neural Information Processing Systems, 2017: 5998–6008.\n[29] Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding”. Octo‐\nber 11, 2018.\n[30] Alammar, Jay. “The Illustrated Transformer”. June 27, 2018.\n[31] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. Cam‐\nbridge: MIT Press, 2016. ISBN: 978-0-262-03561-3\n[32] Varma, Nakul. COMS 4771: Introduction to Machine Learning, Lecture 6, Slide\n7. Last accessed June 15, 2020.\nWrapping Up \n| \n35\n",
        "word_count": 302,
        "char_count": 2225,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 66,
        "text": "[33] Wang, Yaqing, Quanming Yao, James Kwok, and Lionel M. Ni. “Generalizing\nfrom a Few Examples: A Survey on Few-Shot Learning”, (2019).\n[34] Wang, Zhengwei, Qi She, and Tomas E. Ward. “Generative Adversarial Networks\nin Computer Vision: A Survey and Taxonomy”, (2019).\n[35] Olah, Chris, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert,\nKatherine Ye, and Alexander Mordvintsev. “The Building Blocks of Interpretability.”\nDistill 3.3 (March 2018): e10.\n[36] Nan, Kaiming, Sicong Liu, Junzhao Du, and Hui Liu. “Deep Model Compression\nfor Mobile Platforms: A Survey.” Tsinghua Science and Technology 24.6 (2019): 677–\n693.\n[37] TensorFlow. “Get started with TensorFlow Lite”. Last modified March 21, 2020.\n[38] Ganesh, Prakhar, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Deming\nChen, Marianne Winslett, Hassan Sajjad, and Preslav Nakov. “Compressing Large-\nScale Transformer-Based Models: A Case Study on BERT”, (2020).\n[39] Lipton, Zachary C. and Jacob Steinhardt. “Troubling Trends in Machine Learn‐\ning Scholarship”, (2018).\n36 \n| \nChapter 1: NLP: A Primer\n",
        "word_count": 160,
        "char_count": 1078,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 67,
        "text": "CHAPTER 2\nNLP Pipeline\nThe whole is more than the sum of its parts. It is more correct to say that the whole is\nsomething else than the sum of its parts, because summing up is a meaningless procedure,\nwhereas the whole-part relationship is meaningful.\n—Kurt Koffka\nIn the previous chapter, we saw examples of some common NLP applications that we\nmight encounter in everyday life. If we were asked to build such an application, think\nabout how we would approach doing so at our organization. We would normally\nwalk through the requirements and break the problem down into several sub-\nproblems, then try to develop a step-by-step procedure to solve them. Since language\nprocessing is involved, we would also list all the forms of text processing needed at\neach step. This step-by-step processing of text is known as a pipeline. It is the series of\nsteps involved in building any NLP model. These steps are common in every NLP\nproject, so it makes sense to study them in this chapter. Understanding some com‐\nmon procedures in any NLP pipeline will enable us to get started on any NLP prob‐\nlem encountered in the workplace. Laying out and developing a text-processing\npipeline is seen as a starting point for any NLP application development process. In\nthis chapter, we will learn about the various steps involved and how they play impor‐\ntant roles in solving the NLP problem and we’ll see a few guidelines about when and\nhow to use which step. In later chapters, we’ll discuss specific pipelines for various\nNLP tasks (e.g., Chapters 4–7).\nFigure 2-1 shows the main components of a generic pipeline for modern-day, data-\ndriven NLP system development. The key stages in the pipeline are as follows:\n1. Data acquisition\n2. Text cleaning\n3. Pre-processing\n37\n",
        "word_count": 300,
        "char_count": 1758,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (9.3pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 68,
        "text": "4. Feature engineering\n5. Modeling\n6. Evaluation\n7. Deployment\n8. Monitoring and model updating\nFigure 2-1. Generic NLP pipeline\nThe first step in the process of developing any NLP system is to collect data relevant\nto the given task. Even if we’re building a rule-based system, we still need some data\nto design and test our rules. The data we get is seldom clean, and this is where text\ncleaning comes into play. After cleaning, text data often has a lot of variations and\nneeds to be converted into a canonical form. This is done in the pre-processing step.\nThis is followed by feature engineering, where we carve out indicators that are most\nsuitable for the task at hand. These indicators are converted into a format that is\nunderstandable by modeling algorithms. Then comes the modeling and evaluation\nphase, where we build one or more models and compare and contrast them using a\nrelevant evaluation metric(s). Once the best model among the ones evaluated is\nchosen, we move toward deploying this model in production. Finally, we regularly\nmonitor the performance of the model and, if need be, update it to keep up its\nperformance.\nNote that, in the real world, the process may not always be linear as it’s shown in the\npipeline in Figure 2-1; it often involves going back and forth between individual steps\n(e.g., between feature extraction and modeling, modeling and evaluation, and so on).\nAlso, there are loops in between, most commonly going from evaluation to pre-\nprocessing, feature engineering, modeling, and back to evaluation. There is also an\noverall loop that goes from monitoring to data acquisition, but this loop happens at\nthe project level.\nNote that exact step-by-step procedures may depend on the specific task at hand. For\nexample, a text-classification system may require a different feature extraction step\ncompared to a text-summarization system. We will focus on application-specific\n38 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 321,
        "char_count": 1947,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1288,
            "height": 409,
            "ext": "png",
            "size_bytes": 27657
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 69,
        "text": "pipeline stages in subsequent chapters in the book. Also, depending on the phase of\nthe project, different steps can take different amounts of time. In the initial phases,\nmost of the time is used in modeling and evaluation, whereas once the system\nmatures, feature engineering can take far more time.\nFor the rest of this chapter, we’ll look at the individual stages of the pipeline in detail\nalong with examples. We’ll describe some of the most common procedures at each\nstage and discuss some use cases to illustrate them. Let’s start with the first step: data\nacquisition.\nData Acquisition\nData is the heart of any ML system. In most industrial projects, it is often the data\nthat becomes the bottleneck. In this section, we’ll discuss various strategies for gather‐\ning relevant data for an NLP project.\nLet’s say we’re asked to develop an NLP system to identify whether an incoming cus‐\ntomer query (for example, using a chat interface) is a sales inquiry or a customer care\ninquiry. Depending on the type of query, it should be automatically routed to the\nright team. How can one go about building such a system? Well, the answer depends\non the type and amount of data we have to work with.\nIn an ideal setting, we’ll have the required datasets with thousands—maybe even mil‐\nlions—of data points. In such cases, we don’t have to worry about data acquisition.\nFor example, in the scenario we just described, we have historic queries fro m previ‐\nous years, which sales and support teams responded to. Further, the teams tagged\nthese queries as sales, support, or other. So, not only do we have the data, but we also\nhave the labels. However, in many AI projects, one is not so lucky. Let’s look at what\nwe can do in a less-than-ideal scenario.\nIf we have little or no data, we can start by looking at patterns in the data that indicate\nif the incoming message is a sales or support query. We can then use regular expres‐\nsions and other heuristics to match these patterns to separate sales queries from sup‐\nport queries. We evaluate this solution by collecting a set of queries from both\ncategories and calculating what percentage of the messages were correctly identified\nby our system. Say we get OK-ish numbers. We would like to improve the system\nperformance.\nNow we can start thinking about using NLP techniques. For this, we need labeled\ndata, a collection of queries where each one is labeled with sales or support. How can\nwe get such data?\nData Acquisition \n| \n39\n",
        "word_count": 433,
        "char_count": 2481,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 70,
        "text": "Use a public dataset\nWe could see if there are any public datasets available that we can leverage. Take a\nlook at the compilation by Nicolas Iderhoff [1] or search Google’s specialized\nsearch engine for datasets [2]. If you find a suitable dataset that’s similar to the\ntask at hand, great! Build a model and evaluate. If not, then what?\nScrape data\nWe could find a source of relevant data on the internet—for example, a consumer\nor discussion forum where people have posted queries (sales or support). Scrape\nthe data from there and get it labeled by human annotators.\nFor many industrial settings, gathering data from external sources does not suf‐\nfice because the data doesn’t contain nuances like product names or product-\nspecific user behavior and thus might be very different from the data seen in\nproduction environments. This is when we’ll have to start looking for data inside\nthe organization.\nProduct intervention\nIn most industrial settings, AI models seldom exist by themselves. They’re devel‐\noped mostly to serve users via a feature or product. In all such cases, the AI team\nshould work with the product team to collect more and richer data by developing\nbetter instrumentation in the product. In the tech world, this is called product\nintervention.\nProduct intervention is often the best way to collect data for building intelligent\napplications in industrial settings. Tech giants like Google, Facebook, Microsoft,\nNetflix, etc., have known this for a long time and have tried to collect as much\ndata as possible from as many users as possible.\nData augmentation\nWhile instrumenting products is a great way to collect data, it takes time. Even if\nyou instrument the product today, it can take anywhere between three to six\nmonths to collect a decent-sized, comprehensive dataset. So, can we do some‐\nthing in the meantime?\nNLP has a bunch of techniques through which we can take a small dataset and use\nsome tricks to create more data. These tricks are also called data augmentation, and\nthey try to exploit language properties to create text that is syntactically similar to\nsource text data. They may appear as hacks, but they work very well in practice. Let’s\nlook at some of them:\nSynonym replacement\nRandomly choose “k” words in a sentence that are not stop words. Replace these\nwords with their synonyms. For synonyms, we can use Synsets in Wordnet [3, 4].\n40 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 406,
        "char_count": 2414,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 71,
        "text": "Back translation\nSay we have a sentence, S1, in English. We use a machine-translation library like\nGoogle Translate to translate it into some other language—say, German. Let the\ncorresponding sentence in German be S2. Now, we’ll use the machine-translation\nlibrary again to translate back to English. Let the output sentence be S3.\nWe’ll find that S1 and S3 are very similar in meaning but are slight variations of\neach other. Now we can add S3 to our dataset. This trick works beautifully for\ntext classification. Figure 2-2 [5] shows an example of back translation in action.\nFigure 2-2. Back translation\nTF-IDF–based word replacement\nBack translation can lose certain words that are crucial to the sentence. In [5], the\nauthors use TF-IDF, a concept we’ll introduce in Chapter 3, to handle this.\nBigram flipping\nDivide the sentence into bigrams. Take one bigram at random and flip it. For\nexample: “I am going to the supermarket.” Here, we take the bigram “going to”\nand replace it with the flipped one: “to going.”\nReplacing entities\nReplace entities like person name, location, organization, etc., with other entities\nin the same category. That is, replace person name with another person name,\ncity with another city, etc. For example, in “I live in California,” replace “Califor‐\nnia” with “London.”\nAdding noise to data\nIn many NLP applications, the incoming data contains spelling mistakes. This is\nprimarily due to characteristics of the platform where the data is being generated\n(for example, Twitter). In such cases, we can add a bit of noise to data to train\nrobust models. For example, randomly choose a word in a sentence and replace it\nwith another word that’s closer in spelling to the first word. Another source of\nData Acquisition \n| \n41\n",
        "word_count": 291,
        "char_count": 1758,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1377,
            "height": 563,
            "ext": "png",
            "size_bytes": 47774
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 72,
        "text": "noise is the “fat finger” problem [6] on mobile keyboards. Simulate a QWERTY\nkeyboard error by replacing a few characters with their neighboring characters\non the QWERTY keyboard.\nAdvanced techniques\nThere are other advanced techniques and systems that can augment text data.\nSome of the notable ones are:\nSnorkel [7, 8, 52]\nThis is a system for building training data automatically, without manual\nlabeling. Using Snorkel, a large training dataset can be “created”—without\nmanual labeling—using heuristics and creating synthetic data by transform‐\ning existing data and creating new data samples. This approach was shown to\nwork well at Google in the recent past [9].\nEasy Data Augmentation (EDA) [10, 11] and NLPAug [12]\nThese two libraries are used to create synthetic samples for NLP. They pro‐\nvide implementation of various data augmentation techniques, including\nsome techniques that we discussed previously.\nActive learning [13]\nThis is a specialized paradigm of ML where the learning algorithm can inter‐\nactively query a data point and get its label. It is used in scenarios where\nthere is an abundance of unlabeled data but manually labeling is expensive.\nIn such cases, the question becomes: for which data points should we ask for\nlabels to maximize learning while keeping the labeling cost low?\nIn order for most of the techniques we discussed in this section to work well, one key\nrequirement is a clean dataset to start with, even if it’s not very big. In our experience,\ndata augmentation techniques can work really well. Further, in day-to-day ML prac‐\ntice, datasets come from heterogeneous sources. A combination of public datasets,\nlabeled datasets, and augmented datasets are used for building early-stage production\nmodels, as we often may not have large datasets for our custom scenarios to start\nwith. Once we have the data we want for a given task, we proceed to the next step:\ntext cleaning.\nText Extraction and Cleanup\nText extraction and cleanup refers to the process of extracting raw text from the input\ndata by removing all the other non-textual information, such as markup, metadata,\netc., and converting the text to the required encoding format. Typically, this depends\non the format of available data in the organization (e.g., static data from PDF, HTML\nor text, some form of continuous data stream, etc.), as shown in Figure 2-3.\n42 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 389,
        "char_count": 2398,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 73,
        "text": "Text extraction is a standard data-wrangling step, and we don’t usually employ any\nNLP-specific techniques during this process. However, in our experience, it is an\nimportant step that has implications for all other aspects of the NLP pipeline. Further,\nit can also be the most time-consuming part of a project. While the design of text-\nextraction tools is beyond the scope of this book, we’ll look at a few examples to illus‐\ntrate different issues involved in this step in this section. We’ll also touch on some of\nthe important aspects of text extraction from various sources as well as cleanup to\nmake them consumable in downstream pipelines.\nFigure 2-3. (a) PDF invoice, [14] (b) HTML texts, and (c) text embedded in an image\n[15]\nText Extraction and Cleanup \n| \n43\n",
        "word_count": 131,
        "char_count": 772,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 1614,
            "ext": "png",
            "size_bytes": 393335
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 74,
        "text": "HTML Parsing and Cleanup\nSay we’re working on a project where we’re building a forum search engine for pro‐\ngramming questions. We’ve identified Stack Overflow as a source and decided to\nextract question and best-answer pairs from the website. How can we go through the\ntext-extraction step in this case? If we observe the HTML markup of a typical Stack\nOverflow question page, we notice that questions and answers have special tags asso‐\nciated with them. We can utilize this information while extracting text from the\nHTML page. While it may seem like writing our own HTML parser is the way to go,\nfor most cases we encounter, it’s more feasible to utilize existing libraries such as \nBeautiful Soup [16] and Scrapy [17], which provide a range of utilities to parse web\npages. The following code snippet shows how to use Beautiful Soup to address the\nproblem described here, extracting a question and its best-answer pair from a Stack\nOverflow web page:\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlopen\nmyurl = \"https://stackoverflow.com/questions/415511/ \\\n  how-to-get-the-current-time-in-python\"\nhtml = urlopen(myurl).read()\nsoupified = BeautifulSoup(html, \"html.parser\")\nquestion = soupified.find(\"div\", {\"class\": \"question\"})\nquestiontext = question.find(\"div\", {\"class\": \"post-text\"})\nprint(\"Question: \\n\", questiontext.get_text().strip())\nanswer = soupified.find(\"div\", {\"class\": \"answer\"})\nanswertext = answer.find(\"div\", {\"class\": \"post-text\"})\nprint(\"Best answer: \\n\", answertext.get_text().strip())\nHere, we’re relying on our knowledge of the structure of an HTML document to\nextract what we want from it. This code shows the output as follows:\nQuestion:\nWhat is the module/method used to get the current time?\nBest answer:\n Use:\n>>> import datetime\n>>> datetime.datetime.now()\ndatetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n>>> print(datetime.datetime.now())\n2009-01-06 15:08:24.789150\nAnd just the time:\n>>> datetime.datetime.now().time()\ndatetime.time(15, 8, 24, 78915)\n>>> print(datetime.datetime.now().time())\n15:08:24.789150\nSee the documentation for more information.\n44 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 291,
        "char_count": 2136,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 75,
        "text": "To save typing, you can import the datetime object from the datetime module:\n>>> from datetime import datetime\nThen remove the leading datetime. from all of the above.\nIn this example, we had a specific need: extracting a question and its answer. In some\nscenarios—for example, extracting postal addresses from web pages—we would get\nall the text (instead of only parts of it) from the web page first, before doing anything\nelse. Typically, all HTML libraries have some function that can strip off all HTML\ntags and return only the content between the tags. But this often results in noisy out‐\nput, and you may end up seeing a lot of JavaScript in the extracted content as well. In\nsuch cases, we should look to extract content between only those tags that typically\ncontain text in web pages.\nUnicode Normalization\nAs we develop code for cleaning up HTML tags, we may also encounter various Uni‐\ncode characters, including symbols, emojis, and other graphic characters. A handful\nof Unicode characters are shown in Figure 2-4.\nFigure 2-4. Unicode characters [18]\nTo parse such non-textual symbols and special characters, we use Unicode normaliza‐\ntion. This means that the text we see should be converted into some form of binary\nrepresentation to store in a computer. This process is known as text encoding. Ignor‐\ning encoding issues can result in processing errors further in the pipeline.\nThere are several encoding schemes, and the default encoding can be different for dif‐\nferent operating systems. Sometimes (more commonly than you think), especially\nwhen dealing with text in multiple languages, social media data, etc., we may have to\nconvert between these encoding schemes during the text-extraction process. Refer to\n[19] for an introduction to how language is represented on computers and what dif‐\nference an encoding scheme makes. Here is an example of Unicode handling:\nText Extraction and Cleanup \n| \n45\n",
        "word_count": 315,
        "char_count": 1923,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1442,
            "height": 591,
            "ext": "png",
            "size_bytes": 141455
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 76,
        "text": "text = ’I love \n!  Shall we book a \n to gizza?’\nText = text.encode(\"utf-8\")\nprint(Text)\nwhich outputs:\nb'I love Pizza \\xf0\\x9f\\x8d\\x95!  Shall we book a cab \\xf0\\x9f\\x9a\\x95 \n  to get pizza?'\nThis processed text is machine readable and can be used in downstream pipelines.\nWe address issues regarding handling Unicode characters with this same example in\nmore detail in Chapter 8.\nSpelling Correction\nIn the world of fast typing and fat-finger typing [6], incoming text data often has\nspelling errors. This can be prevalent in search engines, text-based chatbots deployed\non mobile devices, social media, and many other sources. While we remove HTML\ntags and handle Unicode characters, this remains a unique problem that may hurt the\nlinguistic understanding of the data, and shorthand text messages in social micro‐\nblogs often hinder language processing and context understanding. Two such exam‐\nples follow:\nShorthand typing: Hllo world! I am back!\nFat finger problem [20]: I pronise that I will not bresk the silence again!\nWhile shorthand typing is prevalent in chat interfaces, fat-finger problems are com‐\nmon in search engines and are mostly unintentional. Despite our understanding of\nthe problem, we don’t have a robust method to fix this, but we still can make good\nattempts to mitigate the issue. Microsoft released a REST API [21] that can be used in\nPython for potential spell checking:\nimport requests\nimport json\napi_key = \"<ENTER-KEY-HERE>\"\nexample_text = \"Hollo, wrld\" # the text to be spell-checked\ndata = {'text': example_text}\nparams = {\n    'mkt':'en-us',\n    'mode':'proof'\n    }\nheaders = {\n    'Content-Type': 'application/x-www-form-urlencoded',\n    'Ocp-Apim-Subscription-Key': api_key,\n    }\nresponse = requests.post(endpoint, headers=headers, params=params, data=data)\njson_response = response.json()\nprint(json.dumps(json_response, indent=4))\n46 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 276,
        "char_count": 1904,
        "fonts": [
          "UbuntuMono-Bold (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 129,
            "height": 154,
            "ext": "png",
            "size_bytes": 2322
          },
          {
            "index": 1,
            "width": 154,
            "height": 100,
            "ext": "png",
            "size_bytes": 1660
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 77,
        "text": "Output (partially shown here):\n\"suggestions\": [\n            {\n               \"suggestion\": \"Hello\",\n               \"score\": 0.9115257530801\n            },\n            {\n               \"suggestion\": \"Hollow\",\n               \"score\": 0.858039839213461\n            },\n            {\n               \"suggestion\": \"Hallo\",\n               \"score\": 0.597385084464481\n            }\nYou can see the full tutorial in [21].\nGoing beyond APIs, we can build our own spell checker using a huge dictionary of\nwords from a specific language. A naive solution would be to look for all words that\ncan be composed with minimal alteration (addition, deletion, substitution) to its con‐\nstituent letters. For example, if ‘“Hello” is a valid word that is already present in the\ndictionary, then the addition of “o” (minimal) to “Hllo” would make the correction.\nSystem-Specific Error Correction\nHTML or raw text scraped from the web are just a couple of sources for textual data.\nConsider another scenario where our dataset is in the form of a collection of PDF\ndocuments. The pipeline in this case starts with extraction of plain text from PDF\ndocuments. However, different PDF documents are encoded differently, and some‐\ntimes, we may not be able to extract the full text, or the structure of the text may get\nmessed up. If we need full text or our text has to be grammatical or in full sentences\n(e.g., when we want to extract relations between various people in the news based on\nnewspaper text), this can impact our application. While there are several libraries,\nsuch as PyPDF [22], PDFMiner [23], etc., to extract text from PDF documents, they\nare far from perfect, and it’s not uncommon to encounter PDF documents that can’t\nbe processed by such libraries. We leave their exploration as an exercise for the\nreader. [24] discusses some of the issues involved in PDF-to-text extraction in detail.\nAnother common source of textual data is scanned documents. Text extraction from\nscanned documents is typically done through optical character recognition (OCR),\nusing libraries such as Tesseract [25, 26]. Consider the example image—a snippet\nfrom a 1950 article in a journal [27]—shown in Figure 2-5.\nText Extraction and Cleanup \n| \n47\n",
        "word_count": 333,
        "char_count": 2218,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 78,
        "text": "Figure 2-5. An example of scanned text\nThe code snippet below shows how the Python library pytesseract can be used to\nextract text from this image:\nfrom PIL import Image\nfrom pytesseract import image_to_string\nfilename = \"somefile.png\"\ntext = image_to_string(Image.open(filename))\nprint(text)\nThis code will print the output as follows, where “\\n” indicates a newline character:\n’in the nineteenth century the only Kind of linguistics considered\\nseriously\nwas this comparative and historical study of words in languages\\nknown or\nbelieved to Fe cognate—say the Semitic languages, or the Indo-\\nEuropean\nlanguages. It is significant that the Germans who really made\\nthe subject what\nit was, used the term Indo-germanisch. Those who know\\nthe popular works of \nOtto Jespersen will remember how fitmly he\\ndeclares that linguistic \nscience is historical. And those who have noticed’\nWe notice that there are two errors in the output of the OCR system in this case.\nDepending on the quality of the original scan, OCR output can potentially have\nlarger amounts of errors. How do we clean up this text before feeding it into the next\nstage of the pipeline? One approach is to run the text through a spell checker such as\npyenchant [28], which will identify misspellings and suggest some alternatives. More\nrecent approaches use neural network architectures to train word/character-based\nlanguage models, which are in turn used for correcting OCR text output based on the\ncontext [29].\nRecall that we saw an example of a voice-based assistant in Chapter 1. In such cases,\nthe source of text extraction is the output of an automatic speech recognition (ASR)\nsystem. Like OCR, it’s common to see some errors in ASR, owing to various factors,\nsuch as dialectical variations, slang, non-native English, new or domain-specific\nvocabulary, etc. The above-mentioned approach of spell checkers or neural language\nmodels can be followed here as well to clean up the extracted text.\nWhat we’ve seen so far are just some examples of potential issues that may come up\nduring the text-extraction and cleaning process. Though NLP plays a very small role\nin this process, we hope these examples illustrate how text extraction and cleanup\n48 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 355,
        "char_count": 2249,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 732,
            "height": 164,
            "ext": "png",
            "size_bytes": 54338
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 79,
        "text": "could pose challenges in a typical NLP pipeline. We’ll also touch on these aspects in\nupcoming chapters for different NLP applications, where relevant. Let’s move on to\nthe next step in our pipeline: pre-processing.\nPre-Processing\nLet’s start with a simple question: we already did some cleanup in the previous step;\nwhy do we still have to pre-process text? Consider a scenario where we’re processing\ntext from Wikipedia pages about individuals to extract biographical information\nabout them. Our data acquisition starts with crawling such pages. However, our\ncrawled data is all in HTML, with a lot of boilerplate from Wikipedia (e.g., all the\nlinks in the left panel), possibly the presence of links to multiple languages (in their\nscript), etc. All such information is irrelevant for extracting features from text (in\nmost cases). Our text-extraction step removed all this and gave us the plain text of the\narticle we need. However, all NLP software typically works at the sentence level and\nexpects a separation of words at the minimum. So, we need some way to split a text\ninto words and sentences before proceeding further in a processing pipeline. Some‐\ntimes, we need to remove special characters and digits, and sometimes, we don’t care\nwhether a word is in upper or lowercase and want everything in lowercase. Many\nmore decisions like this are made while processing text. Such decisions are addressed\nduring the pre-processing step of the NLP pipeline. Here are some common pre-\nprocessing steps used in NLP software:\nPreliminaries\nSentence segmentation and word tokenization.\nFrequent steps\nStop word removal, stemming and lemmatization, removing digits/punctuation,\nlowercasing, etc.\nOther steps\nNormalization, language detection, code mixing, transliteration, etc.\nAdvanced processing\nPOS tagging, parsing, coreference resolution, etc.\nWhile not all steps will be followed in all the NLP pipelines we encounter, the first\ntwo are more or less seen everywhere. Let’s take a look at what each of these steps\nmean.\nPre-Processing \n| \n49\n",
        "word_count": 321,
        "char_count": 2048,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 80,
        "text": "Preliminaries\nAs mentioned earlier, NLP software typically analyzes text by breaking it up into\nwords (tokens) and sentences. Hence, any NLP pipeline has to start with a reliable\nsystem to split the text into sentences (sentence segmentation) and further split a sen‐\ntence into words (word tokenization). On the surface, these seem like simple tasks,\nand you may wonder why they need special treatment. We will see why in the coming\ntwo subsections.\nSentence segmentation\nAs a simple rule, we can do sentence segmentation by breaking up text into sentences\nat the appearance of full stops and question marks. However, there may be abbrevia‐\ntions, forms of addresses (Dr., Mr., etc.), or ellipses (...) that may break the simple\nrule.\nThankfully, we don’t have to worry about how to solve these issues, as most NLP\nlibraries come with some form of sentence and word splitting implemented. A com‐\nmonly used library is Natural Language Tool Kit (NLTK) [30]. The code example\nbelow shows how to use a sentence and word splitter from NLTK and uses the first\nparagraph of this chapter as input:\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nmytext = \"In the previous chapter, we saw examples of some common NLP \napplications that we might encounter in everyday life. If we were asked to \nbuild such an application, think about how we would approach doing so at our \norganization. We would normally walk through the requirements and break the \nproblem down into several sub-problems, then try to develop a step-by-step \nprocedure to solve them. Since language processing is involved, we would also\nlist all the forms of text processing needed at each step. This step-by-step \nprocessing of text is known as pipeline. It is the series of steps involved in\nbuilding any NLP model. These steps are common in every NLP project, so it \nmakes sense to study them in this chapter. Understanding some common procedures\nin any NLP pipeline will enable us to get started on any NLP problem encountered \nin the workplace. Laying out and developing a text-processing pipeline is seen \nas a starting point for any NLP application development process. In this\nchapter, we will learn about the various steps involved and how they play  \nimportant roles in solving the NLP problem and we’ll see a few guidelines\nabout when and how to use which step. In later chapters, we’ll discuss  \nspecific pipelines for various NLP tasks (e.g., Chapters 4–7).\"\nmy_sentences = sent_tokenize(mytext)\n50 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 412,
        "char_count": 2506,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 81,
        "text": "Word tokenization\nSimilar to sentence tokenization, to tokenize a sentence into words, we can start with\na simple rule to split text into words based on the presence of punctuation marks. The \nNLTK library allows us to do that. If we take the previous example:\nfor sentence in my_sentences:\n   print(sentence)\n   print(word_tokenize(sentence))\nFor the first sentence, the output is printed as follows:\nIn the previous chapter, we saw a quick overview of what is NLP, what are some\nof the common applications and challenges in NLP, and an introduction to \ndifferent tasks in NLP.\n['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'a', 'quick', \n'overview', 'of', 'what', 'is', 'NLP', ',', 'what', 'are', 'some', 'of', 'the', \n'common', 'applications', 'and', 'challenges', 'in', 'NLP', ',', 'and', 'an', \n'introduction', 'to', 'different', 'tasks', 'in', 'NLP', '.']\nWhile readily available solutions work for most of our needs and most NLP libraries\nwill have a tokenizer and sentence splitter bundled with them, it’s important to\nremember that they’re far from perfect. For example, consider this sentence: “Mr. Jack\nO’Neil works at Melitas Marg, located at 245 Yonge Avenue, Austin, 70272.” If we run\nthis through the NLTK tokenizer, O, ‘, and Neil are identified as three separate tokens.\nSimilarly, if we run the sentence: “There are $10,000 and €1000 which are there just\nfor testing a tokenizer” through this tokenizer, while $ and 10,000 are identified as\nseparate tokens, €1000 is identified as a single token. In another scenario, if we want\nto tokenize tweets, this tokenizer will separate a hashtag into two tokens: a “#” sign\nand the string that follows it. In such cases, we may need to use a custom tokenizer\nbuilt for our purpose. To complete our example, we’ll perform word tokenization\nafter we perform sentence tokenization.\nA point to note in this context is that NLTK also has a tweet tokenizer; we’ll see how\nit’s useful in Chapters 4 and 8. To summarize, although word- and sentence-\ntokenization approaches appear to be elementary and easy to implement, they may\nnot always meet our specific tokenization needs, as we saw in the above examples.\nNote that we refer to NLTK’s example, but these observations hold true for any other\nlibrary as well. We leave that exploration as an exercise for the reader.\nAs tokenization may differ from one domain to the other, tokenization is also heavily\ndependent on language. Each language can have various linguistic rules and excep‐\ntions. Figure 2-6 shows an example where “N.Y.!” has a total of three punctuations.\nBut in English, N.Y. stands for New York, hence “N.Y.” should be treated as a single\nword and not be tokenized further. Such language-specific exceptions can be speci‐\nfied in the tokenizer provided by spaCy [31]. It’s also possible in spaCy to develop\ncustom rules to handle such exceptions for languages that have high inflections (pre‐\nfixes or suffixes) and complex morphology.\nPre-Processing \n| \n51\n",
        "word_count": 482,
        "char_count": 2988,
        "fonts": [
          "UbuntuMono-Regular (8.5pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 82,
        "text": "Another important fact to keep in mind is that any sentence segmenter and tokenizer\nwill be sensitive to the input they receive. Let’s say we’re writing software to extract\nsome information, such as company, position, and salary, from job offer letters. They\nfollow a certain format, with a To and a From address, a signed note at the end, and\nso on. How will we decide what a sentence is in such a case? Should the entire address\nbe considered a single “sentence”? Or should each line be split separately? Answers to\nsuch questions depend on what you want to extract and how sensitive the rest of the\npipeline is about such decisions. For identifying specific patterns (e.g., dates or\nmoney expressions), well-formed regular expressions are the first step. In many prac‐\ntical scenarios, we may end up using a custom tokenizer or sentence segmenter that\nsuits our text structure instead of or on top of an existing one available in a standard\nNLP library [32].\nFigure 2-6. Language-specific (English here) exceptions during tokenization [31]\nFrequent Steps\nLet’s look at some other frequently performed pre-processing operations in an NLP\npipeline. Say we’re designing software that identifies the category of a news article as\none of politics, sports, business, and other. Assume we have a good sentence seg‐\nmenter and word tokenizer in place. At that point, we would have to start thinking\nabout what kind of information is useful for developing a categorization tool. Some of\nthe frequently used words in English, such as a, an, the, of, in, etc., are not particu‐\nlarly useful for this task, as they don’t carry any content on their own to separate\nbetween the four categories. Such words are called stop words and are typically\n(though not always) removed from further analysis in such problem scenarios. There\nis no standard list of stop words for English, though. There are some popular lists\n(NLTK has one, for example), although what a stop word is can vary depending on\n52 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 340,
        "char_count": 2013,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 948,
            "height": 709,
            "ext": "png",
            "size_bytes": 40660
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 83,
        "text": "what we’re working on. For example, the word “news” is perhaps a stop word for this\nproblem scenario, but it may not be a stop word for the offer letter data in the exam‐\nple mentioned in the previous step.\nSimilarly, in some cases, upper or lowercase may not make a difference for the prob‐\nlem. So, all text is lowercased (or uppercased, although lowercasing is more com‐\nmon). Removing punctuation and/or numbers is also a common step for many NLP\nproblems, such as text classification (Chapter 4), information retrieval (Chapter 7),\nand social media analytics (Chapter 8). We’ll see examples of how and if these steps\nare useful in upcoming chapters.\nThe code example below shows how to remove stop words, digits, and punctuation\nand lowercase a given collection of texts:\nfrom nltk.corpus import stopwords\nFrom string import punctuation\ndef preprocess_corpus(texts):\n    mystopwords = set(stopwords.words(\"english\"))\n    def remove_stops_digits(tokens):\n       return [token.lower() for token in tokens if token not in mystopwords                      \n               not token.isdigit() and token not in punctuation]\n    return [remove_stops_digits(word_tokenize(text)) for text in texts]\nIt’s important to note that these four processes are neither mandatory nor sequential\nin nature. The above function is just an illustration of how to add those processing\nsteps into our project. The pre-processing we saw here, while specific to textual data,\nhas nothing particularly linguistic about it—we’re not looking at any aspect of lan‐\nguage other than frequency (stop words are very frequent words), and we’re remov‐\ning non-alphabetic data (punctuation, digits). Two commonly used pre-processing\nsteps that take the word-level properties into account are stemming and\nlemmatization.\nStemming and lemmatization\nStemming refers to the process of removing suffixes and reducing a word to some\nbase form such that all different variants of that word can be represented by the same\nform (e.g., “car” and “cars” are both reduced to “car”). This is accomplished by apply‐\ning a fixed set of rules (e.g., if the word ends in “-es,” remove “-es”). More such exam‐\nples are shown in Figure 2-7. Although such rules may not always end up in a\nlinguistically correct base form, stemming is commonly used in search engines to\nmatch user queries to relevant documents and in text classification to reduce the fea‐\nture space to train machine learning models.\nThe following code snippet shows how to use a popular stemming algorithm called\nPorter Stemmer [33] using NLTK:\nPre-Processing \n| \n53\n",
        "word_count": 396,
        "char_count": 2584,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 84,
        "text": "from nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\nword1, word2 = “cars”, “revolution” \nprint(stemmer.stem(word1), stemmer.stem(word2))\nThis gives “car” as the stemmed version for “cars,” but “revolut” as the stemmed form\nof “revolution,” even though the latter is not linguistically correct. While this may not\naffect the performance of a search engine, derivation of correct linguistic form\nbecomes useful in some other scenarios. This is accomplished by another process,\ncloser to stemming, called lemmatization.\nLemmatization is the process of mapping all the different forms of a word to its base\nword, or lemma. While this seems close to the definition of stemming, they are, in\nfact, different. For example, the adjective “better,” when stemmed, remains the same.\nHowever, upon lemmatization, this should become “good,” as shown in Figure 2-7.\nLemmatization requires more linguistic knowledge, and modeling and developing\nefficient lemmatizers remains an open problem in NLP research even now.\nFigure 2-7. Difference between stemming and lemmatization [34]\nThe following code snippet shows the usage of a lemmatizer based on WordNet from \nNLTK:\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordnetLemmatizer()\nprint(lemmatizer.lemmatize(\"better\", pos=\"a\")) #a is for adjective\nAnd this code snippet shows a lemmatizer using spaCy:\nimport spacy\nsp = spacy.load('en_core_web_sm')\ntoken = sp(u'better')\nfor word in token:\n   print(word.text,  word.lemma_)\nNLTK prints the output as “good,” whereas spaCy prints “well”—both are correct.\nSince lemmatization involves some amount of linguistic analysis of the word and its\ncontext, it is expected that it will take longer to run than stemming, and it’s also typi‐\ncally used only if absolutely necessary. We’ll see how stemming and lemmatization are\nuseful in the next chapters. The choice of lemmatizer is optional; we can choose\nNLTK or spaCy given what framework we’re using for other pre-processing steps in\norder to use a single framework in the complete pipeline.\n54 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 304,
        "char_count": 2078,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 608,
            "height": 223,
            "ext": "png",
            "size_bytes": 14487
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 85,
        "text": "Remember that not all of these steps are always necessary, and not all of them are per‐\nformed in the order in which they’re discussed here. For example, if we were to\nremove digits and punctuation, what is removed first may not matter much. How‐\never, we typically lowercase the text before stemming. We also don’t remove tokens or\nlowercase the text before doing lemmatization because we have to know the part of\nspeech of the word to get its lemma, and that requires all tokens in the sentence to be\nintact. A good practice to follow is to prepare a sequential list of pre-processing tasks\nto be done after having a clear understanding of how to process our data.\nFigure 2-8 lists the different pre-processing steps we’ve seen in this subsection so far,\nas a quick summary.\nNote that these are the more common pre-processing steps, but they’re by no means\nexhaustive. Depending on the nature of the data, some additional pre-processing\nsteps may be important. Let’s take a look at a few of those steps.\nFigure 2-8. Common pre-processing steps on a blob of text\nOther Pre-Processing Steps\nSo far, we’ve seen a few common pre-processing steps in an NLP pipeline. While we\nhaven’t explicitly stated the nature of the texts, we have assumed that we’re dealing\nwith regular English text. What’s different if that’s not the case? Let’s introduce a few\nmore pre-processing steps to deal with such scenarios, using a few examples.\nPre-Processing \n| \n55\n",
        "word_count": 249,
        "char_count": 1448,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1142,
            "height": 884,
            "ext": "png",
            "size_bytes": 40145
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 86,
        "text": "Text normalization\nConsider a scenario where we’re working with a collection of social media posts to\ndetect news events. Social media text is very different from the language we’d see in,\nsay, newspapers. A word can be spelled in different ways, including in shortened\nforms, a phone number can be written in different formats (e.g., with and without\nhyphens), names are sometimes in lowercase, and so on. When we’re working on\ndeveloping NLP tools to work with such data, it’s useful to reach a canonical repre‐\nsentation of text that captures all these variations into one representation. This is\nknown as text normalization. Some common steps for text normalization are to con‐\nvert all text to lowercase or uppercase, convert digits to text (e.g., 9 to nine), expand\nabbreviations, and so on. A simple way to incorporate text normalization can be\nfound in Spacy’s source code [35], which is a dictionary showing different spellings of\na preset collection of words mapped to a single spelling. We’ll see more examples of\ntext normalization in Chapter 8.\nLanguage detection\nA lot of web content is in non-English languages. For example, say we’re asked to col‐\nlect all reviews about our product on the web. As we navigate different e-commerce\nwebsites and start crawling pages related to our product, we notice several non-\nEnglish reviews showing up. Since a majority of the pipeline is built with language-\nspecific tools, what will happen to our NLP pipeline, which is expecting English text?\nIn such cases, language detection is performed as the first step in an NLP pipeline.\nWe can use libraries like Polyglot [36] for language detection. Once this step is done,\nthe next steps could follow a language-specific pipeline.\nCode mixing and transliteration\nThe discussion above was about a scenario where the content is in non-English lan‐\nguages. However, there’s another scenario where a single piece of content is in more\nthan one language. Many people across the world speak more than one language in\ntheir day-to-day lives. Thus, it’s not uncommon to see them using multiple languages\nin their social media posts, and a single post may contain many languages. As an\nexample of code mixing, we can look at a Singlish (Singapore slang + English) phrase\nfrom LDC [37] in Figure 2-9.\nFigure 2-9. Code mixing in a single Singlish phrase\n56 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 396,
        "char_count": 2374,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1404,
            "height": 238,
            "ext": "png",
            "size_bytes": 30479
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 87,
        "text": "A single popular phrase has words from Tamil, English, Malay, and three Chinese\nlanguage variants. Code mixing refers to this phenomenon of switching between lan‐\nguages. When people use multiple languages in their write-ups, they often type words\nfrom these languages in Roman script, with English spelling. So, the words of another\nlanguage are written along with English text. This is known as transliteration. Both of\nthese phenomena are common in multilingual communities and need to be handled\nduring the pre-processing of text. We’ll discuss more about these in Chapter 8, where\nwe’ll see examples of these phenomena in social media text.\nThis concludes our discussion of common pre-processing steps. While this list is by\nno means exhaustive, we hope it gives you some idea of the different forms of pre-\nprocessing that may be required, depending on the nature of the dataset. Now, let’s\ntake a look at a few more pre-processing steps in the NLP pipeline—ones that need\nadvanced language processing beyond what we’ve seen so far.\nAdvanced Processing\nImagine we’re asked to develop a system to identify person and organization names\nin our company’s collection of one million documents. The common pre-processing\nsteps we discussed earlier may not be relevant in this context. Identifying names\nrequires us to be able to do POS tagging, as identifying proper nouns can be useful in\nidentifying person and organization names. How do we do POS tagging during the\npre-processing stage of the project? We’re not going into the details of how POS tag‐\ngers are developed (see Chapter 8 in [38] for details) in this book. Pre-trained and\nreadily usable POS taggers are implemented in NLP libraries such as NLTK, spaCy\n[39], and Parsey McParseface Tagger [40], and we generally don’t have to develop our\nown POS-tagging solutions. The following code snippet illustrates how to use many\nof the pre-built pre-processing functions we’ve discussed so far using the NLP library\nspaCy:\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(u'Charles Spencer Chaplin was born on 16 April 1889 toHannah Chaplin \n         (born Hannah Harriet\nPedlingham Hill) and Charles Chaplin Sr')\nfor token in doc:\n    print(token.text, token.lemma_, token.pos_,\n          token.shape_, token.is_alpha, token.is_stop)\nIn this simple snippet, we can see tokenization, lemmatization, POS tagging, and sev‐\neral other steps in action! Note that if needed we can add additional processing steps\nwith the same code snippet; we’ll leave that as an exercise for the reader. A point to\nnote is that there may be differences in the output among different NLP libraries for\nthe same pre-processing step. This is due in part to implementation differences and\nalgorithmic variations among different libraries. Which library (or libraries) you’ll\nPre-Processing \n| \n57\n",
        "word_count": 444,
        "char_count": 2840,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 88,
        "text": "eventually want to use in your project is a subjective decision based on the amount of\nlanguage processing you want.\nLet’s now consider a slightly different problem: along with identifying person and\norganization names in our company’s collection of one million documents, we’re also\nasked to identify if a given person and organization are related to each other in some\nway (e.g., Satya Nadella is related to Microsoft through the relation CEO). This is\nknown as the problem of relation extraction, which we’ll discuss in greater detail in\nChapter 5. But for now, think about what kind of pre-processing we need for this\ncase. We need POS tagging, which we already know how to add to our pipeline. We\nneed a way of identifying person and organization names, which is a separate infor‐\nmation extraction task known as named entity recognition (NER), which we’ll discuss\nin Chapter 5. Apart from these two, we need a way to identify patterns indicating\n“relation” between two entities in a sentence. This requires us to have some form of\nsyntactic representation of the sentence, such as parsing, which we saw in Chapter 1.\nFurther, we also want a way to identify and link multiple mentions of an entity (e.g.,\nSatya Nadella, Mr. Nadella, he, etc.). We accomplish this with the pre-processing step\nknown as coreference resolution. We saw an example of this in “An NLP Walkthrough:\nConversational Agents” on page 31. Figure 2-10 shows the output from Stanford Cor‐\neNLP [41], which illustrates a parser output and coreference resolution output for an\nexample sentence, along with other pre-processing steps we discussed previously.\nWhat we’ve seen so far in this section are some of the most common pre-processing\nsteps in a pipeline. They’re all available as pre-trained, usable models in different NLP\nlibraries. Apart from these, additional, customized pre-processing may be necessary,\ndepending on the application. For example, consider a case where we’re asked to\nmine the social media sentiment on our product. We start by collecting data from,\nsay, Twitter, and quickly realize there are tweets that are not in English. In such cases,\nwe may also need a language-detection step before doing anything else.\nAdditionally, what steps we need also depends on a specific application. If we’re creat‐\ning a system to identify whether the reviewer is expressing a positive or negative sen‐\ntiment about a movie from a review they wrote, we might not worry much about\nparsing or coreference resolution, but we would want to consider stop word removal,\nlowercasing, and removing digits. However, if we’re interested instead in extracting\ncalendar events from emails, we’ll probably be better off not removing stop words or\ndoing stemming, but rather including, say, parsing. In the case where we want to\nextract relationships between different entities in the text and events mentioned in it,\nwe would need coreference resolution, as we discussed previously. We’ll see examples\nof cases requiring such steps in Chapter 5.\n58 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 496,
        "char_count": 3049,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 89,
        "text": "Figure 2-10. Output from different stages of NLP pipeline processing\nFinally, we have to consider the step-by-step procedures of pre-processing in each\ncase, as summarized in Figure 2-11.\nPre-Processing \n| \n59\n",
        "word_count": 31,
        "char_count": 210,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1376,
            "height": 1714,
            "ext": "png",
            "size_bytes": 142057
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 90,
        "text": "Figure 2-11. Advanced pre-processing steps on a blob of text\nFor example, POS tagging cannot be preceded by stop word removal, lowercasing,\netc., as such processing affects POS tagger output by changing the grammatical struc‐\nture of the sentence. How a particular pre-processing step is helping a given NLP\nproblem is another question that is specific to the application, and it can only be\nanswered with a lot of experimentation. We’ll discuss more specific pre-processing\nrequired for different NLP applications in upcoming chapters. For now, let’s move on\nto the next step: feature engineering.\nFeature Engineering\nSo far, we’ve seen different pre-processing steps and where they can be useful. When\nwe use ML methods to perform our modeling step later, we’ll still need a way to feed\nthis pre-processed text into an ML algorithm. Feature engineering refers to the set of\nmethods that will accomplish this task. It’s also referred to as feature extraction. The\ngoal of feature engineering is to capture the characteristics of the text into a numeric\nvector that can be understood by the ML algorithms. We refer to this step as “text\nrepresentation” in this book, and it’s the topic of Chapter 3. We also detail feature\nextraction in the context of developing a complete NLP pipeline and iterating to\nimprove performance in Chapter 11. Here, we’ll briefly touch on two different\napproaches taken in practice for feature engineering in (1) a classical NLP and tradi‐\ntional ML pipeline and (2) a DL pipeline. Figure 2-12 (adapted from [42]) distin‐\nguishes the two approaches.\n60 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 264,
        "char_count": 1610,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1206,
            "height": 884,
            "ext": "png",
            "size_bytes": 57944
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 91,
        "text": "Figure 2-12. Feature engineering for classical NLP versus DL-based NLP\nFeature Engineering \n| \n61\n",
        "word_count": 14,
        "char_count": 98,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1397,
            "height": 2046,
            "ext": "png",
            "size_bytes": 190846
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 92,
        "text": "Classical NLP/ML Pipeline\nFeature engineering is an integral step in any ML pipeline. Feature engineering steps\nconvert the raw data into a format that can be consumed by a machine. These trans‐\nformation functions are usually handcrafted in the classical ML pipeline, aligning to\nthe task at hand. For example, imagine a task of sentiment classification on product\nreviews in e-commerce. One way to convert the reviews into meaningful “numbers”\nthat helps predict the reviews’ sentiments (positive or negative) would be to count the\nnumber of positive and negative words in each review. There are statistical measures\nfor understanding if a feature is useful for a task or not; we’ll discuss this in Chap‐\nter 11. The main takeaway for building classical ML models is that the features are\nheavily inspired by the task at hand as well as domain knowledge (for example, using\nsentiment words in the review example). One of the advantages of handcrafted fea‐\ntures is that the model remains interpretable—it’s possible to quantify exactly how\nmuch each feature is influencing the model prediction.\nDL Pipeline\nThe main drawback of classical ML models is the feature engineering. Handcrafted\nfeature engineering becomes a bottleneck for both model performance and the model\ndevelopment cycle. A noisy or unrelated feature can potentially harm the model’s per‐\nformance by adding more randomness to the data. Recently, with the advent of DL\nmodels, this approach has changed. In the DL pipeline, the raw data (after pre-\nprocessing) is directly fed to a model. The model is capable of “learning” features\nfrom the data. Hence, these features are more in line with the task at hand, so they\ngenerally give improved performance. But, since all these features are learned via\nmodel parameters, the model loses interpretability. It’s very hard to explain a DL\nmodel’s prediction, which is a disadvantage in a business-driven use case. For exam‐\nple, when identifying an email as ham or spam, it might be worth knowing which\nword or phrases played the significant role in making the email ham or spam. While\nthis is easy to do with handcrafted features, it’s not easy in the case of DL models.\nAs we’ve already mentioned, feature engineering is heavily task specific, so we discuss\nit throughout the book in the context of textual data and a range of tasks. With a\nhigh-level understanding of feature engineering, now let’s take a look at the next step\nin the pipeline, which we call modeling.\nModeling\nWe now have some amount of data related to our NLP project and a clear idea of\nwhat sort of cleaning up and pre-processing needs to be done and what features are\nto be extracted. The next step is about how to build a useful solution out of this. At\nthe start, when we have limited data, we can use simpler methods and rules. Over\n62 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 483,
        "char_count": 2856,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 93,
        "text": "time, with more data and a better understanding of the problem, we can add more\ncomplexity and improve performance. We’ll cover this process in this section.\nStart with Simple Heuristics\nAt the very start of building a model, ML may not play a major role by itself. Part of\nthat could be due to a lack of data, but human-built heuristics can also provide a great\nstart in some ways. Heuristics may already be part of your system, either implicitly or\nexplicitly. For instance, in email spam-classification tasks, we may have a blacklist of\ndomains that are used exclusively to send spam. This information can be used to filter\nemails from those domains. Similarly, a blacklist of words in an email that denote a\nhigh chance of spam could also be used for this classification.\nSuch heuristics can be found in a range of tasks, especially at the start of applying ML.\nIn an e-commerce setting, we may use a heuristic based on the number of purchases\nfor ordering search results and show products belonging to the same category as rec‐\nommendations while we collect data that could be used to build a larger, collabora‐\ntive, filtering-based system that can recommend products using a range of other\ncharacteristics based on what customers with similar buying profiles purchased.\nAnother popular approach to incorporating heuristics in your system is using regular\nexpressions. Let’s say we’re developing a system to extract different forms of informa‐\ntion from text documents, such as dates and phone numbers, names of people who\nwork in a given organization, etc. While some information, such as email IDs, dates,\nand telephone numbers can be extracted using normal (albeit complex) regular\nexpressions, Stanford NLP’s TokensRegex [43] and spaCy’s rule-based matching [20]\nare two tools that are useful for defining advanced regular expressions to capture\nother information, such as people who work in a specific organization. Figure 2-13\nshows an example of spaCy’s rule-based matcher in action.\nThis shows a pattern that looks for text containing the lemma “match,” appearing as a\nnoun, optionally preceded by an adjective, and followed by any word form of lemma\n“be.” Such patterns are an advanced form of regular expressions, which require some\nof the NLP pre-processing steps we saw earlier in this chapter. In the absence of large\namounts of training data, and when we have some domain knowledge, we can start\nbuilding systems by encoding this knowledge in the form of rules/heuristics. Even\nwhen we’re building ML-based models, we can use such heuristics to handle special\ncases—for example, cases where the model has failed to learn well. Thus, simple heu‐\nristics can give us a good starting point and be useful in ML models. Now, assuming\nwe built such a heuristics-based system, where do we go from there?\nModeling \n| \n63\n",
        "word_count": 468,
        "char_count": 2833,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 94,
        "text": "Figure 2-13. spaCy’s rule-based matcher\nBuilding Your Model\nWhile a set of simple heuristics is a good start, as our system matures, adding newer\nand newer heuristics may result in a complex, rule-based system. Such a system is\nhard to manage, and it can be even harder to diagnose the cause of errors. We need a\nsystem that’s easier to maintain as it matures. Further, as we collect more data, our\nML model starts beating pure heuristics. At that point, a common practice is to com‐\nbine heuristics directly or indirectly with the ML model. There are two broad ways of\ndoing that:\nCreate a feature from the heuristic for your ML model\nWhen there are many heuristics where the behavior of a single heuristic is deter‐\nministic but their combined behavior is fuzzy in terms of how they predict, it’s\nbest to use these heuristics as features to train your ML model. For instance, in\nthe email spam-classification example, we can add features, such as the number\nof words from the blacklist in a given email or the email bounce rate, to the ML\nmodel.\n64 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 192,
        "char_count": 1079,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1109,
            "height": 842,
            "ext": "png",
            "size_bytes": 96169
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 95,
        "text": "i. This is different from the vertical stacking done in neural networks like LSTM.\nPre-process your input to the ML model\nIf the heuristic has a really high prediction for a particular kind of class, then it’s\nbest to use it before feeding the data in your ML model. For instance, if for cer‐\ntain words in an email, there’s a 99% chance that it’s spam, then it’s best to classify\nthat email as spam instead of sending it to an ML model.\nAdditionally, we have NLP service providers, such as Google Cloud Natural Language\n[44], Amazon Comprehend [45], Microsoft Azure Cognitive Services [46], and IBM\nWatson Natural Language Understanding [47], which provide off-the-shelf APIs to\nsolve various NLP tasks. If your project has an NLP problem that’s addressed by these\nAPIs, you can start by using them to get an estimate of the feasibility of the task and\nhow good your existing dataset is. Once you’re comfortable that the task is feasible\nand conclude that the off-the-shelf models give reasonable results, you can move\ntoward building custom ML models and improving them.\nBuilding THE Model\nWe’ve seen examples of getting started building an NLP system by using heuristics or\nexisting APIs, or by building our own ML models. We start with a baseline approach\nand work toward improving it. We may have to do many iterations of the model-\nbuilding process to “build THE model” that gives good performance and is also\nproduction-ready. We cover some of the approaches to address this issue here:\nEnsemble and stacking\nIn our experience, a common practice is not to have a single model, but to use a\ncollection of ML models, often dealing with different aspects of the prediction\nproblem. There are two ways of doing this: we can feed one model’s output as\ninput for another model, thus sequentially going from one model to another and\nobtaining a final output. This is called model stacking.i Alternatively, we can also\npool predictions from multiple models and make a final prediction. This is called\nmodel ensembling. Figure 2-14 demonstrates both of these procedures.\nIn this figure, training data is used to build Models 1, 2, and 3. Outputs of these\nmodels are then combined to be used in a meta-model (a model that uses other\nmodels) to predict the final outcome. For example, in the email spam-\nclassification case, we can assume that we run three different models: a heuristic-\nbased score, Naive Bayes, and LSTM. The output of these three models is then\nfed into the meta-model based on logistic regression, which then gives the chan‐\nces of the email being spam or not. As the product grows in terms of its features,\nthe model will also grow in complexity. So, we may eventually end up using a\nModeling \n| \n65\n",
        "word_count": 469,
        "char_count": 2718,
        "fonts": [
          "MinionPro-Regular (8.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Regular (6.3pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 96,
        "text": "combination of all of these—i.e., heuristics, machine learning, and stacked and\nensemble models—as part of a large product.\nFigure 2-14. Model ensemble and stacking\nBetter feature engineering\nFor both API-based and custom-built models, feature engineering is an impor‐\ntant step, and it evolves throughout the process. A better feature engineering step\nmay lead to better performance. For instance, if there are a lot of features, then\nwe use feature selection to find a better model. We detail strategies for iterating\nfeature engineering to achieve an optimal setting in Chapter 11.\nTransfer learning\nApart from model stacking or ensemble, there is a newer trend that’s becoming\npopular in the NLP community—transfer learning, which we introduced in\nChapter 1. Often, the model needs external knowledge beyond the dataset for the\ntask to understand the language and the problem well. Transfer learning tries to\ntransfer preexisting knowledge from a big, well-trained model to a newer model\nat its initial phase. Afterward, the new model slowly adapts to the task at hand.\nThis is analogous to a teacher transferring wisdom and knowledge to a student.\nTransfer learning provides a better initialization, which helps in the downstream\ntasks, especially when the dataset for the downstream task is smaller. In these\ncases, transfer learning yields better results than just initializing a downstream\nmodel from scratch with random initialization. As an example, for email spam\nclassification, we can use BERT to fine-tune the email dataset. We cover BERT in\ngreater detail in Chapters 4 through 6.\n66 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 255,
        "char_count": 1627,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1409,
            "height": 817,
            "ext": "png",
            "size_bytes": 65947
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 97,
        "text": "Reapplying heuristics\nNo ML model is perfect. Hence, ML models still make mistakes. It’s possible to\nrevisit these cases again at the end of the modeling pipeline to find any common\npattern in errors and use heuristics to correct them. We can also apply domain-\nspecific knowledge that is not automatically captured in the data to refine the\nmodel predictions. An analogy for reapplying heuristics would be our model as a\ntrapeze artist performing great feats and these rules as the safety net so the artist\ndoesn’t fall off.\nBetween the stage of having no data, when we fully rely on heuristics, to a lot of data,\nwhere we can try a range of modeling techniques, we encounter a situation where we\nhave a small amount of data, which is often not sufficient to build good ML models.\nIn such scenarios, one approach to follow is active learning, where we can use user\nfeedback or other such sources to continuously collect new data to build better mod‐\nels. We’ll discuss this in detail in Chapter 4. As we’ve just seen, modeling strategies\ndepend heavily on the data at hand. Table 2-1 provides a range of decision paths\ngiven our data volume and quality, based on our experience.\nTable 2-1. Data attributes and associated decision paths\nData attribute\nDecision path\nExamples\nLarge data volume\nCan use techniques that require more data, like\nDL. Can use a richer set of features as well.\nIf the data is sufficiently large but unlabeled,\nwe can also apply unsupervised techniques.\nIf we have a lot of reviews and metadata\nassociated with them, we can build a sentiment-\nanalysis tool from scratch.\nSmall data volume\nNeed to start with rule-based or traditional ML\nsolutions that are less data hungry. Can also\nadapt cloud APIs and generate more data with\nweak supervision.\nWe can also use transfer learning if there’s a\nsimilar task that has large data.\nThis often happens at the start of a completely\nnew project.\nData quality is poor\nand the data is\nheterogeneous in\nnature\nMore data cleaning and pre-processing might\nbe required.\nThis entails issues like code mixing (different\nlanguages being mixed in the same sentence),\nunconventional language, transliteration, or noise\n(like social media text).\nData quality is good\nCan directly apply off-the-shelf algorithms or\ncloud APIs more easily.\nLegal text or newspapers.\nData consists of full-\nlength documents\nChoose the right strategy for breaking the\ndocument into lower levels, like paragraphs,\nsentences, or phrases, depending on the\nproblem.\nDocument classification, review analysis, etc.\nSo far, we’ve seen an overview of different forms of modeling that can be useful in an\nNLP pipeline and what modeling path to choose based on the data we have. Super‐\nvised learning, especially classification, is the most common modeling process you’ll\nModeling \n| \n67\n",
        "word_count": 468,
        "char_count": 2812,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 98,
        "text": "encounter in the NLP projects you’ll be building in an industry scenario. We’ll discuss\nclassification models in Chapter 4 and models used for different application scenarios\nin NLP in Chapters 5 through 7. Now, let’s take a look at the next step in the pipeline:\nevaluation.\nEvaluation\nA key step in the NLP pipeline is to measure how good the model we’ve built is.\n“Goodness” of a model can have multiple meanings, but the most common interpre‐\ntation is the measure of the model’s performance on unseen data. Success in this\nphase depends on two factors: (1) using the right metric for evaluation, and (2) fol‐\nlowing the right evaluation process. Let’s first focus on 1. Depending on the NLP task\nor problem, the evaluation metrics can vary. They can also vary depending on the\nphase: the model building, deployment, and production phases. Whereas in the first\ntwo phases, we typically use ML metrics, in the final phase, we also include business\nmetrics to measure business impact.\nAlso, evaluations are of two types: intrinsic and extrinsic. Intrinsic focuses on inter‐\nmediary objectives, while extrinsic focuses on evaluating performance on the final\nobjective. For example, consider a spam-classification system. The ML metric will be\nprecision and recall, while the business metric will be “the amount of time users spent\non a spam email.” Intrinsic evaluation will focus on measuring the system perfor‐\nmance using precision and recall. Extrinsic evaluation will focus on measuring the\ntime a user wasted because a spam email went to their inbox or a genuine email went\nto their spam folder.\nIntrinsic Evaluation\nIn this section, we’ll look at some intrinsic evaluation metrics that are commonly used\nto measure NLP systems. For most metrics in this category, we assume a test set\nwhere we have the ground truth or labels (human annotated, correct answers). Labels\ncould be binary (e.g., 0/1 for text classification), one-to-two words (e.g., names for\nnamed entity recognition), or large text itself (e.g., text translated by machine transla‐\ntion). The output of the NLP model on a data point is compared against the corre‐\nsponding label for that data point, and metrics are calculated based on the match (or\nmismatch) between the output and label. For most NLP tasks, the comparison can be\nautomated, hence intrinsic evaluation can be automated. For some cases, like\nmachine translation or summarization, it’s not always possible to automate evaluation\nsince comparison is not subjective.\nTable 2-2 lists various metrics used for intrinsic evaluation across various NLP tasks.\nFor a more detailed discussion of the metrics, refer to the corresponding reference.\n68 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 438,
        "char_count": 2707,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 99,
        "text": "Table 2-2. Popular metrics and NLP applications where they’re used\nMetric\nDescription\nApplications\nAccuracy [48]\nUsed when the output variable is categorical or discrete. It\ndenotes the fraction of times the model makes correct\npredictions as compared to the total predictions it makes.\nMainly used in classification tasks, such as\nsentiment classification (multiclass), natural\nlanguage inference (binary), paraphrase\ndetection (binary), etc.\nPrecision [48]\nShows how precise or exact the model’s predictions are,\ni.e., given all the positive (the class we care about) cases,\nhow many can the model classify correctly?\nUsed in various classification tasks, especially\nin cases where mistakes in a positive class\nare more costly than mistakes in a negative\nclass, e.g., disease predictions in healthcare.\nRecall [48]\nRecall is complementary to precision. It captures how well\nthe model can recall positive class, i.e., given all the\npositive predictions it makes, how many of them are\nindeed positive?\nUsed in classification tasks, especially where\nretrieving positive results is more important,\ne.g., e-commerce search and other\ninformation-retrieval tasks.\nF1 score [49]\nCombines precision and recall to give a single metric,\nwhich also captures the trade-off between precision and\nrecall, i.e., completeness and exactness.\nF1 is defined as (2 × Precision × Recall) / (Precision +\nRecall).\nUsed simultaneously with accuracy in most\nof the classification tasks. It is also used in\nsequence-labeling tasks, such as entity\nextraction, retrieval-based questions\nanswering, etc.\nAUC [48]\nCaptures the count of positive predictions that are correct\nversus the count of positive predictions that are incorrect\nas we vary the threshold for prediction.\nUsed to measure the quality of a model\nindependent of the prediction threshold. It is\nused to find the optimal prediction threshold\nfor a classification task.\nMRR (mean\nreciprocal rank)\n[50]\nUsed to evaluate the responses retrieved given their\nprobability of correctness. It is the mean of the reciprocal\nof the ranks of the retrieved results.\nUsed heavily in all information-retrieval\ntasks, including article search, e-commerce\nsearch, etc.\nMAP (mean\naverage\nprecision) [51]\nUsed in ranked retrieval results, like MRR. It calculates the\nmean precision across each retrieved result.\nUsed in information-retrieval tasks.\nRMSE (root\nmean squared\nerror) [48]\nCaptures a model’s performance in a real-value prediction\ntask. Calculates the square root of the mean of the squared\nerrors for each data point.\nUsed in conjunction with MAPE in the case of\nregression problems, from temperature\nprediction to stock market price prediction.\nMAPE (mean\nabsolute\npercentage\nerror) [52]\nUsed when the output variable is a continuous variable. It\nis the average of absolute percentage error for each data\npoint.\nUsed to test the performance of a regression\nmodel.\nIt is often used in conjunction with RMSE.\nBLEU (bilingual\nevaluation\nunderstudy) [53]\nCaptures the amount of n-gram overlap between the\noutput sentence and the reference ground truth sentence.\nIt has many variants.\nMainly used in machine-translation tasks.\nRecently adapted to other text-generation\ntasks, such as paraphrase generation and\ntext summarization.\nMETEOR [54]\nA precision-based metric to measure the quality of text\ngenerated. It fixes some of the drawbacks of BLEU, such as\nexact word matching while calculating precision. METEOR\nallows synonyms and stemmed words to be matched with\nthe reference word.\nMainly used in machine translation.\nEvaluation \n| \n69\n",
        "word_count": 537,
        "char_count": 3567,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MinionPro-It (10.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 100,
        "text": "Metric\nDescription\nApplications\nROUGE [55]\nAnother metric to compare quality of generated text with\nrespect to a reference text. As opposed to BLEU, it\nmeasures recall.\nSince it measures recall, it’s mainly used for\nsummarization tasks where it’s important to\nevaluate how many words a model can\nrecall.\nPerplexity [56]\nA probabilistic measure that captures how confused an\nNLP model is. It’s derived from the cross-entropy in a next\nword prediction task. The exact definition can be found at\n[56].\nUsed to evaluate language models. It can\nalso be used in language-generation tasks,\nsuch as dialog generation.\nApart from the list of metrics shown in Table 2-2, there are few more metrics and vis‐\nualizations that are often used for solving NLP problems. While we’ve covered these\ntopics briefly here, we encourage you to follow the references and learn more about\nthese metrics.\nIn the case of classification tasks, a commonly used visual evaluation method is a con‐\nfusion matrix. It allows us to inspect the actual and predicted output for different\nclasses in the dataset. The name stems from the fact that it helps to understand how\n“confused” the classification model is in terms of identifying different classes. A con‐\nfusion matrix is in turn used to compute metrics such as precision, recall, F1 score,\nand accuracy. We’ll see how to use confusion matrices in Chapter 4.\nRanking tasks like information search and retrieval mostly uses ranking-based met‐\nrics, such as MRR and MAP, but usual classification metrics can be used, too. In the\ncase of retrieval, we care mainly about recall, so recall at various ranks is calculated.\nFor example, for information retrieval, a common metric is “Recall at rank K”; it\nlooks for the presence of ground truth in top K retrieved results. If present, it’s a\nsuccess.\nWhen it comes to text-generation tasks, there are a number of metrics that are used,\ndepending on the task. Even though BLEU and METEOR are good metrics for\nmachine translation, they may not be good metrics when applied to other generation\ntasks. For example, in the case of dialog generation, the ground truth is one of the\ncorrect answers, but there could be many variations in responses that are not listed.\nIn cases like this, precision-based metrics such as BLEU and METEOR will com‐\npletely fail to capture the task performance faithfully. For these reasons, perplexity is\none metric that’s used extensively to understand a model’s text-generation ability.\nHowever, any evaluation scheme for text generation is not perfect. This is because\nthere could be multiple sentences that have the same meaning, and it’s not possible to\nhave all the variations listed as ground truth. Therefore, the text generated and the\nground truth can have the same meaning but be different sentences. This makes auto‐\nmated evaluation a difficult process. For example, say we build a machine-translation\nmodel that converts sentences from French to English. Consider the following sen‐\ntence in French: “J’ai mangé trois filberts.” In English, this means, “I ate three filberts.”\n70 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 510,
        "char_count": 3112,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 101,
        "text": "So, we put this sentence as the label. Say our model generates the following English\ntranslation: “I ate three hazelnuts.” Since the output does not match the label, auto‐\nmated evaluation will say the output is incorrect. But this evaluation is incorrect\nbecause English speakers are known to refer to filberts as hazelnuts. Even if we add\nthis sentence as a possible label, our model could still generate “I have eaten three\nhazelnuts” as output. Yet again, the automated evaluation will say the model got it\nwrong since the output does not match either of the two labels. This is where human\nevaluation comes into play. But human evaluation can be expensive both in terms of\ntime and money.\nExtrinsic Evaluation\nLike we said earlier, extrinsic evaluation focuses on evaluating the model perfor‐\nmance on the final objective. In industrial projects, any AI model is built with the aim\nof solving a business problem. For example, a regression model is built with the aim\nof ranking the emails of the users and bringing the most important emails to the top\nof the inbox, thereby helping the users of an email service save time. Consider a sce‐\nnario where the regression model does well on the ML metrics but doesn’t really save\na lot of time for the email service users, or where a question-answering model does\nvery well on intrinsic metrics but fails to address a large number of questions in the\nproduction environment. Would we call such models successful? No, because they\nfailed to achieve their business objectives. While this is not an issue for researchers in\nacademia, for practitioners in industry, it’s very important.\nThe way to carry out extrinsic evaluation is to set up the business metrics and the\nprocess to measure them correctly at the start of the project. We’ll see examples of the\nright business metrics in later chapters.\nWe might ask: if extrinsic evaluation is what matters, why do intrinsic evaluation at\nall? The reason we must do intrinsic evaluation before extrinsic evaluation is that\nextrinsic evaluation often includes project stakeholders outside the AI team—some‐\ntimes even end users. Intrinsic evaluation can be done mostly by the AI team itself.\nThis makes extrinsic evaluation a much more expensive process as compared to\nintrinsic evaluation. Therefore, intrinsic evaluation is used as a proxy for extrinsic\nevaluation. Only when we get consistently good results in intrinsic evaluation should\nwe go for extrinsic evaluation.\nAnother thing to remember is that bad results in intrinsic evaluation often imply bad\nresults in extrinsic evaluation. However, the converse may not be true. That is, we can\nhave a model that does very well in intrinsic evaluation but does badly in extrinsic\nevaluation, but it’s unlikely that a model that does well in extrinsic evaluation did\npoorly during intrinsic evaluation. The reasons for poor performance in extrinsic\nevaluation could be many, from setting up the wrong metrics to not having suitable\nEvaluation \n| \n71\n",
        "word_count": 493,
        "char_count": 2995,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 102,
        "text": "data or having wrong expectations. We touched on some of these in Chapter 1 and\nwill discuss them in more detail in Chapter 11.\nSo far, we’ve seen some metrics commonly used for intrinsic evaluation and also dis‐\ncussed the importance of extrinsic evaluation to measure the performance of NLP\nmodels. There are some more metrics that are task specific, which are not seen across\ndifferent NLP application scenarios. We’ll discuss such evaluation measures in detail\nas we cover these specific applications in upcoming chapters. With this, now let’s look\nat the next components of the pipeline: model deployment, monitoring, and\nupdating.\nPost-Modeling Phases\nOnce our model has been tried and tested, we move on to the post-modeling phase:\ndeploying, monitoring, and updating the model. We’ll cover these briefly in this\nsection.\nDeployment\nIn most practical application scenarios, the NLP module we’re implementing is a part\nof a larger system (e.g., a spam-classification system in a larger email application).\nThus, working through the processing, modeling, and evaluation pipeline is only a\npart of the story. Eventually, once we’re happy with one final solution, it needs to be\ndeployed in a production environment as a part of a larger system. Deployment\nentails plugging the NLP module into the broader system. It may also involve making\nsure input and output data pipelines are in order, as well as making sure our NLP\nmodule is scalable under heavy load.\nAn NLP module is typically deployed as a web service. Let’s say we designed a web\nservice that takes a text as input and returns the email’s category (spam or non-spam)\nas output. Now, each time someone gets a new email, it goes to the microservice,\nwhich classifies the email text. This, in turn, can be used to make a decision about\nwhat to do with the email (either show it or send it to the spam folder). In certain\ncircumstances, like batch processing, the NLP module is deployed in the larger task\nqueue. As an example, take a look at task queues in Google Cloud [57] or AWS [58].\nWe’ll cover deployment in more detail in Chapter 11.\nMonitoring\nLike with any software engineering project, extensive software testing has to be done\nbefore final deployment, and the model performance is monitored constantly after\ndeploying. Monitoring for NLP projects and models has to be handled differently\nthan a regular engineering project, as we need to ensure that the outputs produced by\nour models daily make sense. If we’re automatically training the model frequently, we\n72 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 426,
        "char_count": 2564,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 103,
        "text": "have to make sure that the models behave in a reasonable manner. Part of this is done\nthrough a performance dashboard showing the model parameters and key perfor‐\nmance indicators. We’ll discuss this more in Chapter 11.\nModel Updating\nOnce the model is deployed and we start gathering new data, we’ll iterate the model\nbased on this new data to stay current with predictions. We cover this model update\nfor each task throughout the book, especially in Chapters 4 through 7 and Chap‐\nter 11. As a start, Table 2-3 gives some guidance on how to approach the model\nupdating process for different post-deployment scenarios.\nTable 2-3. Project attribute and associated decision paths\nProject attribute\nDecision paths\nExamples\nMore training data is generated\npost-deployment.\nOnce deployed, extracted signals can be used to\nautomatically improve the model. Can also try online\nlearning to train the model automatically on a daily\nbasis.\nAbuse-detection systems where\nusers flag data.\nTraining data is not generated\npost-deployment.\nManual labeling could be done to improve evaluation\nand the models.\nIdeally, each new model has to be manually built and\nevaluated.\nA subset of a larger NLP pipeline\nwith no direct feedback.\nLow model latency is required,\nor model has to be online with\nnear-real-time response.\nNeed to use models that can be inferred quickly.\nAnother option is to create memoization strategies like\ncaching or have substantially bigger computing power.\nSystems that need to respond\nright away, like any chatbot or\nan emergency tracking system.\nLow model latency is not\nrequired, or model can be run in\nan offline fashion.\nCan use more advanced and slower models. This can\nalso help in optimizing costs where feasible.\nSystems that can be run on a\nbatch process, like retail product\ncatalog analysis.\nWorking with Other Languages\nSo far, our discussion has assumed that we’re dealing mostly with English text.\nDepending on the task at hand, we may need to build models and solutions for other\nlanguages as well. How we approach this will change based on what language we’re\ndealing with. The pipeline for some languages may be very similar to English,\nwhereas some languages and scenarios may require us to rethink how we approach\nthe problem. We’ve compiled some action points for dealing with different languages\nin Table 2-4, based on our experiences working on projects that involve non-English\nlanguage processing.\nWorking with Other Languages \n| \n73\n",
        "word_count": 399,
        "char_count": 2465,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 104,
        "text": "Table 2-4. Language attribute and action plan\nLanguage\nattribute\nExample and languages\nAction\nHigh-resource\nlanguages\nLanguages that have both ample data as well\nas pre-built models.\nExamples include English, French, and Spanish.\nPossible to use pre-trained DL models. Easier to use.\nLow-resource\nlanguages\nLanguages that have limited data and recent\ndigital adoption. May not have pre-built\nmodels.\nExamples include Swahili, Burmese, and\nUzbek.\nDepending on the task, may need to label more data\nas well as explore individual components.\nMorphologically\nrich\nLinguistic and grammatical information like\nsubject, object, predicate, tense, and mode are\nnot separate words, but are joined together.\nExamples include Latin, Turkish, Finnish, and\nMalayalam.\nIf the language is not resource rich, we’ll need to\nexplore morphological analyzers that exist for the\nlanguage. In the worst case, manual rules to handle\ncertain cases might be needed.\nVocabulary\nvariation heavy\nNonstandard spellings and high word\nvariation.\nFor Arabic and Hindi, the spellings are\nnonstandard.\nIf the language is not resource rich, then we may need\nto first normalize the words/spellings before training\nany model.\nThis may not be needed for languages with large\ndatasets, as they can still learn of vocabulary variation.\nCJK languages\nThese languages are derived from ancient\nChinese characters. They’re not alphabet based\nand have several thousand characters for basic\nliteracy and over 40,000 characters for larger\ncoverage. Thus, they have to be handled\ndifferently.\nThey include Chinese, Japanese, and Korean,\nhence the name CJK.\nUse specific tokenization schemes in these languages.\nGiven that an ample amount of CJK data is available,\nit’s possible to build NLP models for various tasks from\nscratch.\nThere are also pre-trained models for them.\nTransfer learning from models trained in other\nlanguages beyond CJK may not be useful in this case.\nNext, we’ll turn our attention to a case study that will put all these steps together.\nCase Study\nSo far, we’ve seen different stages of an NLP pipeline. At each stage, we discussed\nwhat it’s about, why it’s useful, and how it fits into the general framework of an NLP\npipeline. However, we tackled these individual stages separately, away from the over‐\nall context. How do all these stages work together in a real-world NLP system pipe‐\nline? Let’s see a case study, using Uber’s tool to improve customer care: Customer\nObsession Ticketing Assistant (COTA).\nUber operates in 400+ cities worldwide, and cbased on the number of people who use\nUber every day, we can expect that their customer support teams receive several hun‐\ndreds of thousands of tickets on different issues each day. There are a couple of solu‐\ntions to choose from for a given ticket. The goal of COTA is to rank these solutions\n74 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 454,
        "char_count": 2857,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 105,
        "text": "and pick the best possible one. Uber developed COTA using ML and NLP techniques\nto enable better customer support and quick and efficient resolution of such tickets.\nFigure 2-15 shows the pipeline in Uber’s COTA and the various NLP components in\nit.\nFigure 2-15. NLP pipeline for ranking tickets in a ticketing system by Uber [59]\nThe information needed to identify the ticket issue and select a solution in this sys‐\ntem comes from three sources, as shown in the figure. Ticket text is, as the name indi‐\ncates, textual content, which is where NLP comes into the picture. After cleaning up\nthe text by removing HTML tags (not shown in the figure), the pre-processing steps\nconsist of tokenization, lowercasing, stop word removal, and lemmatization. We saw\nhow to do all of these earlier in this chapter. After pre-processing, the ticket text is\nrepresented as a collection of words (known as a bag of words and discussed in detail\nin Chapter 3).\nThe next step in this pipeline is feature engineering. The bag of words we obtained\nearlier is fed to two NLP modules—TF-IDF (term frequency and inverse document\nfrequency) and LSI (latent semantic indexing)—which are used to understand the\nmeaning of a text using this bag of words representation. This process comes under\nthe NLP task called topic modeling, which we’ll discuss in Chapter 7. Exactly how\nUber uses these NLP tasks in this context is an interesting idea: Uber collects the his‐\ntorical tickets for each solution from their database, forms a bag-of-words vector rep‐\nresentation for each solution, and creates a topic model based on these\nrepresentations. An incoming ticket is then mapped to this topic space of solutions,\ncreating a vector representation for the ticket. Cosine similarity is a common measure\nof similarity between any two vectors. It is used to create a vector where each element\nindicates the ticket text’s similarity to one solution. Thus, at the end of this feature\nengineering step, we end up with a representation indicating the ticket text’s similar‐\nity to all possible solutions.\nCase Study \n| \n75\n",
        "word_count": 348,
        "char_count": 2088,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1428,
            "height": 648,
            "ext": "png",
            "size_bytes": 57628
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 106,
        "text": "In the next stage, modeling, this representation is combined with ticket information\nand trip data to build a ranking system that shows the three best solutions for the\nticket. Under the hood, this ranking model consists of a binary classification system,\nwhich classifies each ticket-solution combination as a match or mismatch. The\nmatches are then ranked based on a scoring function. [59] describes more details on\nthe implementation of this system pipeline.\nThe next step in our pipeline is evaluation. How does evaluation work in this context?\nWhile the evaluation of model performance itself can be done in terms of an intrinsic\nevaluation measure such as MRR, the overall effectiveness of this approach is evalu‐\nated extrinsically. It’s estimated that COTA’s quick ticket resolution saves Uber tens of\nmillions of dollars every year.\nAs we learned earlier, a model is not built just once. COTA, too, was continually\nexperimented with and improved upon. After exploring a range of DL architectures,\nthe best solution that was ultimately chosen resulted in a 10% greater accuracy com‐\npared to the previous version with the binary classification-based ranking system.\nThe process does not end here, though. As we can see from the COTA team’s article\n[59], it’s a continuous process of model deployment, monitoring, and updating.\nWrapping Up\nIn this chapter, we saw the different steps involved in developing an NLP pipeline for\na given project description and saw a detailed case study of a real-world application.\nWe also saw how a traditional NLP pipeline and a DL-based NLP pipeline differ from\neach other and learned what to do when working with non-English languages. Aside\nfrom the case study, we looked at these steps in a more general manner in this chap‐\nter. Specific details for each step will depend on the task at hand and the purpose of\nour implementation. We’ll look at a few task-specific pipelines from Chapter 4\nonward, describing in detail what’s unique as well as common across different tasks\nwhile designing such pipelines. In the next chapter, we’ll tackle the question of text\nrepresentation that we mentioned briefly earlier in this chapter.\nReferences\n[1] Iderhoff, Nicolas. nlp-datasets: Alphabetical list of free/public domain datasets\nwith text data for use in Natural Language Processing (NLP), (GitHub repo). Last\naccessed June 15, 2020.\n[2] Google. “Dataset Search”. Last accessed June 15, 2020.\n[3] Miller, George A. “WordNet: A Lexical Database for English.” Communications of\nthe ACM 38.11 (1995): 39–41.\n[4] NTLTK documentation. “WordNet Interface”. Last accessed June 15, 2020.\n76 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 423,
        "char_count": 2652,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 107,
        "text": "[5] Xie, Qizhe, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le.\n“Unsupervised Data Augmentation for Consistency Training”. (2019).\n[6] Wikipedia. “Fat-finger error”. Last modified January 26, 2020.\n[7] Snorkel. “Programmatically Building and Managing Training Data”. Last accessed\nJune 15, 2020.\n[8] Ratner, Alexander, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and\nChristopher Ré. “Snorkel: Rapid Training Data Creation with Weak Supervision.” The\nVLDB Journal 29 (2019): 1–22.\n[9] Bach, Stephen H., Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cas‐\nsandra Xia, Souvik Sen et al. “Snorkel DryBell: A Case Study in Deploying Weak\nSupervision at Industrial Scale”. (2018).\n[10] Wei, Jason W., and Kai Zou. “Eda: Easy Data Augmentation Techniques for\nBoosting Performance on Text Classification Tasks”, (2019).\n[11] GitHub repository for [10]. Last accessed June 15, 2020.\n[12] Ma, Edward. nplaug: Data augmentation for NLP, (GitHub repo). Last accessed\nJune 15, 2020.\n[13] Shioulin and Nisha. “A Guide to Learning with Limited Labeled Data”. April 2,\n2019.\n[14] eForms. “Blank Invoice Templates”. Last accessed June 15, 2020.\n[15] Amazon.com. “Amazon Elements Vitamin B12 Methylcobalamin 5000 mcg -\nNormal Energy Production and Metabolism, Immune System Support - 2 Month\nSupply (65 Berry Flavored Lozenges)”. Last accessed June 15, 2020.\n[16] Beautiful Soup. Last accessed June 15, 2020.\n[17] Scrapy.org. Scrapy. Last accessed June 15, 2020.\n[18] Unicode. Last accessed June 15, 2020.\n[19] Dickinson, Markus, Chris Brew, and Detmar Meurers. Language and Computers.\nNew Jersey: John Wiley & Sons, 2012. ISBN: 978-1-405-18305-5\n[20] Explosion.ai. \"Rule-based matching\". Last accessed June 15, 2020.\n[21] Microsoft documentation. “Quickstart: Check spelling with the Bing Spell Check\nREST API and Python”. Last accessed June 15, 2020.\n[22] Stamy, Matthew. PyPDF2: A utility to read and write PDFs with Python, (GitHub\nrepo). Last accessed June 15, 2020.\n[23] pdfminer. pdfminer.six: Community maintained fork of pdfminer, (GitHub\nrepo). Last accessed June 15, 2020.\nWrapping Up \n| \n77\n",
        "word_count": 315,
        "char_count": 2115,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 108,
        "text": "[24] FilingDB. “What’s so hard about PDF text extraction?” Last accessed June 15,\n2020.\n[25] Tesseract-OCR. “Tesseract Open Source OCR Engine (main repository)”, (Git‐\nHub repo). Last accessed June 15, 2020.\n[26] Python-tesseract documentationPython-tesseract. Last accessed June 15, 2020.\n[27] Firth, John Rupert. “Personality and Language in Society.” The Sociological\nReview 42.1 (1950): 37–52.\n[28] pyenchant. Spellchecking library for python, (GitHub repo). Last accessed June\n15, 2020.\n[29] KBNL Research. ochre: Toolbox for OCR post-correction, (GitHub repo). Last\naccessed June 15, 2020.\n[30] “Natural Language ToolKit”. Last accessed June 15, 2020.\n[31] Explosion.ai. “spaCy 101: Everything you need to know”. Last accessed June 15,\n2020.\n[32] Evang, Kilian, Valerio Basile, Grzegorz Chrupała, and Johan Bos. “Elephant:\nSequence Labeling for Word and Sentence Segmentation.” Proceedings of the 2013\nConference on Empirical Methods in Natural Language Processing (2013): 1422–1426.\n[33] Porter, Martin F. “An Algorithm For Suffix Stripping.” Program: electronic library\nand information systems 14.3 (1980): 130–137.\n[34] Padmanabhan, Arvind. “Lemmatization”. October 11, 2019.\n[35] Explosion.ai. “spaCy”. Last accessed June 15, 2020.\n[36] Polyglot documentation.Polyglot Python library. Last accessed June 15, 2020.\n[37] Mair, Victor. “Singlish: alive and well”. May 14, 2016.\n[38] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft), 2018.\n[39] Explosion.ai. “spaCy: Industrial-Strength Natural Language Processing in\nPython”. Last accessed June 15, 2020.\n[40] DeepAI. “Parsey Mcparseface API”. Last accessed June 15, 2020.\n[41] Stanford CoreNLP.Stanford CoreNLP – Natural language software. Last accessed\nJune 15, 2020.\n[42] Ghaffari, Parsa. “Leveraging Deep Learning for Multilingual Sentiment Analy‐\nsis”. July 14, 2016.\n78 \n| \nChapter 2: NLP Pipeline\n",
        "word_count": 263,
        "char_count": 1903,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 109,
        "text": "[43] The Stanford Natural Language Processing Group. “Stanford TokensRegex”. Last\naccessed June 15, 2020.\n[44] Google. “Cloud Natural Language”. Last accessed June 15, 2020.\n[45] Amazon. “AWS Comprehend”. Last accessed June 15, 2020.\n[46] Microsoft. “Azure Cognitive Services documentation”. Last accessed June 15,\n2020.\n[47] IBM. “Watson Natural Language Understanding”. Last accessed June 15, 2020.\n[48] Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The Elements of Statisti‐\ncal Learning, Second Edition. New York: Springer, 2001. ISBN: 978-0-387-84857-0\n[49] Wikipedia. “F1 score”. Last modified April 18, 2020.\n[50] Wikipedia. “Mean reciprocal rank”. Last modified December 6, 2018.\n[51] Wikipedia. “Evaluation measures (information retrieval)”. Last modified Febru‐\nary 12, 2020.\n[52] Wikipedia. “Mean absolute percentage error”. Last modified February 6, 2020.\n[53] Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. “BLEU: A\nMethod for Automatic Evaluation of Machine Translation.” Proceedings of the 40th\nAnnual Meeting on Association for Computational Linguistics (2002): 311–318.\n[54] Banerjee, Satanjeev and Alon Lavie. “METEOR: An Automatic Metric for MT\nEvaluation with Improved Correlation with Human Judgments.” Proceedings of the\nACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation\nand/or Summarization (2005): 65–72.\n[55] Lin, Chin-Yew. “ROUGE: A Package for Automatic Evaluation of Summaries.”\nText Summarization Branches Out (2004): 74–81.\n[56] Wikipedia. “Perplexity”. Last modified February 13, 2020.\n[57] Google Cloud. “Quickstart for Cloud Tasks queues”. Last accessed June 15, 2020.\n[58] Amazon. “Amazon Simple Queue Service”. Last accessed June 15, 2020.\n[59] Zheng, Huaixiu., Yi-Chia Wang, and Piero Molino. “COTA: Improving Uber\nCustomer Care with NLP & Machine Learning”. January 3, 2018.\nWrapping Up \n| \n79\n",
        "word_count": 262,
        "char_count": 1890,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 110,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 111,
        "text": "CHAPTER 3\nText Representation\nIn language processing,\nthe vectors x are derived from textual data,\nin order to reflect various linguistic properties of the text.\n—Yoav Goldberg\nFeature extraction is an important step for any machine learning problem. No matter\nhow good a modeling algorithm you use, if you feed in poor features, you will get\npoor results. In computer science, this is often called “garbage in, garbage out.” In the\nprevious two chapters, we saw an overview of NLP, the different tasks and challenges\ninvolved, and what a typical NLP pipeline looks like. In this chapter, we’ll address the\nquestion: how do we go about doing feature engineering for text data? In other\nwords, how do we transform a given text into numerical form so that it can be fed\ninto NLP and ML algorithms? In NLP parlance, this conversion of raw text to a suit‐\nable numerical form is called text representation. In this chapter, we’ll take a look at\nthe different methods for text representation, or representing text as a numeric vec‐\ntor. With respect to the larger picture for any NLP problem, the scope of this chapter\nis depicted by the dotted box in Figure 3-1.\nFigure 3-1. Scope of this chapter within the NLP pipeline\n81\n",
        "word_count": 211,
        "char_count": 1220,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (9.3pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1075,
            "height": 425,
            "ext": "png",
            "size_bytes": 23034
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 112,
        "text": "Feature representation is a common step in any ML project, whether the data is text,\nimages, videos, or speech. However, feature representation for text is often much\nmore involved as compared to other formats of data. To understand this, let’s look at a\nfew examples of how other data formats can be represented numerically. First, con‐\nsider the case of images. Say we want to build a classifier that can distinguish images\nof cats from images of dogs. Now, in order to train an ML model to accomplish this\ntask, we need to feed it (labeled) images. How do we feed images to an ML model?\nThe way an image is stored in a computer is in the form of a matrix of pixels where\neach cell[i,j] in the matrix represents pixel i,j of the image. The real value stored at\ncell[i,j] represents the intensity of the corresponding pixel in the image, as shown\nin Figure 3-2. This matrix representation accurately represents the complete image.\nVideo is similar: a video is just a collection of frames where each frame is an image.\nHence, any video can be represented as a sequential collection of matrices, one per\nframe, in the same order.\nFigure 3-2. How we see an image versus how computers see it [1]\nNow consider speech—it’s transmitted as a wave. To represent it mathematically, we\nsample the wave and record its amplitude (height), as shown in Figure 3-3.\nFigure 3-3. Sampling a speech wave\n82 \n| \nChapter 3: Text Representation\n",
        "word_count": 251,
        "char_count": 1424,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 601,
            "ext": "png",
            "size_bytes": 672183
          },
          {
            "index": 1,
            "width": 1390,
            "height": 408,
            "ext": "png",
            "size_bytes": 53779
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 113,
        "text": "This gives us a numerical array representing the amplitude of a sound wave at fixed\ntime intervals, as shown in Figure 3-4.\nFigure 3-4. Speech signal represented by a numerical vector\nFrom this discussion, it’s clear that mathematically representing images, video, and\nspeech is straightforward. What about text? It turns out that representing text is not\nstraightforward, hence a whole chapter focusing on various schemes to address this\nquestion. We’re given a piece of text, and we’re asked to find a scheme to represent it\nmathematically. In literature, this is called text representation. Text representation has\nbeen an active area of research in the past decades, especially the last one. In this\nchapter, we’ll start with simple approaches and go all the way to state-of-the-art tech‐\nniques for representing text. These approaches are classified into four categories:\n• Basic vectorization approaches\n• Distributed representations\n• Universal language representation\n• Handcrafted features\nThe rest of this chapter describes these categories one by one, covering various algo‐\nrithms in each. Before we delve deeper into various schemes, consider the following\nscenario: we’re given a labeled text corpus and asked to build a sentiment analysis\nmodel. To correctly predict the sentiment of a sentence, the model needs to under‐\nstand the meaning of the sentence. In order to correctly extract the meaning of the\nsentence, the most crucial data points are:\n1. Break the sentence into lexical units such as lexemes, words, and phrases\n2. Derive the meaning for each of the lexical units\n3. Understand the syntactic (grammatical) structure of the sentence\n4. Understand the context in which the sentence appears\nThe semantics (meaning) of the sentence arises from the combination of the above\npoints. Thus, any good text representation scheme must facilitate the extraction of\nthose data points in the best possible way to reflect the linguistic properties of the\ntext. Without this, a text representation scheme isn’t of much use.\nText Representation \n| \n83\n",
        "word_count": 324,
        "char_count": 2065,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1403,
            "height": 311,
            "ext": "png",
            "size_bytes": 35688
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 114,
        "text": "i. It is sometimes also referred to as the term vector model, but we’ll stick to the notation VSM.\nOften in NLP, feeding a good text representation to an ordinary\nalgorithm will get you much farther compared to applying a top-\nnotch algorithm to an ordinary text representation.\nLet’s take a look at a key concept that carries throughout this entire chapter: the vec‐\ntor space model.\nVector Space Models\nIt should be clear from the introduction that, in order for ML algorithms to work\nwith text data, the text data must be converted into some mathematical form.\nThroughout this chapter, we’ll represent text units (characters, phonemes, words,\nphrases, sentences, paragraphs, and documents) with vectors of numbers. This is\nknown as the vector space model (VSM).i It’s a simple algebraic model used extensively\nfor representing any text blob. VSM is fundamental to many information-retrieval\noperations, from scoring documents on a query to document classification and docu‐\nment clustering [2]. It’s a mathematical model that represents text units as vectors. In\nthe simplest form, these are vectors of identifiers, such as index numbers in a corpus\nvocabulary. In this setting, the most common way to calculate similarity between two\ntext blobs is using cosine similarity: the cosine of the angle between their\ncorresponding vectors. The cosine of 0° is 1 and the cosine of 180° is –1, with the\ncosine monotonically decreasing from 0° to 180°. Given two vectors, A and B, each\nwith n components, the similarity between them is computed as follows:\nsimilarity = cos θ =\nA · B\nA 2 B 2\n=\n∑\ni = 1\nn\nAiBi\n∑\ni = 1\nn\nAi\n2 ∑\ni = 1\nn\nBi\n2\nwhere Ai and Bi are the ith components of vectors A and B, respectively. Sometimes,\npeople also use Euclidean distance between vectors to capture similarity.\nAll the text representation schemes we’ll study in this chapter fall within the scope of\nvector space models. What differentiates one scheme from another is how well the\nresulting vector captures the linguistic properties of the text it represents. With this,\nwe’re ready to discuss various text representation schemes.\n84 \n| \nChapter 3: Text Representation\n",
        "word_count": 366,
        "char_count": 2150,
        "fonts": [
          "MinionPro-Regular (8.0pt)",
          "MinionPro-Bold (10.0pt)",
          "MinionPro-Regular (14.1pt)",
          "MinionPro-It (6.3pt)",
          "MinionPro-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (8.0pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Regular (6.3pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 115,
        "text": "ii. This mapping is arbitrary. Any other mapping works just as well.\nBasic Vectorization Approaches\nLet’s start with a basic idea of text representation: map each word in the vocabulary\n(V) of the text corpus to a unique ID (integer value), then represent each sentence or\ndocument in the corpus as a V-dimensional vector. How do we operationalize this\nidea? To understand this better, let’s take a toy corpus (shown in Table 3-1) with only\nfour documents—D1, D2, D3, D4 —as an example.\nTable 3-1. Our toy corpus\nD1 Dog bites man.\nD2 Man bites dog.\nD3 Dog eats meat.\nD4 Man eats food.\nLowercasing text and ignoring punctuation, the vocabulary of this corpus is com‐\nprised of six words: [dog, bites, man, eats, meat, food]. We can organize the vocabu‐\nlary in any order. In this example, we simply take the order in which the words\nappear in the corpus. Every document in this corpus can now be represented with a\nvector of size six. We’ll discuss multiple ways in which we can do this. We’ll assume\nthat the text is already pre-processed (lowercased, punctuation removed, etc.) and\ntokenized (text string split into tokens), following the pre-processing step in the NLP\npipeline described in Chapter 2. We’ll start with one-hot encoding.\nOne-Hot Encoding\nIn one-hot encoding, each word w in the corpus vocabulary is given a unique integer\nID wid that is between 1 and |V|, where V is the set of the corpus vocabulary. Each\nword is then represented by a V-dimensional binary vector of 0s and 1s. This is done\nvia a |V| dimension vector filled with all 0s barring the index, where index = wid. At\nthis index, we simply put a 1. The representation for individual words is then com‐\nbined to form a sentence representation.\nLet’s understand this via our toy corpus. We first map each of the six words to unique\nIDs: dog = 1, bites = 2, man = 3, meat = 4 , food = 5, eats = 6.ii Let’s consider the\ndocument D1: “dog bites man”. As per the scheme, each word is a six-dimensional\nvector. Dog is represented as [1 0 0 0 0 0], as the word “dog” is mapped to ID 1. Bites\nis represented as [0 1 0 0 0 0], and so on and so forth. Thus, D1 is represented as [ [1 0\n0 0 0 0] [0 1 0 0 0 0] [0 0 1 0 0 0]]. D4 is represented as [ [ 0 0 1 0 0] [0 0 0 0 1 0] [0 0 0\n0 0 1]]. Other documents in the corpus can be represented similarly.\nBasic Vectorization Approaches \n| \n85\n",
        "word_count": 452,
        "char_count": 2356,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MinionPro-Regular (8.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Regular (6.3pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 116,
        "text": "Let’s look at a simple way to implement this in Python from first principles. The note‐\nbook Ch3/OneHotEncoding.ipynb demonstrates an example of this. The code that fol‐\nlows is borrowed from the notebook and implements one-hot encoding. In real-\nworld projects, we mostly use scikit-learn’s implementation of one-hot encoding,\nwhich is much more optimized. We’ve provided the same in the notebook.\nSince we assume that the text is tokenized, we can just split the text on white space in\nthis example:\ndef get_onehot_vector(somestring):\n  onehot_encoded = []\n  for word in somestring.split():\n             temp = [0]*len(vocab)\n             if word in vocab:\n                        temp[vocab[word]-1] = 1\n             onehot_encoded.append(temp)\n  return onehot_encoded\nget_onehot_vector(processed_docs[1])\nOutput: [[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]\nNow that we understand the scheme, let’s discuss some of its pros and cons. On the\npositive side, one-hot encoding is intuitive to understand and straightforward to\nimplement. However, it suffers from a few shortcomings:\n• The size of a one-hot vector is directly proportional to size of the vocabulary, and\nmost real-world corpora have large vocabularies. This results in a sparse repre‐\nsentation where most of the entries in the vectors are zeroes, making it computa‐\ntionally inefficient to store, compute with, and learn from (sparsity leads to\noverfitting).\n• This representation does not give a fixed-length representation for text, i.e., if a\ntext has 10 words, you get a longer representation for it as compared to a text\nwith 5 words. For most learning algorithms, we need the feature vectors to be of\nthe same length.\n• It treats words as atomic units and has no notion of (dis)similarity between\nwords. For example, consider three words: run, ran, and apple. Run and ran have\nsimilar meanings as opposed to run and apple. But if we take their respective vec‐\ntors and compute Euclidean distance between them, they’re all equally apart ( 2).\nThus, semantically, they’re very poor at capturing the meaning of the word in\nrelation to other words.\n• Say we train a model using our toy corpus. At runtime, we get a sentence: “man\neats fruits.” The training data didn’t include “fruit” and there’s no way to represent\nit in our model. This is known as the out of vocabulary (OOV) problem. A\n86 \n| \nChapter 3: Text Representation\n",
        "word_count": 386,
        "char_count": 2413,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 117,
        "text": "one-hot encoding scheme cannot handle this. The only way is to retrain the\nmodel: start by expanding the vocabulary, give an ID to the new word, etc.\nThese days, one-hot encoding scheme is seldom used.\nSome of these shortcomings can be addressed by the bag-of-words approach\ndescribed next.\nBag of Words\nBag of words (BoW) is a classical text representation technique that has been used\ncommonly in NLP, especially in text classification problems (see Chapter 4). The key\nidea behind it is as follows: represent the text under consideration as a bag (collec‐\ntion) of words while ignoring the order and context. The basic intuition behind it is\nthat it assumes that the text belonging to a given class in the dataset is characterized\nby a unique set of words. If two text pieces have nearly the same words, then they\nbelong to the same bag (class). Thus, by analyzing the words present in a piece of text,\none can identify the class (bag) it belongs to.\nSimilar to one-hot encoding, BoW maps words to unique integer IDs between 1 and\n|V|. Each document in the corpus is then converted into a vector of |V| dimensions\nwhere in the ith component of the vector, i = wid, is simply the number of times the\nword w occurs in the document, i.e., we simply score each word in V by their occur‐\nrence count in the document.\nThus, for our toy corpus (Table 3-1), where the word IDs are dog = 1, bites = 2, man\n= 3, meat = 4 , food = 5, eats = 6, D1 becomes [1 1 1 0 0 0]. This is because the first\nthree words in the vocabulary appeared exactly once in D1, and the last three did not\nappear at all. D4 becomes [0 0 1 0 1 1]. The notebook Ch3/Bag_of_Words.ipynb dem‐\nonstrates how we can implement BoW text representation. The following code shows\nthe key parts:\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\n#Build a BOW representation for the corpus\nbow_rep = count_vect.fit_transform(processed_docs)\n#Look at the vocabulary mapping\nprint(\"Our vocabulary: \", count_vect.vocabulary_)\n#See the BOW rep for first 2 documents\nBasic Vectorization Approaches \n| \n87\n",
        "word_count": 365,
        "char_count": 2096,
        "fonts": [
          "MinionPro-It (6.3pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MinionPro-Regular (6.3pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 118,
        "text": "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\nprint(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n#Get the representation using this vocabulary, for a new text\ntemp = count_vect.transform([\"dog and dog are friends\"])\nprint(\"Bow representation for 'dog and dog are friends':\", \ntemp.toarray())\nIf we run this code, we’ll notice that the BoW representation for a sentence like “dog\nand dog are friends” has a value of 2 for the dimension of the word “dog,” indicating\nits frequency in the text. Sometimes, we don’t care about the frequency of occurrence\nof words in text and we only want to represent whether a word exists in the text or\nnot. Researchers have shown that such a representation without considering fre‐\nquency is useful for sentiment analysis (see Chapter 4 in [3]). In such cases, we just\ninitialize CountVectorizer with the binary=True option, as shown in the following\ncode:\ncount_vect = CountVectorizer(binary=True)\nbow_rep_bin = count_vect.fit_transform(processed_docs)\ntemp = count_vect.transform([\"dog and dog are friends\"])\nprint(\"Bow representation for 'dog and dog are friends':\", temp.toarray())\nThis results in a different representation for the same sentence. CountVectorizer\nsupports both word as well as character n-grams.\nLet’s look at some of the advantages of this encoding:\n• Like one-hot encoding, BoW is fairly simple to understand and implement.\n• With this representation, documents having the same words will have their vec‐\ntor representations closer to each other in Euclidean space as compared to docu‐\nments with completely different words. The distance between D1 and D2 is 0 as\ncompared to the distance between D1 and D4, which is 2. Thus, the vector space\nresulting from the BoW scheme captures the semantic similarity of documents.\nSo if two documents have similar vocabulary, they’ll be closer to each other in the\nvector space and vice versa.\n• We have a fixed-length encoding for any sentence of arbitrary length.\nHowever, it has its share of disadvantages, too:\n• The size of the vector increases with the size of the vocabulary. Thus, sparsity\ncontinues to be a problem. One way to control it is by limiting the vocabulary to\nn number of the most frequent words.\n• It does not capture the similarity between different words that mean the same\nthing. Say we have three documents: “I run”, “I ran”, and “I ate”. BoW vectors of\nall three documents will be equally apart.\n88 \n| \nChapter 3: Text Representation\n",
        "word_count": 395,
        "char_count": 2500,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MinionPro-Regular (6.3pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 119,
        "text": "• This representation does not have any way to handle out of vocabulary words\n(i.e., new words that were not seen in the corpus that was used to build the vec‐\ntorizer).\n• As the name indicates, it is a “bag” of words—word order information is lost in\nthis representation. Both D1 and D2 will have the same representation in this\nscheme.\nHowever, despite these shortcomings, due to its simplicity and ease of implementa‐\ntion, BoW is a commonly used text representation scheme, especially for text classifi‐\ncation among other NLP problems.\nBag of N-Grams\nAll the representation schemes we’ve seen so far treat words as independent units.\nThere is no notion of phrases or word ordering. The bag-of-n-grams (BoN) approach\ntries to remedy this. It does so by breaking text into chunks of n contiguous words (or\ntokens). This can help us capture some context, which earlier approaches could not\ndo. Each chunk is called an n-gram. The corpus vocabulary, V, is then nothing but a\ncollection of all unique n-grams across the text corpus. Then, each document in the\ncorpus is represented by a vector of length |V|. This vector simply contains the fre‐\nquency counts of n-grams present in the document and zero for the n-grams that are\nnot present.\nTo elaborate, let’s consider our example corpus. Let’s construct a 2-gram (a.k.a.\nbigram) model for it. The set of all bigrams in the corpus is as follows: {dog bites,\nbites man, man bites, bites dog, dog eats, eats meat, man eats, eats food}. Then, BoN\nrepresentation consists of an eight-dimensional vector for each document. The\nbigram representation for the first two documents is as follows: D1 : [1,1,0,0,0,0,0,0],\nD2 : [0,0,1,1,0,0,0,0]. The other two documents follow similarly. Note that the BoW\nscheme is a special case of the BoN scheme, with n=1. n=2 is called a “bigram model,”\nand n=3 is called a “trigram model.” Further, note that, by increasing the value of n,\nwe can incorporate larger context; however, this further increases the sparsity. In NLP\nparlance, the BoN scheme is also called “n-gram feature selection.”\nThe following code (Ch3/Bag_of_N_Grams.ipynb) shows an example of a BoN repre‐\nsentation considering 1–3 n-gram word features to represent the corpus that we’ve\nused so far. Here, we use unigram, bigram, and trigram vectors by setting\nngram_range = (1,3):\n#n-gram vectorization example with count vectorizer and uni, bi, trigrams\ncount_vect = CountVectorizer(ngram_range=(1,3))\n#Build a BOW representation for the corpus\nbow_rep = count_vect.fit_transform(processed_docs)\nBasic Vectorization Approaches \n| \n89\n",
        "word_count": 417,
        "char_count": 2586,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MinionPro-Regular (6.3pt)",
          "UbuntuMono-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 120,
        "text": "#Look at the vocabulary mapping\nprint(\"Our vocabulary: \", count_vect.vocabulary_)\n#Get the representation using this vocabulary, for a new text\ntemp = count_vect.transform([\"dog and dog are friends\"])\nprint(\"Bow representation for 'dog and dog are friends':\", temp.toarray())\nHere are the main pros and cons of BoN:\n• It captures some context and word-order information in the form of n-grams.\n• Thus, resulting vector space is able to capture some semantic similarity. Docu‐\nments having the same n-grams will have their vectors closer to each other in\nEuclidean space as compared to documents with completely different n-grams.\n• As n increases, dimensionality (and therefore sparsity) only increases rapidly.\n• It still provides no way to address the OOV problem.\nTF-IDF\nIn all the three approaches we’ve seen so far, all the words in the text are treated as\nequally important—there’s no notion of some words in the document being more\nimportant than others. TF-IDF, or term frequency–inverse document frequency,\naddresses this issue. It aims to quantify the importance of a given word relative to\nother words in the document and in the corpus. It’s a commonly used representation\nscheme for information-retrieval systems, for extracting relevant documents from a\ncorpus for a given text query.\nThe intuition behind TF-IDF is as follows: if a word w appears many times in a docu‐\nment di but does not occur much in the rest of the documents dj in the corpus, then\nthe word w must be of great importance to the document di. The importance of w\nshould increase in proportion to its frequency in di, but at the same time, its impor‐\ntance should decrease in proportion to the word’s frequency in other documents dj in\nthe corpus. Mathematically, this is captured using two quantities: TF and IDF. The\ntwo are then combined to arrive at the TF-IDF score.\nTF (term frequency) measures how often a term or word occurs in a given document.\nSince different documents in the corpus may be of different lengths, a term may\noccur more often in a longer document as compared to a shorter document. To nor‐\nmalize these counts, we divide the number of occurrences by the length of the docu‐\nment. TF of a term t in a document d is defined as:\nTF t, d = Number of occurrences of term t in document d\nTotal number of terms in the document d\n90 \n| \nChapter 3: Text Representation\n",
        "word_count": 404,
        "char_count": 2367,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MinionPro-Regular (6.3pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 121,
        "text": "IDF (inverse document frequency) measures the importance of the term across a cor‐\npus. In computing TF, all terms are given equal importance (weightage). However, it’s\na well-known fact that stop words like is, are, am, etc., are not important, even though\nthey occur frequently. To account for such cases, IDF weighs down the terms that are\nvery common across a corpus and weighs up the rare terms. IDF of a term t is calcu‐\nlated as follows:\nIDF t = loge\nTotal number of documents in the corpus\nNumber of documents with term t in them \nThe TF-IDF score is a product of these two terms. Thus, TF-IDF score = TF * IDF.\nLet’s compute TF-IDF scores for our toy corpus. Some terms appear in only one\ndocument, some appear in two, while others appear in three documents. The size of\nour corpus is N=4. Hence, corresponding TF-IDF values for each term are shown in\nTable 3-2.\nTable 3-2. TF-IDF values for our toy corpus\nWord TF score\nIDF score\nTF-IDF score\ndog\n⅓ = 0.33\nlog2(4/3) = 0.4114 0.4114 * 0.33 = 0.136\nbites\n⅙ = 0.17\nlog2(4/2) = 1\n1* 0.17 = 0.17\nman\n0.33\nlog2(4/3) =0.4114\n0.4114 * 0.33 = 0.136\neats\n0.17\nlog2(4/2) =1\n1* 0.17 = 0.17\nmeat\n1/12 = 0.083 log2(4/1) =2\n2* 0.083 = 0.17\nfood\n0.083\nlog2(4/1) =2\n2* 0.083 = 0.17\nThe TF-IDF vector representation for a document is then simply the TF-IDF score\nfor each term in that document. So, for D1 we get\nDog\nbites\nman\neats\nmeat\nfood\n0.136 0.17\n0.136 0\n0\n0\nThe following code (Ch3/TF_IDF.ipynb) shows how to use TF-IDF to represent text:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\nbow_rep_tfidf = tfidf.fit_transform(processed_docs)\nprint(tfidf.idf_) #IDF for all words in the vocabulary\nprint(tfidf.get_feature_names()) #All words in the vocabulary.\ntemp = tfidf.transform([\"dog and man are friends\"])\nprint(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())\nBasic Vectorization Approaches \n| \n91\n",
        "word_count": 324,
        "char_count": 1913,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "MyriadPro-Cond (6.3pt)",
          "MinionPro-It (8.0pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MinionPro-Regular (6.3pt)",
          "ArialUnicodeMS (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 122,
        "text": "There are several variations of the basic TF-IDF formula that are used in practice.\nNotice that the TF-IDF scores that we calculated for our corpus in Table 3-2 might\nnot match the TF-IDF scores given by scikit-learn. This is because scikit-learn uses a\nslightly modified version of the IDF formula. This stems from provisions to account\nfor possible zero divisions and to not entirely ignore terms that appear in all docu‐\nments. An interested reader can look into the TF-IDF vectorizer documentation [4]\nfor the exact formula.\nSimilar to BoW, we can use the TF-IDF vectors to calculate similarity between two\ntexts using a similarity measure like Euclidean distance or cosine similarity. TF-IDF is\na commonly used representation in application scenarios such as information\nretrieval and text classification. However, despite the fact that TF-IDF is better than\nthe vectorization methods we saw earlier in terms of capturing similarities between\nwords, it still suffers from the curse of high dimensionality.\nEven today, TF-IDF continues to be a popular representation\nscheme for many NLP tasks, especially the initial versions of the\nsolution.\nIf we look back at all the representation schemes we’ve discussed so far, we notice\nthree fundamental drawbacks:\n• They’re discrete representations—i.e., they treat language units (words, n-grams,\netc.) as atomic units. This discreteness hampers their ability to capture relation‐\nships between words.\n• The feature vectors are sparse and high-dimensional representations. The dimen‐\nsionality increases with the size of the vocabulary, with most values being zero\nfor any vector. This hampers learning capability. Further, high-dimensionality\nrepresentation makes them computationally inefficient.\n• They cannot handle OOV words.\nWith this, we come to the end of basic vectorization approaches. Now, let’s start look‐\ning at distributed representations.\nDistributed Representations\nIn the previous section, we saw some key drawbacks that are common to all basic vec‐\ntorization approaches. To overcome these limitations, methods to learn low-\ndimensional representations were devised. These methods, covered in this section,\ngained momentum in the past six to seven years. They use neural network\n92 \n| \nChapter 3: Text Representation\n",
        "word_count": 343,
        "char_count": 2283,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 123,
        "text": "architectures to create dense, low-dimensional representations of words and texts.\nBut before we look into these methods, we need to understand some key terms:\nDistributional similarity\nThis is the idea that the meaning of a word can be understood from the context\nin which the word appears. This is also known as connotation: meaning is defined\nby context. This is opposed to denotation: the literal meaning of any word. For\nexample: “NLP rocks.” The literal meaning of the word “rocks” is “stones,” but\nfrom the context, it’s used to refer to something good and fashionable.\nDistributional hypothesis [5]\nIn linguistics, this hypothesizes that words that occur in similar contexts have\nsimilar meanings. For example, the English words “dog” and “cat” occur in simi‐\nlar contexts. Thus, according to the distributional hypothesis, there must be a\nstrong similarity between the meanings of these two words. Now, following from\nVSM, the meaning of a word is represented by the vector. Thus, if two words\noften occur in similar context, then their corresponding representation vectors\nmust also be close to each other.\nDistributional representation [6]\nThis refers to representation schemes that are obtained based on distribution of\nwords from the context in which the words appear. These schemes are based on\ndistributional hypotheses. The distributional property is induced from context\n(textual vicinity). Mathematically, distributional representation schemes use\nhigh-dimensional vectors to represent words. These vectors are obtained from a\nco-occurrence matrix that captures co-occurrence of word and context. The\ndimension of this matrix is equal to the size of the vocabulary of the corpus. The\nfour schemes that we’ve seen so far—one-hot, bag of words, bag of n-grams, and\nTF-IDF—all fall under the umbrella of distributional representation.\nDistributed representation [6]\nThis is a related concept. It, too, is based on the distributional hypothesis. As dis‐\ncussed in the previous paragraph, the vectors in distributional representation are\nvery high dimensional and sparse. This makes them computationally inefficient\nand hampers learning. To alleviate this, distributed representation schemes sig‐\nnificantly compress the dimensionality. This results in vectors that are compact\n(i.e., low dimensional) and dense (i.e., hardly any zeros). The resulting vector\nspace is known as distributed representation. All the subsequent schemes we’ll dis‐\ncuss in this chapter are examples of distributed representation.\nEmbedding\nFor the set of words in a corpus, embedding is a mapping between vector space\ncoming from distributional representation to vector space coming from dis‐\ntributed representation.\nDistributed Representations \n| \n93\n",
        "word_count": 408,
        "char_count": 2744,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 124,
        "text": "Vector semantics\nThis refers to the set of NLP methods that aim to learn the word representations\nbased on distributional properties of words in a large corpus.\nNow that you have a basic understanding of these terms, we can move on to our first\nmethod: word embeddings.\nWord Embeddings\nWhat does it mean when we say a text representation should capture “distributional\nsimilarities between words”? Let’s consider some examples. If we’re given the word\n“USA,” distributionally similar words could be other countries (e.g., Canada, Ger‐\nmany, India, etc.) or cities in the USA. If we’re given the word “beautiful,” words that\nshare some relationship with this word (e.g., synonyms, antonyms) could be consid‐\nered distributionally similar words. These are words that are likely to occur in similar\ncontexts. In 2013, a seminal work by Mikolov et al. [7] showed that their neural net‐\nwork–based word representation model known as “Word2vec,” based on “distribu‐\ntional similarity,” can capture word analogy relationships such as:\nKing – Man + Woman ≈ Queen\nTheir model was able to correctly answer many more analogies like this. Figure 3-5\nshows a snapshot of a system based on Word2vec answering analogies. The\nWord2vec model is in many ways the dawn of modern-day NLP.\nWhile learning such semantically rich relationships, Word2vec ensures that the\nlearned word representations are low dimensional (vectors of dimensions 50–500,\ninstead of several thousands, as with previously studied representations in this chap‐\nter) and dense (that is, most values in these vectors are non-zero). Such representa‐\ntions make ML tasks more tractable and efficient. Word2vec led to a lot of work (both\npure and applied) in the direction of learning text representations using neural\nnetworks. These representations are also called “embeddings.” Let’s build an intuition\nof how they work and how to use them to represent text.\nGiven a text corpus, the aim is to learn embeddings for every word in the corpus such\nthat the word vector in the embedding space best captures the meaning of the word.\nTo “derive” the meaning of the word, Word2vec uses distributional similarity and dis‐\ntributional hypothesis. That is, it derives the meaning of a word from its context:\nwords that appear in its neighborhood in the text. So, if two different words (often)\noccur in similar context, then it’s highly likely that their meanings are also similar.\nWord2vec operationalizes this by projecting the meaning of the words in a vector\nspace where words with similar meanings will tend to cluster together, and words\nwith very different meanings are far from one another.\n94 \n| \nChapter 3: Text Representation\n",
        "word_count": 432,
        "char_count": 2679,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 125,
        "text": "Figure 3-5. Word2vec-based analogy-answering system\nConceptually, Word2vec takes a large corpus of text as input and “learns” to represent\nthe words in a common vector space based on the contexts in which they appear in\nthe corpus. Given a word w and the words appearing in its context C, how do we find\nthe vector that best represents the meaning of the word? For every word w in corpus,\nwe start with a vector vw initialized with random values. The Word2vec model refines\nthe values in vw by predicting vw, given the vectors for words in the context C. It does\nthis using a two-layer neural network. We’ll dive deeper into this by discussing pre-\ntrained embeddings before moving on to train our own.\nPre-trained word embeddings\nTraining your own word embeddings is a pretty expensive process (in terms of both\ntime and computing). Thankfully, for many scenarios, it’s not necessary to train your\nown embeddings, and using pre-trained word embeddings often suffices. What are\npre-trained word embeddings? Someone has done the hard work of training word\nembeddings on a large corpus, such as Wikipedia, news articles, or even the entire\nweb, and has put words and their corresponding vectors on the web. These embed‐\ndings can be downloaded and used to get the vectors for the words you want. Such\nembeddings can be thought of as a large collection of key-value pairs, where keys are\nthe words in the vocabulary and values are their corresponding word vectors. Some\nof the most popular pre-trained embeddings are Word2vec by Google [8], GloVe by\nStanford [9], and fasttext embeddings by Facebook [10], to name a few. Further,\nthey’re available for various dimensions like d = 25, 50, 100, 200, 300, 600.\nDistributed Representations \n| \n95\n",
        "word_count": 293,
        "char_count": 1740,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Regular (6.3pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 629,
            "height": 347,
            "ext": "png",
            "size_bytes": 19310
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 126,
        "text": "Ch3/Pre_Trained_Word_Embeddings.ipynb, the notebook associated with the rest of\nthis section, shows an example of how to load pre-trained Word2vec embeddings and\nlook for the most similar words (ranked by cosine similarity) to a given word. The\ncode that follows covers the key steps. Here, we find the words that are semantically\nmost similar to the word “beautiful”; the last line returns the embedding vector of the\nword “beautiful”:\nfrom gensim.models import Word2Vec, KeyedVectors\npretrainedpath = \"NLPBookTut/GoogleNews-vectors-negative300.bin\"\nw2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True)\nprint('done loading Word2Vec')\nprint(len(w2v_model.vocab)) #Number of words in the vocabulary.\nprint(w2v_model.most_similar['beautiful'])\nW2v_model['beautiful']\nmost_similar('beautiful') returns the most similar words to the word “beautiful.”\nThe output is shown below. Each word is accompanied by a similarity score. The\nhigher the score, the more similar the word is to the query word:\n[('gorgeous', 0.8353004455566406),\n ('lovely', 0.810693621635437),\n ('stunningly_beautiful', 0.7329413890838623),\n ('breathtakingly_beautiful', 0.7231341004371643),\n ('wonderful', 0.6854087114334106),\n ('fabulous', 0.6700063943862915),\n ('loveliest', 0.6612576246261597),\n ('prettiest', 0.6595001816749573),\n ('beatiful', 0.6593326330184937),\n ('magnificent', 0.6591402292251587)]\nw2v_model returns the vector for the query word. For the word “beautiful,” we get the\nvector as shown in Figure 3-6.\nNote that if we search for a word that is not present in the Word2vec model (e.g.,\n“practicalnlp”), we’ll see a “key not found” error. Hence, as a good coding practice, it’s\nalways advised to first check if the word is present in the model’s vocabulary before\nattempting to retrieve its vector. The Python library we used in this code snippet,\ngensim, also supports training and loading GloVe pre-trained models.\nIf you’re new to embeddings, always start by using pre-trained\nword embeddings in your project. Understand their pros and cons,\nthen start thinking of building your own embeddings. Using pre-\ntrained embeddings will quickly give you a strong baseline for the\ntask at hand.\n96 \n| \nChapter 3: Text Representation\n",
        "word_count": 290,
        "char_count": 2236,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "MinionPro-Regular (9.6pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 127,
        "text": "Figure 3-6. Vector representing the word “beautiful” in pre-trained Word2vec\nNow let’s look at training our own word embeddings.\nDistributed Representations \n| \n97\n",
        "word_count": 23,
        "char_count": 164,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 667,
            "height": 851,
            "ext": "png",
            "size_bytes": 94171
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 128,
        "text": "Training our own embeddings\nNow we’ll focus on training our own word embeddings. For this, we’ll look at two\narchitectural variants that were proposed in the original Word2vec approach. The\ntwo variants are:\n• Continuous bag of words (CBOW)\n• SkipGram\nBoth of these have a lot of similarities in many respects. We’ll begin by understanding\nthe CBOW model, then we’ll look at SkipGram. Throughout this section, we’ll use the\nsentence “The quick brown fox jumps over the lazy dog” as our toy corpus.\nCBOW.    In CBOW, the primary task is to build a language model that correctly pre‐\ndicts the center word given the context words in which the center word appears.\nWhat is a language model? It is a (statistical) model that tries to give a probability dis‐\ntribution over sequences of words. Given a sentence of, say, m words, it assigns a\nprobability Pr(w1, w2, ….., wn) to the whole sentence. The objective of a language\nmodel is to assign probabilities in such a way that it gives high probability to “good”\nsentences and low probabilities to “bad” sentences. By good, we mean sentences that\nare semantically and syntactically correct. By bad, we mean sentences that are incor‐\nrect—semantically or syntactically or both. So, for a sentence like “The cat jumped\nover the dog,” it will try to assign a probability close to 1.0, whereas for a sentence like\n“jumped over the the cat dog,” it tries to assign a probability close to 0.0.\nCBOW tries to learn a language model that tries to predict the “center” word from the\nwords in its context. Let’s understand this using our toy corpus. If we take the word\n“jumps” as the center word, then its context is formed by words in its vicinity. If we\ntake the context size of 2, then for our example, the context is given by brown, fox,\nover, the. CBOW uses the context words to predict the target word—jumps—as\nshown in Figure 3-7. CBOW tries to do this for every word in the corpus; i.e., it takes\nevery word in the corpus as the target word and tries to predict the target word from\nits corresponding context words.\nFigure 3-7. CBOW: given the context words, predict the center word\n98 \n| \nChapter 3: Text Representation\n",
        "word_count": 381,
        "char_count": 2165,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (11.5pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Regular (6.3pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1439,
            "height": 242,
            "ext": "png",
            "size_bytes": 37230
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 129,
        "text": "The idea discussed in the previous paragraph is then extended to the entire corpus to\nbuild the training set. Details are as follows: we run a sliding window of size 2k+1\nover the text corpus. For our example, we took k as 2. Each position of the window\nmarks the set of 2k+1 words that are under consideration. The center word in the\nwindow is the target, and k words on either side of the center word form the context.\nThis gives us one data point. If the point is represented as (X,Y), then the context is\nthe X and the target word is the Y. A single data point consists of a pair of numbers:\n(2k indices of words in context, index of word in target). To get the next data point,\nwe simply shift the window to the right on the corpus by one word and repeat the\nprocess. This way, we slide the window across the entire corpus to create the training\nset. This is shown in Figure 3-8. Here, the target word is shown in blue and k=2.\nFigure 3-8. Preparing a dataset for CBOW\nNow that we have the training data ready, let’s focus on the model. For this, we con‐\nstruct a shallow net (it’s shallow since it has a single hidden layer), as shown in\nFigure 3-9. We assume we want to learn D-dim word embeddings. Further, let V be\nthe vocabulary of the text corpus.\nDistributed Representations \n| \n99\n",
        "word_count": 247,
        "char_count": 1294,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1418,
            "height": 495,
            "ext": "png",
            "size_bytes": 42237
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 130,
        "text": "iii. Technically speaking, both E and E’ are two different learned embeddings. You can use either of them or\neven combine the two by simply averaging them.\nFigure 3-9. CBOW model [7]\nThe objective is to learn an embedding matrix E|V| x d.To begin with, we initialize the\nmatrix randomly. Here, |V| is the size of corpus vocabulary and d is the dimension of\nthe embedding. Let’s break down the shallow net in Figure 3-9 layer by layer. In the\ninput layer, indices of the words in context are used to fetch the corresponding rows\nfrom the embedding matrix E|V| x d. The vectors fetched are then added to get a single\nD-dim vector, and this is passed to the next layer. The next layer simply takes this d\nvector and multiplies it with another matrix E’d x |V|.. This gives a 1 x |V| vector, which\nis fed to a softmax function to get probability distribution over the vocabulary space.\nThis distribution is compared with the label and uses back propagation to update\nboth the matrices E and E’ accordingly. At the end of the training, E is the embedding\nmatrixiii we wanted to learn.\n100 \n| \nChapter 3: Text Representation\n",
        "word_count": 202,
        "char_count": 1119,
        "fonts": [
          "MinionPro-Regular (8.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Regular (6.3pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1395,
            "height": 1018,
            "ext": "png",
            "size_bytes": 82137
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 131,
        "text": "SkipGram.    SkipGram is very similar to CBOW, with some minor changes. In Skip‐\nGram, the task is to predict the context words from the center word. For our toy cor‐\npus with context size 2, using the center word “jumps,” we try to predict every word\nin context—“brown,” “fox,” “over,” “the”—as shown in Figure 3-10. This constitutes\none step. SkipGram repeats this one step for every word in the corpus as the center\nword.\nFigure 3-10. SkipGram: given the center word, predict every word in context\nThe dataset to train a SkipGram is prepared as follows: we run a sliding window of\nsize 2k+1 over the text corpus to get the set of 2k+1 words that are under considera‐\ntion. The center word in the window is the X, and k words on either side of the center\nword are Y. Unlike CBOW, this gives us 2k data points. A single data point consists of\na pair: (index of the center word, index of a target word). We then shift the window to\nthe right on the corpus by one word and repeat the process. This way, we slide the\nwindow across the entire corpus to create the training set. This is shown in\nFigure 3-11.\nFigure 3-11. Preparing a dataset for SkipGram\nDistributed Representations \n| \n101\n",
        "word_count": 217,
        "char_count": 1187,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (11.5pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 241,
            "ext": "png",
            "size_bytes": 35164
          },
          {
            "index": 1,
            "width": 1214,
            "height": 822,
            "ext": "png",
            "size_bytes": 52289
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 132,
        "text": "The shallow network used to train the SkipGram model, shown in Figure 3-12, is\nvery similar to the network used for CBOW, with some minor changes. In the input\nlayer, the index of the word in the target is used to fetch the corresponding row from\nthe embedding matrix E|V| x d. The vectors fetched are then passed to the next layer.\nThe next layer simply takes this d vector and multiplies it with another matrix E’d x |V|.\nThis gives a 1 x |V| vector, which is fed to a softmax function to get probability distri‐\nbution over the vocabulary space. This distribution is compared with the label and\nuses back propagation to update both the matrices E and E’ accordingly. At the end of\nthe training, E is the embedding matrix we wanted to learn.\nFigure 3-12. SkipGram architecture [7]\nThere are a lot of other minute details that go into both CBOW and the SkipGram\nmodel. An interested reader can look at the three-part blog post by Sebastian Ruder\n[11]. You can also refer to Rong (2016) [12] for a step-by-step derivation of Word2vec\nparameter learning. Another key aspect to keep in mind is hyperparameters of the\nmodel. There are several hyperparameters: window size, dimensionality of the vectors\nto be learned, learning rate, number of epochs, etc. It’s a well-established fact that\nhyperparameters play a crucial role in the quality of the final model [13, 14].\n102 \n| \nChapter 3: Text Representation\n",
        "word_count": 245,
        "char_count": 1406,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (6.3pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1336,
            "height": 1120,
            "ext": "png",
            "size_bytes": 80199
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 133,
        "text": "To use both the CBOW and SkipGram algorithms in practice, there are several avail‐\nable implementations that abstract the mathematical details for us. One of the most\ncommonly used implementations is gensim [15].\nDespite the availability of several off-the-shelf implementations, we still have to make\ndecisions on several hyperparameters (i.e., the variables that need to be set before\nstarting the training process). Let’s look at two examples.\nDimensionality of the word vectors\nAs the name indicates, this decides the space of the learned embeddings. While\nthere is no ideal number, it’s common to construct word vectors with dimensions\nin the range of 50–500 and evaluate them on the task we’re using them for to\nchoose the best option.\nContext window\nHow long or short the context we look for to learn the vector representation is.\nThere are also other choices we make, such as whether to use CBOW or SkipGram to\nlearn the embeddings. These choices are more of an art than science at this point, and\nthere’s a lot of ongoing research on methods for choosing the right hyperparameters.\nUsing packages like gensim, it’s pretty straightforward from a code point of view to\nimplement Word2vec. The following code shows how to train our own Word2vec\nmodel using a toy corpus called common_texts that’s available in gensim. Assuming\nyou have the corpus for your domain, following this code snippet will quickly give\nyou your own embeddings:\n#Import a test data set provided in gensim to train a model\nfrom gensim.test.utils import common_texts\n#Build the model, by selecting the parameters.\nour_model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4)\n#Save the model\nour_model.save(\"tempmodel.w2v\")\n#Inspect the model by looking for the most similar words for a test word.\nprint(our_model.wv.most_similar('computer', topn=5))\n#Let us see what the 10-dimensional vector for 'computer' looks like.\nprint(our_model['computer'])\nNow, we can get the vector representation for any word in our corpus, provided it’s in\nthe model’s vocabulary—we just look up the word in the model. But what if we have a\nphrase (e.g., “word embeddings”) for which we need a vector?\nGoing Beyond Words\nSo far, we’ve seen examples of how to use pre-trained word embeddings and train our\nown word embeddings. This gives us a compact and dense representation for words\nin our vocabulary. However, in most NLP applications, we seldom deal with atomic\nunits like words—we deal with sentences, paragraphs, or even full texts. So, we need a\nDistributed Representations \n| \n103\n",
        "word_count": 404,
        "char_count": 2562,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 134,
        "text": "way to represent larger units of text. Is there a way we can use word embeddings to\nget feature representations for larger units of text?\nA simple approach is to break the text into constituent words, take the embeddings\nfor individual words, and combine them to form the representation for the text.\nThere are various ways to combine them, the most popular being sum, average, etc.,\nbut these may not capture many aspects of the text as a whole, such as ordering. Sur‐\nprisingly, they work very well in practice (see Chapter 4). As a matter of fact, in\nCBOW, this was demonstrated by taking the sum of word vectors in context. The\nresulting vector represents the entire context and is used to predict the center word.\nIt’s always a good idea to experiment with this before moving to other representa‐\ntions. The following code shows how to obtain the vector representation for text by\naveraging word vectors using the library spaCy [16]:\nimport spacy\nimport en_core_web_sm\n# Load the spacy model. This takes a few seconds.\nnlp = en_core_web_sm.load()\n# Process a sentence using the model\ndoc = nlp(\"Canada is a large country\")\n#Get a vector for individual words\n#print(doc[0].vector) #vector for 'Canada', the first word in the text\nprint(doc.vector) #Averaged vector for the entire sentence\nBoth pre-trained and self-trained word embeddings depend on the vocabulary they\nsee in the training data. However, there is no guarantee that we will only see those\nwords in the production data for the application we’re building. Despite the ease of\nusing Word2vec or any such word embedding to do feature extraction from texts, we\ndon’t have a good way of handling OOV words yet. This has been a recurring prob‐\nlem in all the representations we’ve seen so far. What do we do in such cases?\nA simple approach that often works is to exclude those words from the feature extrac‐\ntion process so we don’t have to worry about how to get their representations. If we’re\nusing a model trained on a large corpus, we shouldn’t see too many OOV words any‐\nway. However, if a large fraction of the words from our production data isn’t present\nin the word embedding’s vocabulary, we’re unlikely to see good performance. This\nvocabulary overlap is a great heuristic to gauge the performance of an NLP model.\nIf the overlap between corpus vocabulary and embedding vocabu‐\nlary is less than 80%, we’re unlikely to see good performance from\nour NLP model.\n104 \n| \nChapter 3: Text Representation\n",
        "word_count": 422,
        "char_count": 2474,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "MinionPro-Regular (9.6pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 135,
        "text": "Even if the overlap is above 80%, the model can still do poorly depending on which\nwords fall in the 20%. If these words are important for the task, then this is very pos‐\nsible. For example, say we want to build a classifier that can classify medical docu‐\nments on cancer from medical documents on the heart. Now, in this case, certain\nterms like heart, cancer, etc., will become important for differentiating the two sets of\ndocuments. If these terms are not present in the word embedding’s vocabulary, our\nclassifier might still do poorly.\nAnother way to deal with the OOV problem for word embeddings is to create vectors\nthat are initialized randomly, where each component is between –0.25 to +0.25, and\ncontinue to use these vectors throughout the application we’re building [17, 18].\nFrom our own experience, this can give us a jump of 1–2% in performance.\nThere are also other approaches that handle the OOV problem by modifying the\ntraining process by bringing in characters and other subword-level linguistic compo‐\nnents. Let’s look at one such approach now. The key idea is that one can potentially\nhandle the OOV problem by using subword information, such as morphological\nproperties (e.g., prefixes, suffixes, word endings, etc.), or by using character represen‐\ntations. fastText [19], from Facebook AI research, is one of the popular algorithms\nthat follows this approach. A word can be represented by its constituent character n-\ngrams. Following a similar architecture to Word2vec, fastText learns embeddings for\nwords and character n-grams together and views a word’s embedding vector as an\naggregation of its constituent character n-grams. This makes it possible to generate\nembeddings even for words that are not present in the vocabulary. Say there’s a word,\n“gregarious,” that’s not found in the embedding’s word vocabulary. We break it into\ncharacter n-grams—gre, reg, ega, ….ous—and combine these embeddings of the n-\ngrams to arrive at the embedding of “gregarious.”\nPre-trained fastText models can be downloaded from their website [20], and gensim’s\nfastText wrapper can be used both for loading pre-trained models or training models\nusing fastText in a way similar to Word2vec. We leave that as an exercise for the\nreader. In Chapter 4, we’ll see how to use fastText embeddings for text classification.\nNow, we’ll take a look at some distributed representations that move beyond words.\nDistributed Representations Beyond Words\nand Characters\nSo far, we’ve seen two approaches to coming up with text representations using\nembeddings. Word2vec learned representations for words, and we aggregated them\nto form text representations. fastText learned representations for character n-grams,\nwhich were aggregated to form word representations and then text representations. A\npotential problem with both approaches is that they do not take the context of words\ninto account. Take, for example, the sentences “dog bites man” and “man bites dog.”\nDistributed Representations Beyond Words and Characters \n| \n105\n",
        "word_count": 478,
        "char_count": 3031,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 136,
        "text": "Both receive the same representation in these approaches, but they obviously have\nvery different meanings. Let’s look at another approach, Doc2vec, which allows us to\ndirectly learn the representations for texts of arbitrary lengths (phrases, sentences,\nparagraphs, and documents) by taking the context of words in the text into account.\nDoc2vec is based on the paragraph vectors framework [21] and is implemented in\ngensim. This is similar to Word2vec in terms of its general architecture, except that,\nin addition to the word vectors, it also learns a “paragraph vector” that learns a\nrepresentation for the full text (i.e., with words in context). When learning with a\nlarge corpus of many texts, the paragraph vectors are unique for a given text (where\n“text” can mean any piece of text of arbitrary length), while word vectors will be\nshared across all texts. The shallow neural networks used to learn Doc2vec embed‐\ndings (Figure 3-13) are very similar to the CBOW and SkipGram architecture of\nWord2vec. The two architectures are called distributed memory (DM) and distributed\nbag of words (DBOW). They are shown in Figure 3-13.\nOnce the Doc2vec model is trained, paragraph vectors for new texts are inferred\nusing the common word vectors from training. Doc2vec was perhaps the first widely\naccessible implementation for getting an embedding representation for the full text\ninstead of using a combination of individual word vectors. Since it models some form\nof context and can encode texts of arbitrary length into a fixed, low-dimensional,\ndense vector, it has found application in a wide range of NLP applications, such as\ntext classification, document tagging, text recommendation systems, and simple chat‐\nbots for FAQs. We’ll see an example of training a Doc2vec representation and using it\nfor text classification in the next chapter. Let’s look at other text representation meth‐\nods that extend this idea of taking full text into account.\nFigure 3-13. Doc2vec architectures: DM (left) and DBOW (right)\n106 \n| \nChapter 3: Text Representation\n",
        "word_count": 327,
        "char_count": 2057,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1419,
            "height": 498,
            "ext": "png",
            "size_bytes": 48140
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 137,
        "text": "Universal Text Representations\nIn all the representations we’ve seen so far, we notice that one word gets one fixed\nrepresentation. Can this be a problem? Well, to some extent, yes. Words can mean\ndifferent things in different contexts. For example, the sentences “I went to a bank to\nwithdraw money” and “I sat by the river bank and pondered about text representa‐\ntions” both use the word “bank.” However, they mean different things in each\nsentence. With the vectorization and embedding approaches that we’ve seen so far,\nthere’s no direct way to capture this information.\nIn 2018, researchers came up with the idea of contextual word representations, which\naddresses this issue. It uses “language modeling,” which is the task of predicting the\nnext likely word in a sequence of words. In its earliest form, it used the idea of n-\ngram frequencies to estimate the probability of the next word given a history of\nwords. The past few years have seen the advent of advanced neural language models\n(e.g., transformers [22, 23]) that make use of the word embeddings we discussed\nearlier but use complex architectures involving multiple passes through the text and\nmultiple reads from left to right and right to left to model the context of language use.\nNeural architectures such as recurrent neural networks (RNNs) and transformers\nwere used to develop large-scale models of language (ELMo [24], BERT [25]), which\ncan be used as pre-trained models to get text representations. The key idea is to lever‐\nage “transfer learning”—that is, to learn embeddings on a generic task (like language\nmodeling) on a massive corpus and then fine-tune learnings on task-specific data.\nThese models have shown significant improvements on some fundamental NLP\ntasks, such as question answering, semantic role labeling, named entity recognition,\nand coreference resolution, to name a few. We briefly described what some of these\ntasks are in Chapter 1. Interested readers can go through the references, including the\nupcoming book by Taher and Collados [26].\nIn the last three sections, we looked at the key ideas behind word embeddings, how to\ntrain them, and how to use pre-trained embeddings to get text representations. We’ll\nlearn more about how to use these representations in different NLP applications in\nthe coming chapters. These representations are very useful and popular in modern-\nday NLP. However, based on our experience, here are a few important aspects to keep\nin mind while using them in your project:\n• All text representations are inherently biased based on what they saw in training\ndata. For example, an embedding model trained heavily on technology news or\narticles is likely to identify Apple as being closer to, say, Microsoft or Facebook\nthan to an orange or pear. While this is anecdotal, such biases stemming from\ntraining data can have serious implications on the performance of NLP models\nand systems that rely on these representations. Understanding biases that may be\npresent in learned embeddings and developing methods for addressing them is\nUniversal Text Representations \n| \n107\n",
        "word_count": 501,
        "char_count": 3098,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 138,
        "text": "very important. An interested reader can look at Tolga et al. [27]. These biases are\nan important factor to consider in any NLP software development.\n• Unlike the basic vectorization approaches, pre-trained embeddings are generally\nlarge-sized files (several gigabytes), which may pose problems in certain deploy‐\nment scenarios. This is something we need to address while using them,\notherwise it can become an engineering bottleneck in performance. The\nWord2vec model takes ~4.5 GB RAM. One good hack is to use in-memory data‐\nbases like Redis [28] with a cache on top of them to address scaling and latency\nissues. Load your embeddings into such databases and use the embeddings as if\nthey’re available in RAM.\n• Modeling language for a real-world application is more than capturing the infor‐\nmation via word and sentence embeddings. We still need ways to encode specific\naspects of text, the relationships between sentences in it, and any other domain-\nand application-specific needs that may not be addressed by the embedding\nrepresentations themselves (yet!). For example, the task of sarcasm detection\nrequires nuances that are not yet captured well by embedding techniques.\n• As we speak, neural text representation is an evolving area in NLP, with rapidly\nchanging state of the art. While it’s easy to get carried away by the next big model\nin the news, a practitioner needs to exercise caution and consider practical issues\nsuch as return on investment from the effort, business needs, and infrastructural\nconstraints before trying to use them in production-grade applications.\nAn interested reader may refer to Smith [29] for a concise summary of the evolution\nof different word representations and the research challenges ahead for neural net‐\nwork models of text representation. Now, let’s move on to techniques for visualizing\nembeddings.\nVisualizing Embeddings\nSo far, we’ve seen various vectorization techniques for representing text. The vectors\nobtained are used as features for the NLP task at hand, be it text classification or a\nquestion-answering system. An important aspect of any ML project is feature explo‐\nration. How can we explore the vectors that we have to work with? Visual exploration\nis a very important aspect of any data-related problem. Is there a way to visually\ninspect word vectors? Even though embeddings are low-dimensional vectors, even\n100 or 300 dimensions are too high to visualize.\nEnter t-SNE [30], or t-distributed Stochastic Neighboring Embedding. It’s a technique\nused for visualizing high-dimensional data like embeddings by reducing them to two-\nor three-dimensional data. The technique takes in the embeddings (or any data) and\nlooks at how to best represent the input data using lesser dimensions, all while main‐\ntaining the same data distributions in original high-dimensional input space and low-\n108 \n| \nChapter 3: Text Representation\n",
        "word_count": 454,
        "char_count": 2895,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 139,
        "text": "dimensional output space. This, therefore, enables us to plot and visualize the input\ndata. It helps to get a feel for the space of word embedding.\nLet’s look at some visualizations using t-SNE. First, we look at feature vectors\nobtained from the MNIST digits dataset [31]. Here, the images are passed through a\nconvolution neural network and the final feature vectors. Figure 3-14 shows the two-\ndimensional plot of the vectors. It clearly shows that our feature vectors are very use‐\nful since vectors of the same class tend to cluster together.\nFigure 3-14. Visualizing MNIST data using t-SNE [32]\nLet’s now visualize word embeddings. In Figure 3-15, we show only a few words. The\ninteresting thing to note is that the words that have similar meanings tend to cluster\ntogether.\nVisualizing Embeddings \n| \n109\n",
        "word_count": 135,
        "char_count": 812,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 643,
            "height": 619,
            "ext": "png",
            "size_bytes": 98362
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 140,
        "text": "Figure 3-15. t-SNE visualizations of word embeddings (eft: numbers, right: job titles [33]\nLet’s look at another word embedding visualization, probably the most famous one in\nthe NLP community. Figure 3-16 shows two-dimensional visualization of embedding\nvectors for a subset of words: man, woman, uncle, aunt, king, queen. Figure 3-16\nshows not only the position of the vectors of these words, but also an interesting\nobservation between the vectors—the arrows capture the “relationship” between\nwords. t-SNE visualization helps greatly in coming up with such nice observations.\nFigure 3-16. t-SNE visualization shows some interesting relationships [7]\nt-SNE works equally well for visualizing document embeddings. For example, we\nmight take Wikipedia articles on various topics, obtain corresponding document vec‐\ntors for each article, then plot these vectors using t-SNE. The visualization in\nFigure 3-17 clearly shows that articles in a given category are grouped together.\nClearly, t-SNE is very useful for eyeballing the quality of feature vectors. We can use\ntools like the embedding projector from TensorBoard [34] to visualize embeddings in\nour day-to-day work. As shown in Figure 3-18, TensorBoard has a nice interface that\nis tailor made for visualizing embeddings. We leave it as an exercise for the reader to\nexplore it further. For more on t-SNE, you can read the excellent article on using t-\nSNE more effectively by Martin et al. [35].\n110 \n| \nChapter 3: Text Representation\n",
        "word_count": 230,
        "char_count": 1492,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1358,
            "height": 505,
            "ext": "png",
            "size_bytes": 24182
          },
          {
            "index": 1,
            "width": 641,
            "height": 474,
            "ext": "png",
            "size_bytes": 12430
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 141,
        "text": "Figure 3-17. Visualization of Wikipedia document vectors [36]\nFigure 3-18. Screenshot of TensorBoard interface for visualizing embeddings [37]\nVisualizing Embeddings \n| \n111\n",
        "word_count": 22,
        "char_count": 174,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 633,
            "height": 550,
            "ext": "png",
            "size_bytes": 213487
          },
          {
            "index": 1,
            "width": 1442,
            "height": 724,
            "ext": "png",
            "size_bytes": 420895
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 142,
        "text": "iv. Unless they’ve been fine-tuned for the task at hand.\nHandcrafted Feature Representations\nSo far in this chapter, we’ve seen various feature representation schemes that are\nlearned from a text corpus, large or small. These feature representations are mostly\nnot dependent on the NLP problem or application domain.iv The same approach\nworks whether we want text representation for information extraction or text classifi‐\ncation and whether we work with a corpus of tweets or scientific articles.\nHowever, in many cases, we do have some domain-specific knowledge about the\ngiven NLP problem, which we would like to incorporate into the model we’re build‐\ning. In such cases, we resort to handcrafted features. Let’s take an example of a real-\nworld NLP system: TextEvaluator [38]. It’s software developed by Educational Testing\nService (ETS). The goal of this tool is to help teachers and educators provide support\nin choosing grade-appropriate reading materials for students and identifying sources\nof comprehension difficulty in texts. Clearly, this is a very specific problem. Having\ngeneral-purpose word embeddings will not help much. It needs specialized features\nextracted from text to model some form of grade appropriateness. The screenshot in\nFigure 3-19 shows some of the specialized features that are extracted from text for\nvarious dimensions of text complexity they model. Clearly, measures such as “syntac‐\ntic complexity,” “concreteness,” etc., cannot be calculated by only converting text into\nBoW or embedding representations. They have to be designed manually, keeping in\nmind both the domain knowledge and the ML algorithms to train the NLP models.\nThis is why we call these handcrafted feature representations.\nFigure 3-19. TextEvaluator software output requiring handcrafted features [38]\n112 \n| \nChapter 3: Text Representation\n",
        "word_count": 279,
        "char_count": 1851,
        "fonts": [
          "MinionPro-Regular (8.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Regular (6.3pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1157,
            "height": 597,
            "ext": "png",
            "size_bytes": 77365
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 143,
        "text": "Another software tool from ETS that’s popular for grading is the automated essay\nscorer used in online exams, such as the Graduate Record Examination (GRE) and\nTest of English as a Foreign Language (TOEFL), to evaluate test-taker essays [39]. \nThis tool, too, requires handcrafted features. Evaluating an essay for various aspects\nof writing requires specialized features that address those needs. One cannot rely only\non n-grams or word embeddings. Another NLP application where one may need\nsuch specialized feature engineering is the spelling and grammar correction we use in\ntools such as Microsoft Word, Grammarly, etc. These are all examples of commonly\nused tools where we often need custom features to incorporate domain knowledge.\nClearly, custom feature engineering is much more difficult to formulate compared to\nother feature engineering schemes we’ve seen so far. It’s for this reason that vectoriza‐\ntion approaches are more accessible to get started with, especially when we don’t have\nenough understanding of the domain. Still, handcrafted features are very common in\nseveral real-world applications. In most industrial application scenarios, we end up\ncombining the problem-agnostic feature representations we saw earlier (basic vecto‐\nrization and distributed representations) with some domain-specific features to\ndevelop hybrid features. Recent studies from IBM Research [40] and Walmart [41]\nshow examples of how heuristics, handcrafted features, and ML are all used together\nin real-world industrial systems dealing with NLP problems. We’ll see some examples\nof how to use such handcrafted features in upcoming chapters, such as text classifica‐\ntion (Chapter 4) and information extraction (Chapter 5).\nWrapping Up\nIn this chapter, we saw different techniques for representing text, starting from the\nbasic approaches to state-of-the-art DL methods. A question that may arise organi‐\ncally at this point would be: when should we go for vectorization features and embed‐\ndings, and when should we look for handcrafted features? The answer is that it\ndepends on the task at hand. For some applications, such as text classification, it’s\nmore common to see vectorization approaches and embeddings as the go-to feature\nrepresentations for text. For some other applications, such as information extraction,\nor in the examples we saw in the previous section, it’s more common to look for\nhandcrafted, domain-specific features. Quite often, a hybrid approach that combines\nboth kinds of features are used in practice. Having said that, vectorization-based\napproaches are a great starting point.\nWe hope the discussion and various perspectives presented in this chapter gave you a\ngood idea about the role of text representation in NLP, different techniques for repre‐\nsenting text, and their respective pros and cons. In the next chapters, we’ll move on to\nsolving some of the essential NLP tasks (Chapters 4–7), where we’ll see different text\nrepresentations being put to use, starting with text representation.\nWrapping Up \n| \n113\n",
        "word_count": 463,
        "char_count": 3048,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 144,
        "text": "References\n[1] Bansal, Suraj. “Convolutional Neural Networks Explained”. December 28, 2019.\n[2] Manning, C., Hinrich Schütze, and Prabhakar Raghavan. Introduction to Informa‐\ntion \nRetrieval. \nCambridge: \nCambridge \nUniversity \nPress, \n2008. \nISBN:\n978-0-521-86571-5\n[3] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft), 2018.\n[4] scikit-learn.org. TFIDF vectorizer documentation. Last accessed June 15, 2020.\n[5] Firth, John R. “A Synopsis of Linguistic Theory 1930–1955.” Studies in Linguistic\nAnalysis (1968).\n[6] Ferrone, Lorenzo, and Fabio Massimo Zanzotto. “Symbolic, Distributed and Dis‐\ntributional Representations for Natural Language Processing in the Era of Deep\nLearning: A Survey”, (2017).\n[7] Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. “Efficient Estimation\nof Word Representations in Vector Space”, (2013).\n[8] Google. Word2vec pre-trained model. Last accessed June 15, 2020.\n[9] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. “GloVe: Global\nVectors for Word Representation”. Last accessed June 15, 2020.\n[10] Facebook. fastText pre-trained model. Last accessed June 15, 2020.\n[11] Ruder, Sebastian. Three-part blog series on word embeddings. https://oreil.ly/\nOkJnx, https://oreil.ly/bjygp, and https://oreil.ly/GHgg9. Last accessed June 15, 2020.\n[12] Rong, Xin. “word2vec parameter learning explained”, (2014).\n[13] Levy, Omer, Yoav Goldberg, and Ido Dagan. “Improving Distributional Similar‐\nity with Lessons Learned from Word Embeddings.” Transactions of the Association for\nComputational Linguistics 3 (2015): 211–225.\n[14] Levy, Omer and Yoav Goldberg. “Neural Word Embedding as Implicit Matrix\nFactorization.” Proceedings of the 27th International Conference on Neural Information\nProcessing Systems 2 (2014): 2177–2185.\n[15] RaRe Technologies. gensim: Topic Modelling for Humans, (GitHub repo). Last\naccessed June 15, 2020.\n[16] Explosion.ai. “spaCy: Industrial-Strength Natural Language Processing in\nPython”. Last accessed June 15, 2020.\n[17] word2vec-toolkit Google Group discussion. Last accessed June 15, 2020.\n114 \n| \nChapter 3: Text Representation\n",
        "word_count": 284,
        "char_count": 2154,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 145,
        "text": "[18] Code for: Kim, Yoon. “Convolutional neural networks for sentence classifica‐\ntion”. (2014).\n[19] Facebook Open Source. “fastText: Library for efficient text classification and rep‐\nresentation learning”. Last accessed June 15, 2020.\n[20] Facebook Open Source. “English word vectors”. Last accessed June 15, 2020.\n[21] Le, Quoc, and Tomas Mikolov. “Distributed Representations of Sentences and\nDocuments.” Proceedings of the 31st International Conference on Machine Learning\n(2014): 1188–1196.\n[22] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention is All You Need.”\nAdvances in Neural Information Processing Systems 30 (NIPS 2017): 5998–6008.\n[23] Wang, Chenguang, Mu Li, and Alexander J. Smola. “Language Models with\nTransformers”, (2019).\n[24] Allen Institute for AI. “ELMo: Deep contextualized word representations”. Last\naccessed June 15, 2020.\n[25] Google Research. bert: TensorFlow code and pre-trained models for BERT, (Git‐\nHub repo). Last accessed June 15, 2020.\n[26] Pilehvar, Mohammad Taher and Jose Camacho-Collados. “Embeddings in Natu‐\nral Language Processing: Theory and Advances in Vector Representation of Mean‐\ning.” Synthesis Lectures on Human Language Technologies. Morgan & Claypool, 2020.\n[27] Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam\nT. Kalai. “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing\nWord Embeddings.” Advances in Neural Information Processing Systems 29 (NIPS\n2016): 4349–4357.\n[28] Redis. Last accessed June 15, 2020.\n[29] Smith, Noah A. “Contextual Word Representations: A Contextual Introduction”,\n(2019).\n[30] Maaten, Laurens van der and Geoffrey Hinton. “Visualizing Data Using t-SNE.”\nJournal of Machine Learning Research 9, Nov. (2008): 2579–2605.\n[31] Le Cun, Yann, Corinna Cortes, and Christopher J.C. Burges. “The MNIST data‐\nbase of handwritten digits”. Last accessed June 15, 2020.\n[32] Rossant, Cyril. “An illustrated introduction to the t-SNE algorithm”. March 3,\n2015.\nWrapping Up \n| \n115\n",
        "word_count": 296,
        "char_count": 2084,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 146,
        "text": "[33] Turian, Joseph, Lev Ratinov, and Yoshua Bengio. “Word Representations: A Sim‐\nple and General Method for Semi-Supervised Learning.” Proceedings of the 48th\nAnnual Meeting of the Association for Computational Linguistics (2020): 384–394.\n[34] TensorFlow. “Word embeddings” tutorial. Last accessed June 15, 2020.\n[35] Wattenberg, Martin, Fernanda Viégas, and Ian Johnson. “How to Use t-SNE\nEffectively.” Distill 1.10 (2016): e2.\n[36] Dai, Andrew M., Christopher Olah, and Quoc V. Le. “Document Embedding\nwith Paragraph Vectors”, (2015).\n[37] TensorFlow. “Embedding Projector”. Last accessed June 15, 2020.\n[38] Educational Testing Service. “TextEvaluator”. Last accessed June 15, 2020.\n[39] Educational Testing Service. “Automated Scoring of Written Responses”, 2019.\n[40] Chiticariu, L., Yunyao Li, and Frederick R. Reiss. “Rule-Based Information\nExtraction is Dead! Long Live Rule-Based Information Extraction Systems!” Proceed‐\nings of the 2013 Conference on Empirical Methods in Natural Language Processing\n(2013): 827–832.\n[41] Suganthan G.C., Paul, Chong Sun, Haojun Zhang, Frank Yang, Narasimhan\nRampalli, Shishir Prasad, Esteban Arcaute, et al. “Why Big Data Industrial Systems\nNeed Rules and What We Can Do About It.” Proceedings of the 2015 ACM SIGMOD\nInternational Conference on Management of Data (2015): 265–276.\n116 \n| \nChapter 3: Text Representation\n",
        "word_count": 192,
        "char_count": 1368,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 147,
        "text": "PART II\nEssentials\n",
        "word_count": 3,
        "char_count": 19,
        "fonts": [
          "MyriadPro-SemiboldCond (28.4pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 148,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 149,
        "text": "CHAPTER 4\nText Classification\nOrganizing is what you do before you do something,\nso that when you do it, it is not all mixed up.\n—A.A. Milne\nAll of us check email every day, possibly multiple times. A useful feature of most\nemail service providers is the ability to automatically segregate spam emails away\nfrom regular emails. This is a use case of a popular NLP task known as text classifica‐\ntion, which is the focus of this chapter. Text classification is the task of assigning one\nor more categories to a given piece of text from a larger set of possible categories. In\nthe email spam–identifier example, we have two categories—spam and non-spam—\nand each incoming email is assigned to one of these categories. This task of categoriz‐\ning texts based on some properties has a wide range of applications across diverse\ndomains, such as social media, e-commerce, healthcare, law, and marketing, to name\na few. Even though the purpose and application of text classification may vary from\ndomain to domain, the underlying abstract problem remains the same. This invari‐\nance of the core problem and its applications in a myriad of domains makes text clas‐\nsification by far the most widely used NLP task in industry and the most researched\nin academia. In this chapter, we’ll discuss the usefulness of text classification and how\nto build text classifiers for our use cases, along with some practical tips for real-world\nscenarios.\nIn machine learning, classification is the problem of categorizing a data instance into\none or more known classes. The data point can be originally of different formats,\nsuch as text, speech, image, or numeric. Text classification is a special instance of the\nclassification problem, where the input data point(s) is text and the goal is to catego‐\nrize the piece of text into one or more buckets (called a class) from a set of pre-\ndefined buckets (classes). The “text” can be of arbitrary length: a character, a word, a\nsentence, a paragraph, or a full document. Consider a scenario where we want to\nclassify all customer reviews for a product into three categories: positive, negative,\n119\n",
        "word_count": 359,
        "char_count": 2126,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (9.3pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 150,
        "text": "and neutral. The challenge of text classification is to “learn” this categorization from a\ncollection of examples for each of these categories and predict the categories for new,\nunseen products and new customer reviews. This categorization need not always\nresult in a single category, though, and there can be any number of categories avail‐\nable. Let’s take a quick look at the taxonomy of text classification to understand this.\nAny supervised classification approach, including text classification, can be further\ndistinguished into three types based on the number of categories involved: binary,\nmulticlass, and multilabel classification. If the number of classes is two, it’s called\nbinary classification. If the number of classes is more than two, it’s referred to as mul‐\nticlass classification. Thus, classifying an email as spam or not-spam is an example of\nbinary classification setting. Classifying the sentiment of a customer review as nega‐\ntive, neutral, or positive is an example of multiclass classification. In both binary and\nmulticlass settings, each document belongs to exactly one class from C, where C is the\nset of all possible classes. In multilabel classification, a document can have one or\nmore labels/classes attached to it. For example, a news article on a soccer match may\nbelong to more than one category, such as “sports” and “soccer,” simultaneously,\nwhereas another news article on US elections may have the labels “politics,” “USA,”\nand “elections.” Thus, each document has labels that are a subset of C. Each article can\nbe in no class, exactly one class, multiple classes, or all of the classes. Sometimes, the\nnumber of labels in the set C can be very large (known as “extreme classification”). In\nsome other scenarios, we may have a hierarchical classification system, which may\nresult in each text getting different labels at different levels in the hierarchy. In this\nchapter, we’ll focus only on binary and multiclass classification, as those are the most\ncommon use cases of text classification in the industry.\nText classification is sometimes also referred to as topic classification, text categoriza‐\ntion, or document categorization. For the rest of this book, we’ll stick to the term “text\nclassification.” Note that topic classification is different from topic detection, which\nrefers to the problem of uncovering or extracting “topics” from texts, which we’ll\nstudy in Chapter 7.\nIn this chapter, we’ll take a closer look at text classification and build text classifiers\nusing different approaches. Our aim is to provide an overview of some of the most\ncommonly applied techniques along with practical advice on handling different sce‐\nnarios and decisions that have to be made when building text classification systems in\npractice. We’ll start by introducing some common applications of text classification,\nthen we’ll discuss what an NLP pipeline for text classification looks like and illustrate\nthe use of this pipeline to train and test text classifiers using different approaches,\nranging from the traditional methods to the state of the art. We’ll then tackle the\nproblem of training data collection/sparsity and different methods to handle it. We’ll\nend the chapter by summarizing what we learned in all these sections along with\nsome practical advice and a case study.\n120 \n| \nChapter 4: Text Classification\n",
        "word_count": 531,
        "char_count": 3369,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 151,
        "text": "Note that, in this chapter, we’ll only deal with the aspect of training and evaluating the\ntext classifiers. Issues related to deploying NLP systems in general and performing\nquality assurance will be discussed in Chapter 11.\nApplications\nText classification has been of interest in a number of application scenarios, ranging\nfrom identifying the author of an unknown text in the 1800s to the efforts of USPS in\nthe 1960s to perform optical character recognition on addresses and zip codes [1]. In\nthe 1990s, researchers began to successfully apply ML algorithms for text classifica‐\ntion for large datasets. Email filtering, popularly known as “spam classification,” is\none of the earliest examples of automatic text classification, which impacts our lives\nto this day. From manual analyses of text documents to purely statistical, computer-\nbased approaches and state-of-the-art deep neural networks, we’ve come a long way\nwith text classification. Let’s briefly discuss some of the popular applications before\ndiving into the different approaches to perform text classification. These examples\nwill also be useful in identifying problems that can be solved using text classification\nmethods in your organization.\nContent classification and organization\nThis refers to the task of classifying/tagging large amounts of textual data. This,\nin turn, is used to power use cases like content organization, search engines, and\nrecommendation systems, to name a few. Examples of such data include news\nwebsites, blogs, online bookshelves, product reviews, tweets, etc.; tagging product\ndescriptions in an e-commerce website; routing customer service requests in a\ncompany to the appropriate support team; and organizing emails into personal,\nsocial, and promotions in Gmail are all examples of using text classification for\ncontent classification and organization.\nCustomer support\nCustomers often use social media to express their opinions about and experien‐\nces of products or services. Text classification is often used to identify the tweets\nthat brands must respond to (i.e., those that are actionable) and those that don’t\nrequire a response (i.e., noise) [2, 3]. To illustrate, consider the three tweets about\nthe brand Macy’s shown in Figure 4-1.\nAlthough all three tweets mention the brand Macy’s explicitly, only the first one\nnecessitates a reply from Macy’s customer support team.\nApplications \n| \n121\n",
        "word_count": 363,
        "char_count": 2410,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 152,
        "text": "Figure 4-1. Tweets reaching out to brands: one is actionable, the other two are noise\nE-commerce\nCustomers leave reviews for a range of products on e-commerce websites like\nAmazon, eBay, etc. An example use of text classification in this kind of scenario is\nto understand and analyze customers’ perception of a product or service based\non their comments. This is commonly known as “sentiment analysis.” It’s used\nextensively by brands across the globe to better understand whether they’re get‐\nting closer to or farther away from their customers. Rather than categorizing cus‐\ntomer feedback as simply positive, negative, or neutral, over a period of time,\nsentiment analysis has evolved into a more sophisticated paradigm: “aspect”-\nbased sentiment analysis. To understand this, consider the customer review of a\nrestaurant shown in Figure 4-2.\nFigure 4-2. A review that praises some aspects and criticizes few\nWould you call the review in Figure 4-2 negative, positive, or neutral? It’s difficult\nto answer this—the food was great, but the service was bad. Practitioners and\nbrands working with sentiment analysis have realized that many products or\nservices have multiple facets. In order to understand overall sentiment, under‐\nstanding each and every facet is important. Text classification plays a major role\n122 \n| \nChapter 4: Text Classification\n",
        "word_count": 211,
        "char_count": 1354,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1199,
            "height": 677,
            "ext": "png",
            "size_bytes": 201052
          },
          {
            "index": 1,
            "width": 575,
            "height": 170,
            "ext": "png",
            "size_bytes": 22284
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 153,
        "text": "in performing such fine-grained analysis of customer feedback. We’ll discuss this\nspecific application in detail in Chapter 9.\nOther applications\nApart from the above-mentioned areas, text classification is also used in several\nother applications in various domains:\n• Text classification is used in language identification, like identifying the lan‐\nguage of new tweets or posts. For example, Google Translate has an auto‐\nmatic language identification feature.\n• Authorship attribution, or identifying the unknown authors of texts from a\npool of authors, is another popular use case of text classification, and it’s used\nin a range of fields from forensic analysis to literary studies.\n• Text classification has been used in the recent past for triaging posts in an\nonline support forum for mental health services [4]. In the NLP community,\nannual competitions are conducted (e.g., clpsych.org) for solving such text\nclassification problems originating from clinical research.\n• In the recent past, text classification has also been used to segregate fake\nnews from real news.\nNote that this section only serves as an illustration of the wide range of applications\nof text classification, and the list is not exhaustive, but we hope it gives you enough\nbackground to identify text classification problems in your workplace projects when\nyou encounter them. Let’s now look at how to build such text classification models.\nA Pipeline for Building Text Classification Systems\nIn Chapter 2, we discussed some of the common NLP pipelines. The text classifica‐\ntion pipeline shares some of its steps with the pipelines we learned in that chapter.\nOne typically follows these steps when building a text classification system:\n1. Collect or create a labeled dataset suitable for the task.\n2. Split the dataset into two (training and test) or three parts: training, validation\n(i.e., development), and test sets, then decide on evaluation metric(s).\n3. Transform raw text into feature vectors.\n4. Train a classifier using the feature vectors and the corresponding labels from the\ntraining set.\n5. Using the evaluation metric(s) from Step 2, benchmark the model performance\non the test set.\n6. Deploy the model to serve the real-world use case and monitor its performance.\nA Pipeline for Building Text Classification Systems \n| \n123\n",
        "word_count": 364,
        "char_count": 2325,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 154,
        "text": "Figure 4-3 shows these typical steps in building a text classification system.\nFigure 4-3. Flowchart of a text classification pipeline\nSteps 3 through 5 are iterated on to explore different variants of features and classifi‐\ncation algorithms and their parameters and to tune the hyperparameters before pro‐\nceeding to Step 6, deploying the optimal model in production.\nSome of the individual steps related to data collection and pre-processing were dis‐\ncussed in past chapters. For example, Steps 1 and 2 were discussed in detail in Chap‐\nter 2. Chapter 3 focused entirely on Step 3. Our focus in this chapter is on Steps 4\nthrough 5. Toward the end of this chapter, we’ll revisit Step 1 to discuss issues specific\nto text classification. We’ll deal with Step 6 in Chapter 11. To be able to perform Steps\n4 through 5 (i.e., to benchmark the performance of a model or compare multiple clas‐\nsifiers), we need the right measure(s) of evaluation. Chapter 2 discussed various gen‐\neral metrics used in evaluating NLP systems. For evaluating classifiers specifically,\namong the metrics introduced in Chapter 2, the following are used more commonly:\nclassification accuracy, precision, recall, F1 score, and area under ROC curve. In this\nchapter, we’ll use some of these measures to evaluate our models and also look at con‐\nfusion matrices to understand the model performance in detail.\nApart from these, when classification systems are deployed in real-world applications,\nkey performance indicators (KPIs) specific to a given business use case are also used\nto evaluate their impact and return on investment (ROI). These are often the metrics\n124 \n| \nChapter 4: Text Classification\n",
        "word_count": 272,
        "char_count": 1681,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1169,
            "height": 997,
            "ext": "png",
            "size_bytes": 64313
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 155,
        "text": "business teams care about. For example, if we’re using text classification to automati‐\ncally route customer service requests, a possible KPI could be the reduction in wait\ntime before the request is responded to compared to manual routing. In this chapter,\nwe’ll focus on the NLP evaluation measures. In Part III of the book, where we’ll dis‐\ncuss NLP use cases specific to industry verticals, we’ll introduce some KPIs that are\noften used in those verticals.\nBefore we start looking at how to build text classifiers using the pipeline we just dis‐\ncussed, let’s take a look at the scenarios where this pipeline is not at all necessary or\nwhere it’s not possible to use it.\nA Simple Classifier Without the Text Classification Pipeline\nWhen we talk about the text classification pipeline, we’re referring to a supervised\nmachine learning scenario. However, it’s possible to build a simple classifier without\nmachine learning and without this pipeline. Consider the following problem sce‐\nnario: we’re given a corpus of tweets where each tweet is labeled with its correspond‐\ning sentiment: negative or positive. For example, a tweet that says, “The new James\nBond movie is great!” is clearly expressing a positive sentiment, whereas a tweet that\nsays, “I would never visit this restaurant again, horrible place!!” has a negative senti‐\nment. We want to build a classification system that will predict the sentiment of an\nunseen tweet using only the text of the tweet. A simple solution could be to create\nlists of positive and negative words in English—i.e., words that have a positive or neg‐\native sentiment. We then compare the usage of positive versus negative words in the\ninput tweet and make a prediction based on this information. Further enhancements\nto this approach may involve creating more sophisticated dictionaries with degrees of\npositive, negative, and neutral sentiment of words or formulating specific heuristics\n(e.g., usage of certain smileys indicate positive sentiment) and using them to make\npredictions. This approach is called lexicon-based sentiment analysis.\nClearly, this does not involve any “learning” of text classification; that is, it’s based on\na set of heuristics or rules and custom-built resources such as dictionaries of words\nwith sentiment. While this approach may seem too simple to perform reasonably well\nfor many real-world scenarios, it may enable us to deploy a minimum viable product\n(MVP) quickly. Most importantly, this simple model can lead to better understanding\nof the problem and give us a simple baseline for our evaluation metric and speed.\nFrom our experience, it’s always good to start with such simpler approaches when\ntackling a new NLP problem, where possible. However, eventually, we’ll need ML\nmethods that can infer more insights from large collections of text data and perform\nbetter than the baseline approach.\nA Pipeline for Building Text Classification Systems \n| \n125\n",
        "word_count": 469,
        "char_count": 2938,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 156,
        "text": "Using Existing Text Classification APIs\nAnother scenario where we may not have to “learn” a classifier or follow this pipeline\nis when our task is more generic in nature, such as identifying a general category of a\ntext (e.g., whether it’s about technology or music). In such cases, we can use existing\nAPIs, such as Google Cloud Natural Language [5], that provide off-the-shelf content\nclassification models that can identify close to 700 different categories of text.\nAnother popular classification task is sentiment analysis. All major service providers\n(e.g., Google, Microsoft, and Amazon) serve sentiment analysis APIs [5, 6, 7] with\nvarying payment structures. If we’re tasked with building a sentiment classifier, we\nmay not have to build our own system if an existing API addresses our business\nneeds.\nHowever, many classification tasks could be specific to our organization’s business\nneeds. For the rest of this chapter, we’ll address the scenario of building our own clas‐\nsifier by considering the pipeline described earlier in this section.\nOne Pipeline, Many Classifiers\nLet’s now look at building text classifiers by altering Steps 3 through 5 in the pipeline\nand keeping the remaining steps constant. A good dataset is a prerequisite to start\nusing the pipeline. When we say “good” dataset, we mean a dataset that is a true rep‐\nresentation of the data we’re likely to see in production. Throughout this chapter,\nwe’ll use some of the publicly available datasets for text classification. A wide range of\nNLP-related datasets, including ones for text classification, are listed online [8]. Addi‐\ntionally, Figure Eight [9] contains a collection of crowdsourced datasets, some of\nwhich are relevant to text classification. The UCI Machine Learning Repository [10]\nalso contains a few text classification datasets. Google recently launched a dedicated\nsearch system for datasets for machine learning [11]. We’ll use multiple datasets\nthroughout this chapter instead of sticking to one to illustrate any dataset-specific\nissues you may come across.\nNote that our goal in this chapter is to give you an overview of different approaches.\nNo single approach is known to work universally well on all kinds of data and all clas‐\nsification problems. In the real world, we experiment with multiple approaches, eval‐\nuate them, and choose one final approach to deploy in practice.\nFor the rest of this section, we’ll use the “Economic News Article Tone and Rele‐\nvance” dataset from Figure Eight to demonstrate text classification. It consists of\n8,000 news articles annotated with whether or not they’re relevant to the US economy\n(i.e., a yes/no binary classification). The dataset is also imbalanced, with ~1,500 rele‐\nvant and ~6,500 non-relevant articles, which poses the challenge of guarding against\nlearning a bias toward the majority category (in this case, non-relevant articles).\nClearly, learning what a relevant news article is is more challenging with this dataset\n126 \n| \nChapter 4: Text Classification\n",
        "word_count": 475,
        "char_count": 3024,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 157,
        "text": "than learning what is irrelevant. After all, just guessing that everything is irrelevant\nalready gives us 80% accuracy!\nLet’s explore how a BoW representation (introduced in Chapter 3) can be used with\nthis dataset following the pipeline described earlier in this chapter. We’ll build classifi‐\ners using three well-known algorithms: Naive Bayes, logistic regression, and support\nvector machines. The notebook related to this section (Ch4/OnePipeline_ManyClassi‐\nfiers.ipynb) shows the step-by-step process of following our pipeline using these three\nalgorithms. We’ll discuss some of the important aspects in this section.\nNaive Bayes Classifier\nNaive Bayes is a probabilistic classifier that uses Bayes’ theorem to classify texts based\non the evidence seen in training data. It estimates the conditional probability of each\nfeature of a given text for each class based on the occurrence of that feature in that\nclass and multiplies the probabilities of all the features of a given text to compute the\nfinal probability of classification for each class. Finally, it chooses the class with maxi‐\nmum probability. A detailed step-by-step explanation of the classifier is beyond the\nscope of this book. However, a reader interested in Naive Bayes with a detailed\nexplanation in the context of text classification can look at Chapter 4 of Jurafsky and\nMartin [12]. Although simple, Naive Bayes is commonly used as a baseline algorithm\nin classification experiments.\nLet’s walk through the key steps of an implementation of the pipeline described ear‐\nlier for our dataset. For this, we use a Naive Bayes implementation in scikit-learn.\nOnce the dataset is loaded, we split the data into train and test data, as shown in the\ncode snippet below:\n#Step 1: train-test split\nX = our_data.text \n#the column text contains textual data to extract features from.\ny = our_data.relevance \n#this is the column we are learning to predict.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n#split X and y into training and testing sets. By default, \nit splits 75% #training and 25% test. random_state=1 for reproducibility.\nThe next step is to pre-process the texts and then convert them into feature vectors.\nWhile there are many different ways to do the pre-processing, let’s say we want to do\nthe following: lowercasing and removal of punctuation, digits and any custom\nstrings, and stop words. The code snippet below shows this pre-processing and con‐\nverting the train and test data into feature vectors using CountVectorizer in scikit-\nlearn, which is the implementation of the BoW approach we discussed in Chapter 3:\n#Step 2-3: Pre-process and Vectorize train and test data\nvect = CountVectorizer(preprocessor=clean) \n#clean is a function we defined for pre-processing, seen in the notebook.\nX_train_dtm = vect.fit_transform(X_train)\nOne Pipeline, Many Classifiers \n| \n127\n",
        "word_count": 445,
        "char_count": 2887,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 158,
        "text": "X_test_dtm = vect.transform(X_test)\nprint(X_train_dtm.shape, X_test_dtm.shape)\nOnce we run this in the notebook, we’ll see that we ended up having a feature vector\nwith over 45,000 features! We now have the data in a format we want: feature vectors.\nSo, the next step is to train and evaluate a classifier. The code snippet below shows\nhow to do the training and evaluation of a Naive Bayes classifier with the features we\nextracted above:\nnb = MultinomialNB() #instantiate a Multinomial Naive Bayes classifier\nnb.fit(X_train_dtm, y_train)#train the mode \ny_pred_class = nb.predict(X_test_dtm)#make class predictions for test data\nFigure 4-4 shows the confusion matrix of this classifier with test data.\nFigure 4-4. Confusion matrix for Naive Bayes classifier\nAs evident from Figure 4-4, the classifier is doing fairly well with identifying the non-\nrelevant articles correctly, only making errors 14% of the time. However, it does not\nperform well in comparison to the second category: relevance. The category is identi‐\nfied correctly only 42% of the time. An obvious thought may be to collect more data.\nThis is correct and often the most rewarding approach. But in the interest of covering\nother approaches, we assume that we cannot change it or collect additional data. This\nis not a far-fetched assumption—in industry, we often don’t have the luxury of col‐\nlecting more data; we have to work with what we have. We can think of a few possible\n128 \n| \nChapter 4: Text Classification\n",
        "word_count": 238,
        "char_count": 1488,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1385,
            "height": 1209,
            "ext": "png",
            "size_bytes": 42063
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 159,
        "text": "reasons for this performance and ways to improve this classifier. These are summar‐\nized in Table 4-1, and we’ll look into some of them as we progress in this chapter.\nTable 4-1. Potential reasons for poor classifier performance\nReason 1 Since we extracted all possible features, we ended up in a large, sparse feature vector, where most features are\ntoo rare and end up being noise. A sparse feature set also makes training hard.\nReason 2 There are very few examples of relevant articles (~20%) compared to the non-relevant articles (~80%) in the\ndataset. This class imbalance makes the learning process skewed toward the non-relevant articles category, as\nthere are very few examples of “relevant” articles.\nReason 3 Perhaps we need a better learning algorithm.\nReason 4 Perhaps we need a better pre-processing and feature extraction mechanism.\nReason 5 Perhaps we should look to tuning the classifier’s parameters and hyperparameters.\nLet’s see how to improve our classification performance by addressing some of the\npossible reasons for it. One way to approach Reason 1 is to reduce noise in the feature\nvectors. The approach in the previous code example had close to 40,000 features\n(refer to the Jupyter notebook for details). A large number of features introduce spar‐\nsity; i.e., most of the features in the feature vector are zero, and only a few values are\nnon-zero. This, in turn, affects the ability of the text classification algorithm to learn.\nLet’s see what happens if we restrict this to 5,000 and rerun the training and evalua‐\ntion process. This requires us to change the CountVectorizer instantiation in the\nprocess, as shown in the code snippet below, and repeat all the steps:\nvect = CountVectorizer(preprocessor=clean, max_features=5000) #Step-1\nX_train_dtm = vect.fit_transform(X_train)#combined step 2 and 3\nX_test_dtm = vect.transform(X_test)\nnb = MultinomialNB() #instantiate a Multinomial Naive Bayes model\n%time nb.fit(X_train_dtm, y_train)\n#train the model(timing it with an IPython \"magic command\")\ny_pred_class = nb.predict(X_test_dtm)\n#make class predictions for X_test_dtm\nprint(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_class))\nFigure 4-5 shows the new confusion matrix with this setting.\nNow, clearly, while the average performance seems lower than before, the correct\nidentification of relevant articles increased by over 20%. At that point, one may won‐\nder whether this is what we want. The answer to that question depends on the prob‐\nlem we’re trying to solve. If we care about doing reasonably well with non-relevant\narticle identification and doing as well as possible with relevant article identification,\nor doing equally well with both, we could conclude that reducing the feature vector\nsize with the Naive Bayes classifier was useful for this dataset.\nOne Pipeline, Many Classifiers \n| \n129\n",
        "word_count": 437,
        "char_count": 2849,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 160,
        "text": "Figure 4-5. Improved classification performance with Naive Bayes and feature selection\nConsider reducing the number of features if there are too many to\nreduce data sparsity.\nReason 2 in our list was the problem of skew in data toward the majority class. There\nare several ways to address this. Two typical approaches are oversampling the instan‐\nces belonging to minority classes or undersampling the majority class to create a bal‐\nanced dataset. Imbalanced-Learn [13] is a Python library that incorporates some of\nthe sampling methods to address this issue. While we won’t delve into the details of\nthis library here, classifiers also have a built-in mechanism to address such imbal‐\nanced datasets. We’ll see how to use that by taking another classifier, logistic regres‐\nsion, in the next subsection.\nClass imbalance is one of the most common reasons for a classifier\nto not do well. We must always check if this is the case for our task\nand address it.\n130 \n| \nChapter 4: Text Classification\n",
        "word_count": 166,
        "char_count": 998,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          },
          {
            "index": 1,
            "width": 1402,
            "height": 1209,
            "ext": "png",
            "size_bytes": 42682
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 161,
        "text": "To address Reason 3, let’s try using other algorithms, beginning with logistic\nregression.\nLogistic Regression\nWhen we described the Naive Bayes classifier, we mentioned that it learns the proba‐\nbility of a text for each class and chooses the one with maximum probability. Such a\nclassifier is called a generative classifier. In contrast, there’s a discriminative classifier\nthat aims to learn the probability distribution over all classes. Logistic regression is an\nexample of a discriminative classifier and is commonly used in text classification, as a\nbaseline in research, and as an MVP in real-world industry scenarios.\nUnlike Naive Bayes, which estimates probabilities based on feature occurrence in\nclasses, logistic regression “learns” the weights for individual features based on how\nimportant they are to make a classification decision. The goal of logistic regression is\nto learn a linear separator between classes in the training data with the aim of maxi‐\nmizing the probability of the data. This “learning” of feature weights and probability\ndistribution over all classes is done through a function called “logistic” function, and\n(hence the name) logistic regression [14].\nLet’s take the 5,000-dimensional feature vector from the last step of the Naive Bayes\nexample and train a logistic regression classifier instead of Naive Bayes. The code\nsnippet below shows how to use logistic regression for this task:\nfrom sklearn.linear_model import LogisticRegression \nlogreg = LogisticRegression(class_weight=\"balanced\")\nlogreg.fit(X_train_dtm, y_train) \ny_pred_class = logreg.predict(X_test_dtm)\nprint(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_class))\nThis results in a classifier with an accuracy of 73.7%. Figure 4-6 shows the confusion\nmatrix with this approach.\nOur logistic regression classifier instantiation has an argument class_weight, which\nis given a value “balanced”. This tells the classifier to boost the weights for classes in\ninverse proportion to the number of samples for that class. So, we expect to see better\nperformance for the less-represented classes. We can experiment with this code by\nremoving that argument and retraining the classifier, to witness a fall (by approxi‐\nmately 5%) in the bottom-right cell of the confusion matrix. However, logistic regres‐\nsion clearly seems to perform worse than Naive Bayes for this dataset.\nReason 3 in our list was: “Perhaps we need a better learning algorithm.” This gives\nrise to the question: “What is a better learning algorithm?” A general rule of thumb\nwhen working with ML approaches is that there is no one algorithm that learns well\non all datasets. A common approach is to experiment with various algorithms and\ncompare them.\nOne Pipeline, Many Classifiers \n| \n131\n",
        "word_count": 412,
        "char_count": 2762,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 162,
        "text": "Figure 4-6. Classification performance with logistic regression\nLet’s see if this idea helps us by replacing logistic regression with another well-known\nclassification algorithm that was shown to be useful for several text classification\ntasks, called the “support vector machine.”\nSupport Vector Machine\nWe described logistic regression as a discriminative classifier that learns the weights\nfor individual features and predicts a probability distribution over the classes. A sup‐\nport vector machine (SVM), first invented in the early 1960s, is a discriminative classi‐\nfier like logistic regression. However, unlike logistic regression, it aims to look for an\noptimal hyperplane in a higher dimensional space, which can separate the classes in\nthe data by a maximum possible margin. Further, SVMs are capable of learning even\nnon-linear separations between classes, unlike logistic regression. However, they may\nalso take longer to train.\nSVMs come in various flavors in sklearn. Let’s see how one of them is used by keep‐\ning everything else the same and altering maximum features to 1,000 instead of the\nprevious example’s 5,000. We restrict to 1,000 features, keeping in mind the time an\nSVM algorithm takes to train. The code snippet below shows how to do this, and\nFigure 4-7 shows the resultant confusion matrix:\n132 \n| \nChapter 4: Text Classification\n",
        "word_count": 210,
        "char_count": 1361,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1385,
            "height": 1209,
            "ext": "png",
            "size_bytes": 42171
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 163,
        "text": "from sklearn.svm import LinearSVC\nvect = CountVectorizer(preprocessor=clean, max_features=1000) #Step-1\nX_train_dtm = vect.fit_transform(X_train)#combined step 2 and 3\nX_test_dtm = vect.transform(X_test)\nclassifier = LinearSVC(class_weight='balanced') #notice the “balanced” option\nclassifier.fit(X_train_dtm, y_train) #fit the model with training data\ny_pred_class = classifier.predict(X_test_dtm)\nprint(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred_class))\nFigure 4-7. Confusion matrix for classification with SVM\nWhen compared to logistic regression, SVMs seem to have done better with the rele‐\nvant articles category, although, among this small set of experiments we did, Naive\nBayes, with the smaller set of features, seems to be the best classifier for this dataset.\nAll the examples in this section demonstrate how changes in different steps affected\nthe classification performance and how to interpret the results. Clearly, we excluded\nmany other possibilities, such as exploring other text classification algorithms, chang‐\ning different parameters of various classifiers, coming up with better pre-processing\nmethods, etc. We leave them as further exercises for the reader, using the notebook as\na playground. A real-world text classification project involves exploring multiple\noptions like this, starting with the simplest approach in terms of modeling, deploy‐\nment, and scaling, and gradually increasing the complexity. Our eventual goal is to\nbuild the classifier that best meets our business needs given all the other constraints.\nOne Pipeline, Many Classifiers \n| \n133\n",
        "word_count": 210,
        "char_count": 1593,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1402,
            "height": 1209,
            "ext": "png",
            "size_bytes": 43420
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 164,
        "text": "Let’s now consider a part of Reason 4 in Table 4-1: better feature representation. So\nfar in this chapter, we’ve used BoW features. Let’s see how we can use other feature\nrepresentation techniques we saw in Chapter 3 for text classification.\nUsing Neural Embeddings in Text Classification\nIn the latter half of Chapter 3, we discussed feature engineering techniques using neu‐\nral networks, such as word embeddings, character embeddings, and document\nembeddings. The advantage of using embedding-based features is that they create a\ndense, low-dimensional feature representation instead of the sparse, high-\ndimensional structure of BoW/TF-IDF and other such features. There are different\nways of designing and using features based on neural embeddings. In this section,\nlet’s look at some ways of using such embedding representations for text\nclassification.\nWord Embeddings\nWords and n-grams have been used primarily as features in text classification for a\nlong time. Different ways of vectorizing words have been proposed, and we used one\nsuch representation in the last section, CountVectorizer. In the past few years, neu‐\nral network–based architectures have become popular for “learning” word represen‐\ntations, which are known as “word embeddings.” We surveyed some of the intuitions\nbehind this in Chapter 3. Let’s now take a look at how to use word embeddings as\nfeatures for text classification. We’ll use the sentiment-labeled sentences dataset from\nthe UCI repository, consisting of 1,500 positive-sentiment and 1,500 negative-\nsentiment sentences from Amazon, Yelp, and IMDB. All the steps are detailed in the\nnotebook Ch4/Word2Vec_Example.ipynb. Let’s walk through the important steps and\nwhere this approach differs from the previous section’s procedures.\nLoading and pre-processing the text data remains a common step. However, instead\nof vectorizing the texts using BoW-based features, we’ll now rely on neural embed‐\nding models. As mentioned earlier, we’ll use a pre-trained embedding model.\nWord2vec is a popular algorithm we discussed in Chapter 3 for training word embed‐\nding models. There are several pre-trained Word2vec models trained on large cor‐\npora available on the internet. Here, we’ll use the one from Google [15]. The\nfollowing code snippet shows how to load this model into Python using gensim:\ndata_path= \"/your/folder/path\"\npath_to_model = os.path.join(data_path,'GoogleNews-vectors-negative300.bin')\ntraining_data_path = os.path.join(data_path, \"sentiment_sentences.txt\")\n#Load W2V model. This will take some time.\nw2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\nprint('done loading Word2Vec')\n134 \n| \nChapter 4: Text Classification\n",
        "word_count": 384,
        "char_count": 2700,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 165,
        "text": "This is a large model that can be seen as a dictionary where the keys are words in the\nvocabulary and the values are their learned embedding representations. Given a\nquery word, if the word’s embedding is present in the dictionary, it will return the\nsame. How do we use this pre-learned embedding to represent features? As we dis‐\ncussed in Chapter 3, there are multiple ways of doing this. A simple approach is just\nto average the embeddings for individual words in text. The code snippet below\nshows a simple function to do this:\n# Creating a feature vector by averaging all embeddings for all sentences\ndef embedding_feats(list_of_lists):\n    DIMENSION = 300\n    zero_vector = np.zeros(DIMENSION)\n    feats = []\n    for tokens in list_of_lists:\n          feat_for_this =  np.zeros(DIMENSION)\n          count_for_this = 0\n          for token in tokens:\n                     if token in w2v_model:\n                          feat_for_this += w2v_model[token]\n                          count_for_this +=1\n          feats.append(feat_for_this/count_for_this)         \n    return feats\ntrain_vectors = embedding_feats(texts_processed)\nprint(len(train_vectors))\nNote that it uses embeddings only for the words that are present in the dictionary. It\nignores the words for which embeddings are absent. Also, note that the above code\nwill give a single vector with DIMENSION(=300) components. We treat the resulting\nembedding vector as the feature vector that represents the entire text. Once this fea‐\nture engineering is done, the final step is similar to what we did in the previous sec‐\ntion: use these features and train a classifier. We leave that as an exercise to the reader\n(refer to the notebook for the full code).\nWhen trained with a logistic regression classifier, these features gave a classification\naccuracy of 81% on our dataset (see the notebook for more details). Considering that\nwe just used an existing word embeddings model and followed only basic pre-\nprocessing steps, this is a great model to have as a baseline! We saw in Chapter 3 that\nthere are other pre-trained embedding approaches, such as GloVe, which can be\nexperimented with for this approach. Gensim, which we used in this example, also\nsupports training our own word embeddings if necessary. If we’re working on a cus‐\ntom domain whose vocabulary is remarkably different from that of the pre-trained\nnews embeddings we used here, it would make sense to train our own embeddings to\nextract features.\nIn order to decide whether to train our own embeddings or use pre-trained embed‐\ndings, a good rule of thumb is to compute the vocabulary overlap. If the overlap\nbetween the vocabulary of our custom domain and that of pre-trained word\nUsing Neural Embeddings in Text Classification \n| \n135\n",
        "word_count": 420,
        "char_count": 2770,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 166,
        "text": "embeddings is greater than 80%, pre-trained word embeddings tend to give good\nresults in text classification.\nAn important factor to consider when deploying models with embedding-based fea‐\nture extraction approaches is that the learned or pre-trained embedding models have\nto be stored and loaded into memory while using these approaches. If the model itself\nis bulky (e.g., the pre-trained model we used takes 3.6 GB), we need to factor this into\nour deployment needs.\nSubword Embeddings and fastText\nWord embeddings, as the name indicates, are about word representations. Even off-\nthe-shelf embeddings seem to work well on classification tasks, as we saw earlier.\nHowever, if a word in our dataset was not present in the pre-trained model’s vocabu‐\nlary, how will we get a representation for this word? This problem is popularly known\nas out of vocabulary (OOV). In our previous example, we just ignored such words\nfrom feature extraction. Is there a better way?\nWe discussed fastText embeddings [16] in Chapter 3. They’re based on the idea of\nenriching word embeddings with subword-level information. Thus, the embedding\nrepresentation for each word is represented as a sum of the representations of indi‐\nvidual character n-grams. While this may seem like a longer process compared to just\nestimating word-level embeddings, it has two advantages:\n• This approach can handle words that did not appear in training data (OOV).\n• The implementation facilitates extremely fast learning on even very large\ncorpora.\nWhile fastText is a general-purpose library to learn the embeddings, it also supports\noff-the-shelf text classification by providing end-to-end classifier training and testing;\ni.e., we don’t have to handle feature extraction separately. The remaining part of this\nsubsection shows how to use the fastText classifier [17] for text classification. We’ll\nwork with the DBpedia dataset [18]. It’s a balanced dataset consisting of 14 classes,\nwith 40,000 training and 5,000 testing examples per class. Thus, the total size of the\ndataset is 560,000 training and 70,000 testing data points. Clearly, this is a much\nlarger dataset than what we saw before. Can we build a fast training model using fast‐\nText? Let’s check it out!\nThe training and test sets are provided as CSV files in this dataset. So, the first step\ninvolves reading these files into your Python environment and cleaning the text to\nremove extraneous characters, similar to what we did in the pre-processing steps for\nthe other classifier examples we’ve seen so far. Once this is done, the process to use\nfastText is quite simple. The code snippet below shows a simple fastText model. The\n136 \n| \nChapter 4: Text Classification\n",
        "word_count": 432,
        "char_count": 2705,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 167,
        "text": "step-by-step process is detailed in the associated Jupyter notebook (Ch4/Fast‐\nText_Example.ipynb):\n## Using fastText for feature extraction and training\nfrom fasttext import supervised\n\"\"\"fastText expects and training file (csv), a model name as input arguments.\nlabel_prefix refers to the prefix before label string in the dataset.\ndefault is __label__. In our dataset, it is __class__.\nThere are several other parameters which can be seen in:\nhttps://pypi.org/project/fasttext/\n\"\"\"\nmodel = supervised(train_file, 'temp', label_prefix=\"__class__\")\nresults = model.test(test_file)\nprint(results.nexamples, results.precision, results.recall)\nIf we run this code in the notebook, we’ll notice that, despite the fact that this is a\nhuge dataset and we gave the classifier raw text and not the feature vector, the train‐\ning takes only a few seconds, and we get close to 98% precision and recall! As an exer‐\ncise, try to build a classifier using the same dataset but with either BoW or word\nembedding features and algorithms like logistic regression. Notice how long it takes\nfor the individual steps of feature extraction and classification learning!\nWhen we have a large dataset, and when learning seems infeasible with the\napproaches described so far, fastText is a good option to use to set up a strong work‐\ning baseline. However, there’s one concern to keep in mind when using fastText, as\nwas the case with Word2vec embeddings: it uses pre-trained character n-gram\nembeddings. Thus, when we save the trained model, it carries the entire character n-\ngram embeddings dictionary with it. This results in a bulky model and can result in\nengineering issues. For example, the model stored with the name “temp” in the above\ncode snippet has a size close to 450 MB. However, fastText implementation also\ncomes with options to reduce the memory footprint of its classification models with\nminimal reduction in classification performance [19]. It does this by doing vocabu‐\nlary pruning and using compression algorithms. Exploring these possibilities could\nbe a good option in cases where large model sizes are a constraint.\nfastText is extremely fast to train and very useful for setting up\nstrong baselines. The downside is the model size.\nWe hope this discussion gives a good overview of the usefulness of fastText for text\nclassification. What we showed here is a default classification model without any tun‐\ning of the hyperparameters. fastText’s documentation contains more information on\nthe different options to tune your classifier and on training custom embedding repre‐\nsentations for a dataset you want. However, both of the embedding representations\nwe’ve seen so far learn a representation of words and characters and collect them\nUsing Neural Embeddings in Text Classification \n| \n137\n",
        "word_count": 432,
        "char_count": 2797,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "MinionPro-Regular (9.6pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 168,
        "text": "together to form a text representation. Let’s see how to learn the representation for a\ndocument directly using the Doc2vec approach we discussed in Chapter 3.\nDocument Embeddings\nIn the Doc2vec embedding scheme, we learn a direct representation for the entire\ndocument (sentence/paragraph) rather than each word. Just as we used word and\ncharacter embeddings as features for performing text classification, we can also use\nDoc2vec as a feature representation mechanism. Since there are no existing pre-\ntrained models that work with the latest version of Doc2vec [20], let’s see how to\nbuild our own Doc2vec model and use it for text classification.\nWe’ll use a dataset called “Sentiment Analysis: Emotion in Text” from figure-\neight.com [9], which contains 40,000 tweets labeled with 13 labels signifying different\nemotions. Let’s take the three most frequent labels in this dataset—neutral, worry,\nhappiness—and build a text classifier for classifying new tweets into one of these\nthree classes. The notebook for this subsection (Ch4/Doc2Vec_Example.ipynb) walks\nyou through the steps involved in using Doc2vec for text classification and provides\nthe dataset.\nAfter loading the dataset and taking a subset of the three most frequent labels, an\nimportant step to consider here is pre-processing the data. What’s different here com‐\npared to previous examples? Why can’t we just follow the same procedure as before?\nThere are a few things that are different about tweets compared to news articles or\nother such text, as we briefly discussed in Chapter 2 when we talked about text pre-\nprocessing. First, they are very short. Second, our traditional tokenizers may not\nwork well with tweets, splitting smileys, hashtags, Twitter handles, etc., into multiple\ntokens. Such specialized needs prompted a lot of research into NLP for Twitter in the\nrecent past, which resulted in several pre-processing options for tweets. One such sol‐\nution is a TweetTokenizer, implemented in the NLTK [21] library in Python. We’ll\ndiscuss more on this topic in Chapter 8. For now, let’s see how we can use a TweetTo\nkenizer in the following code snippet:\ntweeter = TweetTokenizer(strip_handles=True,preserve_case=False)\nmystopwords = set(stopwords.words(\"english\"))\n#Function to pre-process and tokenize tweets\ndef preprocess_corpus(texts):\n    def remove_stops_digits(tokens):\n    #Nested function to remove stopwords and digits\n          return [token for token in tokens if token not in mystopwords \n                  and not token.isdigit()]\n    return [remove_stops_digits(tweeter.tokenize(content)) for content in texts]\nmydata = preprocess_corpus(df_subset['content'])\nmycats = df_subset['sentiment']\n138 \n| \nChapter 4: Text Classification\n",
        "word_count": 393,
        "char_count": 2730,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 169,
        "text": "The next step in this process is to train a Doc2vec model to learn tweet representa‐\ntions. Ideally, any large dataset of tweets will work for this step. However, since we\ndon’t have such a ready-made corpus, we’ll split our dataset into train-test and use the\ntraining data for learning the Doc2vec representations. The first part of this process\ninvolves converting the data into a format readable by the Doc2vec implementation,\nwhich can be done using the TaggedDocument class. It’s used to represent a document\nas a list of tokens, followed by a “tag,” which in its simplest form can be just the file‐\nname or ID of the document. However, Doc2vec by itself can also be used as a nearest\nneighbor classifier for both multiclass and multilabel classification problems using .\nWe’ll leave this as an exploratory exercise for the reader. Let’s now see how to train a\nDoc2vec classifier for tweets through the code snippet below:\n#Prepare training data in doc2vec format:\nd2vtrain = [TaggedDocument((d),tags=[str(i)]) for i, d in enumerate(train_data)]\n#Train a doc2vec model to learn tweet representations. Use only training data!!\nmodel = Doc2Vec(vector_size=50, alpha=0.025, min_count=10, dm =1, epochs=100)\nmodel.build_vocab(d2vtrain)\nmodel.train(d2vtrain, total_examples=model.corpus_count, epochs=model.epochs)\nmodel.save(\"d2v.model\")\nprint(\"Model Saved\")\nTraining for Doc2vec involves making several choices regarding parameters, as seen\nin the model definition in the code snippet above. vector_size refers to the dimen‐\nsionality of the learned embeddings; alpha is the learning rate; min_count is the min‐\nimum frequency of words that remain in vocabulary; dm, which stands for distributed\nmemory, is one of the representation learners implemented in Doc2vec (the other is\ndbow, or distributed bag of words); and epochs are the number of training iterations.\nThere are a few other parameters that can be customized. While there are some\nguidelines on choosing optimal parameters for training Doc2vec models [22], these\nare not exhaustively validated, and we don’t know if the guidelines work for tweets.\nThe best way to address this issue is to explore a range of values for the ones that mat‐\nter to us (e.g., dm versus dbow, vector sizes, learning rate) and compare multiple mod‐\nels. How do we compare these models, as they only learn the text representation? One\nway to do it is to start using these learned representations in a downstream task—in\nthis case, text classification. Doc2vec’s infer_vector function can be used to infer the\nvector representation for a given text using a pre-trained model. Since there is some\namount of randomness due to the choice of hyperparameters, the inferred vectors\ndiffer each time we extract them. For this reason, to get a stable representation, we\nrun it multiple times (called steps) and aggregate the vectors. Let’s use the learned\nmodel to infer features for our data and train a logistic regression classifier:\n#Infer the feature representation for training and test data using \n#the trained model\nmodel= Doc2Vec.load(\"d2v.model\")\n#Infer in multiple steps to get a stable representation\ntrain_vectors =  [model.infer_vector(list_of_tokens, steps=50)\nUsing Neural Embeddings in Text Classification \n| \n139\n",
        "word_count": 498,
        "char_count": 3263,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 170,
        "text": "              for list_of_tokens in train_data]\ntest_vectors = [model.infer_vector(list_of_tokens, steps=50)\n              for list_of_tokens in test_data]\nmyclass = LogisticRegression(class_weight=\"balanced\") \n#because classes are not balanced\nmyclass.fit(train_vectors, train_cats)\npreds = myclass.predict(test_vectors)\nprint(classification_report(test_cats, preds))\nNow, the performance of this model seems rather poor, achieving an F1 score of 0.51\non a reasonably large corpus, with only three classes. There are a couple of interpreta‐\ntions for this poor result. First, unlike full news articles or even well-formed senten‐\nces, tweets contain very little data per instance. Further, people write with a wide\nvariety in spelling and syntax when they tweet. There are a lot of emoticons in differ‐\nent forms. Our feature representation should be able to capture such aspects. While\ntuning the algorithms by searching a large parameter space for the best model may\nhelp, an alternative could be to explore problem-specific feature representations, as\nwe discussed in Chapter 3. We’ll see how to do this for tweets in Chapter 8. An\nimportant point to keep in mind when using Doc2vec is the same as for fastText: if we\nhave to use Doc2vec for feature representation, we have to store the model that\nlearned the representation. While it’s not typically as bulky as fastText, it’s also not as\nfast to train. Such trade-offs need to be considered and compared before we make a\ndeployment decision.\nSo far, we’ve seen a range of feature representations and how they play a role for text\nclassification using ML algorithms. Let’s now turn to a family of algorithms that\nbecame popular in the past few years, known as “deep learning.”\nDeep Learning for Text Classification\nAs we discussed in Chapter 1, deep learning is a family of machine learning algo‐\nrithms where the learning happens through different kinds of multilayered neural\nnetwork architectures. Over the past few years, it has shown remarkable improve‐\nments on standard machine learning tasks, such as image classification, speech recog‐\nnition, and machine translation. This has resulted in widespread interest in using\ndeep learning for various tasks, including text classification. So far, we’ve seen how to\ntrain different machine learning classifiers, using BoW and different kinds of embed‐\nding representations. Now, let’s look at how to use deep learning architectures for text\nclassification.\nTwo of the most commonly used neural network architectures for text classification\nare convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\nLong short-term memory (LSTM) networks are a popular form of RNNs. Recent\napproaches also involve starting with large, pre-trained language models and fine-\ntuning them for the task at hand. In this section, we’ll learn how to train CNNs and\n140 \n| \nChapter 4: Text Classification\n",
        "word_count": 436,
        "char_count": 2907,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 171,
        "text": "LSTMs and how to tune a pre-trained language model for text classification using the\nIMDB sentiment classification dataset [23]. Note that a detailed discussion on how\nneural network architectures work is beyond the scope of this book. Interested read‐\ners can read the textbook by Goodfellow et al. [24] for a general theoretical discussion\nand Goldberg’s book [25] for NLP-specific uses of neural network architectures.\nJurafsky and Martin’s book [12] also provides a short but concise overview of differ‐\nent neural network methods for NLP.\nThe first step toward training any ML or DL model is to define a feature representa‐\ntion. This step has been relatively straightforward in the approaches we’ve seen so far,\nwith BoW or embedding vectors. However, for neural networks, we need further pro‐\ncessing of input vectors, as we saw in Chapter 3. Let’s quickly recap the steps involved\nin converting training and test data into a format suitable for the neural network\ninput layers:\n1. Tokenize the texts and convert them into word index vectors.\n2. Pad the text sequences so that all text vectors are of the same length.\n3. Map every word index to an embedding vector. We do that by multiplying word\nindex vectors with the embedding matrix. The embedding matrix can either be\npopulated using pre-trained embeddings or it can be trained for embeddings on\nthis corpus.\n4. Use the output from Step 3 as the input to a neural network architecture.\nOnce these are done, we can proceed with the specification of neural network archi‐\ntectures and training classifiers with them. The Jupyter notebook associated with this\nsection (Ch4/DeepNN_Example.ipynb) will walk you through the entire process from\ntext pre-processing to neural network training and evaluation. We’ll use Keras, a\nPython-based DL library. The code snippet below illustrates Steps 1 and 2:\n#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer.\n#Tokenizer is fit on training data only, and that is used to tokenize both train \n#and test data.\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(train_texts)\ntrain_sequences = tokenizer.texts_to_sequences(train_texts) \ntest_sequences = tokenizer.texts_to_sequences(test_texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n#Converting this to sequences to be fed into neural network. Max seq. len is \n#1000 as set earlier. Initial padding of 0s, until vector is of \n#size MAX_SEQUENCE_LENGTH\ntrainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\ntrainvalid_labels = to_categorical(np.asarray(train_labels))\ntest_labels = to_categorical(np.asarray(test_labels))\nDeep Learning for Text Classification \n| \n141\n",
        "word_count": 400,
        "char_count": 2797,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 172,
        "text": "i. There are other such pre-trained embeddings available. Our choice in this case is arbitrary.\nStep 3: If we want to use pre-trained embeddings to convert the train and test data\ninto an embedding matrix like we did in the earlier examples with Word2vec and\nfastText, we have to download them and use them to convert our data into the input\nformat for the neural networks. The following code snippet shows an example of how\nto do this using GloVe embeddings, which were introduced in Chapter 3. GloVe\nembeddings come with multiple dimensionalities, and we chose 100 as our dimen‐\nsion here. The value of dimensionality is a hyperparameter, and we can experiment\nwith other dimensions as well:i\nembeddings_index = {}\nwith open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n    for line in f:\n          values = line.split()\n          word = values[0]\n          coefs = np.asarray(values[1:], dtype='float32')\n          embeddings_index[word] = coefs\nnum_words = min(MAX_NUM_WORDS, len(word_index)) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i > MAX_NUM_WORDS:\n          continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n          embedding_matrix[i] = embedding_vector\nStep 4: Now, we’re ready to train DL models for text classification! DL architectures\nconsist of an input layer, an output layer, and several hidden layers in between the\ntwo. Depending on the architecture, different hidden layers are used. The input layer\nfor textual input is typically an embedding layer. The output layer, especially in the\ncontext of text classification, is a softmax layer with categorical output. If we want to\ntrain the input layer instead of using pre-trained embeddings, the easiest way is to call\nthe Embedding layer class in Keras, specifying the input and output dimensions. How‐\never, since we want to use pre-trained embeddings, we should create a custom\nembedding layer that uses the embedding matrix we just built. The following code\nsnippet shows how to do that:\nembedding_layer = Embedding(num_words, EMBEDDING_DIM,\n                        embeddings_initializer=Constant(embedding_matrix),\n                        input_length=MAX_SEQUENCE_LENGTH,\n                        trainable=False)\nprint(\"Preparing of embedding matrix is done\")\nThis will serve as the input layer for any neural network we want to use (CNN or\nLSTM). Now that we know how to pre-process the input and define an input layer,\n142 \n| \nChapter 4: Text Classification\n",
        "word_count": 356,
        "char_count": 2552,
        "fonts": [
          "MinionPro-Regular (8.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MinionPro-Regular (6.3pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 173,
        "text": "let’s move on to specifying the rest of the neural network architecture using CNNs\nand LSTMs.\nCNNs for Text Classification\nLet’s now look at how to define, train, and evaluate a CNN model for text classifica‐\ntion. CNNs typically consist of a series of convolution and pooling layers as the hid‐\nden layers. In the context of text classification, CNNs can be thought of as learning\nthe most useful bag-of-words/n-grams features instead of taking the entire collection\nof words/n-grams as features, as we did earlier in this chapter. Since our dataset has\nonly two classes—positive and negative—the output layer has two outputs, with the\nsoftmax activation function. We’ll define a CNN with three convolution-pooling lay‐\ners using the Sequential model class in Keras, which allows us to specify DL models\nas a sequential stack of layers—one after another. Once the layers and their activation\nfunctions are specified, the next task is to define other important parameters, such as\nthe optimizer, loss function, and the evaluation metric to tune the hyperparameters of\nthe model. Once all this is done, the next step is to train and evaluate the model. The\nfollowing code snippet shows one way of specifying a CNN architecture for this task\nusing the Python library Keras and prints the results with the IMDB dataset for this\nmodel:\nprint('Define a 1D CNN model.')\ncnnmodel = Sequential()\ncnnmodel.add(embedding_layer)\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(MaxPooling1D(5))\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(MaxPooling1D(5))\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(GlobalMaxPooling1D())\ncnnmodel.add(Dense(128, activation='relu'))\ncnnmodel.add(Dense(len(labels_index), activation='softmax'))\ncnnmodel.compile(loss='categorical_crossentropy',\n                    optimizer='rmsprop',\n                    metrics=['acc'])\ncnnmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=1, validation_data=(x_val, y_val))\nscore, acc = cnnmodel.evaluate(test_data, test_labels)\nprint('Test accuracy with CNN:', acc)\nAs you can see, we made a lot of choices in specifying the model, such as activation\nfunctions, hidden layers, layer sizes, loss function, optimizer, metrics, epochs, and\nbatch size. While there are some commonly recommended options for these, there’s\nno consensus on one combination that works best for all datasets and problems. A\ngood approach while building your models is to experiment with different settings\n(i.e., hyperparameters). Keep in mind that all these decisions come with some\nDeep Learning for Text Classification \n| \n143\n",
        "word_count": 348,
        "char_count": 2631,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 174,
        "text": "associated cost. For example, in practice, we have the number of epochs as 10 or\nabove. But that also increases the amount of time it takes to train the model. Another\nthing to note is that, if you want to train an embedding layer instead of using pre-\ntrained embeddings in this model, the only thing that changes is the line cnnmo\ndel.add(embedding_layer). Instead, we can specify a new embedding layer as, for\nexample, cnnmodel.add(Embedding(Param1, Param2)). The code snippet below\nshows the code and model performance for the same:\nprint(\"Defining and training a CNN model, training embedding layer on the fly \n      instead of using pre-trained embeddings\")\ncnnmodel = Sequential()\ncnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n…\n...\ncnnmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=1, validation_data=(x_val, y_val))\nscore, acc = cnnmodel.evaluate(test_data, test_labels)\nprint('Test accuracy with CNN:', acc)\nIf we run this code in the notebook, we’ll notice that, in this case, training the embed‐\nding layer on our own dataset seems to result in better classification on test data.\nHowever, if the training data were substantially small, sticking to the pre-trained\nembeddings, or using the domain adaptation techniques we’ll discuss later in this\nchapter, would be a better choice. Let’s look at how to train similar models using an\nLSTM.\nLSTMs for Text Classification\nAs we saw briefly in Chapter 1, LSTMs and other variants of RNNs in general have\nbecome the go-to way of doing neural language modeling in the past few years. This\nis primarily because language is sequential in nature and RNNs are specialized in\nworking with sequential data. The current word in the sentence depends on its con‐\ntext—the words before and after. However, when we model text using CNNs, this\ncrucial fact is not taken into account. RNNs work on the principle of using this con‐\ntext while learning the language representation or a model of language. Hence, they’re\nknown to work well for NLP tasks. There are also CNN variants that can take such\ncontext into account, and CNNs versus RNNs is still an open area of debate. In this\nsection, we’ll see an example of using RNNs for text classification. Now that we’ve\nalready seen one neural network in action, it’s relatively easy to train another! Just\nreplace the convolutional and pooling parts with an LSTM in the prior two code\nexamples. The following code snippet shows how to train an LSTM model using the\nsame IMDB dataset for text classification:\nprint(\"Defining and training an LSTM model, training embedding layer on the fly\")\nrnnmodel = Sequential()\nrnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n144 \n| \nChapter 4: Text Classification\n",
        "word_count": 423,
        "char_count": 2710,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 175,
        "text": "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nrnnmodel.add(Dense(2, activation='sigmoid'))\nrnnmodel.compile(loss='binary_crossentropy',\n               optimizer='adam',\n               metrics=['accuracy'])\nprint('Training the RNN')\nrnnmodel.fit(x_train, y_train,\n          batch_size=32,\n          epochs=1,\n          validation_data=(x_val, y_val))\nscore, acc = rnnmodel.evaluate(test_data, test_labels,\n                          batch_size=32)\nprint('Test accuracy with RNN:', acc)\nNotice that this code took much longer to run than the CNN example. While LSTMs\nare more powerful in utilizing the sequential nature of text, they’re much more data\nhungry as compared to CNNs. Thus, the relative lower performance of the LSTM on\na dataset need not necessarily be interpreted as a shortcoming of the model itself. It’s\npossible that the amount of data we have is not sufficient to utilize the full potential of\nan LSTM. As in the case of CNNs, several parameters and hyperparameters play\nimportant roles in model performance, and it’s always a good practice to explore mul‐\ntiple options and compare different models before finalizing on one.\nText Classification with Large, Pre-Trained Language Models\nIn the past two years, there have been great improvements in using neural network–\nbased text representations for NLP tasks. We discussed some of these in “Universal\nText Representations” on page 107. These representations have been used successfully\nfor text classification in the recent past by fine-tuning the pre-trained models to the\ngiven task and dataset. BERT, which was mentioned in Chapter 3, is a popular model\nused in this way for text classification. Let’s take a look at how to use BERT for text\nclassification using the IMDB dataset we used earlier in this section. The full code is\nin the relevant notebook (Ch4/BERT_Sentiment_Classification_IMDB.ipynb).\nWe’ll use ktrain, a lightweight wrapper to train and use pre-trained DL models using\nthe TensorFlow library Keras. ktrain provides a straightforward process for all steps,\nfrom obtaining the dataset and the pre-trained BERT to fine-tuning it for the classifi‐\ncation task. Let’s see how to load the dataset first through the code snippet below:\ndataset = tf.keras.utils.get_file(\nfname=\"aclImdb.tar.gz\",   \norigin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n extract=True,)\nOnce the dataset is loaded, the next step is to download the BERT model and pre-\nprocess the dataset according to BERT’s requirements. The following code snippet\nshows how to do this with ktrain’s functions:\nDeep Learning for Text Classification \n| \n145\n",
        "word_count": 355,
        "char_count": 2637,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 176,
        "text": "(x_train, y_train), (x_test, y_test), preproc = \n                       text.texts_from_folder(IMDB_DATADIR,maxlen=500,                            \n   preprocess_mode='bert',train_test_names=['train','test'],\nThe next step is to load the pre-trained BERT model and fine-tune it for this dataset.\nHere’s the code snippet to do this:\nmodel = text.text_classifier('bert', (x_train, y_train), preproc=preproc)\nlearner=ktrain.get_learner(model,train_data=(x_train,y_train),    \n                          val_data=(x_test, y_test), batch_size=6)\nlearner.fit_onecycle(2e-5, 4)\nThese three lines of code will train a text classifier using the BERT pre-trained model.\nAs with other examples we’ve seen so far, we would need to do parameter tuning and\na lot of experimentation to pick the best-performing model. We leave that as an exer‐\ncise for the reader.\nIn this section, we introduced the idea of using DL for text classification using two\nneural network architectures—CNN and LSTM—and showed how we can tune a\nstate-of-the-art, pre-trained language model (BERT) for a given dataset and classifica‐\ntion task. There are several variants to these architectures, and new models are being\nproposed every day by NLP researchers. We saw how to use one pre-trained language\nmodel, BERT. There are other such models, and this is a constantly evolving area in\nNLP research; the state of the art keeps changing every few months (or even weeks!).\nHowever, in our experience as industry practitioners, several NLP tasks, especially\ntext classification, still widely use several of the non-DL approaches we described ear‐\nlier in the chapter. Two primary reasons for this are a lack of the large amounts of\ntask-specific training data that neural networks demand and issues related to com‐\nputing and deployment costs.\nDL-based text classifiers are often nothing but condensed repre‐\nsentations of the data they were trained on. These models are often\nas good as the training dataset. Selecting the right dataset becomes\nall the more important in such cases.\nWe’ll end this section by reiterating what we mentioned earlier when we discussed the\ntext classification pipeline: in most industrial settings, it always makes sense to start\nwith a simpler, easy-to-deploy approach as your MVP and go from there incremen‐\ntally, taking customer needs and feasibility into account.\n146 \n| \nChapter 4: Text Classification\n",
        "word_count": 338,
        "char_count": 2396,
        "fonts": [
          "UbuntuMono-Regular (8.5pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 177,
        "text": "We’ve seen several approaches to building text classification models so far. Unlike\nheuristics-based approaches where the predictions can be justified by tracing back the\nrules applied on the data sample, ML models are treated as a black box while making\npredictions. However, in the recent past, the topic of interpretable ML started to gain\nprominence, and programs that can “explain” an ML model’s predictions exist now.\nLet’s take a quick look at their application for text classification.\nInterpreting Text Classification Models\nIn the previous sections, we’ve seen how to train text classifiers using multiple\napproaches. In all these examples, we took the classifier predictions as is, without\nseeking any explanations. In fact, most real-world use cases of text classification may\nbe similar—we just consume the classifier’s output and don’t question its decisions.\nTake spam classification: we generally don’t look for explanations of why a certain\nemail is classified as spam or regular email. However, there may be scenarios where\nsuch explanations are necessary.\nConsider a scenario where we developed a classifier that identifies abusive comments\non a discussion forum website. The classifier identifies comments that are objectiona‐\nble/abusive and performs the job of a human moderator by either deleting them or\nmaking them invisible to users. We know that classifiers aren’t perfect and can make\nerrors. What if the commenter questions this moderation decision and asks for an\nexplanation? Some method to “explain” the classification decision by pointing to\nwhich feature’s presence prompted such a decision can be useful in such cases. Such a\nmethod is also useful to provide some insights into the model and how it may per‐\nform on real-world data (instead of train/test sets), which may result in better, more\nreliable models in the future.\nAs ML models started getting deployed in real-world applications, interest in the\ndirection of model interpretability grew. Recent research [26, 27] resulted in usable\ntools [28, 29] for interpreting model predictions (especially for classification). Lime\n[28] is one such tool that attempts to interpret a black-box classification model by\napproximating it with a linear model locally around a given training instance. The\nadvantage of this is that such a linear model is expressed as a weighted sum of its fea‐\ntures and is easy to interpret for humans. For example, if there are two features, f1\nand f2, for a given test instance of a binary classifier with classes A and B, a Lime lin‐\near model around this instance could be something like -0.3 × f1 + 0.4 × f2 with a\nprediction B. This indicates that the presence of feature f1 will negatively affect this\nprediction (by 0.3) and skew it toward A. [26] explains this in more detail. Let’s now\nlook at how Lime [28] can be used to understand the predictions of a text classifier.\nInterpreting Text Classification Models \n| \n147\n",
        "word_count": 474,
        "char_count": 2944,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 178,
        "text": "Explaining Classifier Predictions with Lime\nLet’s take a model we already built earlier in this chapter and see how Lime can help\nus interpret its predictions. The following code snippet uses the logistic regression\nmodel we built earlier using the “Economy News Article Tone and Relevance” dataset,\nwhich classifies a given news article as being relevant or non-relevant and shows how\nwe can use Lime (the full code can be accessed in the notebook Ch4/Lime‐\nDemo.ipynb):\nfrom lime import lime_text\nfrom lime.lime_text import LimeTextExplainer\nfrom sklearn.pipeline import make_pipeline\ny_pred_prob = classifier.predict_proba(X_test_dtm)[:, 1]\nc = make_pipeline(vect, classifier)\nmystring = list(X_test)[221] #Take a string from test instance\nprint(c.predict_proba([mystring])) #Prediction is a \"No\" here, i.e., not relevant\nclass_names = [\"no\", \"yes\"] #not relevant, relevant\nexplainer = LimeTextExplainer(class_names=class_names)\nexp = explainer.explain_instance(mystring, c.predict_proba, num_features=6)\nexp.as_list()\nThis code shows six features that played an important role in making this prediction.\nThey’re as follows:\n[('YORK', 0.23416984139912805),\n ('NEW', -0.22724581340890154),\n ('showing', -0.12532906927967377),\n ('AP', -0.08486610147834726),\n ('dropped', 0.07958281943957331),\n ('trend', 0.06567603359316518)]\nThus, the output of the above code can be seen as a linear sum of these six features.\nThis would mean that, if we remove the features “NEW” and “showing,” the predic‐\ntion should move toward the opposite class, i.e., “relevant/Yes,” by 0.35 (the sum of\nthe weights of these two features). Lime also has functions to visualize these predic‐\ntions. Figure 4-8 shows a visualization of the above explanation.\nAs shown in the figure, the presence of three words—York, trend, and dropped—\nskews the prediction toward Yes, whereas the other three words skew the prediction\ntoward No. Apart from some uses we mentioned earlier, such visualizations of classi‐\nfiers can also help us if we want to do some informed feature selection.\nWe hope this brief introduction gave you an idea of what to do if you have to explain\na classifier’s predictions. We also have a notebook (Ch4/Lime_RNN.ipynb) that\nexplains an LSTM model’s predictions using Lime, and we leave this detailed explora‐\ntion of Lime as an exercise for the reader.\n148 \n| \nChapter 4: Text Classification\n",
        "word_count": 338,
        "char_count": 2384,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 179,
        "text": "Figure 4-8. Visualization of Lime’s explanation of a classifier’s prediction\nLearning with No or Less Data and Adapting to\nNew Domains\nIn all the examples we’ve seen so far, we had a relatively large training dataset avail‐\nable for the task. However, in most real-world scenarios, such datasets are not readily\navailable. In other cases, we may have an annotated dataset available, but it might not\nbe large enough to train a good classifier. There can also be cases where we have a\nlarge dataset of, say, customer complaints and requests for one product suite, but\nwe’re asked to customize our classifier to another product suite for which we have a\nvery small amount of data (i.e., we’re adapting an existing model to a new domain). In\nthis section, let’s discuss how to build good classification systems for these scenarios\nwhere we have no or little data or have to adapt to new domain training data.\nNo Training Data\nLet’s say we’re asked to design a classifier for segregating customer complaints for our\ne-commerce company. The classifier is expected to automatically route customer\ncomplaint emails into a set of categories: billing, delivery, and others. If we’re fortu‐\nnate, we may discover a source of large amounts of annotated data for this task within\nthe organization in the form of a historical database of customer requests and their\ncategories. If such a database doesn’t exist, where should we start to build our\nclassifier?\nThe first step in such a scenario is creating an annotated dataset where customer\ncomplaints are mapped to the set of categories mentioned above. One way to\napproach this is to get customer service agents to manually label some of the com‐\nplaints and use that as the training data for our ML model. Another approach is\ncalled “bootstrapping” or “weak supervision.” There can be certain patterns of infor‐\nmation in different categories of customer requests. Perhaps billing-related requests\nLearning with No or Less Data and Adapting to New Domains \n| \n149\n",
        "word_count": 334,
        "char_count": 2004,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1216,
            "height": 552,
            "ext": "png",
            "size_bytes": 23006
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 180,
        "text": "mention variants of the word “bill,” amounts in a currency, etc. Delivery-related\nrequests talk about shipping, delays, etc. We can get started with compiling some\nsuch patterns and using their presence or absence in a customer request to label it,\nthereby creating a small (perhaps noisy) annotated dataset for this classification task.\nFrom here, we can build a classifier to annotate a larger collection of data. Snorkel\n[30], a recent software tool developed by Stanford University, is useful for deploying\nweak supervision for various learning tasks, including classification. Snorkel was used\nto deploy weak supervision–based text classification models at industrial scale at\nGoogle [31]. They showed that weak supervision could create classifiers comparable\nin quality to those trained on tens of thousands of hand-labeled examples! [32] shows\nan example of how to use Snorkel to generate training data for text classification\nusing a large amount of unlabeled data.\nIn some other scenarios where large-scale collection of data is necessary and feasible,\ncrowdsourcing can be seen as an option to label the data. Websites like Amazon\nMechanical Turk and Figure Eight provide platforms to make use of human intelli‐\ngence to create high-quality training data for ML tasks. A popular example of using\nthe wisdom of crowds to create a classification dataset is the “CAPTCHA test,” which\nGoogle uses to ask if a set of images contain a given object (e.g., “Select all images that\ncontain a street sign”).\nLess Training Data: Active Learning and Domain Adaptation\nIn scenarios like the one described earlier, where we collected small amounts of data\nusing human annotations or bootstrapping, it may sometimes turn out that the\namount of data is too small to build a good classification model. It’s also possible that\nmost of the requests we collected belonged to billing and very few belonged to the\nother categories, resulting in a highly imbalanced dataset. Asking the agents to spend\nmany hours doing manual annotation is not always feasible. What should we do in\nsuch scenarios?\nOne approach to address such problems is active learning, which is primarily about\nidentifying which data points are more crucial to be used as training data. It helps\nanswer the following question: if we had 1,000 data points but could get only 100 of\nthem labeled, which 100 would we choose? What this means is that, when it comes to\ntraining data, not all data points are equal. Some data points are more important as\ncompared to others in determining the quality of the classifier trained. Active learn‐\ning converts this into a continuous process.\nUsing active learning for training a classifier can be described as a step-by-step\nprocess:\n1. Train the classifier with the available amount of data.\n2. Start using the classifier to make predictions on new data.\n150 \n| \nChapter 4: Text Classification\n",
        "word_count": 468,
        "char_count": 2891,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 181,
        "text": "3. For the data points where the classifier is very unsure of its predictions, send\nthem to human annotators for their correct classification.\n4. Include these data points in the existing training data and retrain the model.\nRepeat Steps 1 through 4 until a satisfactory model performance is reached.\nTools like Prodigy [33] have active learning solutions implemented for text classifica‐\ntion and support the efficient usage of active learning to create annotated data and\ntext classification models quickly. The basic idea behind active learning is that the\ndata points where the model is less confident are the data points that contribute most\nsignificantly to improving the quality of the model, and therefore only those data\npoints get labeled.\nNow, imagine a scenario for our customer complaint classifier where we have a lot of\nhistorical data for a range of products. However, we’re now asked to tune it to work\non a set of newer products. What’s potentially challenging in this situation? Typical\ntext classification approaches rely on the vocabulary of the training data. Hence,\nthey’re inherently biased toward the kind of language seen in the training data. So, if\nthe new products are very different (e.g., the model is trained on a suite of electronic\nproducts and we’re using it for complaints on cosmetic products), the pre-trained\nclassifiers trained on some other source data are unlikely to perform well. However,\nit’s also not realistic to train a new model from scratch on each product or product\nsuite, as we’ll again run into the problem of insufficient training data. Domain adap‐\ntation is a method to address such scenarios; this is also called transfer learning. Here,\nwe “transfer” what we learned from one domain (source) with large amounts of data\nto another domain (target) with less labeled data but large amounts of unlabeled data.\nWe already saw one example of how to use BERT for text classification earlier in this\nchapter.\nThis approach for domain adaptation in text classification can be summarized as\nfollows:\n1. Start with a large, pre-trained language model trained on a large dataset of the\nsource domain (e.g., Wikipedia data).\n2. Fine-tune this model using the target language’s unlabeled data.\n3. Train a classifier on the labeled target domain data by extracting feature repre‐\nsentations from the fine-tuned language model from Step 2.\nULMFit [34] is another popular domain adaptation approach for text classification.\nIn research experiments, it was shown that this approach matches the performance of\ntraining from scratch with 10 to 20 times more training examples and only 100\nlabeled examples in text classification tasks. When unlabeled data was used to fine-\ntune the pre-trained language model, it matched the performance of using 50 to 100\nLearning with No or Less Data and Adapting to New Domains \n| \n151\n",
        "word_count": 465,
        "char_count": 2862,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 182,
        "text": "times more labeled examples when trained from scratch, on the same text classifica‐\ntion tasks. Transfer learning methods are currently an active area of research in NLP.\nTheir use for text classification has not yet shown dramatic improvements on stan‐\ndard datasets, nor are they the default solution for all classification scenarios in\nindustry setups yet. But we can expect to see this approach yielding better and better\nresults in the near future.\nSo far, we’ve seen a range of text classification methods and discussed obtaining\nappropriate training data and using different feature representations for training the\nclassifiers. We also briefly touched on how to interpret the predictions made by some\ntext classification models. Let’s now consolidate what we’ve learned so far using a\nsmall case study of building a text classifier for a real-world scenario.\nCase Study: Corporate Ticketing\nLet’s consider a real-world scenario and learn how we can apply some of the concepts\nwe’ve discussed in this section. Imagine we’re asked to build a ticketing system for our\norganization that will track all the tickets or issues people face in the organization and\nroute them to either internal or external agents. Figure 4-9 shows a representative\nscreenshot for such a system; it’s a corporate ticketing system called Spoke.\nFigure 4-9. A corporate ticketing system\nNow let’s say our company has recently hired a medical counsel and partnered with a\nhospital. So our system should also be able to pinpoint any medical-related issue and\nroute it to the relevant people and teams. But while we have some past tickets, none\n152 \n| \nChapter 4: Text Classification\n",
        "word_count": 267,
        "char_count": 1661,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1438,
            "height": 825,
            "ext": "png",
            "size_bytes": 188836
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 183,
        "text": "of them are labeled as health related. In the absence of these labels, how will we go\nabout building such a health issue–related classification system?\nLet’s explore a couple of options:\nUse existing APIs or libraries\nOne option is to start with a public API or library and map its classes to what’s\nrelevant to us. For instance, the Google APIs mentioned earlier in the chapter can\nclassify content into over 700 categories. There are 82 categories associated with\nmedical or health issues. These include categories like /Health/Health Condi‐\ntions/Pain Management, /Health/Medical Facilities & Services/Doctors’ Offices, /\nFinance/Insurance/Health Insurance, etc.\nWhile not all categories are relevant to our organization, some could be, and we\ncan map these accordingly. For example, let’s say our company doesn’t consider\nsubstance abuse and obesity issues as relevant for medical counsel. We can\nignore /Health/Substance Abuse and /Health/Health Conditions/Obesity in this\nAPI. Similarly, whether insurance should be a part of HR or referred outside can\nalso be handled with these categories.\nUse public datasets\nWe can also adopt public datasets for our needs. For example, 20 Newsgroups is a\npopular text classification dataset, and it’s also part of the sklearn library. It has a\nrange of topics, including sci.med. We can also use it to train a basic classifier,\nclassifying all other topics in one category and sci.med in another.\nUtilize weak supervision\nWe have a history of past tickets, but they’re not labeled. So, we can consider\nbootstrapping a dataset out of it using the approaches described earlier in this\nsection. For example, consider having a rule: “If the past ticket contains words\nlike fever, diarrhea, headache, or nausea, put them in the medical counsel cate‐\ngory.” This rule can create a small amount of data, which we can use as a starting\npoint for our classifier.\nActive learning\nWe can use tools like Prodigy to conduct data collection experiments where we\nask someone working at the customer service desk to look at ticket descriptions\nand tag them with a preset list of categories. Figure 4-10 shows an example of\nusing Prodigy for this purpose.\nCase Study: Corporate Ticketing \n| \n153\n",
        "word_count": 357,
        "char_count": 2223,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 184,
        "text": "Figure 4-10. Active learning with Prodigy\nLearning from implicit and explicit feedback\nThroughout the process of building, iterating, and deploying this solution, we’re\ngetting feedback that we can use to improve our system. Explicit feedback could\nbe when the medical counsel or hospital says explicitly that the ticket was not rel‐\nevant. Implicit feedback could be extracted from other dependent variables like\nticket response times and ticket response rates. All of these could be factored in\nto improve our model using active learning techniques.\nA sample pipeline summarizing these ideas may look like what’s shown in\nFigure 4-11. We start with no labeled data and use either a public API or a model cre‐\nated with a public dataset or weak supervision as the first baseline model. Once we\nput this model to production, we’ll get explicit and implicit signals on where it’s\nworking or failing. We use this information to refine our model and active learning to\nselect the best set of instances that need to be labeled. Over time, as we collect more\ndata, we can build more sophisticated and deeper models.\n154 \n| \nChapter 4: Text Classification\n",
        "word_count": 190,
        "char_count": 1150,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1429,
            "height": 995,
            "ext": "png",
            "size_bytes": 96823
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 185,
        "text": "Figure 4-11. A pipeline for building a classifier when there’s no training data\nIn this section, we started looking at a practical scenario of not having enough train‐\ning data for building our own text classifier for our custom problem. We discussed\nseveral possible solutions to address the issue. Hopefully, this helps you foresee and\nprepare for some of the scenarios related to data collection and creation in your\nfuture projects related to text classification.\nPractical Advice\nSo far, we’ve shown a range of different methods for building text classifiers and\npotential issues you may run into. We’d like to end this chapter with some practical\nadvice that summarizes our observations and experience with building text classifica‐\ntion systems in industry. Most of these are generic enough to be applied to other top‐\nics in the book as well.\nEstablish strong baselines\nA common fallacy is to start with a state-of-the-art algorithm. This is especially\ntrue in the current era of deep learning, where every day, new approaches/algo‐\nrithms keep coming up. However, it’s always good to start with simpler\napproaches and try to establish strong baselines first. This is useful for three\nmain reasons:\nPractical Advice \n| \n155\n",
        "word_count": 199,
        "char_count": 1232,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1225,
            "height": 992,
            "ext": "png",
            "size_bytes": 62177
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 186,
        "text": "a. It helps us get a better understanding of the problem statement and key chal‐\nlenges.\nb. Building a quick MVP helps us get initial feedback from end users and stake‐\nholders.\nc. A state-of-the-art research model may give us only a minor improvement\ncompared to the baseline, but it might come with a huge amount of technical\ndebt.\nBalance training data\nWhile working with classification, it’s very important to have a balanced dataset\nwhere all categories have an equal representation. An imbalanced dataset can\nadversely impact the learning of the algorithm and result in a biased classifier.\nWhile we cannot always control this aspect of the training data, there are various\ntechniques to fix class imbalance in the training data. Some of them are collecting\nmore data, resampling (undersample from majority classes or oversample from\nminority classes), and weight balancing.\nCombine models and humans in the loop\nIn practical scenarios, it makes sense to combine the outputs of multiple classifi‐\ncation models with handcrafted rules from domain experts to achieve the best\nperformance for the business. In other cases, it’s practical to defer the decision to\na human evaluator if the machine is not sure of its classification decision. Finally,\nthere could also be scenarios where the learned model has to change with time\nand newer data. We’ll discuss some solutions for such scenarios in Chapter 11,\nwhich focuses on end-to-end systems.\nMake it work, make it better\nBuilding a classification system is not just about building a model. For most\nindustrial settings, building a model is often just 5% to 10% of the total project.\nThe rest consists of gathering data, building data pipelines, deployment, testing,\nmonitoring, etc. It is always good to build a model quickly, use it to build a sys‐\ntem, then start improvement iterations. This helps us to quickly identify major\nroadblocks and the parts that need the most work, and it’s often not the modeling\npart.\nUse the wisdom of many\nEvery text classification algorithm has its own strengths and weaknesses. There is\nno single algorithm that always works well. One way to circumvent this is via\nensembling: training multiple classifiers. The data is passed through every classi‐\nfier, and the predictions generated are combined (e.g., majority voting) to arrive\nat a final class prediction. An interested reader can look at the work of Dong et al.\n[35, 36] for a deep dive into ensemble methods for text classification.\n156 \n| \nChapter 4: Text Classification\n",
        "word_count": 412,
        "char_count": 2520,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 187,
        "text": "Wrapping Up\nIn this chapter, we saw how to address the problem of text classification from multi‐\nple viewpoints. We discussed how to identify a classification problem, tackle the vari‐\nous stages in a text classification pipeline, collect data to create relevant datasets, use\ndifferent feature representations, and train several classification algorithms. With\nthis, we hope you’re now well-equipped and ready to solve text classification prob‐\nlems for your use case and scenario and understand how to use existing solutions,\nbuild our own classifiers using various methods, and tackle the roadblocks you may\nface in the process. We focused on only one aspect of building text classification sys‐\ntems in industry applications: building the model. Issues related to the end-to-end\ndeployment of NLP systems will be dealt with in Chapter 11. In the next chapter, we’ll\nuse some of the ideas we learned here to tackle a related but different NLP problem:\ninformation extraction.\nReferences\n[1] United States Postal Service. The United States Postal Service: An American His‐\ntory, 57–60. ISBN: 978-0-96309-524-4. Last accessed June 15, 2020.\n[2] Gupta, Anuj, Saurabh Arora, Satyam Saxena, and Navaneethan Santhanam.\n“Noise reduction and smart ticketing for social media-based communication sys‐\ntems.” US Patent Application 20190026653, filed January 24, 2019.\n[3] Spasojevic, Nemanja and Adithya Rao. “Identifying Actionable Messages on Social\nMedia.” 2015 IEEE International Conference on Big Data: 2273–2281.\n[4] CLPSYCH: Computational Linguistics and Clinical Psychology Workshop. Shared\nTasks 2019.\n[5] Google Cloud. “Natural Language”. Last accessed June 15, 2020.\n[6] Amazon Comprehend. Last accessed June 15, 2020.\n[7] Azure Cognitive Services. Last accessed June 15, 2020.\n[8] Iderhoff, Nicolas. nlp-datasets: Alphabetical list of free/public domain datasets\nwith text data for use in Natural Language Processing (NLP), (GitHub repo). Last\naccessed June 15, 2020.\n[9] Kaggle. “Sentiment Analysis: Emotion in Text”. Last accessed June 15, 2020.\n[10] UC Irvine Machine Learning Repository. A collection of repositories for\nmachine learning. Last accessed June 15, 2020.\n[11] Google. “Dataset Search”. Last accessed June 15, 2020.\nWrapping Up \n| \n157\n",
        "word_count": 334,
        "char_count": 2258,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 188,
        "text": "[12] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft), 2018.\n[13] Lemaître, Guillaume, Fernando Nogueira, and Christos K. Aridas. “Imbalanced-\nlearn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine\nLearning”. The Journal of Machine Learning Research 18.1 (2017): 559–563.\n[14] For a detailed mathematical description of logistic regression, refer to Chapter 5\nin [12].\n[15] Google. Pre-trained word2vec model. Last accessed June 15, 2020.\n[16] Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov.\n“Enriching Word Vectors with Subword Information.” Transactions of the Association\nfor Computational Linguistics 5 (2017): 135–146.\n[17] Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. “Bag of\nTricks for Efficient Text Classification”. (2016).\n[18] Ramesh, Sree Harsha. torchDatasets, (GitHub repo). Last accessed June 15, 2020.\n[19] Joulin, Armand, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve\nJégou, and Tomas Mikolov. “Fasttext.zip: Compressing text classification models”.\n(2016).\n[20] For older Doc2vec versions, there are some pre-trained models; e.g., https://\noreil.ly/kt0U0 (last accessed June 15, 2020).\n[21] Natural Language Toolkit. “NLTK 3.5 documentation”. Last accessed June 15,\n2020.\n[22] Lau, Jey Han and Timothy Baldwin. “An Empirical Evaluation of doc2vec with\nPractical Insights into Document Embedding Generation”. (2016).\n[23] Stanford Artificial Intelligence Laboratory. “Large Movie Review Dataset”. Last\naccessed June 15, 2020.\n[24] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep Learning. Cam‐\nbridge: MIT Press, 2016. ISBN: 978-0-26203-561-3\n[25] Goldberg, Yoav. “Neural Network Methods for Natural Language Processing.”\nSynthesis Lectures on Human Language Technologies 10.1 (2017): 1–309.\n[26] Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “‘Why Should I Trust\nYou?’ Explaining the Predictions of Any Classifier.” Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining (2016):\n1135–1144.\n158 \n| \nChapter 4: Text Classification\n",
        "word_count": 292,
        "char_count": 2129,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 189,
        "text": "[27] Lundberg, Scott M. and Su-In Lee. “A Unified Approach to Interpreting Model\nPredictions.” Advances in Neural Information Processing Systems 30 (NIPS 2017):\n4765–4774.\n[28] Marco Tulio Correia Ribeiro. Lime: Explaining the predictions of any machine\nlearning classifier, (GitHub repo). Last accessed June 15, 2020.\n[29] Lundberg, Scott. shap: A game theoretic approach to explain the output of any\nmachine learning model, (GitHub repo).\n[30] Snorkel. “Programmatically Building and Managing Training Data”. Last\naccessed June 15, 2020.\n[31] Bach, Stephen H., Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao, Cas‐\nsandra Xia, Souvik Sen et al. “Snorkel DryBell: A Case Study in Deploying Weak\nSupervision at Industrial Scale”. (2018).\n[32] Snorkel. “Snorkel Intro Tutorial: Data Labeling”. Last accessed June 15, 2020.\n[33] Prodigy. Last accessed June 15, 2020.\n[34] Fast.ai. “Introducing state of the art text classification with universal language\nmodels”. Last accessed June 15, 2020.\n[35] Dong, Yan-Shi and Ke-Song Han. “A comparison of several ensemble methods\nfor text categorization.” IEEE International Conference on Services Computing (2004):\n419–422.\n[36] Caruana, Rich, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes.\n“Ensemble Selection from Libraries of Models.” Proceedings of the Twenty-First Inter‐\nnational Conference on Machine Learning (2004): 18.\nWrapping Up \n| \n159\n",
        "word_count": 201,
        "char_count": 1404,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 190,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 191,
        "text": "CHAPTER 5\nInformation Extraction\nWhat’s in a name? A rose\nby any other name would smell as sweet.\n—William Shakespeare\nWe deal with a lot of textual content every day, be it short messages on the phone or\ndaily emails or longer texts we read for fun or at work or to catch up on current\naffairs. Such text documents are a rich source of information for us. Depending on\nthe context, “information” can mean multiple things, such as key events, people, or\nrelationships between people, places, or organizations, etc. Information extraction\n(IE) refers to the NLP task of extracting relevant information from text documents.\nAn example of IE put to use in real-world applications are the short blurbs we see to\nthe right when we search for a popular figure’s name on Google.\nWhen compared to structured information sources like databases or tables or semi-\nstructured sources such as webpages (which have some markup), text is a form of\nunstructured data. For example, in a database, we know where to look for something\nbased on its schema. However, to a large extent, text documents typically comprise\nfree-flowing text without a set schema. This makes IE a challenging problem. Texts\nmay contain various kinds of information. In most cases, extracting information that\nhas a fixed pattern (e.g., addresses, phone numbers, dates, etc.) is relatively straight‐\nforward using pattern-based extraction techniques like regular expressions, even\nthough the text itself is considered unstructured data. However, extracting other\ninformation (e.g., names of people, relations between different entities in the text,\ndetails for a calendar event, etc.) may require more advanced language processing.\nIn this chapter, we’ll discuss various IE tasks and the methods for implementing them\nfor our applications. We’ll start with a brief historical background, followed by an\noverview of different IE tasks and applications of IE in the real world. We’ll then\nintroduce the typical NLP processing pipeline for solving any IE task and move on to\n161\n",
        "word_count": 327,
        "char_count": 2034,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (9.3pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 192,
        "text": "discuss how to solve specific IE tasks—key phrase extraction, named entity recogni‐\ntion, named entity disambiguation and linking, and relationship extraction—along\nwith some practical advice on implementing them in your projects. We’ll then present\na case study of how IE is used in a real-world scenario and briefly cover other\nadvanced IE tasks. With this introduction, let’s explore IE, starting with a brief\nhistory.\nApproaches for extracting different kinds of information from documents like scien‐\ntific papers and medical reports have been proposed in the past in the research com‐\nmunity. However, Message Understanding Conferences organized by the US Navy\n(1987–1998) [1] can be considered the starting point for modern-day research on\ninformation extraction from text. This was followed by the Automatic Content\nExtraction Program (1999–2008) [2] and the Text Analysis Conference (2009–2018)\nseries organized by NIST [3], which introduced competitions for extracting different\nkinds of information from text, from recognizing names of different entities to con‐\nstructing large, queryable knowledge bases. Existing libraries and methods for\nextracting various forms of information from text and their use in real-world applica‐\ntions trace their origins back to the research that started in these conference series.\nBefore we start looking at what the methods and libraries for IE are, let’s first take a\nlook at some examples of where IE is used in real-world applications.\nIE Applications\nIE is used in a wide range of real-world applications, from news articles, to social\nmedia, and even receipts. Here, we’ll cover the details of a few of them:\nTagging news and other content\nThere’s a lot of text generated about various events happening around the world\nevery day. In addition to classifying text using methods discussed in Chapter 4,\nit’s useful for some applications, such as search engines and recommendation sys‐\ntems, if such texts are tagged with important entities mentioned within them. For\nexample, look at Figure 5-1, which shows a screenshot from the Google News [4]\nhomepage.\nPeople (e.g., Jean Vanier), organizations (e.g., Progressive Conservative Party of\nOntario), locations (e.g., Canada), and events (e.g., Brexit) currently in the news\nare extracted and shown to the reader so that they can go directly to news about a\nspecific entity. This is one example of information extraction at work in a popular\napplication.\n162 \n| \nChapter 5: Information Extraction\n",
        "word_count": 385,
        "char_count": 2496,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 193,
        "text": "Figure 5-1. Screenshot from the Google News homepage\nChatbots\nA chatbot needs to understand the user’s question in order to generate/retrieve a\ncorrect response. For example, consider the question, “What are the best cafes\naround the Eiffel Tower?” The chatbot needs to understand that “Eiffel Tower”\nand “cafe” are locations, then identify cafes within a certain distance of the Eiffel\nTower. IE is useful in extracting such specific information from a pool of avail‐\nable data. We’ll discuss more on chatbots in Chapter 6.\nApplications in social media\nA lot of information is disseminated through social media channels like Twitter.\nExtracting informative excerpts from social media text may help in decision\nmaking. An example use case is extracting time-sensitive, frequently updated\ninformation, such as traffic updates and disaster relief efforts, based on tweets.\nNLP for Twitter is one of the most useful applications that utilizes the abundant\ninformation present in social media. We’ll touch on some of these applications in\nChapter 8.\nExtracting data from forms and receipts\nMany banking apps nowadays have the feature to scan a check and deposit the\nmoney directly into the user’s account. Whether you’re an individual, small busi‐\nness, or larger business enterprise, it’s not uncommon to use apps that scan bills\nand receipts. Along with optical character recognition (OCR), information\nextraction techniques play an important role in these apps [5, 6]. We won’t dis‐\ncuss this aspect in this chapter, as OCR is the primary step in such applications\nand isn’t part of the processing pipelines in this book.\nNow that we have an idea of what IE is and where it’s useful, let’s move on to under‐\nstanding what the different tasks covered under IE are.\nIE Applications \n| \n163\n",
        "word_count": 288,
        "char_count": 1787,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 375,
            "height": 276,
            "ext": "png",
            "size_bytes": 14506
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 194,
        "text": "IE Tasks\nIE is a term that’s used to refer to a range of different tasks of varying complexity. The\noverarching goal of IE is to extract “knowledge” from text, and each of these tasks\nprovides different information to do that. To understand what these tasks are, con‐\nsider the snippet from a New York Times article shown in Figure 5-2.\nFigure 5-2. A New York Times article from April 30, 2019 [7]\nAs human readers, we find several useful pieces of information in this blurb. For\nexample, we know that the article is about Apple, the company (and not the fruit),\nand that it mentions a person, Luca Maestri, who is the finance chief of the company.\nThe article is about the buyback of stock and other issues related to it. For a machine\nto understand all this involves different levels of IE.\nIdentifying that the article is about “buyback” or “stock price” relates to the IE task of\nkeyword or keyphrase extraction (KPE). Identifying Apple as an organization and Luca\nMaestri as a person comes under the IE task of named entity recognition (NER). Rec‐\nognizing that Apple is not a fruit, but a company, and that it refers to Apple, Inc. and\nnot some other company with the word “apple” in its name is the IE task of named\nentity disambiguation and linking. Extracting the information that Luca Maestri is the\nfinance chief of Apple refers to the IE task of relation extraction.\nThere are a few advanced IE tasks beyond those mentioned above. Identifying that\nthis article is about a single event (let’s call it “Apple buys back stocks”) and being able\nto link it to other articles talking about the same event over time refers to the IE task\nof event extraction. A related task is temporal information extraction, which aims to\n164 \n| \nChapter 5: Information Extraction\n",
        "word_count": 313,
        "char_count": 1771,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 562,
            "height": 416,
            "ext": "png",
            "size_bytes": 23928
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 195,
        "text": "extract information about times and dates, which is also useful for developing calen‐\ndar applications and interactive personal assistants. Finally, many applications, such\nas automatically generating weather reports or flight announcements, follow a stan‐\ndard template with some slots that need to be filled based on extracted data. This IE\ntask is known as template filling.\nEach of these tasks requires different levels of language processing. A range of rule-\nbased methods as well as supervised, unsupervised, and semi-supervised machine\nlearning (including state-of-the-art deep learning approaches) can be used for devel‐\noping solutions to solve these tasks. However, considering that IE is very much\ndependent on the application domain (e.g., finance, news, airlines, etc.), IE in indus‐\ntry is generally implemented as a hybrid system incorporating rule-based and\nlearning-based approaches [8, 9]. IE is still a very active area of research, and not all\nthese tasks are considered “solved” or matured enough to have standard approaches\nthat can be used in real-world application scenarios. Tasks such as KPE and NER are\nmore widely studied than others and have some tried-and-tested solutions. The rest\nof the tasks are relatively more challenging, and it’s more common to rely on pay-as-\nyou-use services from large providers like Microsoft, Google, and IBM.\nAn important point to note regarding IE is that the datasets needed to train IE models\nare typically more specialized than what we saw, for example, in Chapter 4, where all\nwe needed to get started was a collection of texts mapped to some categories. Hence,\nreal-world use cases of IE may not always require us to train models from scratch,\nand we can make use of external APIs for some tasks. Before moving on to specific\ntasks, let’s first take a look at the general NLP pipeline for any IE task.\nThe General Pipeline for IE\nThe general pipeline for IE requires more fine-grained NLP processing than what we\nsaw for text classification (Chapter 4). For example, to identify named entities (per‐\nsons, organizations, etc.), we would need to know the part-of-speech tags of words.\nFor relating multiple references to the same entity (e.g., Albert Einstein, Einstein, the\nscientist, he, etc.), we would need coreference resolution. Note that none of these are\nmandatory steps for building a text classification system. Thus, IE is a task that is\nmore NLP intensive than text classification. Figure 5-3 shows a typical NLP pipeline\nfor IE tasks. Not all steps in the pipeline are necessary for all IE tasks, and the figure\ndemonstrates which IE tasks require what levels of analysis.\nWe discussed the details of the different processing steps illustrated in this figure in\nChapters 1 and 2. As the figure shows, key phrase extraction is the task requiring\nminimal NLP processing (some algorithms also do POS tagging before extracting\nkeyphrases), whereas, other than named entity recognition, all the other IE tasks\nrequire deeper NLP pre-processing followed by models developed for those specific\ntasks. IE tasks are typically evaluated in terms of precision, recall, and F1 scores using\nThe General Pipeline for IE \n| \n165\n",
        "word_count": 511,
        "char_count": 3194,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 196,
        "text": "standard evaluation sets. Considering the different levels of NLP pre-processing\nrequired, IE tasks are also affected by the accuracy of these processing steps them‐\nselves. Collecting relevant training data and training our own models for IE, if neces‐\nsary, should take all these aspects into account. With this background, let’s now start\nlooking at each of the IE tasks, one by one.\nFigure 5-3. IE pipeline illustrating NLP processing needed for some IE tasks\nKeyphrase Extraction\nConsider a scenario where we want to buy a product, which has a hundred reviews,\non Amazon. There’s no way we’re going to read all of them to get an idea of what\nusers think about the product. To facilitate this, Amazon has a filtering feature: “Read\nreviews that mention.” This presents a bunch of keywords or phrases that several peo‐\nple used in these reviews to filter the reviews, as shown in Figure 5-4. This is a good\nexample of where KPE can be useful in an application we all use.\nKeyword and phrase extraction, as the name indicates, is the IE task concerned\nwith extracting important words and phrases that capture the gist of the text from a\ngiven text document. It’s useful for several downstream NLP tasks, such as\n166 \n| \nChapter 5: Information Extraction\n",
        "word_count": 214,
        "char_count": 1256,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1205,
            "height": 1139,
            "ext": "png",
            "size_bytes": 55125
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 197,
        "text": "search/information retrieval, automatic document tagging, recommendation systems,\ntext summarization, etc.\nFigure 5-4. “Read reviews that mention” on Amazon.ca\nKPE is a well-studied problem in the NLP community, and the two most commonly\nused methods to solve it are supervised learning and unsupervised learning. Super‐\nvised learning approaches require corpora with texts and their respective keyphrases\nand use engineered features or DL techniques [10]. Creating such labeled datasets for\nKPE is a time- and cost-intensive endeavor. Hence, unsupervised approaches that do\nnot require a labeled dataset and are largely domain agnostic are more popular for\nKPE. These approaches are also more commonly used in real-world KPE applica‐\ntions. Recent research has also shown that state-of-the-art DL methods for KPE don’t\nperform any better than unsupervised approaches [11].\nAll the popular unsupervised KPE algorithms are based on the idea of representing\nthe words and phrases in a text as nodes in a weighted graph where the weight indi‐\ncates the importance of that keyphrase. Keyphrases are then identified based on how\nconnected they are with the rest of the graph. The top-N important nodes from the\ngraph are then returned as keyphrases. Important nodes are those words and phrases\nthat are frequent enough and also well connected to different parts of the text. The\ndifferent graph-based KPE approaches differ in the way they select potential words/\nphrases from the text (from a large set of possible words and phrases in the entire\ntext) and the way these words/phrases are scored in the graph.\nThere’s a huge body of work on this topic, with some working implementations avail‐\nable. In most cases, existing approaches are a great starting point to meet your\nrequirements. How can we use these to implement a keyphrase extractor in our\nproject? Let ‘s look at an example.\nImplementing KPE\nThe Python library textacy [12], built on top of the well-known library spaCy [13],\ncontains implementations for some of the common graph-based keyword and phrase\nextraction algorithms. The notebook associated with this section (Ch5/KPE.ipynb)\nillustrates the use of textacy to extract keyphrases using two algorithms, TextRank\nKeyphrase Extraction \n| \n167\n",
        "word_count": 351,
        "char_count": 2257,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 623,
            "height": 187,
            "ext": "png",
            "size_bytes": 7753
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 198,
        "text": "[14] and SGRank. We’ll use a text file that talks about the history of NLP as our test\ndocument. The code snippet below illustrates KPE with textacy:\nfrom textacy import *\nimport textacy.ke\nmytext = open(“nlphistory.txt”).read()\nen = textacy.load_spacy_lang(\"en_core_web_sm\", disable=(\"parser\",))\ndoc = textacy.make_spacy_doc(mytext, lang=en)\nprint(\"Textrank output: \", [kps for kps, weights in \ntextacy.ke.textrank(doc, normalize=\"lemma\",  topn=5)])\nprint(\"SGRank output: \", [kps for kps, weights in \ntextacy.ke.sgrank(mydoc, n_keyterms=5)])\nOutput: \nTextrank output:  ['successful natural language processing system', \n'statistical machine translation system', 'natural language system', \n'statistical natural language processing', 'natural language task']\nSGRank output:  ['natural language processing system', \n'statistical machine translation', 'research', 'late 1980', 'early']\nThere are numerous options for how long our n-grams should be in these phrases;\nwhat POS tags should be considered or ignored; what pre-processing should be done\na priori; how to eliminate overlapping n-grams, such as statistical machine transla‐\ntion and machine translation in the above example; and so on. Some of these are\nexplored in the notebook, and we leave the rest as exercises for the reader.\nWe showed one example of implementing KPE with textacy. There are other options,\nthough. For example, the Python library gensim has a keyword extractor based on\nTextRank [15]. [16] shows how to implement TextRank from scratch. You can\nexplore multiple library implementations and compare them before choosing one.\nPractical Advice\nWe’ve seen how keyphrase extraction can be implemented using spaCy and textacy\nand how we can modify it to suit our needs. From a practical point of view, there are\na few caveats to keep in mind when using such graph-based algorithms in produc‐\ntion, though. We’ll list a few of them below, along with some suggestions for working\naround them based on our experience with adding KPE as a feature in software\nproducts:\n• The process of extracting potential n-grams and building the graph with them is\nsensitive to document length, which could be an issue in a production scenario.\nOne approach to dealing with it is to not use the full text, but instead try using\n168 \n| \nChapter 5: Information Extraction\n",
        "word_count": 342,
        "char_count": 2324,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 199,
        "text": "the first M% and the last N% of the text, since we would expect that the introduc‐\ntory and concluding parts of the text should cover the main summary of the text.\n• Since each keyphrase is independently ranked, we sometimes end up seeing over‐\nlapping keyphrases (e.g., “buy back stock” and “buy back”). One solution for this\ncould be to use some similarity measure (e.g., cosine similarity) between the top-\nranked keyphrases and choose the ones that are most dissimilar to one another.\ntextacy already implements a function to address this issue, as shown in the\nnotebook.\n• Seeing counterproductive patterns (e.g., a keyphrase that starts with a preposition\nwhen you don’t want that) is another common problem. This is relatively\nstraightforward to handle by tweaking the implementation code for the algo‐\nrithm and explicitly encoding information about such unwanted word patterns.\n• Improper text extraction can affect the rest of the KPE process, especially when\ndealing with formats such as PDF or scanned images. This is primarily because\nKPE is sensitive to sentence structure in the document. Hence, it’s always a good\nidea to add some post-processing to the extracted key phrases list to create a\nfinal, meaningful list without noise.\nA custom solution could be a combination of an existing graph-based KPE algorithm\nthat addresses the above-mentioned issues and a domain-specific list of heuristics, if\navailable. From our experience, this covers the issues most commonly encountered\nwith KPE in typical NLP projects.\nIn this section, we saw how to use KPE algorithms to extract important words and\nphrases from any document and some ways to overcome potential challenges. While\nsuch keyphrases can potentially capture the names of important entities in the text,\nwe’re not specifically looking for them when we use KPE algorithms. Let’s now look at\nthe next—and perhaps most popular—IE task, which is designed to look specifically\nfor the presence of named entities in the text.\nNamed Entity Recognition\nConsider a scenario where the user asks a search query—“Where was Albert Einstein\nborn?”—using Google search. Figure 5-5 shows a screenshot of what we see before a\nlist of search results.\nTo be able to show “Ulm, Germany” for this query, the search engine needs to deci‐\npher that Albert Einstein is a person before going on to look for a place of birth. This\nis an example of NER in action in a real-world application.\nNamed Entity Recognition \n| \n169\n",
        "word_count": 405,
        "char_count": 2471,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 200,
        "text": "Figure 5-5. Screenshot of a Google search result\nNER refers to the IE task of identifying the entities in a document. Entities are typi‐\ncally names of persons, locations, and organizations, and other specialized strings,\nsuch as money expressions, dates, products, names/numbers of laws or articles, and\nso on. NER is an important step in the pipeline of several NLP applications involving\ninformation extraction. Figure 5-6 illustrates the function of NER using the displaCy\nvisualizer by explosion.ai [17].\nAs seen in the figure, for a given text, NER is expected to identify person names, loca‐\ntions, dates, and other entities. Different categories of entities identified here are some\nof the ones commonly used in NER system development [18]. NER is a prerequisite\nfor being able to do other IE tasks, such as relation extraction or event extraction,\nwhich were introduced earlier in this chapter and will be discussed in greater detail\nlater on. NER is also useful in other applications like machine translation, as names\nneed not necessarily be translated while translating a sentence. So, clearly, there’s a\nrange of scenarios in NLP projects where NER is a major component. It’s one of the\ncommon tasks you’re likely to encounter in NLP projects in industry. How do we\nbuild such an NER system? The rest of this section focuses on this question, consider‐\ning three cases: building our own NER system, using existing libraries, and using\nactive learning.\n170 \n| \nChapter 5: Information Extraction\n",
        "word_count": 244,
        "char_count": 1507,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1029,
            "height": 629,
            "ext": "png",
            "size_bytes": 361956
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 201,
        "text": "Figure 5-6. NER example using the displaCy visualizer\nBuilding an NER System\nA simple approach to building an NER system is to maintain a large collection of per‐\nson/organization/location names that are the most relevant to our company (e.g.,\nnames of all clients, cities in their addresses, etc.); this is typically referred to as a\ngazetteer. To check whether a given word is a named entity or not, just do a lookup in\nthe gazetteer. If a large number of entities present in our data are covered by a gazet‐\nteer, then it’s a great way to start, especially when we don’t have an existing NER sys‐\ntem available. There are a few questions to consider with such an approach. How\ndoes it deal with new names? How do we periodically update this database? How does\nit keep track of aliases, i.e., different variations of a given name (e.g., USA, United\nStates, etc.)?\nAn approach that goes beyond a lookup table is rule-based NER, which can be based\non a compiled list of patterns based on word tokens and POS tags. For example, a\npattern “NNP was born,” where “NNP” is the POS tag for a proper noun, indicates\nthat the word that was tagged “NNP” refers to a person. Such rules can be program‐\nmed to cover as many cases as possible to build a rule-based NER system. Stanford\nNLP’s RegexNER [19] and spaCy’s EntityRuler [20] provide functionalities to imple‐\nment your own rule-based NER.\nA more practical approach to NER is to train an ML model, which can predict the\nnamed entities in unseen text. For each word, a decision has to be made whether or\nnot that word is an entity, and if it is, what type of the entity it is. In many ways, this is\nvery similar to the classification problems we discussed in detail in Chapter 4. The\nNamed Entity Recognition \n| \n171\n",
        "word_count": 319,
        "char_count": 1763,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1243,
            "height": 697,
            "ext": "png",
            "size_bytes": 81118
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 202,
        "text": "only difference here is that NER is a “sequence labeling” problem [21]. The typical\nclassifiers we saw in Chapter 4 predict labels for texts independent of their surround‐\ning context. Consider a classifier that classifies sentences in a movie review into posi‐\ntive/negative/neutral categories based on their sentiment. This classifier does not\n(usually) take into account the sentiment of previous (or subsequent) sentences when\nclassifying the current sentence. In a sequence classifier, such context is important. A\ncommon use case for sequence labeling is POS tagging, where we need information\nabout the parts of speech of surrounding words to estimate the part of speech of the\ncurrent word. NER is traditionally modeled as a sequence classification problem,\nwhere the entity prediction for the current word also depends on the context. For\nexample, if the previous word was a person name, there’s a higher probability that the\ncurrent word is also a person name if it’s a noun (e.g., first and last names).\nTo illustrate the difference between a normal classifier and a sequence classifier, con‐\nsider the following sentence: “Washington is a rainy state.” When a normal classifier\nsees this sentence and has to classify it word by word, it has to make a decision as to\nwhether Washington refers to a person (e.g., George Washington) or the State of\nWashington without looking at the surrounding words. It’s possible to classify the\nword “Washington” in this particular sentence as a location only after looking at the\ncontext in which it’s being used. It’s for this reason that sequence classifiers are used\nfor training NER models.\nConditional random fields (CRFs) is one of the popular sequence classifier training\nalgorithms. The notebook associated with this section (Ch5/NERTraining.ipynb)\nshows how we can use CRFs to train an NER system. We’ll use CONLL-03, a popular\ndataset used for training NER systems [22], and an open source sequence labeling\nlibrary called sklearn-crfsuite [23], along with a set of simple word- and POS tag–\nbased features, which provide contextual information we need for this task.\nTo perform sequence classification, we need data in a format that allows us to model\nthe context. Typical training data for NER looks like Figure 5-7, which is a sentence\nfrom the CONLL-03 dataset.\nThe labels in the figure follow what’s known as a BIO notation: B indicates the begin‐\nning of an entity; I, inside an entity, indicates when entities comprise more than one\nword; and O, other, indicates non-entities. Peter Such is a name with two words in the\nexample shown in Figure 5-7. Thus, “Peter” gets tagged as a B-PER, and “Such” gets\ntagged as an I-PER to indicate that Such is a part of the entity from the previous\nword. The remaining entities in this example, Essex, Yorkshire, and Headingley, are\nall one-word entities. So, we only see B-ORG and B-LOC as their tags. Once we\nobtain a dataset of sentences annotated in this form and we have a sequence classifier\nalgorithm, how should we train an NER system?\n172 \n| \nChapter 5: Information Extraction\n",
        "word_count": 508,
        "char_count": 3087,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 203,
        "text": "The steps are the same as those for the text classifiers we saw in Chapter 4:\n1. Load the dataset\n2. Extract the features\n3. Train the classifier\n4. Evaluate it on a test set\nFigure 5-7. NER training data format example\nNamed Entity Recognition \n| \n173\n",
        "word_count": 47,
        "char_count": 253,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 276,
            "height": 801,
            "ext": "png",
            "size_bytes": 21047
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 204,
        "text": "Loading the dataset is straightforward. This particular dataset is also already split into\na train/dev/test set. So, we’ll train the model using the training set. We saw a range of\nfeature representation techniques in Chapter 3. Let’s look at an example using hand‐\ncrafted features this time. What features seem intuitively relevant for this task? To\nidentify names of people or places, for example, patterns such as whether the word\nstarts with an uppercase character or whether it’s preceded or succeeded by a verb/\nnoun, etc., can be used as starting points to train an NER model. The following code\nsnippet shows a function that extracts the previous and next words’ POS tags for a\ngiven sentence. The notebook has a more elaborate feature set:\ndef sent2feats(sentence):\n    feats = []\n    sen_tags = pos_tag(sentence)\n    for i in range(0,len(sentence)):\n         word = sentence[i]\n         wordfeats = {}\n         #POS tag features: current tag, previous and next 2 tags.\n         wordfeats['tag'] = sen_tags[i][1]\n         if i == 0:\n          wordfeats[\"prevTag\"] = \"<S>\"\n         elif i == 1:\n          wordfeats[\"prevTag\"] = sen_tags[0][1]\n         else:\n          wordfeats[\"prevTag\"] = sen_tags[i - 1][1]\n         if i == len(sentence) - 2:\n          wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n         elif i == len(sentence) - 1:\n          wordfeats[\"nextTag\"] = \"</S>\"\n         else:\n          wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n         feats.append(wordfeats)\n    return feats\nAs you can see from the wordfeats variable in this code sample, each word is trans‐\nformed into a dictionary of features, and therefore each sentence will look like a list of\ndictionaries (the variable feats in the code), which will be used by the CRF classifier.\nThe following code snippet shows a function to train an NER system with a CRF\nmodel and evaluates the model performance on the development set:\n#Train a sequence model\ndef train_seq(X_train,Y_train,X_dev,Y_dev):\n    crf = CRF(algorithm='lbfgs', c1=0.1, c2=10, max_iterations=50)\n    crf.fit(X_train, Y_train)\n    labels = list(crf.classes_)\n    y_pred = crf.predict(X_dev)\n    sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n    print(metrics.flat_f1_score(Y_dev,y_pred,average='weighted',\n                          labels=labels))\n174 \n| \nChapter 5: Information Extraction\n",
        "word_count": 312,
        "char_count": 2361,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 205,
        "text": "Training this CRF model gave an F1 score of 0.92 on the development data, which is a\nvery good score! The notebook shows more detailed evaluation measures and how to\ncalculate them. Here, we showed some of the most commonly used features in learn‐\ning an NER system and used a popular training method and a publicly available data‐\nset. Clearly, there’s a lot to be done in terms of tuning the model and developing\n(even) better features; this example only serves to illustrate one way of developing an\nNER model quickly using one particular library in case you need to and you have a\nrelevant dataset. MITIE [24] is another such library to train NER systems.\nRecent advances in NER research either exclude or augment the kind of feature engi‐\nneering we did in this example with neural network models. NCRF++ [25] is another\nlibrary that can be used to train your own NER using different neural network archi‐\ntectures. A notebook that uses the BERT model for training an NER system using the\nsame dataset is available in the GitHub repo (Ch5/BERT_CONLL_NER.ipynb). We\nleave working through that as an exercise for the reader.\nWe took a quick tour of how to train our own NER system. However, in real-world\nscenarios, using the trained model by itself won’t be sufficient, as the data keeps\nchanging and new entities keep getting added, and there will also be some domain-\nspecific entities or patterns that were not seen in generic training datasets. Hence,\nmost NER systems deployed in real-world scenarios use a combination of ML mod‐\nels, gazetteers, and some pattern matching–based heuristics to improve their perfor‐\nmance [26]. [24] shows an example of how Rasa, a company that builds intelligent\nchatbots, improves its entity extraction using lookup tables.\nClearly, to build these NER systems ourselves, we need large, annotated datasets in a\nformat similar to the one shown in Figure 5-7. While datasets like CONLL-03 are\navailable, they work with a limited set of entities (person, organization, location, mis‐\ncellaneous, other) and in limited domains. There are other such datasets, such as\nOntoNotes [27], which are much larger and cover different kinds of text. However,\nthey’re not freely available and usually need to be purchased under expensive license\nagreements, which may not always be supported by our organizations’ budgets. So,\nwhat should we do?\nNER Using an Existing Library\nWhile all this discussion about training an NER system may make building and\ndeploying it look like a long process (starting with procuring a dataset), thankfully,\nNER has been well researched over the past few decades, and we have off-the-shelf\nlibraries to start with. Stanford NER [28], spaCy, and AllenNLP [29] are some well-\nknown NLP libraries that can be used to incorporate a pre-trained NER model into a\nsoftware product. The code snippet below illustrates using NER from spaCy:\nNamed Entity Recognition \n| \n175\n",
        "word_count": 484,
        "char_count": 2924,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 206,
        "text": "import spacy\nnlp = spacy.load(\"en_core_web_lg\")\ntext_from_fig = \"On Tuesday, Apple announced its plans for another major chunk \n                 of the money: It will buy back a further $75 billion in stock.\"\ndoc = nlp(text_from_fig)\nfor ent in doc.ents:\n    if ent.text:\n         print(ent.text, \"\\t\", ent.label_)\nRunning this code snippet will show Tuesday as DATE, Apple as ORG, and $75 bil‐\nlion as MONEY. Considering that spaCy’s NER is based on a state-of-the-art neural\nmodel coupled with some pattern matching and heuristics, it’s a good starting point.\nHowever, we may run into two issues:\n1. As mentioned earlier, we may be using NER in a specific domain, and the pre-\ntrained models may not capture the specific nature of our own domain.\n2. Sometimes, we may want to add new categories to the NER system without hav‐\ning to collect a large dataset for all the common categories.\nWhat should we do in such cases?\nNER Using Active Learning\nFrom our experience, the best approach to NER when we want customized solutions\nbut don’t want to train everything from scratch is to start with an off-the-shelf prod‐\nuct and either augment it with customized heuristics for our problem domain (using\ntools such as RegexNER or EntityRuler) and/or use active learning using tools like\nProdigy (like we saw in Chapter 4 for text classification). This allows us to improve an\nexisting pre-trained NER model by manually tagging a few example sentences con‐\ntaining new NER categories or correct a few model predictions manually and use\nthese to retrain the model. [30] shows some examples of going through this process\nusing Prodigy.\nIn general, in most cases, we don’t always have to think about developing an NER sys‐\ntem from scratch. If we do have to develop an NER system from scratch, the first\nthing we would need, as we saw in this section, is a large collection of annotated data\nof sentences where each word/token is tagged with its category (entity type or other).\nOnce such a dataset is available, the next step is to use it to obtain handcrafted and/or\nneural feature representations and feed them to a sequence labeling model. Chapters\n8 and 9 in [31] deal with specific methods to learn from such sequences. In the\nabsence of such data, rule-based NER is the first step.\n176 \n| \nChapter 5: Information Extraction\n",
        "word_count": 390,
        "char_count": 2323,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 207,
        "text": "Start with a pre-trained NER model and enhance it with heuristics,\nactive learning, or both.\nPractical Advice\nSo far, we’ve taken a quick look at how to use existing NER systems, discussed some\nways of augmenting them, and discussed how to train our own NER from scratch.\nDespite the fact that state-of-the-art NER is highly accurate (with F1 scores over 90%\nusing standard evaluation frameworks for NER in NLP research), there are several\nissues to keep in mind when using NER in our own software applications. Here are a\ncouple caveats based on our own experience with developing NER systems:\n• NER is very sensitive to the format of its input. It’s more accurate with well-\nformatted plain text than with, say, a PDF document from which plain text needs\nto be extracted first. While it’s possible to build custom NER systems for specific\ndomains or for data like tweets, the challenge with PDFs comes from the failure\nto be 100% accurate in extracting text from them while preserving the structure.\n[32] illustrates some of the challenges with PDF-to-text extraction. Why do we\nneed to be so accurate in properly extracting the structure from PDFs, though?\nIn PDFs, partial sentences, headings, and formatting are common, and they can\nall mess up NER accuracy. There’s no single solution for this. One approach is to\ndo custom post-processing of PDFs to extract blobs of text, then run NER on the\nblobs.\nIf you’re working with documents, such as reports, etc., pre-\nprocess them to extract text blobs, then run NER on them.\n• NER is also very sensitive to the accuracy of the prior steps in its processing pipe‐\nline: sentence splitting, tokenization, and POS tagging (refer back to Figure 5-2).\nTo understand how improper sentence splitting can result in poor NER results,\ntry taking the content from the screenshot back in Figure 5-1 and looking at the\noutput from spaCy (see the notebook Ch5/NERIssues.ipynb for a short illustra‐\ntion). So, some amount of pre-processing may be necessary before passing a\npiece of text into an NER model to extract entities.\nDespite such shortcomings, NER is immensely useful for many IE scenarios, such as\ncontent tagging, search, and mining social media to identify customer feedback about\nspecific products, to name a few. While NER (and KPE) serve the useful task of\nNamed Entity Recognition \n| \n177\n",
        "word_count": 394,
        "char_count": 2343,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 208,
        "text": "identifying important words, phrases, and entities in documents, some NLP applica‐\ntions require further analysis of language, which leads us to more advanced NLP\ntasks. One such IE task is entity disambiguation or entity linking, and it’s the topic of\nthe next section.\nNamed Entity Disambiguation and Linking\nConsider a scenario where we’re working on the data science team of a large newspa‐\nper publication (say, The New York Times). We’re charged with the task of building a\nsystem that creates visual representation of news stories by connecting different enti‐\nties mentioned in the stories to what they refer to in the real world, as shown in\nFigure 5-8.\nFigure 5-8. Entity linking by IBM [33]\nDoing this requires knowledge of several IE tasks beyond what we’ve seen with NER\nand KPE. As a first step, we have to know what these entities or keywords actually\nrefer to in the real world. Let’s take another example to illustrate why this could be\nchallenging. Consider this sentence: “Lincoln drives a Lincoln Aviator and lives on\nLincoln Way.” All three mentions of “Lincoln” here refer to different entities and dif‐\nferent types of entities: the first Lincoln is a person, the second one is a vehicle, and\nthe third is a location. How can we reliably link the three Lincolns to their correct\nWikipedia pages like in Figure 5-8?\n178 \n| \nChapter 5: Information Extraction\n",
        "word_count": 233,
        "char_count": 1380,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 698,
            "height": 416,
            "ext": "png",
            "size_bytes": 273035
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 209,
        "text": "Named entity disambiguation (NED) refers to the NLP task of achieving exactly this:\nassigning a unique identity to entities mentioned in the text. It’s also the first step in\nmoving toward more sophisticated tasks to address the scenario mentioned above by\nidentifying relationships between entities. NER and NED together are known as\nnamed entity linking (NEL). Some other NLP applications that would need NEL\ninclude question answering and constructing large knowledge bases of connected\nevents and entities, such as the Google Knowledge Graph [34].\nSo, how do we build an IE system for performing NEL? Just as NER identifies entities\nand their spans using contextual information encoded by a range of features, NEL\nalso relies on context. However, it requires going beyond POS tagging in terms of the\nNLP pre-processing needed. At a minimum, NEL needs some form of parsing to\nidentify linguistic items like subject, verb, and object. Additionally, it may also need\ncoreference resolution to resolve and link multiple references to the same entity (e.g.,\nAlbert Einstein, the scientist, Einstein, etc.) to the same reference in a large, encyclo‐\npedic knowledge base (e.g., Wikipedia). This is typically modeled as a supervised ML\nproblem and evaluated in terms of precision, recall, and F1 scores on standard test\nsets.\nState-of-the-art NEL uses a range of different neural network architectures [35].\nClearly, learning an NEL model requires the presence of a large, annotated dataset as\nwell as some kind of encyclopedic resource to link to. Further, NEL is a much more\nspecialized NLP task compared to what we’ve seen so far (text representation, text\nclassification, NER, KPE). In our experience as industry practitioners, it’s more com‐\nmon to use off-the-shelf, pay-as-you-use services offered by big providers such as\nIBM (Watson) and Microsoft (Azure) for NEL rather than developing an in-house\nsystem. Let’s look at an example of using one such service.\nNEL Using Azure API\nThe Azure Text Analytics API is one of the popular APIs for NEL. DBpedia Spotlight\n[36] is a freely available tool to do the same. The following code snippet (Ch5/\nEntityLinking-AzureTextAnalytics.ipynb) shows how to access the Azure API to per‐\nform entity linking on a text. Azure comes with a seven-day free trial, which is a good\nway to explore the API to understand whether it meets your requirements:\nimport requests\nmy_api_key = 'XXXXXXX'\ndef print_entities(text):\n    url = \"https://westcentralus.api.cognitive.microsoft.com/text/analytics/\\\n    v2.1/entities\"\n    documents = {'documents':[{'id':'1', 'language':'en', 'text':text}]}\n    headers = {'Ocp-Apim-Subscription-Key': my_api_key}\n    response = requests.post(url, headers=headers, json=documents)\n    entities = response.json()\n    return entities\nNamed Entity Disambiguation and Linking \n| \n179\n",
        "word_count": 420,
        "char_count": 2849,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 210,
        "text": "mytext = open(\"nytarticle.txt\").read() #file is in the github repo.\nentities = print_entities(mytext)\nfor document in entities['documents']:\n    print(\"Entities in this document: \")\n    for entity in document['entities']:\n          if entity['type'] in [\"Person\", \"Location\", \"Organization\"]:\n                     print(entity['name'], \"\\t\", entity['type'])\n                     if 'wikipediaUrl' in entity.keys():\n                          print(entity['wikipediaUrl'])\nThe result of running this code using the Azure API is shown in Figure 5-9; it lists\nentities in the text along with their Wikipedia links wherever available.\nFigure 5-9. Output of entity linking for a news article from The New York Times\nWe see that San Francisco is a location, but a specific location, which is indicated by\nits Wikipedia page. Alex Jones is not any other Alex Jones, but an American TV show\nhost, as can be seen from the Wikipedia page. This is clearly much more informative\nthan stopping at NER, and it can be used for better information extraction. This\ninformation can then be used for understanding the relationship between these enti‐\nties, which we’ll see later in this chapter.\nSo, we now have a way to incorporate NEL in our NLP system. How good is this solu‐\ntion? Based on our experience using off-the-shelf NEL systems, there are a few\nimportant things to keep in mind while using NEL in your project:\n• Existing NEL approaches are not perfect, and they’re unlikely to fare well with\nnew names or domain-specific terms. Since NEL also requires further linguistic\nprocessing, including syntactic parsing, its accuracy is also affected by how well\nthe different processing steps are done.\n• Like with other IE tasks, the first step in any NLP pipeline—text extraction and\ncleanup—affects what we see as output for NEL as well. When we use third-party\n180 \n| \nChapter 5: Information Extraction\n",
        "word_count": 284,
        "char_count": 1893,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 411,
            "height": 255,
            "ext": "png",
            "size_bytes": 21234
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 211,
        "text": "services, we have little control over adapting them to our domain, if needed, or\nunderstanding their internal workings to modify them to our needs.\nWith this overview, now that we know how to introduce NEL into our project’s NLP\npipeline if necessary, let’s move on to the next IE task that has NEL as a prerequisite:\nrelationship extraction.\nRelationship Extraction\nImagine we’re working at a company that mines tons of news articles to derive, say,\nfinancial insights. To be able to do such analysis on thousands of news texts every day,\nwe would need a constantly updated knowledge base that connects different people,\norganizations, and events based on the news content. A use case for this knowledge\nbase could be to analyze stock markets based on documents released by companies\nand the news articles about them. How will we get started building such a tool? The\nIE tasks we’ve seen so far—KPE, NER, and NEL—are all useful to a certain extent in\nhelping identify entities, events, keyphrases, etc. But how do we go from there to the\nnext step of “connecting” them with some relation? What exactly are the relations?\nHow will we extract them? Let’s revisit Figure 5-2, which shows a screenshot of a New\nYork Times article. One relation that can be extracted is: (Luca Maestri, finance chief,\nApple). Here, we connect Luca Maestri to Apple with the relationship of finance\nchief.\nRelationship extraction (RE) is the IE task that deals with extracting entities and rela‐\ntionships between them from text documents. It’s an important step in building a\nknowledge base, and it’s also useful in improving search and developing question-\nanswering systems. Figure 5-10 shows an example of a working RE system by Rosette\nText Analytics [37] for the following text snippet [38]:\nSatya Narayana Nadella is an Indian-American business executive. He currently serves\nas the Chief Executive Officer (CEO) of Microsoft, succeeding Steve Ballmer in 2014.\nBefore becoming chief executive, he was Executive Vice President of Microsoft’s Cloud\nand Enterprise Group, responsible for building and running the company’s computing\nplatforms.\nThis output shows that Narayana Nadella is a person related to Microsoft as an\nemployee, related to India and America as a citizen, and so on. How does one pro‐\nceed with extracting such relationships from a piece of text? Clearly, it’s more chal‐\nlenging than the other IE tasks we’ve seen so far in this chapter and requires deeper\nknowledge of language processing as compared to every other task we’ve covered in\nthis book so far. Apart from identifying what entities there are and disambiguating\nthem, we need to model the process of extracting the relationships between them by\nconsidering the words connecting the entities in a sentence, their sense of usage, and\nso on. Further, an important question that needs to be resolved is: what constitutes a\n“relation”? Relations can be specific to a given domain. For example, in the medical\nRelationship Extraction \n| \n181\n",
        "word_count": 491,
        "char_count": 3000,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 212,
        "text": "domain, relations could include type of injury, location of injury, cause of injury,\ntreatment of injury, etc. In the financial domain, relations could mean something\ncompletely different. A few generic relations between people, locations, and organiza‐\ntions are: located in, is a part of, founder of, parent of, etc. How do we extract them?\nFigure 5-10. Relation extraction demo\nApproaches to RE\nIn NLP, RE is a well-researched topic, and—starting from handwritten patterns to\ndifferent forms of supervised, semi-supervised, and unsupervised learning—various\nmethods have been explored (and are still being used) for building RE systems.\nHand-built patterns consist of regular expressions that aim to capture specific rela‐\ntionships. For example, a pattern such as “PER, [something] of ORG” can indicate a\nsort of “is-a-part-of” relation between that person and organization. Such patterns\nhave the advantage of high precision, but they often have less coverage, and it could\nbe challenging to create such patterns to cover all possible relations within a domain.\n182 \n| \nChapter 5: Information Extraction\n",
        "word_count": 167,
        "char_count": 1109,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1346,
            "height": 1163,
            "ext": "png",
            "size_bytes": 55522
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 213,
        "text": "Hence, RE is often treated as a supervised classification problem. The datasets used to\ntrain RE systems contain a set of pre-defined relations, similar to classification data‐\nsets. This consists of modeling it as a two-step classification problem:\n1. Whether two entities in a text are related (binary classification).\n2. If they are related, what is the relation between them (multiclass classification)?\nThese are treated as a regular text classification problem, using handcrafted features,\ncontextual features like in NER (e.g., words around a given entity), syntactic structure\n(e.g., a pattern such as NP VP NP, where NP is a noun phrase and VP is a verb\nphrase), and so on. Neural models typically use different embedding representations\n(which we saw in Chapter 3) followed by an architecture like recurrent neural net‐\nworks (which we saw in Chapter 4).\nBoth supervised approaches and pattern-based approaches are typically domain spe‐\ncific, and getting large amounts of annotated data each time we start on a new\ndomain can be both challenging and expensive. As we saw in Chapter 4, bootstrap‐\nping can be used in such scenarios, starting with a small set of seed patterns and gen‐\neralizing by learning new patterns based on the sentences extracted using these seed\npatterns. An extension of such weak supervision approaches is called distant supervi‐\nsion. In this, instead of using a small set of seed patterns, large databases such as\nWikipedia, Freebase, etc., are used to first collect thousands of examples of many rela‐\ntions (e.g., using Wikipedia infoboxes), thereby creating a large dataset of relations.\nThis can then be followed by a regular supervised relation extraction approach. Even\nthis works only when such large databases exist. [39] illustrates how to use Snorkel,\nwhich we saw in Chapters 2 and 4, to learn specific relations in the absence of any\ntraining data. We leave its exploration as an exercise for the reader.\nIn scenarios where we cannot procure training data for supervised approaches, we\ncan resort to unsupervised approaches. Unsupervised RE (also known as “open IE”)\naims to extract relations from the web without relying on any training data or any list\nof relations. The relations extracted are in the form of <verb, argument1, argument2>\ntuples. Sometimes, a verb may have more arguments. Figure 5-11 shows an example\nof the output of such an open IE system by AllenNLP [40, 41].\nRelationship Extraction \n| \n183\n",
        "word_count": 400,
        "char_count": 2465,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 214,
        "text": "Figure 5-11. Open IE demo by AllenNLP\nIn this example, we see the relation as a verb and its three arguments <published,\nalbert einstein, the theory of relativity, in 1915>. We can also extract the relation\ntuples <published, albert einstein, the theory of relativity>, <published, albert ein‐\nstein, in 1915>, and <published, theory of relativity, 1915>. Obviously, in such a sys‐\ntem, we see at least as many (typically, more) such tuples/quadruples as the number\nof verbs. While this is an advantage in the sense that it can extract all such relations,\nthe challenge with this approach lies in mapping the extracted versions to some stan‐\ndardized set of relations (e.g., fatherOf, motherOf, inventorOf, etc.) from a database.\nTo then extract specific relations from this information (if we need to), we would\nhave to devise our own procedures combining the outputs of NER/NEL, coreference\nresolution, and open IE.\nRE with the Watson API\nRE is a hard problem, and it would be challenging and time consuming to develop\nour own relation extraction systems from scratch. A solution commonly used in NLP\nprojects in the industry is to rely on the Natural Language Understanding service\nprovided by IBM Watson [42]. The next code snippet (Ch5/REWatson.ipynb) shows\nhow to extract relationships between entities with IBM Watson using a text snippet\nfrom the Wikipedia page referenced earlier in this section:\nmytext3 = \"\"\"Nadella attended the Hyderabad Public School, Begumpet [12] before \nreceiving a bachelor's in electrical engineering[13] from the Manipal Institute \nof Technology (then part of Mangalore University)in Karnataka in 1988.\"\"\"\nresponse = natural_language_understanding.analyze(text=mytext3,\n            features=Features(relations=RelationsOptions())).get_result()\nfor item in response['relations']:\n         print(item['type'])\n         for subitem in item['arguments']:\n          print(subitem['entities'])\nFigure 5-12 shows the output of this code in terms of the relations it extracted. The\nrelations are extracted using a supervised model and contain a preset list of relations\n[43]. Thus, anything outside that list of relations will not be extracted.\n184 \n| \nChapter 5: Information Extraction\n",
        "word_count": 320,
        "char_count": 2215,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1214,
            "height": 412,
            "ext": "png",
            "size_bytes": 27273
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 215,
        "text": "Figure 5-12. Relation extraction output from IBM Watson\nThis output information, showing relations between different entities, can then be\nused to construct a knowledge base for the organization’s data. As we can see, RE is\nnot a completely solved problem yet, and the performance of the approach is also\ndomain dependent. What worked for a Wikipedia article may not work for a general\nnews article or, say, social media text. [44] summarizes the state of the art in RE.\nStart with pattern-based approaches and use some form of weak\nsupervision in scenarios where pre-trained supervised models may\nnot work.\nWe hope this gave an overview of where RE is useful and how to approach the prob‐\nlem if you encounter it at your workplace. Let’s now take a look at a few other IE tasks\nbefore concluding the discussion on this topic.\nOther Advanced IE Tasks\nSo far, we’ve discussed different information extraction tasks, where they’re useful,\nand how we can build them into our NLP projects if required. While this list of tasks\nis by no means exhaustive, they’re the tasks most commonly used across industry use\ncases. In this section, let’s take a quick look at a few more specialized IE tasks. These\nare not very common and are used sparingly in NLP projects in the industry, so we’ll\nonly introduce them briefly in this section. We would advise the reader to start with\n[26] to get further guidance on the different approaches for solving these tasks. Let’s\nlook at an overview of three other IE tasks: temporal IE, event extraction, and tem‐\nplate filling.\nOther Advanced IE Tasks \n| \n185\n",
        "word_count": 273,
        "char_count": 1588,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          },
          {
            "index": 1,
            "width": 880,
            "height": 307,
            "ext": "png",
            "size_bytes": 30243
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 216,
        "text": "Temporal Information Extraction\nConsider an email text: “Let us meet at 3 p.m. today and decide on what to present at\nthe meeting on Friday.” Say we’re working on an application to identify and populate\ncalendars with events extracted from such conversations, much like we see in Gmail.\nFigure 5-13 shows a screenshot of this utility in Gmail.\nTo build a similar application, in addition to extracting date and time information (3\npm, today, Friday) from the text, we should also convert the extracted data into some\nkind of standard form (e.g., mapping the expression “on Friday” to the exact date,\nbased on context, and “today” to today’s date). While extracting date and time infor‐\nmation can be done using a collection of handcrafted patterns in the form of regex, or\nby applying supervised sequence labeling techniques like we did for NER, normaliza‐\ntion of extracted date and time into a standard date-time format can be challenging.\nTogether, these tasks are referred to as temporal IE and normalization. Contemporary\napproaches to such temporal expression normalization are primarily rule based and\ncoupled with semantic analysis [26].\nFigure 5-13. Identifying and extracting temporal events from emails [45]\nDuckling [46] is a Python library recently released by Facebook’s bots team that was\nused to build bots for Facebook Messenger. The package is designed to parse text and\nget structured data. Among the many tasks it can do, it can process the natural lan‐\nguage text data to extract temporal events. Figure 5-14 shows the output when we run\nthe sentence “Let us meet at 3 p.m. today and decide on what to present at the meet‐\ning on Friday” through Duckling. It’s able to map “3 p.m. today” to the correct time\non a given day.\nDuckling supports multiple languages. From our experience, it works very well and is\na great off-the-shelf package to begin with if you want to incorporate some form of\ntemporal IE into your project. Other packages, such as SUTime [47] by Stanford NLP,\n186 \n| \nChapter 5: Information Extraction\n",
        "word_count": 341,
        "char_count": 2040,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 572,
            "height": 309,
            "ext": "png",
            "size_bytes": 26047
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 217,
        "text": "Natty [48], Parsedatetime [49], and Chronic [50], are also capable of processing\nhuman-readable dates and times. We leave it to the reader to explore these packages\nfurther and see how useful they are for temporal IE. Now, let’s move on to the next IE\ntask: event extraction.\nFigure 5-14. Sample output of temporal IE via Duckling\nEvent Extraction\nIn the email-text example we discussed in the previous section, the aim of extracting\ntemporal expressions is to eventually extract information about an “event.” Events can\nbe anything that happens at a certain point in time: meetings, increase in fuel prices\nin a region at a certain time, presidential elections, the rise and fall of stocks, life\nevents like birth, marriage, and demise, and so on. Event extraction is the IE task that\ndeals with identifying and extracting events from text data. Figure 5-15 shows an\nexample of extracting life events from people’s Twitter feeds.\nThere are many business applications of event extraction. Consider a finance-lending\ncompany that reaches out to people for education loans. Wouldn’t they love to have a\nsystem that can scan Twitter feeds and identify “university admission” events? Or\nconsider a trade analyst in a hedge fund who needs to keep tabs on major events\naround the world. It’s believed that Bloomberg Terminal [51] has a submodule that\nreports major events that are identified from thousands of news sources and social\nchannels like Twitter in real time across the globe. A popular, fun application of event\nextraction is the congratsbot [52]. The bot reads through tweets and responds with a\n“congrats” message if it sees any event that one should be congratulated on. See\nFigure 5-16 for an example.\nOther Advanced IE Tasks \n| \n187\n",
        "word_count": 287,
        "char_count": 1743,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 992,
            "height": 547,
            "ext": "png",
            "size_bytes": 42964
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 218,
        "text": "Figure 5-15. Examples of extracting life events from Twitter data [53]\nFigure 5-16. Congratsbot in action\nSo, how should we approach this problem? Event extraction is treated as a supervised\nlearning problem in NLP literature. Contemporary approaches use sequence tagging\nand multilevel classifiers, much like we saw earlier with relationship extraction. The\nultimate goal is to identify various events over time periods, connect them, and create\na temporally ordered event graph. This is still an active area of research, and working\nsolutions for event extraction like those mentioned previously only work for specific\nscenarios; i.e., there are no relatively generic solutions like we saw for RE, NER, etc.\nTo the best of our knowledge, there are no off-the-shelf services or packages for this\ntask. If you end up doing a project that requires event extraction, the best way for‐\nward is to first start with a rule-based approach based on domain knowledge, then\nfollow it up with weak supervision. As you start accumulating more data, you can\nmove toward ML approaches.\n188 \n| \nChapter 5: Information Extraction\n",
        "word_count": 177,
        "char_count": 1115,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1418,
            "height": 538,
            "ext": "png",
            "size_bytes": 59575
          },
          {
            "index": 1,
            "width": 623,
            "height": 235,
            "ext": "png",
            "size_bytes": 36090
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 219,
        "text": "Template Filling\nIn some application scenarios, such as weather forecasts and financial reports, the\ntext format is fairly standard, and what changes are the specific details pertaining to\nthat situation. For example, consider a scenario where we work in an organization\nthat sends reports on companies’ stock prices on a daily basis. The format of these\nreports will be similar for most companies. An example of one such “template” sen‐\ntence is: “Company X’s stock is up by Y% since yesterday,” where X and Y change but\nthe sentence pattern remains the same. If we’re asked to automate the report genera‐\ntion process, how should we approach it? Such scenarios are good use cases for an IE\ntask called template filling, where the task is to model text generation as a slot-filling\nproblem. Figure 5-17 shows an example of template filling and how it can be used to\nbuild an entity graph.\nFigure 5-17. Example of template filling [54]\nOther Advanced IE Tasks \n| \n189\n",
        "word_count": 165,
        "char_count": 968,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1129,
            "height": 1317,
            "ext": "png",
            "size_bytes": 92789
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 220,
        "text": "Generally, the templates to fill are pre-defined. This is typically modeled as a two-\nstage, supervised ML problem, similar to relation extraction. The first step involves\nidentifying whether a template is present in a given sentence, and the second step\ninvolves identifying slot fillers for that template, with a separate classifier trained for\neach slot. Work is being done in the direction of automatically inducing templates.\nSince this is a specialized, domain-dependent case, we’re not aware of any off-the-\nshelf service provider for this task. As with other tasks in this section, we recommend\nyou start with the chapter on IE in [31] to gain further understanding.\nA recent real-world example of template filling–based text generation is the BBC’s\ncoverage of the 2019 UK elections. BBC created a template and created news stories\nautomatically for all of the UK’s 650 electoral areas. [55] and [56] discuss this project\nin greater detail.\nWith this, we conclude our discussion of most IE tasks. So far, we’ve seen a wide\nrange of IE tasks and how to incorporate some of them individually in your code.\nHow do these tasks connect with one another in a real-world application? Let’s dis‐\ncuss a case study.\nCase Study\nImagine we work for a large, traditional enterprise. We communicate via email and\nenterprise messaging platforms like Slack or Yammer. A lot of discussions about\nmeetings happen as part of email threads. There are the three main types of meetings:\nteam meeting, one-on-one meeting, and talk/presentation, plus their associated ven‐\nues. Say we’re tasked with building a system that automatically finds relevant meet‐\nings, books the venue or conference hall, and notifies people. Let’s look at how the IE\ntasks we’ve discussed would be useful in this scenario. We’ll assume that there’s only\none meeting per email. Look at the email exchange in Figure 5-18 for our scenario\ndescription. How would we go about starting to build that?\nAs a caveat, we might need to restrict what we’re building at the start and solve a\nmore focused problem. For instance, an email my contain multiple meeting men‐\ntions, like in this example: “MountLogan was a good venue. Let us meet there tomor‐\nrow and have an all hands in MountRainer on Thursday.” Let’s assume there’s only\none meeting per email in our case study and start thinking about how to approach\nthe problem of building a simple system as an MVP to get started.\nFirst, we’ll need some amount of labeled data. We can start building labeled data in\nmultiple ways. Imagine we have access to past calendar and conference booking\ninformation as well as email. Does comparing booking information and the emails\nyield positive matches? If so, we could try hardcoded weak supervision, similar to the\none described in Chapter 4. Alternatively, we could try bootstrapping with pre-built\nservices like Google Cloud NLP or AWS Comprehend. For example, Google Cloud\n190 \n| \nChapter 5: Information Extraction\n",
        "word_count": 489,
        "char_count": 2967,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 221,
        "text": "NLP has an entity extraction service that returns events, and we can use it to generate\na dataset. However, as such automatically created datasets may not be perfect, we’ll\nneed manual verification.\nLet’s say we’re dealing with the following entities and have collected some data with\nthese annotations: Room Name (Meeting Location), Meeting Date, Meeting Time,\nMeeting Type (derived field), Meeting Invitees. For our first model, we can use a\nsequence labeling model like conditional random fields (CRFs), which are also used\nfor NER. To classify the type of meeting, we can start with a rule-based classifier\nbased on features such as room size (larger rooms may generally imply larger meet‐\nings), number of invitees, etc.\nFigure 5-18. Meeting information extraction from email (representative image)\nCase Study \n| \n191\n",
        "word_count": 129,
        "char_count": 823,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 560,
            "height": 763,
            "ext": "png",
            "size_bytes": 70067
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 222,
        "text": "Once our system is in deployment, we can start collecting feedback in the form of\nexplicit tagging or more implicit feedback. These may include meeting accept/reject\nrates and meeting conflict rates on the calendar and for the room. All this informa‐\ntion can be used to collect more data so we can apply more sophisticated models.\nOnce we have enough data (5–10K labeled sentences from emails), we can start\nexploring more powerful language understanding models. If enough compute power\nis available, we may take advantage of a powerful pre-trained model like BERT and\ncan fine-tune it on the new labeled dataset. The pipeline for this process is depicted in\nFigure 5-19.\nFigure 5-19. Pipeline for meeting information extraction system development\n192 \n| \nChapter 5: Information Extraction\n",
        "word_count": 126,
        "char_count": 791,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 995,
            "height": 1553,
            "ext": "png",
            "size_bytes": 93857
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 223,
        "text": "Now let’s consider the more complex case we discussed at the beginning, where we\nmay have mentions of multiple entities (room names) and also loose mentions of\nmultiple meetings happening at different times. We want to tackle this problem as a\nmulticlass, multilabel classification problem. The linguistic ambiguity could be hard\nto decipher via handcrafted feature engineering like the presence of some specific\nentity, fixed vocabulary, etc. A reasonable way to approach this problem would be to\nuse a deeper neural network with recurrence, such as an LSTM or a GRU network.\nThese networks will model contextual information around each word and encode\nthat knowledge into the hidden vectors we would use to finally classify the email.\nWhile all this discussion is specific to one real-world IE problem, it’s possible to\nincrementally implement and improve a solution to any IE problem using the\napproach outlined in this section.\nWrapping Up\nIn this chapter, we looked at information extraction and its usefulness in different\nreal-world scenarios and discussed how to implement solutions for different IE tasks,\nincluding keyphrase extraction, named entity recognition, named entity linking, and\nrelationship extraction. We also introduced the tasks of temporal information extrac‐\ntion, event extraction, and template filling. Compared to what we saw with text classi‐\nfication, an important difference with IE is that these tasks rely on resources beyond\nlarge annotated corpora and also require more domain knowledge. Hence, in a prac‐\ntical scenario, it’s more common to use pre-trained models and solutions from large\nservice providers rather than developing IE systems of our own from scratch, unless\nwe’re working on a super-specialized domain that needs custom solutions. Another\nimportant point to note, which we also reiterated several times throughout the chap‐\nter, is the role that a good text extraction and cleanup process plays in all these tasks.\nWhile we didn’t take up specific end-to-end examples involving multiple IE tasks\n(some of which will be covered in Part III of the book), we hope that this chapter\ngives you enough of an idea about IE and the things to keep in mind when imple‐\nmenting IE tasks in your projects. In the next chapter, we’ll take a look at how to\nbuild chatbots for different use cases you may encounter in your workplace.\nReferences\n[1] Wikipedia. “Message Understanding Conference”. Last modified November 20,\n2019.\n[2] Linguistic Data Consortium. “ACE”. Last accessed June 15, 2020.\n[3] NIST. “Text Analysis Conference”. Last accessed June 15, 2020.\n[4] Google News. Last accessed June 15, 2020.\nWrapping Up \n| \n193\n",
        "word_count": 422,
        "char_count": 2667,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 224,
        "text": "[5] Sarno, Adrian. “Information Extraction from Receipts with Graph Convolutional\nNetworks”, Nanonets (blog), 2020.\n[6] Sensibill. Last accessed June 15, 2020.\n[7] Nicas, Jack. “Apple’s Plan to Buy $75 Billion of Its Stock Fuels Spending Debate”.\nNew York Times, April 30, 2019.\n[8] Chiticariu, Laura, Yunyao Li, and Frederick Reiss. “Rule-Based Information\nExtraction is Dead! Long Live Rule-Based Information Extraction Systems!” Proceed‐\nings of the 2013 Conference on Empirical Methods in Natural Language Processing\n(2013): 827–832.\n[9] Chiticariu, L. et al. “Web Information Extraction”. In Liu, L. and Özsu, M.T. (eds),\nEncyclopedia of Database Systems, New York: Springer, 2018.\n[10] Hasan, Kazi Saidul and Vincent Ng. “Automatic Keyphrase Extraction: A Survey\nof the State of the Art.” Proceedings of the 52nd Annual Meeting of the Association for\nComputational Linguistics 1: (2014): 1262–1273.\n[11] Çano, Erion and Ondřej Bojar. “Keyphrase Generation: A Text Summarization\nStruggle.” Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies 1 (2019):\n666–672.\n[12] Chartbeat Labs Projects. textacy: NLP, before and after spaCy, (GitHub repo).\nLast accessed June 15, 2020.\n[13] Explosion.ai. “SpaCy: Industrial-Strength Natural Language Processing in\nPython”. Last accessed June 15, 2020.\n[14] Mihalcea, Rada and Paul Tarau. “Textrank: Bringing Order into Text.” Proceed‐\nings of the 2004 Conference on Empirical Methods in Natural Language Processing\n(2004): 404–411.\n[15] Gensim. “summarization.keywords—Keywords for TextRank summarization\nalgorithm”. Last accessed June 15, 2020.\n[16] Chowdhury, Jishnu Ray. “Implementation of TextRank”. Last accessed June 15,\n2020.\n[17] Explosion.ai. “displaCy Named Entity Visualizer”. Last accessed June 15, 2020.\n[18] spaCy. Common entity categories in NER development. Last accessed June 15,\n2020.\n[19] The Stanford Natural Language Processing Group. “Stanford RegexNER”. Last\naccessed June 15, 2020.\n[20] Explosion.ai. spacy’s EntityRuler. Last accessed June 15, 2020.\n194 \n| \nChapter 5: Information Extraction\n",
        "word_count": 300,
        "char_count": 2150,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 225,
        "text": "[21] Wikipedia. “Sequence labeling”. Last modified January 18, 2017.\n[22] Sang, Erik F. and Fien De Meulder. “Introduction to the CoNLL-2003 Shared\nTask: Language-Independent Named Entity Recognition.” Proceedings of the Seventh\nConference on Natural Language Learning at HLT-NAACL (2003).\n[23] Team HG-Memex. sklearn-crfsuite: scikit-learn inspired API for CRFsuite, (Git‐\nHub repo). Last accessed June 15, 2020.\n[24] MIT-NLP. MITIE: library and tools for information extraction, (GitHub repo).\nLast accessed June 15, 2020.\n[25] Yang, Jie. NCRF++: a Neural Sequence Labeling Toolkit, (GitHub repo). Last\naccessed June 15, 2020.\n[26] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft), 2018, Chapter 18.\n[27] Linguistic Data Consortium. “OntoNotes Release 5.0”. Last accessed June 15,\n2020.\n[28] The Stanford Natural Language Processing Group. “Stanford Named Entity Rec‐\nognizer (NER)”. Last accessed June 15, 2020.\n[29] Allen Institute for AI. “AllenNLP: An open-source NLP research library, built on\nPyTorch”. Last accessed June 15, 2020.\n[30] Explosion.ai. Prodigy’s NER Recipes. Last accessed June 15, 2020.\n[31] Jurafsky, Daniel and James H. Martin. Speech and Language Processing: An Intro‐\nduction to Natural Language Processing, Computational Linguistics and Speech Recog‐\nnition. Upper Saddle River, NJ: Prentice Hall, 2008.\n[32] FilingDB. “What’s so hard about PDF text extraction?” Last accessed June 15,\n2020.\n[33] IBM Research Editorial Staff. “Making sense of language. Any language”. Octo‐\nber 28, 2016.\n[34] Wikipedia. “Knowledge Graph”. Last modified April 12, 2020.\n[35] NLP-progress. “Entity Linking”. Last accessed June 15, 2020.\n[36] DBpedia Spotlight. “Shedding light on the web of documents”. Last accessed\nJune 15, 2020.\n[37] Rosette Text Analytics. “Relationship Extraction”. Last accessed June 15, 2020.\n[38] Wikipedia. “Satya Nadella”. Last modified April 10, 2020.\n[39] Snorkel. “Detecting spouse mentions in sentences”. Last accessed June 15, 2020.\nWrapping Up \n| \n195\n",
        "word_count": 294,
        "char_count": 2039,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 226,
        "text": "[40] Allen Institute for AI. “Reading Comprehension: Demo”. Last accessed June 15,\n2020.\n[41] AllenNLP’s GitHub repository. Last accessed June 15, 2020.\n[42] IBM Cloud. “Watson Natural Language Understanding”. Last accessed June 15,\n2020.\n[43] IBM Cloud. Relation types. Last accessed June 15, 2020.\n[44] NLP-progress. “Relationship Extraction”. Last accessed June 15, 2020.\n[45] BetterCloud. “Hidden Shortcuts for Creating Calendar Events Right from\nGmail”. Last accessed June 15, 2020.\n[46] Wit.ai. Duckling. Last accessed June 15, 2020.\n[47] The Stanford Natural Language Processing Group. “Stanford Temporal Tagger”.\nLast accessed June 15, 2020.\n[48] Stelmach, Joe. “Natty”. Last accessed June 15, 2020.\n[49] Taylor, Mike. “parsedatetime”. Last accessed June 15, 2020.\n[50] Preston-Warner, Tom. Chronic: a pure Ruby natural language date parser, (Git‐\nHub repo). Last accessed June 15, 2020.\n[51] Bloomberg Professional Services. “Event-Driven Feeds”. Last accessed June 15,\n2020.\n[52] Twitter. Congratulatron (@congratsbot). Last accessed June 15, 2020.\n[53] Li, Jiwei, Alan Ritter, Claire Cardie, and Eduard Hovy. “Major Life Event Extrac‐\ntion from Twitter based on Congratulations/Condolences Speech Acts”. Proceedings\nof the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP) (2014): 1997–2007.\n[54] Jean-Louis, Ludovic, Romaric Besançon, and Olivier Ferret. “Text Segmentation\nand Graph-based Method for Template Filling in Information Extraction.” Proceed‐\nings of 5th International Joint Conference on Natural Language Processing (2011): 723–\n731.\n[55] Molumby, Conor and Joe Whitwell. “General Election 2019: Semi-Automation\nMakes It a Night of 689 Stories”. BBC News Labs, December 13, 2019.\n[56] Reiter, Ehud. “Election Results: Lessons from a Real-World NLG System”, Ehud\nReiter’s Blog, December 23, 2019.\n196 \n| \nChapter 5: Information Extraction\n",
        "word_count": 265,
        "char_count": 1889,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 227,
        "text": "CHAPTER 6\nChatbots\nOne machine can do the work of fifty ordinary men.\nNo machine can do the work of one extraordinary man.\n—Elbert Green Hubbard\nChatbots are interactive systems that allow users to interact in natural language. They\ngenerally interact via text but can also use speech interfaces. Early 2016 saw the intro‐\nduction of the first wave of chatbots that soon became ubiquitous. Platforms like\nFacebook Messenger, Google Assistant, and Amazon Alexa are some examples of\nchatbots. There are now tools that allow developers to create custom chatbots [1] for\ntheir brand or service so that consumers can carry out some of their daily actions\nfrom within their messaging platforms.\nThe introduction of chatbots into society has brought us to the beginning of a new\nera in technology: the era of the conversational interface. It’s an interface that soon\nwon’t require a screen or a mouse to use. There will be no need to click or swipe; just\nthe use of voice will be enough. This interface will be completely conversational, and\nthose conversations will be indistinguishable from the conversations we have with\nour friends and family. Since chatbots deal with text under the hood, it’s all about\nunderstanding the text responses coming from users and producing reasonable\nreplies. From understanding to generation, NLP plays a significant role, which we’ll\nsee throughout this chapter.\nThe history of chatbots and of artificial intelligence in general are pretty intertwined.\nIn the 1950s and ’60s, computer scientists Alan Turing and Joseph Weizenbaum con‐\ntemplated the concept of computers communicating like humans do. Later, in 1966,\nJoseph Weizebaum built Eliza [2], the first chatterbot ever coded, using only 200 lines\nof code. Eliza imitated the language of a Rogerian Psychotherapist using regular\nexpressions and rules. Humans knew they were interacting with a computer program,\n197\n",
        "word_count": 303,
        "char_count": 1900,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (9.3pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 228,
        "text": "and yet, through the emotional responses Eliza would offer, still grew emotionally\nattached to the program during trials.\nLater, in the advent of powerful signal processing tools, researchers focused on build‐\ning spoken dialog tools with the goal of improving user experience. Many spoken dia‐\nlog systems were built between 1980 and 2000 and started as military-based projects\n(by DARPA) aimed mainly at improving automated communication with soldiers.\nThe systems were used to provide instructions, which later translated into chatbots\ncapable of helping users get answers to frequently asked questions for various serv‐\nices. The bots were still handcrafted such that responses they generated were fixed,\nand the bots were not good at handling the context provided in the conversation.\nIn recent years, chatbots have become more feasible and useful, both due to the ubiq‐\nuity of smartphones and recent advances in ML and DL. In addition to APIs to create\nchatbots on popular messaging platforms like Facebook Messenger, we now have var‐\nious platforms to create the AI and logic behind the chatbots. This has allowed folks\nand companies with limited AI background and experience to deploy their own chat‐\nbots easily.\nThis chapter aims to cover the underlying systems and theory of chatbots, along with\npractical, hands-on experience building chatbots using different scenarios. We’ll end\nwith some state-of-the-art research that may bring major advances to this entire para‐\ndigm. We’ll motivate our readers by introducing popular applications of chatbots.\nApplications\nChatbots can be used for many different tasks in many different industries, from\nretail, to news, and even the medical field. We’ll briefly discuss various applications of\nchatbots. Many of these use cases have become more mature in recent years, while\nsome are still in their infancy. These use cases include:\nShopping and e-commerce\nRecently, chatbots are being used for various e-commerce operations, including\nplacing or modifying an order, payment, etc. Bots for recommending various\nitems are also of great interest to the e-commerce industry. Industries are focused\non building conversational recommendation systems to provide a more seamless\nuser experience.\nNews and content discovery\nSimilar to e-commerce, chatbots can be used in news and content discovery.\nUsers may specify various nuances of their search in a conversational manner,\nand the bot should be able respond with relevant articles.\n198 \n| \nChapter 6: Chatbots\n",
        "word_count": 385,
        "char_count": 2510,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 229,
        "text": "Customer service\nCustomer service is another area where bots are used heavily. They’re used to\nlodge complaints, help answer FAQs, and navigate queries in pre-defined conver‐\nsational flows set by the business requirements.\nMedical\nIn health and medical applications, FAQ bots are of great use. These bots can\nhelp patients fetch relevant information quickly based on their symptoms.\nRecently, there has also been interest in building chatbots that elicit useful infor‐\nmation from patients, especially older patients, regarding their health conditions\nby asking relevant questions.\nLegal\nIn legal applications, bots can also be used to serve FAQs for users. They can even\nbe used for more complex goals, such as asking follow-up questions. For exam‐\nple, if a user asks for legal articles to follow up on a case, a bot might ask specific\nquestions regarding the nature of the case to find a more appropriate match.\nHere’s a more elaborate example of an FAQ bot, which is common in many service\nplatforms, to help users by providing answers to frequently asked questions.\nA Simple FAQ Bot\nA FAQ bot is generally a search-based system where, given a question, it looks for\ncorrect answers and provides them to the user. It’s essentially a bot that allows a user\nto ask questions in different ways to get a response. Such bots are quite useful for pro‐\nviding a conversational interface to a complex set of questions.\nAs an example, we’ll consider a subset of Amazon Machine Learning Frequently\nAsked Questions. A machine needs to learn to provide the correct answer given simi‐\nlar questions, so it’s a good idea to have some paraphrases of each question. See\nTable 6-1 for some input-output examples for such a chatbot.\nTable 6-1. Amazon ML FAQ to be used for a FAQ bot [3]\nQuestions\nAnswer\nWhat can I do with Amazon Machine Learning?\nHow can I use Amazon Machine Learning?\nWhat can Amazon Machine Learning do?\nYou can use Amazon Machine Learning to create a wide variety of\npredictive applications. For example, you can use Amazon Machine\nLearning to help you build applications that flag suspicious transactions,\ndetect fraudulent orders, forecast demand, etc.\nWhat algorithm does Amazon Machine Learning\nuse to generate models? How does Amazon\nMachine Learning build models?\nAmazon Machine Learning currently uses an industry-standard logistic\nregression algorithm to generate models.\nAre there limits to the size of the dataset I can\nuse for training?\nWhat is the maximum size of training dataset?\nAmazon Machine Learning can train models on datasets up to 100 GB in\nsize.\nApplications \n| \n199\n",
        "word_count": 428,
        "char_count": 2598,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MinionPro-It (9.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 230,
        "text": "Figure 6-1 is a working version of such an FAQ bot. Later in the chapter, we’ll learn\nhow to build such a bot for various applications step by step.\nFigure 6-1. An FAQ bot\nNow, we’ll transition to the taxonomy of chatbots and explain various categories of\nchatbot based on their usage.\nA Taxonomy of Chatbots\nLet’s expand on chatbots of various uses and their applicability to various domains.\nChatbots can be classified in many ways, which affects how they’re built and where\nthey’re used. A way of looking at these chatbots is how they interact with the user:\nExact answer or FAQ bot with limited conversations\nThese chatbots are linked to a fixed set of responses and retrieve a correct\nresponse based on understanding the user’s query. For example, if we build an\nFAQ bot, the bot has to understand the question and retrieve a fixed, correct\nanswer for it. Generally, one response from the user does not depend on the pre‐\nvious responses. Take a look at Figure 6-2. In the FAQ bot example, we see that,\nin the first two turns, the bot provides a fixed response to similar questions that\nare asked with slight variations. For a different question, it pulls out a different\nanswer.\n200 \n| \nChapter 6: Chatbots\n",
        "word_count": 214,
        "char_count": 1213,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 394,
            "height": 515,
            "ext": "png",
            "size_bytes": 102285
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 231,
        "text": "Flow-based bot\nFlow-based conversational bots are generally more complex than FAQ bots in\nterms of the variability of their responses. Users may gradually express their\nopinions or requests over the course of conversations. For example, when order‐\ning a pizza, a user may express their requested toppings, pizza size, and other\nnuances gradually. The bot should understand and track this information\nthroughout the conversation to successfully generate a response every time. In\nFigure 6-2, for the flow-based bot, we see that the bot asks a specific set of ques‐\ntions to achieve the goal of making a pizza order. This flow was pre-defined, and\nthe bot asks relevant questions to fulfill the order. We’ll discuss such a flow-based\nbot in greater detail later in this chapter.\nOpen-ended bot\nOpen-ended bots are intended mainly for entertainment, where the bot is sup‐\nposed to converse with the user about various topics. The bot doesn’t have to\nmaintain specific directions or flows of the conversation. In Figure 6-2, the open-\nended bot carries out a conversation without any pre-existing template or fixed\nquestion-answer pairs. It transitions fluently from one topic to another to main‐\ntain the interesting conversation. This example of an open-ended bot was built by\none of the authors for a popular digital assistant platform.\nFigure 6-2. Types of chatbots\nA Taxonomy of Chatbots \n| \n201\n",
        "word_count": 225,
        "char_count": 1398,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1411,
            "height": 1012,
            "ext": "png",
            "size_bytes": 217088
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 232,
        "text": "Chatbots are classified into two broad categories: (1) goal-oriented dialogs and (2)\nchitchats. FAQ bots and flow-based bots fall into the first category, whereas open-\nended bots are mainly chitchat types. Both of these types of bots are used heavily in\nindustry and are also in the active area of research in academia.\nGoal-Oriented Dialog\nThe natural human purpose of having a conversation is to accomplish a goal via rele‐\nvant information seeking. In the similar line of thought, it’s easy to design any chatbot\nor conversation agent for a specific use case where the end goal is known. Most of the\nchatbots we’ve discussed so far (those typically used in research or industry) are goal-\noriented chatbots. The user interacting with the chatbot should have complete infor‐\nmation about what they want to achieve after the conversation. For example, looking\nfor a movie recommendation or booking flight reservations through chatbots or con‐\nversational agents are examples of goal-oriented dialog where the goal is to watch a\nmovie or book a flight.\nNow, by definition, the goal-oriented systems are domain-specific, which requires\ndomain-specific knowledge in the system. This hampers the generalizability and scal‐\nability of the chatbot framework. Research from Facebook [4] recently presented an\nend-to-end framework for training all components from the dialogs themselves to\nmitigate that limitation. This research proposes an automatic manipulation of the\ndata—for example, question-answer pairs to carry out a meaningful conversation via\nrequired API calls. This is one of the newest approaches that researchers and industry\npractitioners have started to follow.\nChitchats\nApart from goal-oriented conversations, humans also engage in unstructured, open-\ndomain conversations without any specific goals. These human-human conversations\ninvolve free-form, opinionated discussions about various topics. Having a conversa‐\ntional agent that can have a chitchat with a human is challenging due to the absence\nof objective goals. A conversational agent must generate coherent, on-topic, and fac‐\ntually correct responses to make the dialog more natural.\nThe application of chitchat bots is futuristic but holds immense potential. For exam‐\nple, these bots could be used to elicit useful but sensitive information in the case of a\nmedical emergency for geriatric care. The free-form conversational bot could also be\nused to address the long-standing issue of loneliness and depression among teenagers\nand elderly people. Some of the market-leader companies, such as Amazon, Apple,\nand Google, to name a few, are investing heavily in building such bots for worldwide\ncustomers.\n202 \n| \nChapter 6: Chatbots\n",
        "word_count": 411,
        "char_count": 2710,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 233,
        "text": "So far, we’ve discussed various kinds of chatbots and their usage in various industries.\nThis will allow us to appreciate various components of chatbots based on usage and\nalso help us implement some of the components as we need them. Now, we’ll deep-\ndive into the chatbot development pipeline and discuss details of various\ncomponents.\nA Pipeline for Building Dialog Systems\nWe discussed various NLP tasks, such as classification and entity detection, through‐\nout Chapters 4 and 5. Now, we’ll utilize some of them to describe an example pipeline\nto build a dialog system. Figure 6-3 depicts a complete pipeline of a dialog system\nwith various components. We’ll discuss the utility of each component and data flow\nthrough the pipeline.\nFigure 6-3. Pipeline for a dialog system\nSpeech recognition\nUsually, the dialog system works as an interface between human and machine, so\nthe input into the dialog system is human speech. Speech recognition algorithms\ntranscribe speech to natural text. In industrial dialog systems, state-of-the-art [5]\nspeech-to-text models are used, which is beyond the scope of this book. If you’re\ninterested in speech models, refer to [5] for an overall view.\nNatural language understanding (NLU)\nAfter transcribing, the system tries to analyze and “understand” the transcribed\ntext. This module encompasses various natural language understanding tasks.\nExamples of such tasks are sentiment detection, named entity extraction, corefer‐\nence resolution, etc. This module is primarily responsible for gathering all possi‐\nble information that is implicitly (sentiment) or explicitly (named entities)\npresent in the input text.\nA Pipeline for Building Dialog Systems \n| \n203\n",
        "word_count": 260,
        "char_count": 1700,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1390,
            "height": 515,
            "ext": "png",
            "size_bytes": 39445
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 234,
        "text": "Dialog and task manager\nOnce we obtain information from the input, a dialog manager, as shown in the\nfigure, gathers and systematically decides which pieces of information are impor‐\ntant or not. A dialog manager is a module that controls and guides the flow of the\nconversation. Imagine this as a table containing information extracted in NLU\nsteps and stored concurrently for all utterances in the ongoing conversation. The\ndialog manager develops a strategy via rules or other complex mechanisms, such\nas reinforcement learning, to effectively utilize the information obtained from\nthe input. Dialog managers are mostly prevalent in goal-oriented dialogs since\nthere’s a definite objective to reach via the conversation.\nNatural language generation\nFinally, as the dialog manager decides a strategy for responding, the natural lan‐\nguage generation module generates a response in a human-readable form\naccording to the strategy devised by the dialog manager. The response generator\ncould be template based or a generative model learned from data. After this, a\nspeech synthesis module converts the text back to speech to the end user. For\nmore information on speech synthesis tasks, take a look at [6] and [7].\nAny chatbot can be built using such a pipeline. For text-based chat‐\nbots, we can remove the speech processing components. While the\nNLU and generation component can be complex, a dialog manager\ncould simply be rules routing the bot to an appropriate response\ngenerator.\nAlthough the pipeline in Figure 6-3 assumes the chatbot is voice based, a similar\npipeline without the speech processing modules will work for text-based chatbots.\nBut in all industrial applications, we’re moving toward eventually having more and\nmore voice-based systems, so the pipeline discussed here is more general, and it\napplies to a variety of applications we described previously (including the case study\nin Chapter 1). Now that we’ve briefly discussed the various components of a chatbot\nand how a conversation flow takes place, let’s deep dive to understand these compo‐\nnents in detail.\nDialog Systems in Detail\nThe main idea of a dialog system or chatbot is to understand a user’s query or input\nand to provide an appropriate response. This is different from typical question-\nanswering systems where, given a question, there has to be an answer. In a dialog\nsetup, users may ask their queries in “turns.” In each turn, a user reveals their interest\nabout the topic based on what the bot may have responded with. So, in a dialog sys‐\ntem, the most important thing is understanding nuances from the user’s input in a\nturn-by-turn way and storing them in context to generate responses.\n204 \n| \nChapter 6: Chatbots\n",
        "word_count": 438,
        "char_count": 2711,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 235,
        "text": "Before we get into the details of bots and dialog systems, we’ll cover the terminology\nused in dialog systems and chatbot development more broadly.\nDialog act or intent\nThis is the aim of a user command. In traditional systems, the intent is a primary\ndescriptor. Often, several other things, such as sentiment, can be linked to the\nintent. The intent is also called a “dialog act” in some literature. In the first exam‐\nple in Figure 6-4, orderPizza is the intent of the user command. Similarly, in the\nsecond example, the user wants to know about a stock, so the intent is getStock‐\nQuote. These intents are usually pre-defined based on the chatbot’s domain of\noperation.\nSlot or entity\nThis is the fixed ontological construct that holds information regarding specific\nentities related to the intent. The information related to each slot that’s surfaced\nin the original utterance is “value.” The slots and value together are sometimes\ndenoted as an “entity.” Figure 6-4 shows two examples of entities. The first exam‐\nple looks for specific attributes of the pizza to be ordered: “medium” and “extra\ncheese.” On the other hand, the second example looks for the related entities for\ngetStockQuote: the stock name and the time period the chatbot is asked for.\nDialog state or context\nA dialog state is an ontological construct that contains both the information\nabout the dialog act as well as state-value pairs. Similarly, context can be viewed\nas a set of dialog states that also captures previous dialog states as history.\nFigure 6-4. Example of different terminology used in chatbots\nDialog Systems in Detail \n| \n205\n",
        "word_count": 270,
        "char_count": 1621,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 602,
            "height": 407,
            "ext": "png",
            "size_bytes": 72875
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 236,
        "text": "Now, let’s complete a walkthrough using a cloud API called Dialogflow [8] for a fic‐\ntional pizza shop to enable users to converse with a chatbot to order pizza. This is a\ngoal-oriented system where the goal is to accommodate the user’s request and order a\npizza.\nPizzaStop Chatbot\nDialogflow is a conversational agent–building platform by Google. By providing the\ntools to understand and generate natural language and manage the conversation, Dia‐\nlogflow enables us to easily create conversational experiences. While there are many\nother tools available, we chose this one because it’s easy to use, mature, and is being\nimproved constantly.\nImagine there’s a fictional pizza shop called PizzaStop, and we have to build a chatbot\nthat can take an order from a customer. A pizza can have multiple toppings (like\nonions, tomatoes, and peppers), and it can come in different sizes. An order can also\ncontain one or more items from the sides, appetizers, and/or beverages section of the\nmenu. Now that we understand the requirements, let’s begin building our bot using\nthe Dialogflow framework.\nBuilding our Dialogflow agent\nBefore we begin creating our agent, we need to create an account and set up a few\nthings. For this, open the official Dialogflow website [9], log in with your Google\naccount, and provide the required permissions. Navigate to V2 of the API [10]. Click\non “try it for free” and you’ll be directed to the free tier of Google Cloud Services,\nthen you can follow the registration process.\n1. First, we need to create an agent. Click the Create Agent button, then enter the\nname of the agent. You can provide any name, but it’s good practice to provide a\nname that gives an idea of what the agent is used for. For our PizzaStop project,\nwe’ll name our agent “Pizza.” Now, set the time zone and click the Create button.\nFigure 6-5 shows the UI you’ll see while creating an agent.\n206 \n| \nChapter 6: Chatbots\n",
        "word_count": 329,
        "char_count": 1923,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 237,
        "text": "Figure 6-5. Creating an agent using Dialogflow\n2. You’ll then be redirected to another page with options that allow you to create the\nbot. Figure 6-6 shows the UI of Dialogflow, which we’ll use multiple times while\ncreating our agent. By default, we’ll already have two intents: Default Fallback\nIntent and Default Welcome Intent. Default Fallback Intent is the default response\nif some internal API fails and Default Welcome Intent will generate a welcome\nmessage.\nFigure 6-6. Dialogflow UI after creating an agent\nDialog Systems in Detail \n| \n207\n",
        "word_count": 89,
        "char_count": 549,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 997,
            "height": 327,
            "ext": "png",
            "size_bytes": 28190
          },
          {
            "index": 1,
            "width": 1362,
            "height": 661,
            "ext": "png",
            "size_bytes": 62398
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 238,
        "text": "3. Now, we need to add the intents and entities we care about to our agent. To add\nan intent, hover over the Intents block and click the + button. You’ll see some‐\nthing similar to Figure 6-7. These intents and entities are what we defined earlier\nin the section.\nFigure 6-7. Dialogflow UI after clicking the “+” button\n4. Now, we’ll create the first intent: orderPizza. As we create a new intent, we have\nto provide training examples, called “training phrases,” to enable the bot to detect\nvariations of responses that belong to the intent. We also need to provide “con‐\ntext”: a piece of information that can be remembered over the span of a conversa‐\ntion and that will be used for subsequent intent detection.\nExamples of training phrases are “I want to order a pizza” or “medium with\ncheese please.” The first one denotes a simple intent of pizza ordering, whereas\nthe second one consists of entities that are useful to remember, such as medium\nsize and cheese topping.\n208 \n| \nChapter 6: Chatbots\n",
        "word_count": 177,
        "char_count": 1003,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1161,
            "height": 905,
            "ext": "png",
            "size_bytes": 30937
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 239,
        "text": "Figure 6-8 shows sample training phrases added to the agent.\nFigure 6-8. Adding training phrases for intents\n5. Since we’ve included intent, we need to add the respective entities to remember\nimportant information provided by the user. Create an entity named pizzaSize,\nenable “fuzzy matching” (which matches entities even if they’re only approxi‐\nmately the same), and provide the necessary values. Similarly, create a pizzaTop‐\nping entity, but this time, also enable “Define synonyms” (this lets us define\nsynonyms while allowing us to match several words, defined as synonyms, to the\nsame entity).\nThese two together will help us detect “medium size” and “cheese toppings,” as\nshown in Figures 6-9 and 6-10.\nDialog Systems in Detail \n| \n209\n",
        "word_count": 118,
        "char_count": 745,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 715,
            "height": 530,
            "ext": "png",
            "size_bytes": 19874
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 240,
        "text": "Figure 6-9. Creating the pizzaSize entity\nFigure 6-10. Creating the pizzaTopping entity\n6. Now, let’s go back to the Intents block to add additional information to the\nAction and Parameters section. We need both the topping and size to complete\nthe order, so we need to check the Required box on those. One pizza can’t be\nmultiple sizes, but one pizza can have multiple toppings. So, enable the isList\noption for toppings to allow it to have multiple values.\nA user might only mention the size or the topping. To gather the complete infor‐\nmation, we need to add a prompt that asks follow-up questions, such as, “What\nsize of pizza would you like?” as a prompt for pizzaSize. This is shown in\nFigure 6-11.\n210 \n| \nChapter 6: Chatbots\n",
        "word_count": 130,
        "char_count": 734,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 723,
            "height": 326,
            "ext": "png",
            "size_bytes": 11540
          },
          {
            "index": 1,
            "width": 761,
            "height": 444,
            "ext": "png",
            "size_bytes": 21125
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 241,
        "text": "Figure 6-11. Actions and parameters for orderPizza intent\n7. We also need to provide sample responses, as shown in Figure 6-12, that the\nagent will give the user. We can ask the user if they need drinks, appetizers, or\nsides. If we were creating something like a billing intent, we could end the con‐\nversation after it by enabling the “Set this intent as end of conversation” slider in\nthe Responses block.\nFigure 6-12. Adding the appropriate responses our agent should use\n8. So far, we’ve added a simple intent and entities. Now we can look at a complex\nentity with context. Consider the statement, “I want to order 2 L of juice and 3\nwings.” Our agent needs to recognize the quantity and the item ordered. This is\ndone by adding a custom entity in Dialogflow. We’ve created an entity called\ncompositeSide, and it can handle all of these combinations. For example, in\n“@sys.number-integer:number-integer @appetizer:appetizer”, the first entity\ndeals with recognizing how many of the appetizers are ordered, and the next one\nDialog Systems in Detail \n| \n211\n",
        "word_count": 179,
        "char_count": 1060,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 731,
            "height": 312,
            "ext": "png",
            "size_bytes": 17082
          },
          {
            "index": 1,
            "width": 737,
            "height": 356,
            "ext": "png",
            "size_bytes": 13857
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 242,
        "text": "deals with the type of appetizer, as shown in Figures 6-13 and 6-14. As you can\nsee, the signatures of these entities are given as regular expressions.\nFigure 6-13. Creating the compositeSide entity\nFigure 6-14. Example of a complex statement with multiple entities and context\n9. We can add many more intents and entities to make our agent robust. In\nFigure 6-15 and Figure 6-16, take a look at examples of other intents and entities\nwe added to enrich and enhance the user’s pizza-buying experience.\n212 \n| \nChapter 6: Chatbots\n",
        "word_count": 90,
        "char_count": 530,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 775,
            "height": 323,
            "ext": "png",
            "size_bytes": 21204
          },
          {
            "index": 1,
            "width": 291,
            "height": 501,
            "ext": "png",
            "size_bytes": 16578
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 243,
        "text": "Figure 6-15. All the intents for this agent\nFigure 6-16. All the entities for this agent\nNow that we’ve gone through the steps to build a bot for PizzaStop, we’ll test our bot\nto see how it works in various scenarios.\nDialog Systems in Detail \n| \n213\n",
        "word_count": 47,
        "char_count": 251,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 769,
            "height": 336,
            "ext": "png",
            "size_bytes": 10288
          },
          {
            "index": 1,
            "width": 766,
            "height": 420,
            "ext": "png",
            "size_bytes": 12862
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 244,
        "text": "Testing our agent\nNow, let’s test our agent in a website setting. For this, we need to open it in “web\ndemo” mode. Click the Integrations block and scroll down until you reach Web\nDemo. Click the link in the pop-up window, and that’s it! Feel free to test your agent\nto your heart’s content. Figure 6-17 shows snippets of the one we built. Testing our\nbot is important for validating that it’s working. We’ll analyze a few cases of varying\ndifficulty.\nFigure 6-17. Making a simple order using our agent\nWe can see in Figure 6-17 that our bot is able to handle simple queries to order a\npizza. As we have tested the bot end to end, we can also test various components of it\nindividually. Testing individual components helps to prototype quickly and catch\nedges cases before the end-to-end testing.\nNow, let’s go through a more complex example, which will be tested with an integra‐\ntion of this bot with Google Assistant. In the example shown in Figure 6-17, our\nagent identifies the intent to order a pizza and recognizes the toppings we ordered.\nThe pizzaSize entity is not fulfilled, so it asks a question regarding the size of the pizza\nto fulfill the entity’s requirement. With the orderPizza intent fulfilled, the agent then\nproceeds to ask us about sides and appetizers. Based on the statement we provided,\nthe agent needs to fulfill the orderSize intent and should be able to identify the quan‐\ntity of juice and the appetizer. This shows that the agent is able to handle complex\nentities. Finally, we move on to the conversation for selecting the type of payment.\n214 \n| \nChapter 6: Chatbots\n",
        "word_count": 282,
        "char_count": 1600,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 932,
            "ext": "png",
            "size_bytes": 121421
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 245,
        "text": "Figures 6-18 and 6-19 show how internal state and extracted entities work in another\nconversation.\nFigure 6-18. Texting complex statements with multiple entities\nFigure 6-19. Testing with a complex entity and context\nDialog Systems in Detail \n| \n215\n",
        "word_count": 38,
        "char_count": 250,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1442,
            "height": 723,
            "ext": "png",
            "size_bytes": 152381
          },
          {
            "index": 1,
            "width": 290,
            "height": 483,
            "ext": "png",
            "size_bytes": 16160
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 246,
        "text": "Dialogflow allows us to build goal-oriented chatbots. It’s important\nto have an extensive ontology (possible slots and intents) for our\ndomain, as it will make our bot rich in responding to varied user\nqueries.\nWe’ve shown how to build a fully functional chatbot using the Dialogflow API. We\nlearned about intents and entities—the two main building blocks of understanding\ndialog. Now, we’ll delve deep into building custom models for intent/dialog act classi‐\nfication and entity/slot identification.\nDeep Dive into Components of a Dialog System\nSo far, we’ve seen how to build a chatbot using Dialogflow and how to add various\nfeatures to handle complex entities and context. Now, we want to deep-dive into the\nmachine learning aspect of the internals of a dialog system. As we discussed while\ndescribing the pipeline for a dialog system, understanding the context (i.e., the user\nresponse) in light of conversation history is one of the most important tasks for\nbuilding a dialog system.\nUnderstanding context can be broken down into understanding the user’s intent and\ndetecting corresponding entities for that particular intent. These internal components\ncorrespond to the natural language understanding component in the chatbot pipe‐\nline. To illustrate this, we’ll go through a sample of a conversation on restaurant book‐\ning and describe how to model different components for context understanding.\nFigure 6-20 shows an example of a user looking for a restaurant reservation. As we\ncan see, there are labels available for each response. The labels indicate intents and\nentities for these responses. We want to use such annotations to train our ML models.\nFigure 6-20. Conversation about restaurant booking [11]\n216 \n| \nChapter 6: Chatbots\n",
        "word_count": 274,
        "char_count": 1748,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          },
          {
            "index": 1,
            "width": 564,
            "height": 444,
            "ext": "png",
            "size_bytes": 57525
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 247,
        "text": "Before we go into the model, we’ll formally define two natural understanding tasks\nrelated to context understanding for dialogs. Since this involves understanding the\nnuances of language underneath, these are also attributed as natural language under‐\nstanding (NLU) tasks.\nDialog Act Classification\nDialog act classification is a task to identify how the user utterance plays a role in the\ncontext of dialog. This informs what “act” the user is performing. For example, a sim‐\nple example of dialog acts would be to identify a “yes/no” question. If the user asks,\n“Are you going to school today?”, this would be classified as a yes/no question. On the\nother hand, if the user asks, “What is the depth of the ocean?”, that may not be classi‐\nfied as a yes/no question. We’ve seen that intents or dialog acts are important for\nbuilding a chatbot, even in Cloud APIs. Identifying intent helps to understand what\nthe user is asking for and to take actions accordingly.\nBuilding dialog act classification and slot identification from\nscratch can be a complex and data-consuming process. Doing so\nmakes sense when our dialog acts and slots are more open-ended\nin nature than a Cloud API or existing framework can solve. Hav‐\ning complete control of dialog internals can yield better results over\ntime in such problems.\nThis can be reframed as a classification problem: given a dialog utterance, classify it\ninto dialog acts or labels. In our example from Figure 6-20, we define a dialog act pre‐\ndiction task where labels include inform, request, etc. The utterance “Where is it?”\ncan be classified as a dialog act “request.” On the other hand, the utterance “I’m look‐\ning for a cheaper restaurant” can be classified as an “inform” dialog act. Drawing on\nwhat we learned in Chapter 4, we can use any classifier we like to solve this task. We’ll\ndiscuss the models pertaining to this task with a complete dataset example in “Dialog\nExamples with Code Walkthrough” on page 219.\nIdentifying Slots\nOnce we’ve extracted the intents, we want to move on to extracting entities. Extract‐\ning entities is also important for generating correct and appropriate responses to the\nuser’s input. We also saw in our Dialogflow example that extracting entities along\nwith the intents creates a full understanding of the user’s input.\nIn the example in Figure 6-20—”I’m looking for a cheaper restaurant”—we want to\nidentify “cheaper” as a price slot and take its value verbatim—i.e., the value of the slot\nis “cheaper.” If we know ontologies for slot-value pairs, a more normalized form can\nultimately be restored, such as “cheaper” -> “cheap.” We have seen similar tasks in\nChapter 5, where we learned how to extract entities from sentences. We can take a\nDeep Dive into Components of a Dialog System \n| \n217\n",
        "word_count": 467,
        "char_count": 2788,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 248,
        "text": "similar approach (i.e., a sequence labeling approach) here as well to extract these enti‐\nties.\nPreviously, in our Dialogflow examples, we saw that slots have to be pre-defined\nbeforehand. But here, we want to build this component on our own using an ML\nalgorithm. Recall the algorithms discussed in the context of NER in Chapter 5. We’ll\nuse similar algorithms for slot detection and labeling. We’ll use an open source\nsequence labeling library called sklearn-crfsuite [12], which we introduced in Chap‐\nter 5, for this task. We’ll discuss details of this experiment in a later section.\nWe can choose a range of ontologies for annotating entities. Imag‐\nine we’re building a travel bot. The choice of entity for the destina‐\ntion can be city or airport. To make it robust, we must detect\nairports as an entity since one city can have multiple airports. On\nthe other hand, in the case of a restaurant-booking bot, detecting\ncities as an entity is probably suitable.\nOne of the disadvantages of these methods is that they need a lot of labeled data for\nboth intent and entity detection. Also, we need dedicated models for both of the tasks.\nThis can make the system slow during deployment. Getting fine-grained labels for\nentities is also expensive. These issues limit the scalability of the pipeline for more\ndomains.\nRecent research [11] on spoken language understanding revealed that joint under‐\nstanding and tracking is better than individual classification and sequence labeling\nparts. This joint model is lightweight at deployment as compared to individual mod‐\nels. For joint modeling, we can utilize dialog states, which is “inform(price - cheap)”\nin our example in Figure 6-20. We can aim to rank or score each candidate pair\njointly with dialog act (in combination, a dialog state) to jointly determine the state.\nJoint determination is more complex and requires better representation learning\ntechniques, which are beyond the scope of this book. Interested readers can learn\nmore about this at [11]. Now that we’ve discussed NLU components, let’s move on to\nresponse generation.\nResponse Generation\nOnce we identify the slots and intent, the final step is for a dialog system to generate\nan appropriate response. There are many ways to generate a response: fixed respon‐\nses, using templates, and automatic generation.\nFixed responses\nFAQ bots mainly use fixed responses. Based on the intent and values for the slots,\na dictionary lookup is made on a pool of responses and retrieves the best\nresponse. A simple case would be to discard the slot information and have one\n218 \n| \nChapter 6: Chatbots\n",
        "word_count": 429,
        "char_count": 2609,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 249,
        "text": "response per intent. For more complex retrieval, a ranking mechanism can be\nbuilt that ranks the pool of responses based on the detected intent and slot-value\npairs (or the dialog state).\nUse of templates\nTo make responses dynamic, a templates-based approach is often taken. Tem‐\nplates are very useful when the follow-up response is a clarifying question. Slot\nvalues can be used to come up with a follow-up question or a fact-driven answer.\nFor example, “The House serves cheap Thai food” can be constructed using a\ntemplate as <restaurant name> serves <price-value> <food-value> food. Once we\nidentify slots and their values, we populate this template to finally generate an\nappropriate response.\nAutomatic generation\nMore natural and fluent generation can be learned using a data-driven approach.\nUpon obtaining the dialog state, a conditional generative model can be built that\ntakes a dialog state as an input and generates the next response for the agent.\nThese models can be graphical models or DL-based language models. Later, we’ll\nbriefly cover end-to-end approaches for dialogs that are similar to automatic gen‐\nerations.\nWhile automatic generation is robust, template generation has\nadvantages over it. It might be hard to distinguish between the two,\nespecially when the template variety is high, Template-based\nresponses contain fewer grammatical errors and are easier to train.\nNow that we’ve deep-dived into various components of a dialog system, let’s walk\nthrough examples of dialog act classification and slot predictions.\nDialog Examples with Code Walkthrough\nNow, we’ll go through instances of various real-world dialog datasets that are publicly\navailable and discuss their usage to model various aspects of a dialog system. Then\nwe’ll use two of those datasets to show how to implement models for two tasks we\ndescribed for context understanding: dialog act prediction or intent classification and\nslot identification or entity detection. We’ll explore a couple of models for each task\nand show via comparisons how these models can be improved gradually. All the\nmodels are inspired from the NLU tasks (classification and information extraction)\nwe discussed in Chapters 4 and 5.\nDeep Dive into Components of a Dialog System \n| \n219\n",
        "word_count": 352,
        "char_count": 2258,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 250,
        "text": "Datasets\nTable 6-2 is a brief summary of various datasets that are used for benchmarking algo‐\nrithms for goal-oriented dialog tasks. As we’re interested in various NLU tasks in dia‐\nlogs, we present four datasets for goal-oriented dialogs that act as benchmarks for\ndialog-based NLU tasks.\nTable 6-2. Goal-oriented datasets from various domains and their usage\nDataset\nDomain\nUsage\nATIS [13]\nAir Ticket\nBooking\nBenchmark for intent classification and slot filling. This is a single-domain dataset, hence\nentities and intents are restricted to one domain.\nSNIPS [14]\nMultidomain\nBenchmark for intent classification and slot filling. This is a multidomain dataset, hence the\nentities belong to multiple domains. Multiple-domain datasets are challenging to model\ndue their variability.\nDSTC [15]\nRestaurants\nBenchmark for dialog state tracking or joint determination of intent and slots. This is\nsimilarly a single-domain dataset, but the entities are expressed more in terms of\nannotations and contain more metadata.\nMultiWoZ\n[16]\nMultidomain\nBenchmark for dialog state tracking or joint determination of intent and slots that spans\nover multiple domains. For the similar reason of variability, modeling this dataset is more\nchallenging than modeling single-domain ones.\nIn addition to these datasets, several datasets of varying scale (i.e., number of sample\nconversations) are available [17] for various other subtasks in a dialog pipeline. Later\nin this section, we’ll discuss how to gather such a dataset and apply it to a domain-\nspecific scenario. For now, we’ll focus on goal-oriented dialogs since they have direct\nusage in industry and the state-of-the-art research is well established.\nDespite the existence of many open source datasets, there are only\na few datasets that reflect the naturalness of human conversation.\nDatasets collected by online annotators like Mechanical Turkers\nsuffer from templatish and forced conversation, which affects the\ndialog quality. Also, domain-specific dialog datasets are still not\navailable for many domains, such as healthcare, law, etc.\nDialog act prediction\nDialog act classification or intent detection is the task we described in the previous\nsection as a part of the NLU component in a dialog system. This is a classification\ntask, and we’ll follow our classification pipeline from Chapter 4 to solve it.\nLoading the dataset.   We’ll use ATIS (Airline Travel Information Systems) for the\nintent detection task. ATIS is a dataset that’s used heavily for spoken language under‐\nstanding and performing various NLU tasks. The dataset consists of 4,478 training\nutterances and 893 test utterances with a total of 21 intents. We’ve chosen 17 intents,\n220 \n| \nChapter 6: Chatbots\n",
        "word_count": 414,
        "char_count": 2726,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "MinionPro-Regular (9.6pt)",
          "MyriadPro-SemiboldCond (11.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 394,
            "height": 514,
            "ext": "png",
            "size_bytes": 7986
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 251,
        "text": "which appear in both the train and test set. Hence, our task is a 17-class classification\ntask. An instance of the dataset looks like the following code:\nQuery text: BOS please list the flights from charlotte to long beach arriving \n after lunch time EOS\nIntent label:  flight\nModels.   Since it’s a classification task, we’ll use one of the DL techniques we used in\nChapter 4 directly: a CNN model. Using CNN is useful here because it captures the n-\ngram features via its dense representations. N-grams such as “list of flights” is indica‐\ntive of a “flight” label:\natis_cnnmodel = Sequential()\natis_cnnmodel.add(embedding_layer)\natis_cnnmodel.add(Conv1D(128, 5, activation='relu'))\natis_cnnmodel.add(MaxPooling1D(5))\natis_cnnmodel.add(Conv1D(128, 5, activation='relu'))\natis_cnnmodel.add(MaxPooling1D(5))\natis_cnnmodel.add(Conv1D(128, 5, activation='relu'))\natis_cnnmodel.add(GlobalMaxPooling1D())\natis_cnnmodel.add(Dense(128, activation='relu'))\natis_cnnmodel.add(Dense(num_classes), activation='softmax'))\natis_cnnmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics= ['acc'])\nWe obtain an accuracy of 72% with the use of a CNN on the test, averaged over all\nclasses. If we use an RNN model, the accuracy shoots up to 96%. We believe that\nRNN is able to capture the interdependency of words across the input sentence. RNN\ncaptures the importance of a word with respect to the context it’s seen before. The\nelaborate details of these models and the dataset code are given in ch6/\nCNN_RNN_ATIS_intents.ipynb:\natis_rnnmodel = Sequential()\natis_rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\natis_rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\natis_rnnmodel.add(Dense(num_classes), activation='sigmoid'))\natis_rnnmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics= ['accuracy'])\nAs we know, recent transformer pre-trained models (such as BERT) are more power‐\nful. So, we’ll try to use BERT to improve the obtained performance so far. BERT can\ncapture the context better and has more parameters, so it’s more expressive and mod‐\nels the intricacies of the language. To use BERT, we use a BERT-style input tokeniza‐\ntion scheme:\n# For data:\nsentence = \" [CLS] \" + query + \" [SEP]\"\nTokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n                                         do_lower_case=True)\nDeep Dive into Components of a Dialog System \n| \n221\n",
        "word_count": 300,
        "char_count": 2465,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MyriadPro-SemiboldCond (11.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 252,
        "text": "tokenizer.tokenize(sentence)\n# For model:\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                     num_labels=num_classes)\nSince BERT is pre-trained, the representation of content is much better than any\nmodels we train from scratch, such as CNNs or RNNs. We see that BERT achieves\n98.8% accuracy, beating both CNN and RNN for the dialog act prediction task. Fol‐\nlow the notebook ch6/BERT_ATIS_intents.ipynb for the complete code for model and\ndata preparation.\nSlot identification\nSlot identification is another task we described in the previous section as a part of the\nNLU component in a dialog system. We described why we can pose this as a sequence\nlabeling task. We need to find the slot values given the input, and we’ll follow our\nsequence labelling pipeline from Chapter 5 to solve this task.\nLoading the dataset.   We’ll use SNIPS for this slot identification task. SNIPS is a dataset\ncurated by Snips, an AI voice platform for connected devices. It contains 16,000\ncrowdsourced queries and is a popular benchmark for slot identification tasks. We’ll\nload both training and test examples, and an instance of the dataset looks like the\ncode below:\nQuery text: [Play, Magic, Sam, from, the, thirties]  # tokenized\nSlots: [O, artist-1, artist-2, O, O, year-1]\nAs we discussed in Chapter 5, we’re using the BIO scheme to annotate the slots. Here,\nO denotes “other,” and artist-1 and artist-2 denote the two words for artist name.\nThe same goes for the year.\nModels.   Since a slot identification task can be viewed as a sequence labeling task, we’ll\nuse one of the popular techniques we used in Chapter 5: a CRF++ model from the\nsklearn package. We also use word vectors instead of creating handcrafted features to\nfeed into a CRF. CRFs are a popular sequence labeling technique and are used heavily\nin information extraction.\nWe use word features that will be useful for this particular task. We see that the con‐\ntext for each word is important in addition to the meaning of the word itself. So, we\nuse the previous two words and next two words for a given word as features. We also\nuse the word embedding vectors retrieved from GloVe pre-trained embeddings (dis‐\ncussed in Chapter 3) as additional features. Features for each word are concatenated\nacross words in an input. This input representation is passed to a CRF model for\nsequence labeling:\n222 \n| \nChapter 6: Chatbots\n",
        "word_count": 392,
        "char_count": 2457,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "MyriadPro-SemiboldCond (11.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 253,
        "text": "def sent2feats(sentence):\n    feats = []\n    sen_tags = pos_tag(sentence) #This format is specific to this POS tagger!\n    for i in range(0,len(sentence)):\n        word = sentence [i]\n        wordfeats = {}\n        #word features: word, prev 2 words, next 2 words in the sentence.\n        wordfeats ['word'] = word\n        if i == 0:\n            wordfeats [\"prevWord\"] = wordfeats [\"prevSecondWord\"] = \"<S>\"\n        elif i==1:\n            wordfeats [\"prevWord\"] = sentence [0]\n            wordfeats [\"prevSecondWord\"] = \"</S>\"\n        else:\n            wordfeats [\"prevWord\"] = sentence [i-1]\n            wordfeats [\"prevSecondWord\"] = sentence [i-2]\n        #next two words as features\n        if i == len(sentence)-2:\n            wordfeats [\"nextWord\"] = sentence [i+1]\n            wordfeats [\"nextNextWord\"] = \"</S>\"\n        elif i==len(sentence)-1:\n            wordfeats [\"nextWord\"] = \"</S>\"\n            wordfeats [\"nextNextWord\"] = \"</S>\"\n        else:\n            wordfeats [\"nextWord\"] = sentence [i+1]\n            wordfeats [\"nextNextWord\"] = sentence [i+2]\n        #Adding word vectors\n        vector = get_embeddings(word)\n        for iv,value in enumerate(vector):\n            wordfeats ['v{}'.format(iv)]=value\n        feats.append(wordfeats)\n    return feats\n# training\ncrf = CRF(algorithm='lbfgs', c1=0.1, c2=10, max_iterations=50)\n# Fit on training data\ncrf.fit(X_train, Y_train)\nWe obtain an F1 of 85.5 with the use of a CRF++ model. More details can be found in\nthe notebook ch6/CRF_SNIPS_slots.ipynb. Similar to the previous classification task,\nwe’ll try to use BERT to improve the performance obtained so far. BERT can capture\nthe context better, even in the case of a sequence labeling task. We use all the hidden\nrepresentations for all the words in the query to predict a label for each. Hence, at the\nend, we input a sequence of words into the model and obtain a sequence of labels (of\nthe same length as the input), which can be inferred as predicted slots with the words\nas values:\n# For data:\nsentence = \" [CLS] \" + query + \" [SEP]\"\nDeep Dive into Components of a Dialog System \n| \n223\n",
        "word_count": 280,
        "char_count": 2114,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 254,
        "text": "Tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n                                         do_lower_case=True)\ntokenizer.tokenize(sentence)\n# For model:\nmodel = BertForTokenClassification.from_pretrained(\"bert-base-uncased\",\n                                                  num_labels=num_tags)\nBut, we find that BERT achieves only 73 F1. This could be due to the presence of\nmany named entities in the input that were not well represented by the original BERT\nparameters. On the other hand, the features we obtained for the CRF were strong\nenough for this dataset to capture the necessary patterns. This is an interesting exam‐\nple where a simpler model beats BERT. See the complete model details in the note‐\nbook ch6/BERT_SNIPS_slots.ipynb.\nAs we’ve seen before and here as well, pre-trained models help get\nbetter performance over other DL models learned from scratch.\nThere could be exceptions, as pre-trained models are sensitive to\nthe size of the data. Pre-trained models may overfit on smaller\ndatasets, and handcrafted features may generalize well in those\ncases.\nSo far, we’ve learned how to build various NLU components for a goal-oriented dia‐\nlog using popular datasets. We’ve seen how various DL models perform relatively well\nin these tasks. With these, we’ll be able to run such custom models in our own dataset\nand explore various models to pick the best one. We also introduced four datasets\nthat are popular benchmarks for goal-oriented dialog modeling. They can be used for\nprototyping newer models to verify their performance against state-of-the-art mod‐\nels. Now, we’ll transition to other dialog models that are generally used beyond goal-\noriented settings, and we’ll discuss their advantages and disadvantages.\nOther Dialog Pipelines\nSo far, we’ve discussed the modular pipeline we introduced in Figure 6-3. But there\nare many other pipelines that can be used in various scenarios, especially in the case\nof an open-ended chatbot. The initial pipeline in Figure 6-3 sometimes lacks in terms\nof ease of trainability due to multiple components, as each of them has to be individ‐\nually trained and they need separate annotated datasets for each component. Besides\nthat, in a modular pipeline, one needs to define the ontology explicitly and it does not\ncapture latent patterns from the data. That is why we will briefly touch upon other\nexisting pipelines that may be promising in future.\n224 \n| \nChapter 6: Chatbots\n",
        "word_count": 363,
        "char_count": 2457,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "UbuntuMono-Italic (8.5pt)",
          "MinionPro-Regular (9.6pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 394,
            "height": 514,
            "ext": "png",
            "size_bytes": 7986
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 255,
        "text": "End-to-End Approach\nSequence-to-sequence models (we’ll call it seq2seq) have seen huge acceptance in\ncritical NLP tasks such as neural machine translation, named entity recognition, etc.\nThe seq2seq models generally take a sequence as input and output another sequence.\nIn a translation task, imagine our input sentence is in one language and output is in\nthe language we want to translate it to.\nSimilar to other tasks, we can build a chatbot using seq2seq models. Imagine that the\ninput of the model is the user utterance: a sequence of words. As the output, it gener‐\nates another sequence of words, which is the response from the bot. Seq2seq models\nare end-to-end trainable, so we don’t have to maintain multiple modules, and they are\ngenerally LSTM based. Recently, state-of-the-art transformers have been used for\nseq2seq tasks, so they can also be applied in the case of dialog.\nUsually, we use tokenization to create word tokens and create a sequence out of a\nquestion. Seq2seq is capable of capturing the inherent order of the token in the\nsequence—this is important, as it ensures that we capture the right meaning of the\nquestion in order to answer it correctly. See Figure 6-21 for some examples from a\nwork by Google [11] on such an end-to-end model. They input the questions to the\nmodel, and the model generated the corresponding outputs.\nFigure 6-21. Example of work done by Google on seq2seq models [18]\nDeep Reinforcement Learning for Dialogue Generation\nIf you’re wondering how a machine would generate a diverse set of answers given any\nkind of question, you’re not alone. [19] studied the drawbacks of typical seq2seq\nmodels and discovered that they often kept generating the generic output, “I don’t\nknow.” These models generated utterances without considering how to respond in\nOther Dialog Pipelines \n| \n225\n",
        "word_count": 301,
        "char_count": 1833,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1330,
            "height": 726,
            "ext": "png",
            "size_bytes": 86495
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 256,
        "text": "order to have a good conversation. Doing so requires futuristic knowledge about the\ngoodness of the conversation, which will ultimately help the user achieve their goal.\nThe concept of goodness is abstract, so it’s typically defined based on the objective of\nthe conversation. For example, with a goal-oriented dialog setup, we have a defined\ngoal to achieve, whereas in a chitchat setup, goodness is defined by how interesting\nthe conversation will be.\nHere, we see the combination of two ideas: goal-oriented dialog and seq2seq-based\ngeneration. Reinforcement learning can help us here. Each time the machine utters a\nresponse is nothing but it performing a specific action. A set of such actions can be\nmade in a way that ensures the goal is finally achieved via the conversation. In rein‐\nforcement learning based on exploration and exploitation, the machine tries to learn\nto generate the best response based on a futuristic reward defined by the user, which\nis directly related to how likely the current response is to achieve the final goal.\nFigure 6-22 shows how the reinforcement learning–based model performed well\ncompared to the typical seq2seq-based model. On the right-hand side, you can see\nthat the reinforcement learning–based model generated a more diverse response\ninstead of collapsing into a generic default response.\nFigure 6-22. Comparison of deep reinforcement learning and a seq2seq model [19]\nHuman-in-the-Loop\nSo far, we’ve talked about machines generating answers in response to questions\nasked, without human intervention. The machine may improve its performance if\nhumans intervene in its learning process and reward or penalize based on the correct\nor incorrect response. These rewards or penalties act as feedback for the model.\nAnswering a natural language query typically follows three steps: understand the\nquery, perform an action, and respond to utterances. While doing this, the machine\n226 \n| \nChapter 6: Chatbots\n",
        "word_count": 304,
        "char_count": 1953,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1212,
            "height": 568,
            "ext": "png",
            "size_bytes": 103526
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 257,
        "text": "might need human intervention in various scenarios—for example, if the question is\nout of the chatbot’s scope, if the action it took was not correct, or if the understanding\nof the query was wrong. Typically, when humans intervene in a machine’s learning\nprocess, it’s termed as human-in-the-loop.\nIn the context of chatbots, Facebook has performed an exercise [20] of using humans\nto inject partial rewards when the bot is learning in a reinforcement learning setup.\nAs we discussed in the previous subsection, the ultimate goal of the bot is to fulfill the\nuser’s needs. But with human-in-the-loop, while exploring various actions, the bot\nreceives additional input from a human “teacher,” which clearly improves the quality\nof the response, as shown in Figure 6-23.\nFigure 6-23. Humans providing additional signals during dialog learning [20]\nHuman-in-the-loop is ultimately a more practical system to deploy\nthan a completely automated dialog generation system. End-to-end\nmodels are efficient to train, but they may not be reliable in pro‐\nducing factually correct outputs. Hence, a hybrid system with the\ncombination of end-to-end dialog generation framework and with\nhuman resources will be more reliable and robust.\nWe’ve discussed various techniques beyond goal-oriented dialog. Many of these\nmethods are built by industry and are usable in practical settings. These end-to-end\nmodels can grow large in terms of parameters (via the use of new transformer archi‐\ntecture) and can therefore become infeasible to deploy in small-scale applications.\nBut we also saw here that even LSTM models can generate reasonable outputs.\nHuman-in-the-loop is also a feasible technique that can be adopted regardless of the\ncomputing power available.\nRasa NLU\nSo far, we’ve discussed how to build two main components of a dialog system: dialog\nact prediction and slot filling. Beyond these two components, there are several inte‐\ngration steps to tie them into a complete pipeline for dialog. Also, we can build wrap‐\nping logic around these components and create a comprehensive dialog experience\nfor users.\nRasa NLU \n| \n227\n",
        "word_count": 330,
        "char_count": 2118,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          },
          {
            "index": 1,
            "width": 1059,
            "height": 225,
            "ext": "png",
            "size_bytes": 51101
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 258,
        "text": "Building such a complete dialog system requires significant engineering work. But\nthe good news is there are frameworks available that allow us to build custom NLP\nmodels as various components of the system and that provide overhead engineering\ntools and supports to build a functioning bot. One example of such a framework is\nRasa. Rasa offers a suite of features [21] that can be essential in building a chatbot for\nindustrial use. Figure 6-24 shows the Rasa chatbot interface along with its interactive\nlearning framework, which we’ll discuss later.\nFigure 6-24. Rasa chatbot interface and interactive learning framework [21]\nWe’ll briefly touch on Rasa’s available features and discuss how they can be used to\nimprove the user’s experience with a chatbot:\nContext-based conversations\nThe Rasa framework allows users to capture and utilize the conversation context\nor dialog state. Internally, Rasa performs NLU and captures required slots and\ntheir values, which can be utilized in response generation.\nInteractive learning\nRasa offers an interactive interface that can be used for two purposes. One is to\ncreate more training data for the internal models by chatting with the bot. The\nsecond is to provide feedback when the models make mistakes. This feedback\ncan be used as negative samples for the model to improve performance in chal‐\nlenging cases.\nData annotation\nRasa presents a highly interactive and easy-to-use interface to annotate more data\nto improve the model training. Data annotation can be done from scratch or\nmodified from examples where labels are already predicted by the existing\n228 \n| \nChapter 6: Chatbots\n",
        "word_count": 258,
        "char_count": 1634,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 727,
            "ext": "png",
            "size_bytes": 241369
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 259,
        "text": "models. See Figure 6-25 for an example of the data annotation step in Rasa.\nWrapper frameworks are built on Rasa NLU, which eases the data annotation\nprocess to generate large-scale dialog datasets. Once such framework is Chatette\n[22], which is a tool that accepts templates and then spawns dialog instances\nusing those templates at scale.\nFigure 6-25. Data annotation and API integrations\nAPI integration\nFinally, the dialog service can also be integrated with other APIs as well as chat\nplatforms like Slack, Facebook, Google Home, and Amazon Alexa. The next sec‐\ntion includes a case study where we’ll produce recipe recommendations via con‐\nversations and integrate a faceted search API endpoint into the bot to facilitate\nthe recommendation process.\nRasa NLU \n| \n229\n",
        "word_count": 123,
        "char_count": 773,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 1937,
            "ext": "png",
            "size_bytes": 328255
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 260,
        "text": "Customize your models in Rasa\nApart from the framework, Rasa also allows us to customize our models by\nchoosing from a pool of models. For example, for intent/dialog act detection, we\ncan choose “sklearn classifier” [23] or “mitie classifier” [24], or we can write our\nown classifier and add that to the building pipeline for Rasa to use it. Various\noptions for embeddings such as spaCy and Rasa’s own are available with the\nframework.\nWe can also harness the power of transformer models as we see performance\nimprovement while building our individual components. Rasa provides BERT\n(and various distilled versions to improve on latency) for both classification and\nsequence labeling tasks [25, 26]. Overall, this makes Rasa a very powerful tool for\nbuilding a dialog system from scratch.\nRasa enables us to build our chatbot in a modular way. For exam‐\nple, we can start with existing pre-trained models and later use cus‐\ntom models built on our specific datasets as needed. Similarly, we\ncan start default API integrations and conversation channels and\nmodify them when needed.\nNow, let’s go over a complete case study with a real scenario discussing what steps are\nnecessary to create a conversational system from scratch in an industrial setup,\nincluding data setup, model building, and deployment.\nA Case Study: Recipe Recommendations\nCooks often look for specific recipes tailored to their culinary and dietary preferen‐\nces. A conversational interface where cooks can find their recipe of choice by fleshing\nout their preferences via a conversation with the agent would be a good user experi‐\nence. In this case study, we’ll discuss all the components we’ve covered in this chapter\nalong with the frameworks required to build them. We’ll see the evolving need for\ndata and modeling complexity of the business problem and address them via various\ntools we’ve learned about in this chapter.\nImagine we’re part of a recipe and food aggregator site. We’ve been tasked with build‐\ning a chatbot. Users can talk about the kind of food they’re craving or want to cook.\nThis is an uncharted problem, so how will we go about building this? Figure 6-26\nshows some example suggestions of recipes for various user preferences.\nWe need to convert this business problem into a technical problem with objectives\nand constraints. As a user will interact with the system, our goal is to create a fully\ndefined query that can fetch a suitable recipe. The recipe can come from an API end‐\npoint or a generative model. This query is made of a set of attributes that define the\ndish, such as ingredients, cuisine, calorie level, cooking time, etc. We also know that\n230 \n| \nChapter 6: Chatbots\n",
        "word_count": 447,
        "char_count": 2681,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 261,
        "text": "users can reveal their preferences through turns in a conversation, so we need to track\ntheir preferences and update the internal dialog state as the conversation proceeds.\nFigure 6-26. Example of a recipe-suggestion site: Allrecipes.com\nUtilizing Existing Frameworks\nWe’ll start with Dialogflow, the cloud API we described earlier in the chapter since it’s\neasy to build. Before we start, we need to define entities like we did before, such as\ningredients, cuisine, calorie level, cooking time. We can build an ontology for the\ncooking domain and identify the number of slots we’d like our chatbot to support.\nA Case Study: Recipe Recommendations \n| \n231\n",
        "word_count": 104,
        "char_count": 656,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 960,
            "height": 996,
            "ext": "png",
            "size_bytes": 929715
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 262,
        "text": "Initially, it will be good to keep an exhaustive list of these entities. Here are some\nexamples of training instances that capture nuances in this early phase of bot\nbuilding:\n• I want a low calorie dessert that is vegan.\n• I have peas, carrots, and chicken in my kitchen. What can I make with it in 30\nminutes?\nDialogflow is capable of handling the user’s preference and identifying the slots and\nvalues necessary to look for a correct recipe. Also, due to the conversational nature of\nthe user’s interaction, the bot will maintain its dialog state or context to fully under‐\nstand the user’s input. We’ll assume a database of recipes has been pre-defined and\nprefilled. Now, once the entities are captured via the bot, we need to feed them into an\nAPI endpoint. This endpoint will do a faceted search on the database and retrieve the\nbest-ranked recipes.\nAs we collect more data, Dialogflow will slowly become better. But due to its lack of\ncustom models, it can’t solve more complex conversations related to this task. Some\nexamples where a Dialogflow-based bot will eventually fail are:\n• I have a chicken with me, what can I cook with it besides chicken lasagna?\n• Give me a recipe for a chocolate dessert that can be made in just 10 mins instead\nof the regular half an hour.\nThese examples show a presence of more than one value for one slot, and only one of\nthem is correct—for example, “10 mins” is correct, while “half an hour” isn’t.\nMatching-based methods in Dialogflow will fail in such cases. That’s why we need to\nbuild custom models so that these examples can be added as adversarial examples in\ntheir training pipeline. In a Rasa pipeline with custom models, we can add such\nadversarial examples in order for the model to learn to identify correct slots and their\nvalues. It’s also possible to generate such adversarial examples from the data we’ve\ngathered using data augmentation techniques and including them though the data\nannotation techniques of the Rasa framework, as shown in Figure 6-27.\nWith this updated training data, new custom models will be able to pick the correct\nvalues to fully describe the user’s ask for the recipe. Once slots and values are cap‐\ntured, the rest of the process will be similar to how it was before (i.e., an API end‐\npoint can use this information to query an appropriate recipe).\n232 \n| \nChapter 6: Chatbots\n",
        "word_count": 414,
        "char_count": 2364,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 263,
        "text": "Figure 6-27. How Rasa can facilitate complex annotations\nOpen-Ended Generative Chatbots\nOur solution is good enough to be deployed on a real website where millions of users\ninteract regularly. Now we can focus on solving more challenging tasks with the\nobjective of improving the user experience even more. So far, we’ve been providing\nusers with specific recipes that are stored in a datastore beforehand. What if we want\nto make the chatbot more open ended by generating recipes instead of searching for\nthem from a pre-existing pool? The advantage of such systems is their ability to han‐\ndle unknown attribute values and customize recipes to fit the personalized tastes of\nthe users.\nOpen-ended chatbots are generally harder to evaluate because\nmany variants of a response can be correct given the context.\nHuman evaluation seems to be most efficient, but it’s irreproduci‐\nble and therefore harder to compare to other systems. A mix of\nautomatic and human evaluation is the right way to evaluate gener‐\native dialog systems.\nHere, we can utilize powerful seq2seq generative models that can condition their gen‐\neration on the various desired attributes the user has described for the recipe prefer‐\nence. Researchers (including one of the authors) have shown [27] that these seq2seq\nmodels are capable of generating personalized recipes based on preferences and\nA Case Study: Recipe Recommendations \n| \n233\n",
        "word_count": 225,
        "char_count": 1412,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          },
          {
            "index": 1,
            "width": 1442,
            "height": 901,
            "ext": "png",
            "size_bytes": 206589
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 264,
        "text": "previous recipe interactions. These models are capable of incorporating nuances and\npotentially generating a novel recipe that’s valid but unique to the user’s culinary\ntaste. Figure 6-28 shows such an example of a newly generated recipe incorporating a\nuser’s preference. The user’s preference can be just a list of recipes that they’ve inter‐\nacted with before. For example, in this figure, the user had previously interacted with\nmojito, martini, and Bloody Mary. The personalized model added an extra garnishing\nstep (highlighted in gray) to make it more personalized.\nFigure 6-28. Recipes generated personalized to user’s preferences [27]\nMerging such generative models with other dialog components can really boost the\nuser experience. While we’ve discussed one specific recipe-recommendation problem,\nsimilar approaches can be taken in developing similar applications. We’ve discussed\nnecessary tools and models that can be used together to build a bot according to the\nbusiness problem at hand. We started with a very simple approach using Dialogflow\nand gradually added more complexity to tackle dialog nuances in the way users may\nexpress their queries and choices. Finally, we went the extra mile to build an end-to-\nend personalized chatbot.\nWrapping Up\nIn this chapter, we discussed chatbots and their applicability in various domains. We\nwent through a pipeline approach and delved deep into its various components. We\ntalked about a complete flow-based bot with a cloud-based API, then implemented\nML components of NLU modules. Finally, we analyzed a business problem and pro‐\nvided some pathways to approach it incrementally.\nBut as far as dialog systems and chatbots are concerned, there are many challenges\nthat are still unsolved. Hence, this is a very active area of research in the NLP com‐\nmunity. In addition to academic research, industrial research groups are also looking\nfor scalable solutions to existing approaches so that chatbots can be built reliably and\n234 \n| \nChapter 6: Chatbots\n",
        "word_count": 312,
        "char_count": 2015,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 568,
            "ext": "png",
            "size_bytes": 274162
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 265,
        "text": "deployed to users. Still today, many industrial chatbots fail to be robust and suffer in\nthe issue of natural language understanding and natural language generation. We\nmention these challenges in order to provide a broader picture of the domain of\nchatbots.\nThe major problem right now in building dialog systems is a lack of datasets that\nreflect natural conversations. Many times, personal data can’t be collected for privacy\nreasons. Other times, a lack of such conversational interfaces hinders data collection\ncapability. Also, existing datasets, especially ones that claim to be real-world datasets,\nlack naturalness. These datasets are created mainly by online annotators, and most of\nthe time, they sound scripted due to the nature of objective data collection. This\nproblem is very different from other NLP tasks. For instance, annotating a correct\nclass to a datapoint in a classification task or pointing out the relevant information in\nan information extraction task is more objective and easy to get than labels via\ncrowd-sourced online annotators. In the case of dialog, many times the task is subjec‐\ntive hence the data collection process becomes complex.\nFurthermore, the current generative models are not capable enough to generate fac‐\ntually correct statements, which becomes a critical problem in the case of chatbots. In\nthe short span of a conversation, factually incorrect generation may hinder the qual‐\nity of the conversation. Hence, future research and industrial efforts should be toward\nboth gathering better representative datasets and improving both natural language\nunderstanding and generation models that can be used in a chatbot pipeline.\nIn summary, we discussed the foundations of dialog systems, starting with an overall\npipeline, and developed a dialog system using Dialogflow, a cloud API; dove deep\ninto building custom models for understanding dialog context; and finally, used all of\nthem to solve a case study. While we anticipate that the area will continue evolving\nand improving, this chapter will be a good start for you to adapt to the new solutions\nthat keep coming. Now let’s turn to a few other common NLP problem scenarios in\nthe next chapter.\nReferences\n[1] ParlAI. Last accessed June 15, 2020.\n[2] Wallace, Michal and George Dunlop. Eliza, The Rogerian Therapist. Last accessed\nJune 15, 2020.\n[3] Amazon. “Build a Machine Learning Model”. Last accessed June 15, 2020.\n[4] Miller, Alexander H., Will Feng, Adam Fisch, Jiasen Lu, Dhruv Batra, Antoine\nBordes, Devi Parikh, and Jason Weston. “ParlAI: A Dialog Research Software Plat‐\nform.” Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations (2017): 79–84.\nWrapping Up \n| \n235\n",
        "word_count": 427,
        "char_count": 2739,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 266,
        "text": "[5] Pratap, Vineel, Awni Hannun, Qiantong Xu, Jeff Cai, Jacob Kahn, Gabriel Syn‐\nnaeve, Vitaliy Liptchinsky, and Ronan Collobert. “wav2letter++: The Fastest Open-\nsource Speech Recognition System”, (2018).\n[6] Google Cloud. “Cloud Text-to-Speech”. Last accessed June 15, 2020.\n[7] van den Oord, Aäron and Dieleman, Sander. “WaveNet: A Generative Model for\nRaw Audio”, DeepMind (blog), September 8, 2016.\n[8] Dialogflow. Last accessed June 15, 2020.\n[9] Dialogflow login page. Last accessed June 15, 2020.\n[10] Google Cloud. Dialogflow V2 API. Last accessed June 15, 2020.\n[11] Mrkšić, Nikola, Diarmuid O. Séaghdha, Tsung-Hsien Wen, Blaise Thomson, and\nSteve Young. “Neural Belief Tracker: Data-Driven Dialogue State Tracking.” Proceed‐\nings of the 55th Annual Meeting of the Association for Computational Linguistics 1\n(2016): 1777–1788.\n[12] Team HG-Memex. “sklearn-crfsuite: scikit-learn inspired API for CRFsuite”.\nLast accessed June 15, 2020.\n[13] Hemphill, Charles T., John J. Godfrey, and George R. Doddington. “The ATIS\nSpoken Language Systems Pilot Corpus.” Speech and Natural Language: Proceedings of\na Workshop Held at Hidden Valley, Pennsylvania, June 24–27, 1990.\n[14] Coucke, Alice, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier,\nDavid Leroy, Clément Doumouro et al. “Snips Voice Platform: an embedded Spoken\nLanguage Understanding system for private-by-design voice interfaces”, (2018).\n[15] Williams, Jason, Antoine Raux, and Matthew Henderson. “The Dialog State\nTracking Challenge Series: A Review.” Dialogue & Discourse 7.3 (2016): 4–33.\n[16] Budzianowski, Paweł, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva,\nStefan Ultes, Osman Ramadan, and Milica Gašić. “MultiWOZ - A Large-Scale Multi-\nDomain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling”, (2018).\n[17] Serban, Iulian Vlad, Ryan Lowe, Peter Henderson, Laurent Charlin, and Joelle\nPineau. “A Survey of Available Corpora for Building Data-Driven Dialogue Systems”,\n(2015).\n[18] Vinyals, Oriol and Quoc Le. “A Neural Conversational Model”, (2015).\n[19] Li, Jiwei, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Juraf‐\nsky. “Deep Reinforcement Learning for Dialogue Generation”, (2016).\n[20] Weston, Jason E. “Dialog-Based Language Learning.” Proceedings of the 30th\nInternational Conference on Neural Information Processing Systems (2016): 829–837.\n[21] Rasa. Last accessed June 15, 2020.\n236 \n| \nChapter 6: Chatbots\n",
        "word_count": 340,
        "char_count": 2433,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 267,
        "text": "[22] SimGus. Chatette: A powerful dataset generator for Rasa NLU, inspired by Cha‐\ntito, (GitHub repo). Last accessed June 15, 2020.\n[23] scikit-learn. “Classifier comparison.” Last accessed June 15, 2020.\n[24] MIT-NLP. MITIE: library and tools for information extraction, (GitHub repo).\nLast accessed June 15, 2020.\n[25] Sucik, Sam. “Compressing BERT for faster prediction”. Rasa (blog), August 8,\n2019.\n[26] Ganesh, Prakhar, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Deming\nChen, Marianne Winslett, Hassan Sajjad, and Preslav Nakov. “Compressing Large-\nScale Transformer-Based Models: A Case Study on BERT”, (2020).\n[27] Majumder, Bodhisattwa Prasad, Shuyang Li, Jianmo Ni, and Julian McAuley.\n“Generating Personalized Recipes from Historical User Preferences”, (2019).\nWrapping Up \n| \n237\n",
        "word_count": 114,
        "char_count": 798,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 268,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 269,
        "text": "CHAPTER 7\nTopics in Brief\nThe problems are solved, not by giving new information,\nbut by arranging what we have known since long.\n—Ludwig Wittgenstein, Philosophical Investigations\nSo far in Part II of this book, we’ve discussed a few common application scenarios of\nNLP: text classification, information extraction, and chatbots (Chapters 4 through 6).\nWhile these are the most common use cases for NLP we’re likely to encounter in\nindustry projects, there are many other NLP tasks that are relevant in building real-\nworld applications involving large collections of documents. We’ll take a quick look at\nsome of these topics in this chapter. Let’s first start with a few largely unrelated sce‐\nnarios you may encounter in your workplace projects. We’ll discuss them in more\ndetail throughout the chapter.\nIf someone asks us to find out what NLP is and we have no idea, where do we start?\nIn the pre-internet era, we would’ve hit the nearest library to do some research. How‐\never, now the first place we’d go is to a search engine. Search involves a lot of human-\ncomputer interaction using natural language, so it gives rise to very interesting use\ncases for NLP.\nOur client is a big law firm. When a new case comes up, they sometimes have to\nresearch lots and lots of documents related to the case to get a bigger picture of what\nit’s about. Many times, there isn’t enough time for a thorough manual review. Our cli‐\nent wants us to develop software that can provide a quick overview of the topics dis‐\ncussed in large document collections. Topic modeling is a technique that’s used to\naddress this problem of finding latent topics in a large collection of documents.\nThe same client’s firm has another problem: case report documents they receive are\nusually quite long, and it’s difficult even for an experienced lawyer to get the gist\nquickly. So our client wants a solution to automatically create summaries of text\n239\n",
        "word_count": 330,
        "char_count": 1928,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (9.3pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 270,
        "text": "documents. Text summarization approaches are used to address such use cases in the\nindustry.\nMany of us read news online every day. A common feature of many news websites is\nthe “related articles” feature, which shows articles that are topically related to the arti‐\ncle we’re reading. Consider a related scenario where we’re shown jobs related to a\ngiven job based on the profile descriptions. Recommendation methods using NLP are\nkey to building solutions for such use cases.\nWe live in an increasingly multicultural world, and many organizations have clients\nor customers across the globe. This results in the need to translate documents (at\nscale) in all supported languages in the organization. Machine translation (MT) is use‐\nful in such scenarios. Streaming services like Amazon, Netflix, and YouTube use MT\nextensively for generating subtitles in various languages. Tools like Google Translate\nhelp tourists across the globe communicate in local languages.\nWe use search engines for many reasons in day-to-day life. Sometimes, we want to\nknow answers to questions. Try asking a factual question such as, “Who wrote Ani‐\nmal Farm?” to your favorite search engine. Google shows “George Orwell” as its top\nresult along with some biographical details about him, followed by other regular\nsearch results. Try asking a somewhat descriptive question, say, “How do I calm down\na crying baby?” Among the answers, you’ll also see a blurb from some website that\nlists a number of ways to calm a baby. This is an example of question answering, where\nthe task is to locate the most appropriate answer to the user query instead of showing\na collection of documents. Note that this is slightly different from the FAQ chatbot we\nsaw in Chapter 6, where the scope of answers lies within a much smaller dataset (i.e.,\nFAQs) instead of a large collection of documents (such as the web).\nThese are the topics we’ll discuss in this chapter. While they may seem very different\nfrom one another, we’ll see the similarities between them as we progress through the\nchapter. This collection is not an exhaustive list, but these are some common scenar‐\nios encountered when developing NLP-based solutions for industrial applications.\nThe first four tasks (search, topic modeling, text summarization, and recommenda‐\ntion) are more common in real-world applied NLP scenarios, so we’ll discuss them in\ngreater detail than the other two. With large-scale question answering and machine\ntranslation, you’re unlikely to encounter a scenario where you’d have to develop solu‐\ntions from scratch, so we’ll only introduce them so you’ll know where to get started to\nbuild an MVP quickly. Table 7-1 summarizes the topics we’ll cover in this chapter,\nalong with example usage scenarios and the kind of data they work on.\n240 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 461,
        "char_count": 2828,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 271,
        "text": "Table 7-1. List of the topics covered in this chapter\nNLP task\nUse\nNature of data\nSearch\nFind relevant content for a given user query.\nWorld wide web/large collection of\ndocuments\nTopic modeling\nFind topics and hidden patterns in a set of documents.\nLarge collection of documents\nText summarization\nCreate a shorter version of the text with the most\nimportant content\nTypically a single document\nRecommendations\nShowing related articles\nLarge collection of documents\nMachine translation\nTranslate from one language to another\nA single document\nQuestion answering\nsystem\nGet answers to queries directly instead of a set of\ndocuments.\nA single document or a large\ncollection of documents\nWith this overview, let us start introducing these topics one by one in a little bit more\ndetail. Our first topic is search and information retrieval.\nSearch and Information Retrieval\nA search engine is an important component of everyone’s online activity. We search\nfor information to decide on the best items to purchase, nice places to eat out, and\nbusinesses to frequent, just to name a few examples. We also rely heavily on search to\nsift through our emails, documents, and financial transactions. A lot of these search\ninteractions happen through text (or speech converted to text in voice input). This\nmeans that a lot of language processing happens inside a search engine. Thus, we can\nsay that NLP plays an important role in modern search engines.\nLet’s start with a quick look into what happens when we search. When a user searches\nusing a query, the search engine collects a ranked list of documents that matches the\nquery. For this to happen, an “index” of documents and vocabulary used in them\nshould be constructed first and is then used to search and rank results. One popular\nform of indexing textual data and ranking search results for search engines is some‐\nthing we studied in Chapter 3: TF-IDF. Recent developments in DL models for NLP\ncan also be used for this purpose. For example, Google recently started ranking\nsearch results and showing search snippets using the BERT model. They claim that\nthis has improved the quality and relevance of their search results [1]. This is an\nimportant example of NLP’s usefulness in a modern-day search engine.\nApart from this major function of storing data and ranking search results, several fea‐\ntures in a modern search engine involve NLP. For example, consider the screenshot of\na Google search result shown in Figure 7-1, which illustrates some features that use\nNLP.\nSearch and Information Retrieval \n| \n241\n",
        "word_count": 422,
        "char_count": 2561,
        "fonts": [
          "MyriadPro-Cond (9.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 272,
        "text": "Figure 7-1. Screenshot of a Google search query\n1. Spelling correction: The user entered an incorrect spelling, and the search engine\noffered a suggestion showing the correct spelling.\n2. Related queries: The “People also ask” feature shows other related questions peo‐\nple ask about Marie Curie.\n3. Snippet extraction: All the search results show a text snippet involving the query.\n4. Biographical information extraction: On the right-hand side, there’s a small snip‐\npet showing Marie Curie’s biographical details along with some specific informa‐\ntion extracted from text. There are also some quotes and a list of people related to\nher in some way.\n5. Search results classification: On top, there are categories of search results: all,\nnews, images, videos, and so on.\nHere, we see a range of concepts we’ve learned about in this book being put to use.\nWhile these are by no means the only places NLP is used in search engines, they are\nexamples of where NLP is useful in the user interface aspect of search. However,\nthere’s much more to search than NLP, and building a search engine seems like a mas‐\nsive endeavor requiring a lot of infrastructure. This may make one wonder: when do\nyou need to build a search engine, and how? Do we always build search engines as\nmassive as Google? Let’s take a look at two scenarios to answer these questions.\n242 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 237,
        "char_count": 1387,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 892,
            "height": 763,
            "ext": "png",
            "size_bytes": 460617
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 273,
        "text": "Imagine we work for a company like Broad Reader. Our company wants to develop a\nsearch engine that crawls forums and discussion boards from all over the web and lets\nusers query this large collection. Consider another scenario: let’s say our client is a law\nfirm where loads of legal documents from clients and other legal sources are\nuploaded every day. We’re asked to develop a custom search engine for the client to\nsearch through their database. How are these two scenarios different?\nThe first scenario requires that we build what we call a generic search engine, where\nwe would have to set up a way to scrape different websites, keep looking for new con‐\ntent and new websites, and constantly build and update our “index.” The second sce‐\nnario is an example of an enterprise search engine, as we don’t have to scout for\ncontent to index. Thus, these two types of search engines are distinguished as follows:\n• Generic search engines, such as Google and Bing, that crawl the web and aim to\ncover as much as possible by constantly looking for new webpages\n• Enterprise search engines, where our search space is restricted to a smaller set of\nalready existing documents within an organization\nIn our experience, the second form of search is the most common use case you may\nencounter at your workplace, so we’ll only briefly introduce a generic search engine\nby discussing a few basic components that are also relevant to enterprise search.\nComponents of a Search Engine\nHow does a search engine work? What are some of the basic components? We briefly\nintroduce them through Figure 7-2, taken from the now-famous 1998 research paper\non the architecture of Google [2].\nSearch and Information Retrieval \n| \n243\n",
        "word_count": 293,
        "char_count": 1713,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 274,
        "text": "Figure 7-2. Early architecture of the Google search engine [2]\nThere are several small and large components inside a search engine, as shown in the\nfigure. The three major components that can be considered its building blocks (and a\nfourth component that is now also common) are:\nCrawler\nCollects all the content for the search engine. The crawler’s job is to traverse the\nweb following a bunch of seed URLs and build its collection of URLs through\nthem in a breadth-first way. It visits each URL, saves a copy of the document,\ndetects the outgoing hyperlinks, then adds them to the list of URLs to be visited\nnext. Typical decisions that need to be made when designing a crawler include\nidentifying what to crawl, when to stop crawling, when to re-crawl, what to re-\ncrawl, and how to make sure we don’t crawl duplicate content. From our experi‐\nence, even when you have to develop some sort of generic search engine (say, a\nblog search engine), you’re unlikely to encounter a scenario where you should\ndesign your own crawler. Production-ready crawlers, such as Apache Nutch [3]\nand Scrapy [4], can be customized and used for your project in such scenarios.\n244 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 207,
        "char_count": 1195,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1091,
            "height": 1114,
            "ext": "png",
            "size_bytes": 130861
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 275,
        "text": "Indexer\nParses and stores the content that the crawler collects and builds an “index” so it\ncan be searched and retrieved efficiently. While it’s possible to index videos,\naudio, images, etc., text indexing is the most common type of indexing in real-\nworld projects. Data structures for a search engine index are developed keeping\nin mind the need for fast and efficient search of its crawl in response to a user\nquery. An example of a popular indexing algorithm used in web search engines is\nan “inverted index,” which stores the list of documents associated with each word\nin its vocabulary. As with crawlers, you’re unlikely to encounter a situation where\nyou have to develop your own indexer. Software like Apache Solr [5] and Elastic‐\nsearch [6] are typically used in the industry to build an index and search over it.\nSearcher\nSearches the index and ranks the search results for the user query based on the\nrelevance of the results to the query. A typical search query on Google or Bing\nwill likely yield hundreds and thousands of results. As users, we can’t go through\nthem manually to decide whether the result is relevant to our query. This is\nwhere a ranking of search results becomes important. An intuitive approach to\nranking based on what we’ve seen so far in this book is to obtain a vector repre‐\nsentation of the result document and user query and rank the documents based\non some measure of similarity. In fact, as we mentioned at the start of the chap‐\nter, TF-IDF, which we saw in detail in Chapter 3 and used for text classification\nin Chapter 4, is one of the popular methods of searching and ranking search\nresults.\nFeedback\nA fourth component, which is now common in all search engines, that tracks and\nanalyzes user interactions with the search engine, such as click-throughs, time\nspent on searching and on each clicked result, etc., and uses it for continuous\nimprovement of the search system.\nWe hope this short discussion gave a quick glance into what a typical search engine is\nmade up of. Information retrieval is a major research area in itself, and search engine\ndevelopment is a massive undertaking involving a lot of computation and infrastruc‐\nture. All the topics discussed above are not completely solved problems yet. In this\nsection, we only provided an overview of how a search engine works in order to lead\ninto a discussion on where NLP comes into the picture and how to develop custom\nsearch engines. Interested readers can refer to [7] for a detailed discussion on the\nalgorithms and data structures behind search engine development.\nWith this introduction, let’s move on to what a typical search engine pipeline looks\nlike in the use cases you may encounter in your workplace and what NLP methods\nwe’ve learned so far can be put to use in this pipeline.\nSearch and Information Retrieval \n| \n245\n",
        "word_count": 490,
        "char_count": 2842,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 276,
        "text": "A Typical Enterprise Search Pipeline\nSay we work for a large newspaper and are tasked with developing a search engine for\nits website. We already mentioned that Solr and ElasticSearch are typically used for\naddressing such scenarios. How will we use them? Let’s do a step-by-step walk‐\nthrough and also discuss which NLP tools we’ll need in this process.\nCrawling/content acquisition\nWe don’t really need a crawler in this case, as we don’t need data from external\nwebsites. What we need is a way to read data from the location where all the news\narticles are stored (e.g., in a local database or in some cloud location).\nText normalization\nOnce we collect the content, depending on its format, we start by first extracting\nthe main text and discarding additional information (e.g., newspaper headers).\nIt’s also common to do some pre-processing steps, such as tokenizing, lowercas‐\ning, stop word removal, stemming, etc., before vectorizing.\nIndexing\nFor indexing, we have to vectorize the text. TF-IDF is a popular scheme for this,\nas we discussed earlier. However, like Google, we can also use BERT instead. How\ndo we use BERT for search? We can use BERT to get a vector representation of\nthe query and documents and generate a ranked list of closest documents for a\ngiven query in terms of vector distance. [8] shows how we can use such text\nembeddings to index and search using Elasticsearch.\nIn addition to indexing the entire content of an article, we can also add additional\nfields/facets to the index for each document and later search by these facets. For\nexample, for a newspaper, this can be the news category, other tags like the state\ninvolved (e.g., California for a news article about something in the USA), and so on.\nText classification approaches we saw in Chapter 4 can be used to get such categories\nand tags, if necessary. At the time of displaying the search results, we can combine\nthis with filters like date to enrich the user experience. We’ll see an example of such a\nfaceted search in Chapter 9.\nSo, let’s assume we’ve built our search engine following the above process. What next?\nWhat happens when the user types a query? At this point, the pipeline typically con‐\nsists of the following steps:\n1. Query processing and execution: The search query is passed through the text nor‐\nmalization process as above. Once the query is framed, it’s executed, and results\nare retrieved and ranked according to some notion of relevance. Search engine\nlibraries like Elasticsearch even provide custom scoring functions to modify the\nranking of documents retrieved for a given query [9].\n246 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 446,
        "char_count": 2641,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 277,
        "text": "2. Feedback and ranking: To evaluate search results and make them more relevant to\nthe user, user behavior is recorded and analyzed, and signals such as click action\non result and time spent on a result page are used to improve the ranking algo‐\nrithm. An example in our newspaper’s case could be to learn the reader’s prefer‐\nence (e.g., the reader prefers reading local news from Region X) and show them a\npersonalized ranking of suggested articles.\nWe hope this newspaper use case shows what a typical enterprise search engine devel‐\nopment pipeline looks like. Like with many software applications, recent develop‐\nments in the field of machine learning have also influenced enterprise search. We\nbriefly mentioned how BERT and other such embedding-based text representations\ncan be used with Elasticsearch. Amazon Kendra [10], an enterprise search engine\npowered by machine learning, is a recent addition to this space.\nSetting Up a Search Engine: An Example\nNow that we have an idea of the components of a search engine and how they work\ntogether in an example scenario, let’s take a quick look at building a small search\nengine using Elasticsearch’s Python API. We’ll use the CMU Book Summaries dataset\n[11], which consists of plot summaries of over 16,000 books extracted from Wikipe‐\ndia pages. We’ll illustrate the process using 500 documents, but the notebook associ‐\nated with this section (Ch7/ElasticSearch.ipynb) can be used to build a search engine\nwith the full dataset. We already have our content in place, so we don’t need a crawler.\nTaking a simple use case that doesn’t involve additional pre-processing (no stemming,\nfor example), the following code snippet shows how to build an index using Elastic‐\nsearch:\n#Build an index from booksummaries dataset, using only 500 documents.\npath = \"../booksummaries/booksummaries.txt\"\ncount = 1\nfor line in open(path):\n    fields = line.split(\"\\t\")\n    doc = {'id' : fields[0],\n          'title': fields[2],\n          'author': fields[3],\n          'summary': fields[6]\n          }\n      #Index is called myindex\n    res = es.index(index=\"myindex\", id=fields[0], body=doc)\n    count = count+1\n    if count%100 == 0:\n          print(\"indexed 100 documents\")\n    if count == 501:\n          break\nres = es.search(index=\"myindex\", body={\"query\": {\"match_all\": {}}})\nprint(\"Your index has %d entries\" % res['hits']['total']['value'])\nSearch and Information Retrieval \n| \n247\n",
        "word_count": 361,
        "char_count": 2431,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 278,
        "text": "This code builds an index with four fields per document—id, title, author, and sum\nmary—which are all available in the dataset itself. Once the index is built, it runs a\nquery to check the size of the index. In this case, the output will show as 500 entries.\nOnce the index is built, we have to figure out how to use it to perform search. While\nwe won’t go into the user interface design aspects of the search process, the following\ncode snippet illustrates how to search with Elasticsearch:\n#match query works as a OR query when the query string has multiple words\n#match_phrase looks for exact matches. So using that here.\nwhile True:\n    query = input(\"Enter your search query: \")\n    if query == \"STOP\":\n        break\n    res = es.search(index=\"myindex\", body={\"query\": {\"match_phrase\":\n                                            {\"summary\": query}}})\n    print(\"Your search returned %d results:\"\n                     %res['hits']['total']['value'])\n    for hit in res[\"hits\"][\"hits\"]:\n          print(hit[\"_source\"][\"title\"])\n          #to get a snippet 100 characters before and after the match\n          loc = hit[\"_source\"][\"summary\"].lower().index(query)\n          print(hit[\"_source\"][\"summary\"][:100])\n          print(hit[\"_source\"][\"summary\"][loc-100:loc+100])\nThis snippet keeps asking the user to enter a search query until the word STOP is\ntyped and shows the search results, along with a short snippet containing the search\nphrase. For example, if the user searches for the word “countess,” the results look as\nfollows:\nEnter your search query: countess\nYour search returned 7 results:\nAll's Well That Ends Well\n71\n Helena, the orphan daughter of a famous physician, is the ward of the Countess\n of Rousillon, and ho\n…\n…\n…\nEnter your search query: STOP\nElasticsearch has many features to alter the scoring function, to change the search\nprocess in terms of query formulation (e.g., exact match versus fuzzy match), to add\npre-processing steps like stemming during the indexing process, and so on. We leave\nthem as further exercises for the reader. Now, let’s look at a case study of building an\nenterprise search engine from scratch and improving it.\n248 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 320,
        "char_count": 2203,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 279,
        "text": "A Case Study: Book Store Search\nImagine a scenario where we have a new e-commerce store focused on books and we\nhave to build its search pipeline. We have metadata like author, title, and summary.\nThe search functionality we saw earlier can serve as the baseline at the start. We can\nset up our own search engine backend or use online services like Elasticsearch [12] or\nElastic on Azure [13].\nThis default search output might have a bunch of issues. For instance, it may show\nthe results with exact query matches in title or summary to be higher than more rele‐\nvant results that aren’t an exact match. Some of the exact matches might be poorly\nwritten books with bad reviews, which we’re not accounting for in our search rank‐\ning. For example, consider these two books on Marie Curie: Marie Curie Biography\nand The Life of Marie Curie. The latter is an authoritative biography on Marie Curie,\nwhile the former is a new and poorly reviewed book. But while querying for “marie\ncurie biography,” the less-relevant book, Marie Curie Biography, is ranked higher than\nthe popular The Life of Marie Curie.\nWe can incorporate real-world metrics that account for this into our search engine.\nFor instance, the number of times a book is viewed and sold, the number of reviews,\nand the book’s rating can all be incorporated into the search ranking function. In\nElasticsearch, this can be done by using function scoring and giving a manually\nselected weightage to number of ratings, number of books sold, and average rating.\nSo, we might want to give more weightage to books sold than the number of times it\nwas viewed. These heuristics will provide more relevant results as more books get\nsold and reviewed. This method of manually defining search relevance weights can be\na good starting point when there’s no data or the data is limited.\nWe should start collecting user interactions with the search engine to improve it fur‐\nther. These interactions can include the search query, the kind of user, and their\nactions on the books. When recording such granular search information, various pat‐\nterns can be found—for example, when searching for “science books for children,”\nscientists’ biographies get purchased at a higher rate even when they’re ranked lower.\nOver time, with increasing amounts of data, we can learn relevance ranking from\nthese logs. We can use a tool like Elasticsearch Learning to Rank [14] to learn this\ninformation and improve search relevance. Over time, more advanced techniques\nlike neural embeddings can also be incorporated into search query analysis [15].\nAs more user information is collected, search results can also be personalized based\non the user’s past preferences. Generally, such systems are built as a layer over the ini‐\ntial ranking retrieved from the search engine.\nAnother point to consider in this journey of building an advanced search engine is\nhow important it is to keep complete control of the system and data. If such a search\nengine is not a core part of your offering and the organization is comfortable with\nSearch and Information Retrieval \n| \n249\n",
        "word_count": 521,
        "char_count": 3095,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 280,
        "text": "data sharing, many of these features also come as a managed service. These managed\nsearch engine services include Algolia [16] and Swiftype [17].\nSince the implementation of a search engine involves many other factors beyond NLP\nand is typically reserved for larger-sized datasets, we’re not showing a running exam‐\nple covering all the aspects of a search engine in this book. However, we hope this\nshort introduction gave you an overview of how to get started developing custom\nsearch engines involving textual data and of where the NLP techniques you learned\nso far may play a role. For more details on implementing search engines with Elastic‐\nsearch, refer to [18]. Now, let’s move on to the second topic of this chapter: topic\nmodeling.\nTopic Modeling\nTopic modeling is one of the most common applications of NLP in industrial use\ncases. For analyzing different forms of text from news articles to tweets, from visual‐\nizing word clouds (see Chapter 8) to creating graphs of connected topics and docu‐\nments, topic models are useful for a range of use cases. Topic models are used\nextensively for document clustering and organizing large collections of text data.\nThey’re also useful for text classification.\nBut what is topic modeling? Say we’re given a large collection of documents, and we’re\nasked to “make sense” out of it. What will we do? Clearly, the task is not well defined.\nGiven the large volume of documents, going through each of them manually is not an\noption. One way to approach it is to bring out some words that best describe the cor‐\npus, like the most common words in the corpus. This is called a word cloud. The key\nto a good word cloud is to remove stop words. If we take any English text corpus and\nlist out the most frequent k words, we won’t get any meaningful insights, as the most\nfrequent words will be stop words (the, is, are, am, etc.). After doing appropriate pre-\nprocessing, the word cloud may yield some meaningful insights depending on the\ndocument collection.\nAnother approach is to break the documents into words and phrases, then group\nthese words and phrases together based on some notion of similarity between them.\nThe resulting groups of words and phrases can then be used to build some under‐\nstanding of what the corpus is about. Intuitively, if we pick one word from each\ngroup, then the set of selected words represents (in a semantic sense) what the corpus\nis about. Another possibility is to use TF-IDF (See Chapter 3). Consider a corpus of\ndocuments wherein some documents are on farming. Then, terms like “farm,”\n“crops,” “wheat,” and “agriculture” should form the “topics” in the documents on\nfarming. What’s the easiest way to find these terms that occur frequently in a docu‐\nment but do not occur much in other documents in the corpus?\n250 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 487,
        "char_count": 2833,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 281,
        "text": "Topic modeling operationalizes this intuition. It tries to identify the “key” words\n(called “topics”) present in a text corpus without prior knowledge about it, unlike the\nrule-based text mining approaches that use regular expressions or dictionary-based\nkeyword searching techniques. Figure 7-3 shows a visualization of a topic model’s\nresults for a humanities corpus.\nFigure 7-3. Illustration a of topic modeling visualization [19].\nIn this figure, we see a collection of keywords for individual humanities disciplines—\nand also how some keywords overlap between disciplines—obtained through a topic\nmodel. This is an example of how we can use a topic model to discover what the top‐\nics in a large corpus are about. It has to be noted that there’s no single topic model.\nTopic modeling generally refers to a collection of unsupervised statistical learning\nmethods to discover latent topics in a large collection of text documents. Some of the\npopular topic modeling algorithms are latent Dirichlet allocation (LDA), latent\nsemantic analysis (LSA), and probabilistic latent semantic analysis (PLSA). In prac‐\ntice, the technique that’s most commonly used is LDA.\nTopic Modeling \n| \n251\n",
        "word_count": 182,
        "char_count": 1188,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 682,
            "height": 596,
            "ext": "png",
            "size_bytes": 513447
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 282,
        "text": "What does LDA do? Let’s start with a toy corpus [20]. Say we have a collection of\ndocuments, D1 to D5, and each document consists of a single sentence:\n• D1: I like to eat broccoli and bananas.\n• D2: I ate a banana and salad for breakfast.\n• D3: Puppies and kittens are cute.\n• D4: My sister adopted a kitten yesterday.\n• D5: Look at this cute hamster munching on a piece of broccoli.\nLearning a topic model on this collection using LDA may produce an output like this:\n• Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching\n• Topic B: 20% puppies, 20% kittens, 20% cute, 15% hamster\n• Document 1 and 2: 100% Topic A\n• Document 3 and 4: 100% Topic B\n• Document 5: 60% Topic A, 40% Topic B\nThus, topics are nothing but a mixture of keywords with a probability distribution,\nand documents are made up of a mixture of topics, again with a probability distribu‐\ntion. A topic model only gives a collection of keywords per topic. What exactly the\ntopic represents and what it should be named is typically left to human interpretation\nin an LDA model. Here, we might look at Topic A and say, “it is about food.” Likewise,\nfor topic B, we might say, “it is about pets.”\nHow does LDA achieve this? LDA assumes that the documents under consideration\nare produced from a mixture of topics. It further assumes the following process gen‐\nerates these documents: at the start, we have a list of topics with a probability distri‐\nbution. For every topic, there’s an associated list of words with a probability\ndistribution. We sample k topics from topic distribution. For each of the k topics\nselected, we sample words from the corresponding distribution. This is how each\ndocument in the collection is generated.\nNow, given a set of documents, LDA tries to backtrack the generation process and fig‐\nure out what topics would generate these documents in the first place. The topics are\ncalled “latent” because they’re hidden and must be discovered. How does LDA do this\nbacktracking? It does so by factorizing a document-term matrix (M) that keeps count\nof words across all documents. It has all the m documents D1, D2, D3 … Dm arranged\nalong the rows and all the n words W1,W2, ..,Wn in the corpus vocabulary arranged as\ncolumns. M[i,j] is the frequency count of word Wj in Document Di. Figure 7-4 shows\none such matrix for a hypothetical corpus consisting of five documents, with a\nvocabulary of six words.\n252 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 439,
        "char_count": 2441,
        "fonts": [
          "MinionPro-It (6.3pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Regular (6.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 283,
        "text": "Figure 7-4. Document–term matrix (M)\nNote that if each word in the vocabulary represents a unique dimension and the total\nvocabulary is of size n, then the ith row of this matrix is a vector that represents the ith\ndocument in this n-dimensional space. LDA factorizes M into two submatrices: M1\nand M2. M1 is a document–topics matrix and M2 is a topic–terms matrix, with\ndimensions (M, K) and (K, N), respectively. With four topics (K1–K4), the submatri‐\nces for M may look like the ones shown in Figure 7-5. Here, k is the number of topics\nwe’re interested in finding.\nFigure 7-5. Factorized matrices\nk, the number of topics, is a hyperparameter. The optimal value for\nk is found by trial and error.\nTopic Modeling \n| \n253\n",
        "word_count": 128,
        "char_count": 724,
        "fonts": [
          "MinionPro-It (9.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Regular (6.3pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          },
          {
            "index": 1,
            "width": 1117,
            "height": 385,
            "ext": "png",
            "size_bytes": 16107
          },
          {
            "index": 2,
            "width": 1117,
            "height": 758,
            "ext": "png",
            "size_bytes": 27224
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 284,
        "text": "These submatrices can then be used to understand the topic structure of a document\nand the keywords a topic is made up of. Now that we have some idea of what happens\nbehind the scenes when we train a topic model, let’s look at how to build one.\nTraining a Topic Model: An Example\nWe’ve seen the intuition behind LDA. How do we build one ourselves? Here, we’ll use\nan LDA implementation from the Python library gensim [21] and the CMU Book\nSummary Dataset [11] we used earlier for demonstrating how to create a search\nengine. The notebook associated with this section (Ch5/TopicModeling.ipynb) con‐\ntains more details. The following code snippet shows how to train a topic model\nusing LDA:\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom gensim.models import LdaModel\nfrom gensim.corpora import Dictionary\nfrom pprint import pprint\n#tokenize, remove stopwords, non-alphabetic words, lowercase\ndef preprocess(textstring):\n   stops =  set(stopwords.words('english'))\n   tokens = word_tokenize(textstring)\n   return [token.lower() for token in tokens if token.isalpha() \n          and token not in stops]\ndata_path = \"/PATH/booksummaries/booksummaries.txt\"\nsummaries = []\nfor line in open(data_path, encoding=\"utf-8\"):\n   temp = line.split(\"\\t\")\n   summaries.append(preprocess(temp[6]))\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(summaries)\n# Filter infrequent or too frequent words.\ndictionary.filter_extremes(no_below=10, no_above=0.5)\ncorpus = [dictionary.doc2bow(summary) for summary in summaries]\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n# Train the topic model\nmodel = LdaModel(corpus=corpus, id2word=id2word,iterations=400, num_topics=10)\ntop_topics = list(model.top_topics(corpus))\npprint(top_topics)\nIf we visually inspect the topics, one of them shows words such as police, case, mur‐\ndered, killed, death, body, etc. While topics themselves will not get names in a topic\nmodel, in looking at the keywords, we may infer that this relates to the topic of crime/\nthriller novels.\n254 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 301,
        "char_count": 2173,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 285,
        "text": "How do you evaluate the results? Given the topic–term matrix for LDA, we sort each\ntopic from highest to lowest term weights and then select the first n terms for each\ntopic. We then measure the coherence for terms in each topic, which essentially meas‐\nures how similar these words are to one another. Additionally, in this example, we\nmade a few choices for the model parameters, such as number of iterations, number\nof topics, and so on, and did not do any fine-tuning. The notebook associated with\nthis section (Ch7/TopicModeling.ipynb) shows how to evaluate the coherence of topic\nmodels.\nAs with any real-world project, we need to experiment with different parameters and\ntopic models before choosing a final model to deploy. Gensim’s tutorial on LDA [22]\nprovides more information on how to build, tune, and evaluate a topic model.\nRemoving words with low frequency or keeping only those words\nthat are nouns and verbs are some ways of improving a topic\nmodel. If the corpus is big, divide it into batches of fixed sizes and\nrun topic modeling for each batch. The best output comes from the\nintersection of topics from each batch.\nWhat’s Next?\nNow that we know how to build a topic model, how exactly can we use it? In our\nexperience, some of the use cases for topic models are:\n• Summarizing documents, tweets, etc., in the form of keywords based on learned\ntopic distributions\n• Detecting social media trends over a period of time\n• Designing recommender systems for text\nAlso, the distribution of topics for a given document can be used as a feature vector\nfor text classification.\nAlthough there is clearly a range of use cases for topic models in industry projects,\nthere are a few challenges associated with their use. The evaluation and interpretation\nof topic models is still challenging, and there’s no consensus on it yet. Parameter tun‐\ning for topic models can also take a lot of time. In the above example, we provided\nthe number of topics manually. As mentioned previously, there’s no straightforward\nprocedure to know the number of topics; we explore with multiple values based on\nour estimates about the topics in the dataset. Another thing to keep in mind is that\nmodels like LDA typically work only with long documents and perform poorly on\nshort documents, such as a corpus of tweets.\nDespite all these challenges, topic models are an important tool in any NLP engineer’s\ntoolbox, and they have a wider reach in terms of where they can be used. We hope we\nTopic Modeling \n| \n255\n",
        "word_count": 432,
        "char_count": 2505,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 286,
        "text": "gave you enough information to help you identify its suitable use cases at your work‐\nplace. An interested reader can start at [23] to delve deeper into this topic. Let’s move\non to the next topic of this chapter: text summarization.\nText Summarization\nText summarization refers to the task of creating a summary of a longer piece of text.\nThe goal of this task is to create a coherent summary that captures the key ideas in the\ntext. It’s useful to do a quick read of large documents, store only relevant information,\nand facilitate better retrieval of information. NLP research on the problem of auto‐\nmatic text summarization was actively pursued by different research groups around\nthe world starting in the early 2000s as a part of the Document Understanding Con‐\nference [24] series. This series of conferences held competitions to solve several sub‐\ntasks within the larger realm of text summarization. Some of them are listed below:\nExtractive versus abstractive summarization\nExtractive summarization refers to selecting important sentences from a piece of\ntext and showing them together as a summary. Abstractive summarization refers\nto the task of generating an abstract of the text; i.e., instead of picking sentences\nfrom within the text, a new summary is generated.\nQuery-focused versus query-independent summarization\nQuery-focused summarization refers to creating the summary of the text\ndepending on the user query, whereas query-independent summarization creates\na general summary.\nSingle-document versus multi-document summarization\nAs the names indicate, single-document summarization is the task of creating a\nsummary from a single document, whereas multi-document summarization cre‐\nates a summary from a collection of documents.\nWe’ll look at some use cases to help you understand how these can be applied to\nactual tasks.\nSummarization Use Cases\nIn our experience, the most common use case for text summarization is a single-\ndocument, query-independent, extractive summarization. This is typically used to\ncreate short summaries of longer documents for human readers or a machine (e.g., in\na search engine to index summaries instead of full texts). A well-known example of\nsuch a summarizer in action in a real-world product is the autotldr bot on Reddit\n[25], a screenshot of which is shown in Figure 7-6. The autotldr bot summarizes long\nReddit posts by selecting and ranking the most important sentences in the post.\n256 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 389,
        "char_count": 2480,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 287,
        "text": "Figure 7-6. Screenshot of Reddit’s autotldr bot\nTwo other use cases one of the authors implemented in their past workplaces are:\n• An automatic sentence highlighter for news articles that colors “summary” sen‐\ntences (i.e., sentences that capture the gist of the text) instead of creating full-\nlength summaries.\n• A text summarizer to index only the summaries of documents instead of the full\ncontent, with the goal of reducing the size of a search engine’s index.\nYou may encounter similar scenarios for implementing a text summarizer at your\nworkplace. Let’s look at an example of how we can leverage existing libraries to imple‐\nment a single-document, query-independent, extractive summarizer.\nSetting Up a Summarizer: An Example\nResearch in this area has explored rule-based, supervised, and unsupervised\napproaches and, more recently, DL-based architectures. However, popular extractive\nsummarization algorithms used in real-world scenarios use a graph-based sentence-\nranking approach. Each sentence in a document is given a score based on its relation\nto other sentences in the text, and this is captured differently in different algorithms.\nThe Top N sentences are then returned as a summary. Sumy [26] is a Python library\nthat contains implementations of several popular query-independent, extractive sum‐\nmarization algorithms. The code snippet below shows an example of how to use\nText Summarization \n| \n257\n",
        "word_count": 216,
        "char_count": 1421,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1204,
            "height": 738,
            "ext": "png",
            "size_bytes": 108774
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 288,
        "text": "sumy’s implementation of a popular summarization algorithm, TextRank [27], to\nsummarize a Wikipedia page:\nfrom sumy.parsers.html import HtmlParser\nfrom sumy.nlp.tokenizers import Tokenizer\nfrom sumy.summarizers.text_rank import TextRankSummarizer\nurl = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\nparser = HtmlParser.from_url(url, Tokenizer(\"english\"))\nsummarizer = TextRankSummarizer()\nfor sentence in summarizer(parser.document, 5):\n    print(sentence)\nThis library takes care of HTML parsing and tokenization for the given URL, then\nuses TextRank to choose the most important sentences as the summary of the text.\nRunning this code shows the five most important sentences in the Wikipedia page on\nautomatic summarization.\nSumy is not the only library with such implementations of summarization algo‐\nrithms. Another popular library is gensim, which implements an improvised version\nof TextRank [28]. The following code snippet shows how to use gensim’s summarizer\nto summarize a given text:\nfrom gensim.summarization import summarize\ntext = \"some text you want to summarize\"\nprint(summarize(text))\nNote that, unlike sumy, gensim does not come with an HTML parser, so we’ll have to\nincorporate an HTML parsing step if we want to parse web pages. Gensim’s summa‐\nrizer also allows us to experiment with the length of the summaries. We’ll leave the\nexploration of other summarization algorithms in sumy and further investigation of\ngensim as exercises for the reader.\nSo, now we know how to implement a summarizer in our projects. However, there\nare a few things to keep in mind when using these libraries to deploy a working sum‐\nmarizer. Let’s take a look at some of them based on our experiences with building\nsummarizers for various application scenarios.\nPractical Advice\nIf you encounter a scenario where you have to deploy a summarizer as a product fea‐\nture, there are a few things to keep in mind. It’s very likely that you’ll use one of the\noff-the-shelf summarizers like in the example above rather than implementing your\nown summarizer from scratch. However, if existing algorithms don’t suit your project\nscenario or if they perform poorly, you may have to develop your own summarizer. A\nmore common reason to work on your own summarizer is if you’re in an R&D orga‐\nnization, working toward pushing the state of the art in summarization systems. So,\nassuming you’re using off-the-shelf summarizers, how do you compare the multiple\n258 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 375,
        "char_count": 2492,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 289,
        "text": "summarization algorithms available and choose the one that works best for your use\ncase?\nIn research, summarization approaches are evaluated using a common dataset of ref‐\nerence summaries created by humans. Recall-Oriented Understudy for Gisting Evalu‐\nation (ROUGE) [29] is a common set of metrics based on n-gram overlaps used for\nevaluating automatic summarization systems. However, such datasets may or may not\nsuit your exact use case. Hence, the best way to compare different approaches is to\ncreate your own evaluation set or ask human annotators to rate the summaries pro‐\nduced by different algorithms in terms of coherence, accuracy of the summary, etc.\nThere are a few practical issues to keep in mind when deploying a summarizer:\n• Pre-processing steps like sentence splitting (or HTML parsing in the above\nexample) play a very important role in what comes out as output summary. Most\nlibraries have built-in sentence splitters, but even those can do erroneous sen‐\ntence splitting for different input data (e.g., what if there’s a news article with a\nletter quoted in the middle?). To our knowledge, there’s no one-stop solution for\nsuch issues, and you may need to develop custom solutions for the data formats\nyou encounter in your project.\n• Most summarization algorithms are sensitive to the size of the text given as\ninput. For example, TextRank runs in polynomial time, so it can easily take up a\nlot of computing time to generate summaries for larger pieces of text. You need\nto be aware of this limitation when using a summarizer with very large texts. A\nworkaround could be to run the summarizer on partitions of the large text and\nstringing the summaries together. Another alternative could be to run the sum‐\nmarizer on the top M% and bottom N% of the text instead of the whole text\n(assuming that these parts contain the gist of a long document).\nSummarizers are sensitive to text length. So, it may make sense to\nrun a summarizer on selected parts of the text.\nSo far, we’ve only seen examples of extractive summarization. In comparison, abstrac‐\ntive summarization is more of a research topic than a practical application. Three\ninteresting use cases that come up frequently in abstractive summarization research\nare: news headline generation, news summary generation, and question answering.\nDeep learning and reinforcement learning approaches have shown some promising\nresults for abstractive summarization in the recent past [30]. Because this topic has so\nfar been primarily a research bastion and is restricted to academics and organizations\nwith dedicated AI teams, we won’t discuss it in further detail in this book. However,\nText Summarization \n| \n259\n",
        "word_count": 437,
        "char_count": 2688,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 290,
        "text": "we hope this discussion gave you enough of an overview about summarization to get\nstarted with an MVP in case you need one. Now, let’s take a look at another interest‐\ning problem where NLP is useful: offering recommendations for textual data.\nRecommender Systems for Textual Data\nWe’re all familiar with seeing related searches, related news articles, related jobs,\nrelated products, and other such features on the various websites we browse in our\nday-to-day lives, and it’s not unusual for clients to request them. How do these\n“related texts” features work?\nNews articles, job descriptions, product descriptions, and search queries all contain a\nlot of text. Hence, textual content and the similarities or relatedness between different\ntexts is important to consider when developing recommender systems for textual\ndata. A common approach to building recommendation systems is a method called\ncollaborative filtering. It shows recommendations to users based on their past history\nand on what users with similar profiles preferred in the past. For example, Netflix\nrecommendations use this type of approach at a large scale.\nIn contrast, there are content-based recommendation systems. An example of one\nsuch recommendation is the “related articles” feature on newspaper websites. Look at\nan example from CBC, a Canadian news website, shown in Figure 7-7.\nBelow the article text, we see a collection of related stories that are topically similar to\nthe source article, which is titled “How Desmond Cole wrote a bestselling book about\nbeing black in Canada.” As you can see, the related stories cover black history and\nracism in Canada and list another article about Desmond Cole. How do we build\nsuch a feature based on content similarity among texts? One approach to building\nsuch a content-based recommendation system is to use a topic model like we saw ear‐\nlier in this chapter. Texts similar to the current text in terms of topic distribution can\nbe shown as “related” texts. However, the advent of neural text representations has\nchanged the ways we can show such recommendations. Let’s take a look at how we\ncan use a neural text representation to show related text recommendations.\n260 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 359,
        "char_count": 2228,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 291,
        "text": "Figure 7-7. Screenshot showing the related stories feature on cbc.ca [31]\nCreating a Book Recommender System: An Example\nWe’ve seen a few examples of neural network–based text representations (Chapter 3)\nand how some of them can be useful for text classification (Chapter 4). One of the\nrepresentations we saw was Doc2vec. The following code snippet shows how to use\nDoc2vec for serving related book recommendations using the CMU Book Summary\nDataset we used earlier in this chapter for topic modeling and the Python libraries\nNLTK (for tokenization) and gensim (for Doc2vec implementation):\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n# Read the dataset’s README to understand the data format.\ndata_path = \"/DATASET_FOLDER_PATH/booksummaries.txt\"\nmydata = {} #titles-summaries dictionary object\nfor line in open(data_path, encoding=\"utf-8\"):\n    temp = line.split(\"\\t\")\n    mydata[temp[2]] = temp[6]\n# Prepare the data for doc2vec, build and save a doc2vec model.\nd2vtrain = [TaggedDocument((word_tokenize(mydata[t])), tags=[t]) \n                          for t in mydata.keys()]\nmodel = Doc2Vec(vector_size=50, alpha=0.025, min_count=10, dm =1, epochs=100)\nRecommender Systems for Textual Data \n| \n261\n",
        "word_count": 166,
        "char_count": 1258,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 624,
            "height": 517,
            "ext": "png",
            "size_bytes": 50740
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 292,
        "text": "model.build_vocab(train_doc2vec)\nmodel.train(train_doc2vec, total_examples=model.corpus_count, \n  epochs=model.epochs)\nmodel.save(\"d2v.model\")\n# Use the model to look for similar texts.\nmodel= Doc2Vec.load(\"d2v.model\")\n# This is a sentence from the summary of “Animal Farm” on Wikipedia:\n# https://en.wikipedia.org/wiki/Animal_Farm\nsample = \"\"\"\nNapoleon enacts changes to the governance structure of the farm, replacing\nmeetings with a committee of pigs who will run the farm.\n \"\"\"\nnew_vector = model.infer_vector(word_tokenize(sample))\nsims = model.docvecs.most_similar([new_vector]) #gives 10 most similar titles\nprint(sims)\nThis prints the output as:\n[('Animal Farm', 0.6960548758506775), (\"Snowball's Chance\", 0.6280543208122253), \n('Ponni', 0.583295464515686), ('Tros of Samothrace', 0.5764356255531311), \n('Payback: Debt and the Shadow Side of Wealth', 0.5714253783226013),\n('Settlers in Canada', 0.5685930848121643), ('Stone Tables', \n0.5614138245582581), ('For a New Liberty: The Libertarian Manifesto', \n0.5510331988334656), ('The God Boy', 0.5497804284095764), \n('Snuff', 0.5480046272277832)]\nNote that we just tokenized the text in this example and did not do any other pre-\nprocessing, nor did we do any model tuning. This is just an example of how we can\napproach the development of a recommendation system, not a detailed analysis.\nMore recent approaches to implementing such systems use BERT or other such mod‐\nels to calculate document similarity. We also briefly mentioned text similarity–based\nsearch options in Elasticsearch earlier in this section; that’s another option for imple‐\nmenting a recommender system for our use case. We’ll leave exploring them further\nas an exercise for the reader.\nNow that we have an idea of how to build a recommendation system for textual data,\nlet’s take a look at some practical advice for building such recommendation systems\nbased on our past experiences.\nPractical Advice\nWe just saw a simple example of a textual recommendation system. This kind of\napproach will work for some use cases, such as recommending related news articles.\nHowever, we may have to consider aspects beyond text in many applications where\nwe need to provide more personalized recommendations or where other non-textual\naspects of the item need to be considered. An example of such a case is similar listing\nrecommendations in Airbnb, where they combine embedding-based neural text\n262 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 337,
        "char_count": 2448,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 293,
        "text": "representations with other information, such as location, price, etc., to provide per‐\nsonalized recommendations [32].\nHow do we know our recommendation system is working? In a real-world project,\nthe impact of recommendations can be measured by performance indicators, such as\nuser click-through rates, conversion into a purchase (if relevant), customer engage‐\nment on the website, etc. A/B tests where different groups of users are exposed to dif‐\nferent recommendations are used to compare these performance indicators. A third\n(and perhaps more time consuming) way is to conduct carefully designed user stud‐\nies where participants are shown specific recommendations and asked to rate them.\nFinally, if we have a small test set with appropriate recommendations for a given item,\nwe can evaluate a recommendation system by comparing it to this test set. In our\nexperience, a combination of these indicators, along with an analytics platform like\nGoogle Analytics, is used in evaluating industry-scale recommendation systems.\nLast but not least, our pre-processing decisions play a significant role in the recom‐\nmendations served by our system. So, we need to know what we want before going\nahead with an approach. In the example above, we just did plain tokenization. In the\nreal world, it’s not uncommon to see lowercasing, removal of special characters, etc.,\nas parts of the pre-processing pipeline.\nThis concludes our overview of text recommendation systems. We hope this provides\nyou enough information to identify suitable use cases at your workplace and build\nrecommendation systems for them. Let’s move to the next topic of this chapter:\nmachine translation.\nMachine Translation\nMachine translation (MT)—translating text from one language to another automati‐\ncally—is one of the original problems of NLP research. Early MT systems employed\nrule-based approaches that required a lot of linguistic knowledge, including the\ngrammars of source and target languages, to be explicitly coded along with resources\nlike dictionaries between languages. This was followed by several years of research\nand application development using statistical methods that relied on the existence of\nlots and lots of parallel data between languages. Such datasets were usually collected\nfrom resources where texts were translated into multiple languages, such as European\nparliamentary proceedings. The past five years have seen explosive growth in DL-\nbased neural MT approaches, which have become the state of the art in both research\nand production-scale MT systems. Google Translate is a popular example. However,\nowing to the amount of data and resources required to build them, research and\ndevelopment of such systems has been primarily the bastion of large organizations.\nMachine Translation \n| \n263\n",
        "word_count": 423,
        "char_count": 2798,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 294,
        "text": "Clearly, MT is a large research area, and building MT systems seems like a large\neffort. Where is MT useful in the industry? Here are two example scenarios where\nMT may be required to develop solutions:\n• Our client’s products are used by people around the world who leave reviews on\nsocial media in multiple languages. Our client wants to know the general senti‐\nment of those reviews. For this, instead of looking for sentiment analysis tools in\nmultiple languages, one option is to use an MT system, translate all the reviews\ninto one language, and run sentiment analysis for that language.\n• We work with a lot of social media data (e.g., tweets) on a regular basis and\nnotice that it’s unlike the kind of text we encounter in typical text documents. For\nexample, consider the sentence, “am gud,” which, in formal, well-formed English\nis, “I am good.” (More details on how social media text differs from normal, well-\nformed text are in Chapter 8.) MT can be used to map these two sentences by\ntreating the conversion from “am gud” to “I am good” as an informal-to-\ngrammatical English translation problem.\nWhile we may or may not develop our own MT systems, there are many scenarios\nwhere we may need to implement an MT solution in our NLP projects. [33] discusses\nsome of the industry use cases of MT. So what should we do, then, if we face a similar\nsituation? Let’s look at an example of how to set up an MT system in our project.\nUsing a Machine Translation API: An Example\nBuilding an MT system from scratch is a time- and resource-consuming exercise. A\nmore common way to set up an MT system for a project is to use one of the pay-per-\nuse translation services APIs provided by large research organizations such as Google\nor Microsoft, which are powered by state-of-the-art neural MT models. The follow‐\ning code snippet shows how to use the Bing Translate API [34] (after obtaining the\nsubscription key and the endpoint URL by registering) to translate from English to\nGerman:\nimport os, requests, uuid, json\nsubscription_key = \"XXXXX\"\nendpoint = \"YYYYY\"\npath = '/translate?api-version=3.0'\nparams = '&to=de' #From English to German (de)\nconstructed_url = endpoint + path + params\nheaders = {\n    'Ocp-Apim-Subscription-Key': subscription_key,\n    'Content-type': 'application/json',\n    'X-ClientTraceId': str(uuid.uuid4())\n}\n264 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 397,
        "char_count": 2374,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 295,
        "text": "body = [{'text' : 'How good is Machine Translation?'}]\nrequest = requests.post(constructed_url, headers=headers, json=body)\nresponse = request.json()\nprint(json.dumps(response, sort_keys=True, indent=4, separators=(',', ': ')))\nThis example requests a translation of the sentence “How good is Machine Transla‐\ntion?” from English to German. The output in JSON format is shown below:\n[\n    {\n    \"detectedLanguage\": {\n          \"language\": \"en\",\n          \"score\": 1.0\n    },\n    \"translations\": [\n          {\n               \"text\": \"Wie gut ist maschinelle Übersetzung?\",\n               \"to\": \"de\"\n          }\n    ]\n    }\n]\nThis shows the translated sentence in German as “Wie gut ist maschinelle Überset‐\nzung?” We can use the service as we need it by calling the Bing Translate API. Similar\nsetups exist for other providers of such services. Before concluding this topic, let’s\ntake a look at some practical advice for readers who want to incorporate MT into an\nNLP project.\nPractical Advice\nFirst, as we explained earlier, don’t build your own MT system if you don’t have to. It’s\nmore practical to make use of translation APIs. When using such APIs, it’s important\nto pay close attention to pricing policies. Considering the costs involved, it might be a\ngood idea to store the translations of frequently used text (called a translation mem‐\nory or a translation cache).\nMaintain a translation memory, which can be used for translations\nthat repeat frequently.\nWhen working with an entirely new language, or, say, a new domain where existing\ntranslation APIs do poorly, it makes sense to start with a domain knowledge–based,\nrule-based translation system addressing the restricted scenario we’re dealing with.\nAnother approach to addressing such data-scarce scenarios is to augment our\nMachine Translation \n| \n265\n",
        "word_count": 266,
        "char_count": 1818,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 296,
        "text": "training data by doing “back translation.” Let’s say we want to translate from English\nto the Navajo language. English is a popular language for MT, while Navajo is not, but\nwe do have a few examples of English–Navajo translation. In such a case, we can\nbuild an MT model between Navajo and English, then use this system to translate a\nfew Navajo sentences into English. At this point, these machine-translated Navajo–\nEnglish pairs can be added as additional training data to the English–Navajo MT sys‐\ntem. This results in a translation system with more examples to train on (even though\nsome of these examples are synthetic). In general, though, if accuracy of translation is\nparamount, it might make sense to form a hybrid MT system that combines the neu‐\nral models with rules and some form of post-processing.\nData augmentation is a useful approach to collect more training\ndata for building an MT system.\nMT is a large area of research with dedicated annual conferences, journals, and data-\ndriven competitions where academics and industry groups involved in MT research\ncompete and evaluate their systems. We’ve only scratched the surface to give you\nsome idea about the topic. A collection of learning materials on MT [35] are available\nfor readers interested in further study. With this overview of MT, let’s move on to the\nnext topic of this chapter: question-answering systems.\nQuestion-Answering Systems\nWhen searching online with a search engine such as Google or Bing, for some of the\nqueries, we see “answers” along with a bunch of search results. These answers can be\na few words or a listing or definition. In Chapter 5, we saw some examples of one\nsuch query to illustrate named entity recognition’s role in search. Let’s now go a little\nbit farther than that. Consider the screenshot in Figure 7-8 from Google search for\nthe query “who invented penicillin.”\n266 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 321,
        "char_count": 1913,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 297,
        "text": "Figure 7-8. Screenshot for the query “who invented penicillin”\nHere, the search engine performs an additional task of question answering along with\ninformation retrieval. If we follow the search engine pipeline described earlier with\nthe aim of answering such a question, the processing steps look like the ones shown\nin Figure 7-9.\nClearly, NLP plays an important role in understanding the user query, deciding what\nkind of question it is and what kind of answer is needed, and identifying where the\nanswers are in a given document after retrieving documents relevant to the query.\nWhile this is an example of a large, generic search engine, we may also encounter sce‐\nnarios where we have to implement a question-answering system for internal con‐\nsumption, using a company’s data or some other custom setting. Following the\npipeline approach mentioned earlier in “Search and Information Retrieval” on page\n241 can lead us toward a solution in such cases.\nThere may be other relatively simpler scenarios of question answering in the work‐\nplace, too. A common scenario is an FAQ-answering system. We saw how this works\nin Chapter 6. Let’s briefly discuss one more scenario, based on one of the author’s past\nexperiences at their workplace.\nQuestion-Answering Systems \n| \n267\n",
        "word_count": 206,
        "char_count": 1277,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1156,
            "height": 692,
            "ext": "png",
            "size_bytes": 94176
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 298,
        "text": "Figure 7-9. Answer extraction\nDeveloping a Custom Question-Answering System\nLet’s say we’re asked to develop a question-answering system that answers all user\nquestions about computers. We’ve identified a few websites with question-and-\nanswer discussions (e.g., Stack Overflow) and have a crawler in place. At this point,\nhow can we get started with the first version of the question-answering system? One\nway to build an MVP is to start looking at the markup structure of the websites. Gen‐\nerally, the questions and answers are distinguished using different HTML elements.\nCollecting this information and using it specifically to build an index of question–\nanswer pairs will get us started on a question-answering system for this task. The next\nstep could be using text embeddings and performing a similarity-based search using\nElasticsearch.\nLooking for Deeper Answers\nIn the approaches described above, we would still expect the user question to have a\nsignificant amount of exact overlap with the indexed question and answer. However,\nDL-based text embeddings, which we’ve seen in different chapters throughout this\n268 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 178,
        "char_count": 1158,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 986,
            "height": 1163,
            "ext": "png",
            "size_bytes": 60675
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 299,
        "text": "book so far, are capable of going beyond exact matches and capturing semantic simi‐\nlarities. Such a neural question-answering approach looks for the answer span in a\ntext by comparing the question’s embedding with that of the text’s subunits (words,\nsentences, and paragraphs). Question answering using deep neural networks is very\nmuch an active area of research and is typically studied as a supervised ML problem\nusing specific datasets designed for this task, such as the SQuAD [36] dataset.\nDeepQA, which is a part of Allen NLP [37], is a popular library for developing exper‐\nimental question-answering systems using DL architectures.\nAnother approach to question answering is knowledge-based question answering,\nwhich relies on the presence of a huge knowledge database and a way to map user\nqueries to the database. This is typically used for answering short, factual questions.\nReal-world question-answering systems like IBM Watson, which beat human partici‐\npants in the popular quiz show Jeopardy!, use a combination of both approaches. Bing\nAnswer Search API [38], which allows subscribed users to query the system for\nanswers, is an example of a research system that follows such a hybrid approach.\nDeveloping any such question-answering system that can model deeper knowledge at\nweb scale requires a substantial amount of data and computing resources coupled\nwith a lot of experimentation. It’s not yet a common scenario in a typical software\ncompany working on NLP projects, so we won’t discuss it further in this book. To get\na historical overview of question answering along with the most recent developments\nin research, we recommend reading Chapter 25 of the upcoming edition of the popu‐\nlar NLP textbook, Speech and Language Processing [39]. If you want to implement a\nDL-based question-answering system for your own dataset (e.g., internal documents\nin an organization), libraries such as CDQA-Suite [40] provide the backbone to get\nstarted.\nAs can be seen from this discussion, question answering is an area of search that has a\nwide-ranging array of solutions, ranging from simple and straightforward approaches\nlike extracting markup, to complex, DL-based solutions. We hope this overview pro‐\nvided you with enough examples of the use cases you may encounter in your work‐\nplace to develop question-answering systems.\nWrapping Up\nIn this chapter, we saw how NLP plays a role in a range of problem scenarios, starting\nfrom search engines to question answering. We saw how some of the topics we\nlearned earlier in the book can be used to address these problems. While these topics\nseem disparate at first glance, some of them are also related to one another—for\nexample, search, recommendation systems, and question answering are all some form\nof information retrieval. Even summarization can be treated as such, as we retrieve\nrelevant sentences from a given text. Additionally, all of them, except machine trans‐\nlation, typically do not require large, annotated datasets. Thus, we can see some\nWrapping Up \n| \n269\n",
        "word_count": 483,
        "char_count": 3042,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 300,
        "text": "similarities among these topics. Note that each of the topics we discussed are still\nactive research questions in NLP, and a lot of new developments happen every day, so\nthe treatment of topics in this chapter is not exhaustive. However, we hope this gave\nyou enough of an overview to get started should you encounter a related use case at\nwork.\nWith this, we’ve reached the end of the “Essentials” part of the book. In the next part,\nwe’ll take a look at how all these different topics come together in specific domains.\nReferences\n[1] Nayak, Pandu. “Understanding Searches Better than Ever Before”. The Keyword\n(blog), October 25, 2019.\n[2] Brin, Sergey and Lawrence Page. “The Anatomy of a Large-Scale Hypertextual\nWeb Search Engine.” Computer Networks and ISDN Systems 30.1–7 (1998): 107–117.\n[3] Apache Nutch. Last accessed June 15, 2020.\n[4] Scrapy, a fast and powerful scraping and web crawling framework. Last accessed\nJune 15, 2020.\n[5] Apache Solr, an open source search engine. Last accessed June 15, 2020.\n[6] Elasticsearch, an open source search engine. Last accessed June 15, 2020.\n[7] Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. Introduc‐\ntion to Information Retrieval. Cambridge: Cambridge University Press, 2008. ISBN:\n978-0-52186-571-5\n[8] Tibshirani, Julie. “Text similarity search with vector fields”. Elastic (blog), August\n27, 2019.\n[9] Elasticsearch. “Function score query” documentation. Last accessed June 15, 2020.\n[10] Amazon Kendra. Last accessed June 15, 2020.\n[11] Bamman, David and Noah Smith. “CMU Book Summary Dataset”, 2013.\n[12] Amazon Elasticsearch Service. Last accessed June 15, 2020.\n[13] Elastic on Azure. Last accessed June 15, 2020.\n[14] Elasticsearch. “Elasticsearch Learning to Rank: the documentation”. Last\naccessed June 15, 2020.\n[15] Mitra, Bhaskar and Nick Craswell. “An introduction to neural information\nretrieval.” Foundations and Trends in Information Retrieval 13.1 (2018): 1–126.\n[16] Search engine services by Algolia. Last accessed June 15, 2020.\n270 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 316,
        "char_count": 2061,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 301,
        "text": "[17] Search engine services by Swiftype, and Amazon Kendra. Last accessed June 15,\n2020.\n[18] Gormley, Clinton and Zachary Tong. Elasticsearch: The Definitive Guide. Boston:\nO’Reilly, 2015. ISBN: 978-1-44935-854-9\n[19] “EH Topic Modeling II”. Last accessed June 15, 2020.\n[20] Keshet, Joseph. “Latent Dirichlet Allocation”. Lecture from Advanced Techni‐\nques in Machine Learning (89654), Bar Ilan University, 2016.\n[21] RaRe Consulting. “Genism: topic modelling for humans”. Last accessed June 15,\n2020.\n[22] Gensim’s LDA tutorial. Last accessed June 15, 2020.\n[23] Topic modeling is a broad area, with entire books written on the topic, so we\nwon’t discuss how they work in this book. Interested readers can refer to the follow‐\ning article as a starting point: Blei, David M. “Probabilistic Topic Models.” Communi‐\ncations of the ACM 55.4 (2012): 77–84.\n[24] NIST. Document Understanding Conference series. Last accessed June 15, 2020.\n[25] Reddit. autotldr bot. Last accessed June 15, 2020.\n[26] Sumy, an automatic text summarizer. Last accessed June 15, 2020.\n[27] Mihalcea, Rada and Paul Tarau. “TextRank: Bringing Order into Text.” Proceed‐\nings of the 2004 Conference on Empirical Methods in Natural Language Processing\n(2004): 404–411.\n[28] Mortensen, Ólavur. “Text Summarization with Gensim”. RARE Technologies\n(blog), August 24, 2015.\n[29] Wikipedia. “ROUGE (metric)”. Last updated September 3, 2019.\n[30] Paulus, Romain, Caiming Xiong, and Richard Socher. “Your TLDR by an ai: a\nDeep Reinforced Model for Abstractive Summarization”. Salesforce Research (blog),\n2017.\n[31] Patrick, Ryan B. “How Desmond Cole Wrote a Bestselling Book about Being\nBlack in Canada”. CBC, February 27, 2020.\n[32] Grbovic, Mihajlo et al. “Listing Embeddings in Search Ranking”. Airbnb Engi‐\nneering & Data Science (blog), March 13, 2018.\n[33] Way, Andy. “Traditional and Emerging Use-Cases for Machine Translation.” Pro‐\nceedings of Translating and the Computer 35 (2013): 12.\n[34] Azure Cognitive Services. Translator Text API v3.0. Last accessed June 15, 2020.\n[35] Machine Translation courses. Last accessed June 15, 2020.\nWrapping Up \n| \n271\n",
        "word_count": 320,
        "char_count": 2134,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 302,
        "text": "[36] SQuAD2.0. “The Stanford Question Answering Dataset”. Last accessed June 15,\n2020.\n[37] Allen Institute for AI. AllenNLP. Last accessed June 15, 2020.\n[38] Microsoft. Project Answer Search API. Last accessed June 15, 2020.\n[39] Jurafsky, Dan and James H. Martin. Speech and Language Processing, Third Edi‐\ntion (Draft). 2018.\n[40] CDQA-Suite, a library to help build a QA system for your dataset. Last accessed\nJune 15, 2020.\n272 \n| \nChapter 7: Topics in Brief\n",
        "word_count": 75,
        "char_count": 465,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 303,
        "text": "PART III\nApplied\n",
        "word_count": 3,
        "char_count": 17,
        "fonts": [
          "MyriadPro-SemiboldCond (28.4pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 304,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 305,
        "text": "CHAPTER 8\nSocial Media\nIn today’s world, we don’t need to speak English\nbecause we have social media.\n—Vir Das\nSocial media platforms (Twitter, Facebook, Instagram, WhatsApp, etc.) have revolu‐\ntionized the way we communicate with individuals, groups, communities, corpora‐\ntions, government agencies, media houses, etc. This, in turn, has changed established\nnorms and etiquette and the day-to-day practices of how businesses and government\nagencies carry out things like sales, marketing, public relations, and customer sup‐\nport. Given the huge volume and variety of data generated daily on social media plat‐\nforms, there’s a huge body of work focused on building intelligent systems to\nunderstand communication and interaction on these platforms. Since a large part of\nthis communication happens in text, NLP has a fundamental role to play in building\nsuch systems. In this chapter, we’ll focus on how NLP is useful for analyzing social\nmedia data and how to build such systems.\nTo give an idea of the volume of data that’s generated on these platforms [1, 2, 3],\nconsider the following numbers:\nVolume: 152 million monthly active users on Twitter; for Facebook, it’s 2.5 billion\nVelocity: 6,000 tweets/second; 57,000 Facebook posts/second\nVariety: Topic, language, style, script\nThe infographic shown in Figure 8-1 presents how much data is generated per\nminute across different platforms [4].\n275\n",
        "word_count": 217,
        "char_count": 1404,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (9.3pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 306,
        "text": "Figure 8-1. Data generated in one minute on various social platforms\nGiven these numbers, social platforms have to be the largest generators of unstruc‐\ntured natural language data. It’s not possible to manually analyze even a fraction of\nthis data. Since a lot of content is text, the only way forward is to design NLP-based\nintelligent systems that can work with social data and bring out insights. This is the\nfocus of this chapter. We’ll look at some important business applications, such as\ntopic detection, sentiment analysis, customer support, and fake news detection, to\nname a few. A large part of this chapter will be about how the text from social plat‐\nforms is different from other sources of data and how we can design subsystems to\nhandle these differences. Let’s begin by looking at some of the important applications\nthat use NLP to extract insights from social media data.\n276 \n| \nChapter 8: Social Media\n",
        "word_count": 156,
        "char_count": 923,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1361,
            "height": 1447,
            "ext": "png",
            "size_bytes": 390951
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 307,
        "text": "Applications\nThere’s a wide variety of NLP applications that use data from social platforms, includ‐\ning sentiment detection, customer support, and opinion mining, to name a few. This\nsection will briefly discuss some of the popular ones to give an idea of where we could\nbegin applying these applications for our own needs:\nTrending topic detection\nThis deals with identifying the topics that are currently getting the most traction\non social networks. Trending topics tell us what content people are attracted to\nand what they think is noteworthy. This information is of immense importance\nto media houses, retailers, first responders, government entities, and many more.\nIt helps them fine-tune their strategies of engaging with their users. Imagine the\ninsights it could bring when done at the level of specific geolocations.\nOpinion mining\nPeople often use social media to express their opinions about a product, service,\nor policy. Gathering this information and making sense of it is of great value to\nbrands and organizations. It’s impossible to go through thousands of tweets and\nposts manually to understand the larger opinion of the masses. In such scenarios,\nbeing able to summarize thousands of social posts and extract the key insights is\nhighly valuable.\nSentiment detection\nSentiment analysis of social media data has to be by far the most popular applica‐\ntion of NLP on social data. Brands rely extensively on using signals from social\nmedia to better understand their users’ sentiments about their products and serv‐\nices and that of their competitors. They use it to better understand their custom‐\ners, from using sentiment to identify the cohorts of customers they should\nengage with to understanding the shift in the sentiment of its customer base over\na long duration of time.\nRumor/fake news detection\nGiven their fast and far reach, social networks are also misused to spread false\nnews. In the past few years, there have been instances where social networks were\nused to sway the opinion of masses using false propaganda. There is a lot of work\ngoing on toward understanding and identifying fake news and rumors. This is\npart of both preventive and corrective measures to control this menace.\nAdult content filtering\nSocial media also suffers from people using social networks to spread inappropri‐\nate content. NLP is used extensively to identify and filter out inappropriate con‐\ntent, such as nudity, profanity, racism, threats, etc.\nApplications \n| \n277\n",
        "word_count": 398,
        "char_count": 2485,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 308,
        "text": "Customer support\nOwing to the widespread use of social media and its public visibility, customer\nsupport on social media has evolved into a must-have for every brand across the\nglobe. Users reach out to brands with their complaints and concerns via social\nchannels. NLP is used extensively to understand, categorize, filter, prioritize, and\nin some cases even automatically respond to the complaints.\nThere are many other applications that we haven’t dug into, such as geolocation\ndetection, sarcasm detection, event and topic detection, emergency situation aware‐\nness, and rumor detection, to name a few. Our aim here is to give you a good idea of\nthe landscape of applications that can be built using social media text data (SMTD).\nNow, let’s look into why building NLP applications using SMTD is not a straightfor‐\nward application of the concepts we’ve learned so far in this book and why SMTD\nrequires special treatment.\nUnique Challenges\nUntil now, we’ve (implicitly) assumed that the input text (most of the time, if not\nalways) follows the basic tenets of any language, namely:\n• Single language\n• Single script\n• Formal\n• Grammatically correct\n• Few or no spelling errors\n• Mostly text-like (very few non-textual elements, such as emoticons, images,\nsmileys, etc.)\nThese assumptions essentially stem from the properties and characteristics of the\ndomain(s) from which the input text data comes. Standard NLP systems assume that\nthe language they deal with is highly structured and formal. When it comes to text\ndata coming from social platforms, most of the above assumptions go for a toss. This\nis because users can be extremely terse when posting on social media; this extreme\nbrevity is a hallmark of social posts. For example, users may write “are” as “r,” “we” as\n“v,” “laugh out loud” as “lol,” etc. This brevity has given rise to a new recipe for lan‐\nguage: one that’s very informal and consists of nonstandard spellings, hashtags, emo‐\nticons, new words and acronyms, code-mixing, transliteration, etc. These\ncharacteristics make the language used on social platforms so unique that it’s alto‐\ngether considered a new language—the “language of social.”\n278 \n| \nChapter 8: Social Media\n",
        "word_count": 357,
        "char_count": 2204,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 309,
        "text": "Because of this, the NLP tools and techniques designed for standard text data don’t\nwork well with SMTD. To illustrate this point better, let’s look at some sample tweets,\nshown in Figures 8-2 and 8-3. Notice how the language used here is very different\nfrom the language used in newspapers, blog posts, emails, book chapters, etc.\nFigure 8-2. Examples of new words being introduced in vocabulary\nFigure 8-3. New recipe for language: nonstandard spellings, emoticons, code-mixing,\ntransliteration [5]\nUnique Challenges \n| \n279\n",
        "word_count": 82,
        "char_count": 527,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1184,
            "height": 718,
            "ext": "png",
            "size_bytes": 191030
          },
          {
            "index": 1,
            "width": 353,
            "height": 432,
            "ext": "png",
            "size_bytes": 172804
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 310,
        "text": "These differences pose challenges to standard NLP systems. Let’s look at the key dif‐\nferences in detail:\nNo grammar\nAny language is known to strictly follow the rules of grammar. However, conver‐\nsations on social media don’t follow any grammar and are characterized by\ninconsistent (or absent) punctuation and capitalization, emoticons, incorrect or\nnonstandard spelling, repetition of the same character multiple times, and ram‐\npant abbreviations. This departure from standard languages makes basic pre-\nprocessing steps like tokenization, POS tagging, and identification of sentence\nboundaries difficult. Modules specialized to work with SMTD are required to\nachieve these tasks.\nNonstandard spelling\nMost languages have a single way of writing any word, so writing a word in any\nother way is a spelling mistake. In SMTD, words can have many spelling varia‐\ntions. As an example, consider the following different ways in which the English\nword “tomorrow” is written on social [6]—tmw, tomarrow, 2mrw, tommorw,\n2moz, tomorro, tommarrow, tomarro, 2m, tomorrw, tmmrw, tomoz, tommorow,\ntmrrw, tommarow, 2maro, tmrow, tommoro, tomolo, 2mor, 2moro, 2mara, 2mw,\ntomaro, tomarow, tomoro, 2morr, 2mro, tmoz, tomo, 2morro, 2mar, 2marrow,\ntmr, tomz, tmorrow, 2mr, tmo, tmro, tommorrow, tmrw, tmrrow, 2mora, tomm‐\nrow, tmoro, 2ma, 2morrow, tomrw, tomm, tmrww, 2morow, 2mrrw, tomorow.\nFor an NLP system to work well, it needs to understand that all these words refer\nto the same word.\nMultilingual\nTake any article from a newspaper or a book, and you’ll probably find it’s written\nin a single language. Seldom will you see where large parts of it are written in\nmore than one language. On social media, people often mix languages. Consider\nthe following example from a social media website [7]:\nYaar tu to, GOD hain. tui\nJU te ki korchis? Hail u man!\nIt means, “Buddy you are GOD. What are you doing in JU? Hail u man!” The text\nis a mix of three languages: English (normal font), Hindi (italics), and Bengali\n(boldface). For Bengali and Hindi, phonetic typing has been used.\nTransliteration\nEach language is written in its own script, which refers to how the characters are\nwritten. However, on social media, people often write the characters of one script\nusing characters of another script. This is called “transliteration.” For example,\nconsider the Hindi word “आप” (devanagari script, pronounced as “aap”). In\n280 \n| \nChapter 8: Social Media\n",
        "word_count": 386,
        "char_count": 2439,
        "fonts": [
          "ArialUnicodeMS (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MinionPro-Bold (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 311,
        "text": "English, it means “you” (roman script). But people often write it in roman script\nas “aap.” Transliteration is common in SMTD, usually due to the typing interface\n(keyboard) being in roman script but the language of communication being non-\nEnglish.\nSpecial characters\nSMTD is characterized by the presence of many non-textual entities, such as spe‐\ncial characters, emojis, hashtags, emoticons, images and gifs, non-ASCII charac‐\nters, etc. For example, look at the tweets shown in Figure 8-4. From an NLP\nstandpoint, one needs modules in the pre-processing pipelines to handle such\nnon-textual entities.\nFigure 8-4. Special characters in social media data\nEver-evolving vocabulary\nMost languages add either no or very few new words to their vocabulary every\nyear. But when it comes to the language of social, the vocabulary increases at a\nvery fast rate. New words pop up every single day. This means that any NLP sys‐\ntem processing SMTD sees a lot of new words that were not part of the vocabu‐\nlary of the training data. This has an adverse impact on the performance of the\nNLP system and is known as the out of vocabulary (OOV) problem.\nIn order to get an idea of the severity of this problem, look at the infographic\nshown in Figure 8-5. We did this experiment [8] a few years ago, where we collec‐\nted a large corpus of tweets and quantified the amount of “new vocabulary” seen\nUnique Challenges \n| \n281\n",
        "word_count": 241,
        "char_count": 1412,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1276,
            "height": 832,
            "ext": "png",
            "size_bytes": 271604
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 312,
        "text": "on a month-by-month basis. The figure shows the fraction of new words seen in\na month as compared to the previous month’s data. As evident from the image,\nwhen compared to the vocabulary of the previous month, there are 10–15% new\nwords every month.\nFigure 8-5. Fraction of new vocabulary words seen every month [8]\n282 \n| \nChapter 8: Social Media\n",
        "word_count": 61,
        "char_count": 348,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1427,
            "height": 1882,
            "ext": "png",
            "size_bytes": 107891
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 313,
        "text": "Length of text\nThe average length of text on social media platforms is much smaller compared\nto other channels of communication like blogs, product reviews, emails, etc. The\nreason is that shorter text can be typed quickly while preserving understandabil‐\nity. This was driven primarily by Twitter’s 140-character restriction. For example,\n“This is an example for texting language” might be written as “dis is n eg 4 txtin\nlang.” Both mean the same, but the former is 39 characters long while the latter\nhas only 24 characters. As Twitter’s popularity and adoption grew, being terse on\nsocial platforms became the norm. This way of condensed writing has become so\npopular that now it can be seen in every informal communication, such as mes‐\nsages and chats.\nNoisy data\nSocial media posts are full of spam, ads, promoted content, and all manner of\nother unsolicited, irrelevant, or distracting content. Thus, we cannot take raw\ndata from social platforms and consume it as is. Filtering out noisy data is a vital\nstep. For example, imagine we’re collecting data for an NLP task (say, sarcasm\ndetection) from a Twitter handle or Facebook page, either by scraping or using\nthe Twitter API. It’s important to run a check that no spam, ads, or irrelevant\ncontent has come into our dataset.\nIn short, text data from social media is highly informal compared to text data from\nblogs, books, etc., and this lack of formality can manifest in the various ways\ndescribed above. All of these can have adverse impacts on the performance of NLP\nsystems that don’t have built-in ways to handle them. Figure 8-6 [5] shows the spec‐\ntrum of formalism in text data and where different sources of text data appear on it.\nFigure 8-6. Spectrum of formalism in text data depending on data sources [5]\nOwing to the characteristics and peculiarities that stem from the informal nature of\nthe language of social, standard NLP tools and techniques face difficulties when\napplied to SMTD. NLP for SMTD relies on either converting the text from social to\nstandard text (normalization) or building systems that are specifically designed to\ntackle SMTD. We’ll see how to go about doing this while building various applica‐\ntions in the next section.\nUnique Challenges \n| \n283\n",
        "word_count": 377,
        "char_count": 2246,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1357,
            "height": 363,
            "ext": "png",
            "size_bytes": 19430
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 314,
        "text": "It’s important to identify, understand, and address the language\npeculiarities found in SMTD. Building submodules that can handle\nthese peculiarities often goes a long way toward improving the per‐\nformance of models working with SMTD.\nNow, let’s focus on building business applications using SMTD.\nNLP for Social Data\nWe’ll now take a deep dive into applying NLP to SMTD to build some interesting\napplications that we can apply to a variety of problems. We may need to know how\ncustomers are responding to a particular announcement or product we’ve released, or\nbe able to identify user demographics, for example. We’ll start with simple applica‐\ntions like word clouds and ramp up to more complex ones, like understanding senti‐\nment in posts on social media platforms like Twitter.\nWord Cloud\nA word cloud is a pictorial way of capturing the most significant words in a given\ndocument or corpus. It’s nothing but an image composed of words (in different sizes)\nfrom the text under consideration, where the size of the word is proportional to its\nimportance (frequency) in the text corpus. It’s a quick way to understand the key\nterms in a corpus. If we run a word cloud algorithm on this book, we’re likely to get a\nword cloud similar to one shown in Figure 8-7.\nFigure 8-7. Word cloud for Chapter 4 of this book\n284 \n| \nChapter 8: Social Media\n",
        "word_count": 232,
        "char_count": 1348,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          },
          {
            "index": 1,
            "width": 710,
            "height": 730,
            "ext": "png",
            "size_bytes": 471865
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 315,
        "text": "Words like NLP, natural language processing, and linguistics occur many times com‐\npared to other words in the book, so they show up prominently in the corresponding\nword cloud. So, how do we create word clouds from a collection of tweets? What’s the\nNLP pipeline for this?\nHere’s a step-by-step process for building a word cloud:\n1. Tokenize a given corpus or document\n2. Remove stop words\n3. Sort the remaining words in descending order of frequency\n4. Take the top k words and plot them “aesthetically”\nThe following code snippet illustrates how to implement this pipeline in practice (the\ncomplete code can be found in Ch8/wordcloud.ipynb). For this, we’ll use a library\ncalled wordcloud [9] that has a built-in function for generating word clouds:\n    from wordcloud import WordCloud\ndocument_file_path = ‘./twitter_data.txt’\ntext_from_file = open(document_file_path).read()\nstop_words = set(nltk.corpus.stopwords.words('english'))\nword_tokens = twokenize(text_from_file)\nfiltered_sentence = [w for w in word_tokens if not w in stop_words]\nwl_space_split = \" \".join(filtered_sentence)\nmy_wordcloud = WordCloud().generate(wl_space_split)\nplt.imshow(my_wordcloud)\nplt.axis(\"off\")\nplt.show()\nDepending on the styling, we can generate word clouds in various shapes to suit our\napplication [10], as shown in Figure 8-8.\nFigure 8-8. The same word cloud in various shapes\nNLP for Social Data \n| \n285\n",
        "word_count": 200,
        "char_count": 1398,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 508,
            "ext": "png",
            "size_bytes": 379568
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 316,
        "text": "Tokenizer for SMTD\nOne of the key steps in the above process is to correctly tokenize the text data. For\nthis, we used twokenize [11] to get tokens from the text corpus. This is a specialized\nfunction for getting tokens from tweets’ text data. This function is part of a set of NLP\ntools specially designed to work with SMTD [12, 13]. Now, we might ask: why do we\nneed a specialized tokenizer, and why not use the standard tokenizer available in\nNLTK? We discussed this briefly in Chapters 3 and 4, but it’s worth spending time on\nagain. The answer lies in the fact that the tokenizer available in NLTK is designed to\nwork with standard English language. Specifically in the English language, two words\nare separated by space. This might not necessarily be true for English used on Twitter.\nThis suggests that a tokenizer that uses space as a way to identify word boundaries\nmight not do well on SMTD. Let’s understand this with an example. Consider the fol‐\nlowing tweet: “Hey @NLPer! This is a #NLProc tweet :-D”. The ideal way to tokenize\nthis text is as follows: ['Hey', '@NLPer', '!', ‘This', ‘is', ‘a', '#NLProc', ‘tweet', ':-D']. Using\na tokenizer designed for the English language, like nltk.tokenize.word_tokenize, we’ll\nget the following tokens: ['Hey', '@', ‘NLPer', '!', ‘This', ‘is', ‘a', '#', ‘NLProc', ‘tweet', ':', '-\nD']. Clearly, the set of tokens given by the tokenizer in NLTK is not correct. It’s\nimportant to use a tokenizer that gives correct tokens. twokenize is specifically\ndesigned to deal with SMTD.\nOnce we have the correct set of tokens, frequency counting is straightforward. There\nare a number of specialized tokenizers available to work with SMTD. Some of the\npopular ones are nltk.tokenize.TweetTokenizer [14], Twikenizer [15], Twokenizer by\nARK at CMU [12], and twokenize [11]. For a given input tweet, each of them can give\nslightly different output. Use the one that gives the best output for your corpus and\nuse case.\nNow, we’ll move on to the next application, where we’ll try to extract topics that are\ntrending.\nTrending Topics\nJust a couple of years ago, keeping yourself updated with the latest topics was pretty\nstraightforward—pick up the day’s newspaper, read through the headlines, and you’re\ndone. Social media has changed this. Given the volume of traffic, what is trending can\n(and often does) change within a few hours. Keeping track of what’s trending by the\nhour may not be so important for an individual, but for a business entity, it can be\nvery important.\nHow can we keep track of trending topics? In the lingo of social media, any conversa‐\ntion around a topic is often associated with a hashtag. Thus, finding trending topics is\nall about finding the most popular hashtags in a given time window. In Figure 8-9, we\nshow a snapshot of trending topics in the area of New York.\n286 \n| \nChapter 8: Social Media\n",
        "word_count": 489,
        "char_count": 2865,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 317,
        "text": "Figure 8-9. Snapshot of trending topics on Twitter [16]\nSo how do we implement a system that can collect trending topics? One of the sim‐\nplest ways to do this is using a Python API called Tweepy [17]. Tweepy gives a simple\nfunction, trends_available, to fetch the trending topics. It takes the geolocation\n(WOEID, or Where On Earth Identifier) as an input and returns the trending topics\nin that geolocation. The function trends_available returns the top-10 trending top‐\nics for a given WOEID, on the condition that the trending information for the given\nWOEID is available. The response of this function call is an array of objects that are\n“trending.” In response, each object encodes the following information: name of the\ntopic that’s trending, the corresponding query parameters that can be used to search\nfor the topic using Twitter search, and the URL to Twitter search. Below is a code\nsnippet that demonstrates how we can use Tweepy to fetch trending topics (full code\nat Ch8/TrendingTopics.ipynb):\nimport tweepy, json\nCONSUMER_KEY = 'key'\nCONSUMER_SECRET = 'secret'\nACCESS_KEY = 'key'\nACCESS_SECRET = 'secret'\nauth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\nauth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\nNLP for Social Data \n| \n287\n",
        "word_count": 193,
        "char_count": 1257,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 281,
            "height": 537,
            "ext": "png",
            "size_bytes": 37758
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 318,
        "text": "api = tweepy.API(auth)\n# Where On Earth ID for the entire world is 1.\n# See https://dev.twitter.com/docs/api/1.1/get/trends/place and\n# http://developer.yahoo.com/geo/geoplanet/\nWORLD_WOE_ID = 1\nCANADA_WOE_ID = 23424775 # WOEID for Canada\nworld_trends = api.t\ntrends_place(_id=WORLD_WOE_ID)\ncanada_trends = api.trends_place(_id=CANADA_WOE_ID )\nworld_trends_set = set([trend['name'] for trend in world_trends[0]['trends']])\ncanada_trends_set = set([trend['name'] for trend incanada_trends[0]['trends']])\n# This gives the top trending hashtags for both world and Canada.\ncommon_trends = world_trends_set.intersection(us_trends_set)\ntrend_queries = [trend['query'] for trend in results[0]['trends']]\nfor trend_query in trend_queries:\n    print(api.search(q=trend_query))\n# this will return the tweets for each of the trending topic\nThis small snippet of code will give us the live top trends for a given location. The\nonly problem is that Tweepy is a free API, so it has rate limits. Twitter imposes rate\nlimits on how many requests an application can make to any given API resource\nwithin a given time window—you can’t make thousands of requests. Twitter’s rate\nlimits are well documented. In case you need to make calls beyond the rate limits,\nlook at Gnip [18], a paid data hosepipe from Twitter.\nNow, let’s see how to implement another popular NLP application: sentiment analysis\nwith social media data.\nUnderstanding Twitter Sentiment\nWhen it comes to NLP and social media, one of the most popular applications has to\nbe sentiment analysis. For businesses and brands across the globe, it’s crucial to listen\nto what people are saying about them and their products and services. It’s even more\nimportant to know whether people’s opinion is positive or negative and if this senti‐\nment polarity is changing over time. In the pre-social era, this was done using cus‐\ntomer surveys, including door-to-door visits. In today’s world, social media is a great\nway to understand people’s sentiment about a brand. Even more important is how\nthis sentiment changes over time. Figure 8-10 shows how sentiment changes over\ntime for a given organization. Visualizations like these provide great insights to mar‐\n288 \n| \nChapter 8: Social Media\n",
        "word_count": 325,
        "char_count": 2232,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 319,
        "text": "keting teams and organizations—dissecting their audience’s reactions to their cam‐\npaigns and events helps them plan strategically for future campaigns and content.\nFigure 8-10. Tracking change in sentiment over time [19]\nIn this section, we’ll focus on building sentiment analysis for Twitter data using a\ndataset from the public domain. There are many datasets available on the internet—\nfor example, the University of Michigan Sentiment Analysis competition on Kaggle\n[20] and Twitter Sentiment Corpus by Niek Sanders [21].\nHow is sentiment analysis for Twitter different from the sentiment analysis models\nwe built in Chapter 4? The key difference lies in the dataset. In Chapter 4, we used the\nIMDB dataset, which consists of sentences that are well formed and have a structure\nto them. On the other hand, the data in the Twitter sentiment corpus consists of\ntweets written informally. This leads to the various issues we discussed in “Unique\nChallenges” on page 278. These issues, in turn, impact the performance of the model.\nA great experiment is to run the sentiment analysis pipeline from Chapter 4 on our\nTwitter corpus and take a deep dive into the kind of mistakes the model makes. We\nleave this as an exercise for the reader.\nNLP for Social Data \n| \n289\n",
        "word_count": 209,
        "char_count": 1268,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1321,
            "height": 1050,
            "ext": "png",
            "size_bytes": 62856
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 320,
        "text": "We’ll move forward by building a system for sentiment analysis and setting up a base‐\nline. For this, we’ll use TextBlob [22], which is a Python-based NLP toolkit built on\ntop of NLTK and Pattern. It comes with an array of modules for text processing, text\nmining, and text analysis. All it takes is five lines of code to get a basic sentiment\nclassifier:\nfrom textblob import TextBlob\nfor tweet_text in tweets_text_collection:\n    print(tweet_text)\n    analysis = TextBlob(tweet_text)\n    print(analysis.sentiment)\nThis will give us polarity and subjectivity values of each of the tweets in the corpus.\nPolarity is a value between [–1.0, 1.0] and tells how positive or negative the text is.\nSubjectivity is within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very\nsubjective.\nIt uses a simple idea: tokenize the tweet and compute polarity and subjectivity for\neach of the tokens. Then combine the polarity and subjectivity numbers to arrive at a\nsingle value for the whole sentence. We leave it to the reader to get into the finer\ndetails. This simple sentiment classifier might not work well, primarily because of the\ntokenizer used by TextBlob. Our data comes from social media, so it will most likely\nnot follow formal English. Thus, after tokenization, many of the tokens may not be\nstandard words found in the English dictionary, so we won’t have the polarity and\nsubjectivity for all such tokens.\nSay we’ve been asked to improve our classifier. We can try various techniques and\nalgorithms we learned in Chapter 4. However, we might not see a great improvement\nin performance because of the noise present in the data (discussed in “Unique Chal‐\nlenges” on page 278). Thus, the key to improving the system lies in better cleaning\nand pre-processing of the text data. This is crucial for SMTD. Below, we’ll discuss\nsome important parts of pre-processing for SMTD. For the rest of the pipeline, we\ncan follow the pipeline discussed in Chapter 4.\nPre-processing and data cleaning are crucial when working with\nSMTD. This step is likely to provide the most gains in model per‐\nformance.\nPre-Processing SMTD\nMost NLP systems that work with SMTD have a rich pre-processing pipeline that\nincludes many steps. In this section, we’ll cover some of the steps that come up often\nin dealing with SMTD.\n290 \n| \nChapter 8: Social Media\n",
        "word_count": 390,
        "char_count": 2343,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 321,
        "text": "Removing markup elements\nIt’s not surprising to see markup elements (HTML, XML, XHTML, etc.) in SMTD,\nand it’s important to remove them. A great way to achieve this is to use a library\ncalled Beautiful Soup [23]:\nfrom bs4 import BeautifulSoup\nmarkup = '<a href=\"http://nlp.com/\">\\nI love <i>nlp</i>\\n</a>'\nsoup = BeautifulSoup(markup)\nsoup.get_text()\nThis gives the output \\nI love nlp\\n.\nHandling non-text data\nSMTD is often full of symbols, special characters, etc., and they’re often in encodings\nsuch as Latin and Unicode. In order to understand them, it’s important to convert the\nsymbols present in the data to simple and easier-to-understand characters. This is\noften done by converting to a standard encoding format like UTF-8. In the example\nbelow, we see how the entire text is converted into a machine-readable form:\ntext = 'I love Pizza 🍕!  Shall we book a cab 🚕 to gizza?'\nText = text.encode(\"utf-8\")\nprint(Text)\nb'I love Pizza \\xf0\\x9f\\x8d\\x95!  \nShall we book a cab \\xf0\\x9f\\x9a\\x95 to get pizza?'\nHandling apostrophes\nAnother hallmark of SMTD is the use of the apostrophe; it’s quite common to see sce‐\nnarios like ‘s, ‘re, ‘r, etc. The way to handle this is to expand apostrophes. This\nrequires a dictionary that can map apostrophes to full forms:\nApostrophes_expansion = {\n“'s\" : \" is\",\n\"'re\" : \" are\",\n\"'r\" : \" are\", ...} ## Given such a dictionary\nwords = twokenize(tweet_text)\nprocessed_tweet_text = [Apostrophes_expansion[word] if word \n                       in Apostrophes_expansion else word for word in words]\nprocessed_tweet_text = \" \".join(processed_tweet_text)\nTo the best of our knowledge, such a mapping between apostrophes and their expan‐\nsion is not available anywhere off the shelf, so it needs to be created manually.\nNLP for Social Data \n| \n291\n",
        "word_count": 280,
        "char_count": 1782,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "Symbola (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 322,
        "text": "Handling emojis\nEmojis are at the very core of communication over social channels. One small image\ncan completely describe one or more human emotions. However, they pose a huge\nchallenge for machines. How can we design subsystems that can understand the\nmeaning of an emoji? A naive thing to do during pre-processing would be to remove\nall emojis. This could result in significant loss of meaning.\nA good way to achieve this is to replace the emoji with corresponding text explaining\nthe emoji. For example, replace “” with “fire”. To do so, we need a mapping\nbetween emojis and their corresponding elaboration in text. Demoji [24] is a Python\npackage that does exactly this. It has a function, findall(), that gives a list of all\nemojis in the text along with their corresponding meanings.\ntweet = \"#startspreadingthenews yankees win great start by \n going 5strong\ninnings with 5k’s\n \n solo homerun \n with 2 solo homeruns \nand\n 3run homerun… \n \n \n with rbi’s … \n \n and \n \nto close the game\n!!!….WHAT A GAME!! \"\ndemoji.findall(tweet)\n{\n    \"\n\": \"fire\",\n    \"\n\": \"volcano\",\n    \"\n\": \"man judge: medium skin tone\",\n    \"\n\": \"Santa Claus: medium-dark skin tone\",\n    \"\n\": \"flag: Mexico\",\n    \"\n\": \"ogre\",\n    \"\n\": \"clown face\",\n    \"\n\": \"flag: Nicaragua\",\n    \"\n\": \"person rowing boat: medium-light skin tone\",\n    \"\n\": \"ox\",\n}\nWe can use the output of findall() to replace all emojis in a text with their corre‐\nsponding meaning in words.\nSplit-joined words\nAnother peculiarity of SMTD is that users sometimes combine multiple words into a\nsingle word, where the word disambiguation is done by using capital letters, for\nexample GoodMorning, RainyDay, PlayingInTheCold, etc. This is simple to handle.\nThe following code snippet does the job for us:\n    processed_tweet_text = “ “.join(re.findall(‘[A-Z][^A-Z]*’, tweet_text))\nFor GoodMorning, this will return “Good Morning.”\n292 \n| \nChapter 8: Social Media\n",
        "word_count": 303,
        "char_count": 1906,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 154,
            "height": 154,
            "ext": "png",
            "size_bytes": 2212
          },
          {
            "index": 1,
            "width": 154,
            "height": 113,
            "ext": "png",
            "size_bytes": 1778
          },
          {
            "index": 2,
            "width": 146,
            "height": 154,
            "ext": "png",
            "size_bytes": 1809
          },
          {
            "index": 3,
            "width": 154,
            "height": 116,
            "ext": "png",
            "size_bytes": 939
          },
          {
            "index": 4,
            "width": 154,
            "height": 154,
            "ext": "png",
            "size_bytes": 4624
          },
          {
            "index": 5,
            "width": 154,
            "height": 150,
            "ext": "png",
            "size_bytes": 4842
          },
          {
            "index": 6,
            "width": 154,
            "height": 154,
            "ext": "png",
            "size_bytes": 2913
          },
          {
            "index": 7,
            "width": 154,
            "height": 131,
            "ext": "png",
            "size_bytes": 1556
          },
          {
            "index": 8,
            "width": 154,
            "height": 140,
            "ext": "png",
            "size_bytes": 3308
          },
          {
            "index": 9,
            "width": 154,
            "height": 113,
            "ext": "png",
            "size_bytes": 2425
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 323,
        "text": "Removal of URLs\nAnother common feature of SMTD is the use of URLs. Depending on the application,\nwe might want to remove the URL all together. The code snippet replaces all URLs\nwith a constant; in this case, constant_url. While in simpler cases, we could use a\nregex, such as http\\S+, in most cases, we’ll have to write a custom regex like the one\nshown in the following snippet. This code is complex because some social posts con‐\ntain tiny URLs instead of full URLs:\ndef process_URLs(tweet_text):\n    '''\n    replace all URLs in the tweet text\n    '''\n    UrlStart1 = regex_or('https?://', r'www\\.')\n    CommonTLDs  = regex_or( 'com','co\\\\.uk','org','net','info','ca','biz',\n                          'info','edu','in','au')\n    UrlStart2 = r'[a-z0-9\\.-]+?' + r'\\.' + CommonTLDs + \n                pos_lookahead(r'[/ \\W\\b]')\n    # * not + for case of:  \"go to bla.com.\" -- don't want period\n    UrlBody = r'[^ \\t\\r\\n<>]*?'\n    UrlExtraCrapBeforeEnd = '%s+?' % regex_or(PunctChars, Entity)\n    UrlEnd = regex_or( r'\\.\\.+', r'[<>]', r'\\s', '$')\n    Url =      (optional(r'\\b') +\n          regex_or(UrlStart1, UrlStart2) +\n          UrlBody +\n    pos_lookahead( optional(UrlExtraCrapBeforeEnd) + UrlEnd))\n    Url_RE = re.compile(\"(%s)\" % Url, re.U|re.I)\n    tweet_text = re.sub(Url_RE, \" constant_url \", tweet_text)\n    # fix to handle unicodes in URL\n    URL_regex2 = r'\\b(htt)[p\\:\\/]*([\\\\x\\\\u][a-z0-9]*)*'\n    tweet_text = re.sub(URL_regex2, \" constant_url \", tweet_text)\n    return tweet_text\nNonstandard spellings\nOn social media, people often write words that are technically spelling mistakes. For\nexample, people often write one or more characters multiple times, as in “yessss” or\n“ssssh” (instead of “yes” or “ssh”). This repetition of characters is very common in\nSMTD. Below is a simple way to fix this. We use the fact that, in the English language,\nthere are hardly any words that have the same character three times consecutively. So\nwe trim accordingly:\ndef prune_multple_consecutive_same_char(tweet_text):\n    '''\n    yesssssssss  is converted to yes\n    ssssssssssh is converted to ssh\n    '''\n          tweet_text = re.sub(r'(.)\\1+', r'\\1\\1', tweet_text)\n          return tweet_text\nNLP for Social Data \n| \n293\n",
        "word_count": 296,
        "char_count": 2229,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 324,
        "text": "This gives the output yess ssh.\nAnother idea is to use spelling-correction libraries. Most of them use some form of\ndistance metric, such as edit distance or Levenshtein distance. TextBlob itself has\nsome spelling-correction capabilities:\nfrom textblob import TextBlob\ndata = \"His sellection is bery antresting\"\noutput = TextBlob(data).correct()\nprint(output)\nThis gives the output: His selection is very interesting.\nWe hope this gives you a good idea of why, when it comes to SMTD, pre-processing is\nso important, and of how it can be accomplished. This is by no means an exhaustive\nlist of pre-processing steps. Now, we’ll focus on the next step in our NLP pipeline\n(from back in Figure 2-1): feature engineering.\nText Representation for SMTD\nPreviously, we saw how to make a simple sentiment classifier for tweets using Text‐\nBlob [22]. Now, let’s try to build a more sophisticated classifier. Let’s say we’ve imple‐\nmented all the pre-processing steps we discussed in the previous section. Now what?\nNow we need to break the text into tokens and then represent them mathematically.\nFor tokenization, we use twokenize [11], which is a specialized tokenizer designed to\nwork with Twitter data. How do we represent the tokens we get? We can try various\ntechniques we learned in Chapter 3.\nIn our experience, basic vectorization approaches like BoW and TF-IDF do not work\nwell with SMTD, primarily due to noise and variation in text data (e.g., the variations\nof “tomorrow” we discussed earlier in this chapter). The noise and variations lead to\nextremely sparse vectors. This leaves us with the option of using embeddings. As we\nsaw in Chapter 3, training our own embeddings is very expensive. So, we can begin\nby using pre-trained embeddings. In Chapter 4, we saw how to use Google’s pre-\ntrained word embeddings to build a sentiment classifier. Now, if we run the same\ncode on our dataset collected from social media platforms, we may not get impressive\nnumbers like we got there. One of the reasons may be that the vocabulary of our data‐\nset is dramatically different from the vocabulary of the Word2vec model. To verify\nthis, we just tokenize our text corpus and build a set on all tokens, then compare it\nwith the vocabulary of Word2vec. The following code snippet does this:\ncombined = tokenizer(train_test_X)\n# This is one way to create vocab set from our dataset.\nflat_list = chain(*combined)\ndataset_vocab = set(flat_list)\nlen(dataset_vocab)\n294 \n| \nChapter 8: Social Media\n",
        "word_count": 408,
        "char_count": 2486,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Italic (8.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 325,
        "text": "w2v_vocab = set(w2v_model.vocab.keys())\nprint(dataset_vocab - w2v_vocab)\nHere, train_test_X is the combined set of reviews from training and test chunks of\nour corpus. Now, you may ask why this wasn’t true when we worked with the IMDB\nmovie review dataset. The reason is that Google’s Word2vec is trained on Wikipedia\nand news articles. The language and vocabulary used in these articles is similar to the\nlanguage and vocabulary used in the IMDB movie review dataset. This is unlikely to\nbe true with our dataset from social media. So, it’s highly likely that, for our dataset\nfrom social media, the set difference will be pretty high.\nSo, how do we fix this? There are a few ways:\n1. Use pre-trained embeddings from social data, such as the ones from Stanford’s\nNLP group [25]. They trained word embeddings on two billion tweets [26].\n2. Use a better tokenizer. We highly recommend the twokenize tokenizer from\nAllen Ritter’s work [11].\n3. Train your own embeddings. This option should be the last resort and done only\nif you have lots and lots of data (at least 1 to 1.5 million tweets). Even after train‐\ning your own embeddings, you may not get any considerable bump in perfor‐\nmance metrics.\nIn our experience, if you’re going for word-based embeddings, (1)\nand (2) can give you the best return on investment for your efforts.\nEven if you get a considerable boost in the performance metrics, as the time gap\nbetween training data and production data keeps increasing, the performance can\nkeep going down. This is because as the time gap increases, the overlap between the\nvocabulary of the training data and production data keeps reducing. One of the main\nreasons for this is the fact that the vocabulary of social media is always evolving—new\nwords and acronyms are created and used all the time. You might think that new\nwords get added only once in a while, but, surprisingly, this is far from true.\nFigure 8-11 shows how fast the vocabulary on social media can evolve [8]. The plot\non the left shows the percentage of unseen tokens on a month-by-month basis. This\nanalysis was done using approximately 2 million tweets over a span of 27 months.\nThe plot in the middle shows the same statistics as a bar plot of total versus new\ntokens on a monthly basis. The plot on the right is a cumulative bar chart. On aver‐\nage, approximately 20% of the vocabulary for any month are new words.\nNLP for Social Data \n| \n295\n",
        "word_count": 422,
        "char_count": 2421,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 326,
        "text": "Figure 8-11. Plots depicting how fast the vocabulary of social media can evolve [8]\nWhat does this mean for us? No matter how good our word embeddings are, because\nof the ever-evolving vocabulary of social media, within a couple of months, our\nembeddings will become obsolete (i.e., a large portion of our vocabulary won’t be\npresent in our word embeddings). This means that when we query the embedding\nmodel with a word to fetch its embedding, it will return null since the query word\nwas not present in the training data when the embeddings were trained. This is analo‐\ngous to saying that all such words were completely ignored. This, in turn, will dra‐\nmatically reduce the accuracy of our sentiment classifier with time, because with time,\nmore and more words will end up getting ignored.\nWord embeddings are not the best way to represent SMTD, espe‐\ncially when you want to use them for more than four to six months.\nResearchers working in this area identified this problem pretty early and tried various\nways to circumvent it. One of the better ways to deal with this persistent OOV prob‐\nlem with SMTD is to use character n-gram embeddings. We discussed this idea when\nwe covered fastText in Chapters 3 and 4. Each character n-gram in the corpus has an\nembedding for it. Now, if the word is present in the vocabulary of the embeddings,\nthen we use the word embedding directly. If not—i.e., the word is OOV—we break\nthe word into character n-grams and combine all these embeddings to come up with\nthe embedding for the word. fastText has pre-trained character n-gram embeddings\n296 \n| \nChapter 8: Social Media\n",
        "word_count": 280,
        "char_count": 1617,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          },
          {
            "index": 1,
            "width": 960,
            "height": 543,
            "ext": "png",
            "size_bytes": 270520
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 327,
        "text": "but they’re not not Twitter or SMTD specific. Researchers have also tried character\nembeddings. An interested reader can look into various works along these lines [27,\n28, 29, 30].\nCustomer Support on Social Channels\nFrom its inception to present day, social media has evolved as a channel of communi‐\ncation. It started primarily with the objective of helping people across the globe get\nconnected and express themselves. But the widespread adoption of social media has\nforced brands and organizations to take another look at their communication strate‐\ngies. A great example of this is brands providing customer support on social plat‐\nforms like Twitter and Facebook. Brands never intended to do this to begin with.\nEarly in this decade, as the adoption of social platforms grew, brands started to create\nand own properties and assets like Twitter handles and Facebook pages primarily to\nreach out to their customers and users and run branding and marketing campaigns.\nHowever, over time brands saw that users and customers were reaching out to them\nwith complaints and grievances. As the volume of the complaints and issues grew,\nthis prompted brands to create dedicated handles and pages to handle support traffic.\nFigure 8-12 shows the support pages of Apple and Bank of America. Twitter and\nFacebook have launched various features to support brands [31], and most customer\nrelationship management (CRM) tools support customer service on social channels.\nA brand can connect their social channels to the CRM tool and use the tool to\nrespond to inbound messages.\nFigure 8-12. Example of brands’ support pages on Twitter [32]\nOwing to the public nature of conversations, brands are obligated to respond quickly.\nHowever, brands’ support pages receive a lot of traffic. Some of this is genuine ques‐\ntions, grievances, and requests. These are popularly known as “actionable conversa‐\ntions,” as customer support teams should act on them quickly. On the other hand, a\nlarge portion of traffic is simply noise: promos, coupons, offers, opinions, troll\nNLP for Social Data \n| \n297\n",
        "word_count": 334,
        "char_count": 2081,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1394,
            "height": 560,
            "ext": "png",
            "size_bytes": 419103
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 328,
        "text": "messages, etc. This is popularly called “noise.” Customer support teams cannot\nrespond to noise and want to steer clear of all such messages. Ideally, they want only\nactionable messages to be converted into tickets in their CRM tools. Figure 8-13\nshows examples of both actionable messages and noise.\nFigure 8-13. Example of actionable versus noisy messages [8]\nImagine we work at a CRM product organization and are asked to build a model to\nsegregate actionable messages from noise. How can we go about it? The problem of\nidentifying noise versus actionable messages is analogous to the spam classification\nproblem or sentiment classification problem. We can build a model that can look at\nthe inbound messages. The pipeline will be very similar:\n1. Collect a labeled dataset\n2. Clean it\n3. Pre-process it\n4. Tokenize it\n5. Represent it\n6. Train a model\n7. Test model\n8. Put it in production\n298 \n| \nChapter 8: Social Media\n",
        "word_count": 156,
        "char_count": 925,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1280,
            "height": 899,
            "ext": "png",
            "size_bytes": 216899
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 329,
        "text": "We’ve already discussed various aspects of this pipeline in this chapter. Much like sen‐\ntiment analysis on SMTD, the key here, too, is the pre-processing step. With this,\nwe’re now ready to move to the last topic of this chapter: identifying controversial\ncontent on social platforms.\nMemes and Fake News\nUsers on social platforms are known to share various kinds of information and\nthoughts in various ways. These platforms were initially designed to be self-\nregulating. However, over time, users have evolved to behave beyond community\nnorms; this is known as “trolling.” A large portion of posts on social platforms are full\nof controversial content such as trolls, memes, internet slang, and fake news. Some of\nit might be advocacy of propaganda, or it could be just for fun. In any case, this con‐\ntent needs to be monitored and filtered out. In this section, we’ll discuss how to study\nthe trends of such content and the role NLP has to play in it.\nIdentifying Memes\nMemes are one of the most interesting elements that have been curated by social\nmedia users to communicate messages with fun or satire. These memes get reused\nwith minimal changes in form, such as the image of “grumpy cat” (Figure 8-14),\nwhich has been used in many scenarios with different text associated with it. This\nresembles the original concept of “genes” as coined by Richard Dawkins [33]. Lada\nAdamic from Facebook studied information flow via these memes in the Facebook\nnetwork [34]; she says, “…memes propagating via a manual copy and paste mecha‐\nnism can be exact, or they might contain a “mutation,” an accidental or intentional\nmodification.” Figure 8-14 shows examples of two popular memes that you might\nhave come across.\nFigure 8-14. Examples of memes [35]\nMemes and Fake News \n| \n299\n",
        "word_count": 300,
        "char_count": 1779,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 601,
            "ext": "png",
            "size_bytes": 1062540
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 330,
        "text": "Before we cover some key methods for understanding the trends in memes, let’s dis‐\ncuss why it’s important to understand these trends. Misuse of trolling memes in a live\nfeed of a professional network platform like LinkedIn may not be desirable. This is\nsimilar to groups on Facebook or Google that intend to spread awareness or informa‐\ntion related to official processes or group events (e.g., a Facebook page for a fund‐\nraiser event or a Google group for helping students applying to their graduate\nschool). Identifying content that could be a meme that’s heckling others or is other‐\nwise offensive or violating other group or platform rules is important. There are two\nprimary ways in which a meme could be identified:\nContent-based\nContent-based meme identification uses content to match with other memes of\nsimilar patterns that have already been identified. For example, in a community,\nit has been identified that “This is Bill. Be like Bill” (Figure 8-14) has emerged as\na meme. To identify if a new post belongs to the same template, we can extract\nthe text and use a similarity metric like Jaccard distance to identify problematic\ncontent. In this way, it’s possible to identify memes of this pattern: “This is Per‐\nsonX. Be like PersonX.” In our running example, even a regular expression would\nbe able to identify such templates from a new post.\nBehavior-based\nBehavior-based meme identification is done mainly using the activity on the\npost. Studies have shown that the sharing behavior of a meme changes drastically\nfrom its inception to later hours. Usually, viral content can be identified by ana‐\nlyzing the number of shares, comments, likes for a particular post. In general,\nthese numbers often go beyond the average metrics for other non-meme posts.\nThis is more in the realm of anomaly detection. An interested reader can read the\nsurvey of such methods studied extensively on the Facebook network [34].\nNow that we’ve discussed the basic definition of memes in the context of social media\nand briefly touched on how to identify or measure their effects, we’ll now move to\nanother important and pressing issue in social media: fake news.\nFake News\nIn the last few years, fake news on social platforms has become a huge issue. The\nnumber of incidents related to fake news has risen significantly along with the rise in\nusers on social platforms. This consists of users both creating fake content and con‐\nstantly sharing it on social networks to make it viral. In this section, we’ll take a look\nat how we can detect fake news using the NLP techniques we’ve learned so far.\nLet’s look at an example of such fake news: “Lottery Winner Arrested for Dumping\n$200,000 of Manure on Ex-Boss’ Lawn.” [36] This got over 2.3 million Facebook\nshares in 2018 [37].\n300 \n| \nChapter 8: Social Media\n",
        "word_count": 475,
        "char_count": 2809,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 331,
        "text": "Various media houses and content moderators are actively working on detecting and\nweeding out such fake news. There are some principled approaches that can be used\nto tackle this menace:\n1. Fact verification using external data sources: Fact verification deals with validating\nvarious facts in a news article. It can be treated as a language understanding task\nwhere, given a sentence and a set of facts, a system needs to find out if the set of\nfacts supports the claim or not.\nConsider we have access to external data sources, such as Wikipedia, where we\nassume the facts have been entered correctly. Now, given a piece of news text,\nsuch as, “Einstein was born in 2000,” we should be able to verify it using data\nsources consisting of facts. Note that, at the beginning, we don’t know which\npiece of information could be wrong, so this cannot be solved trivially just by\npattern matching.\nAmazon Research at Cambridge created a curated dataset to deal with such cases\nof misinformation present in natural text [38]. The dataset consists of examples\nthat look like:\n{\n    \"id\": 78526,\n    \"label\": \"REFUTES\",\n    \"claim\": \"Lorelai Gilmore's father is named Robert.\",\n    \"attack\": \"Entity replacement\",\n    \"evidence\": [\n        [\n            [<annotation_id>, <evidence_id>, \"Lorelai_Gilmore\", 3]\n        ]\n    ]\n}\nAs you might be able to see, we can develop a model that takes the {claim, evi\ndence} as an input and produces the label REFUTES. This is more of a classifica‐\ntion task with three labels: AGREES, REFUTES, and NONE. The evidence set contains\nthe Wikipedia URL of the related entities of the sentence, and 3 denotes the sen‐\ntence that has the correct fact in the corresponding Wikipedia article.\nA similar dataset could be built by individual media houses to extract knowledge\nfrom existing articles related to their domains. For example, a sports news com‐\npany might build a set primarily containing facts related to sports.\nWe can use BoW-based methods to represent both the claim and the evidence\nand pass them as a pair through a logistic regression to obtain a classification\nlabel. More advanced techniques include using DL methods such as LSTM or\npre-trained BERT to obtain encodings of these inputs. We can then concatenate\nthese embeddings and pass it to a neural network to classify the claim. An inter‐\nested reader can look at [39, 40, 41].\nMemes and Fake News \n| \n301\n",
        "word_count": 395,
        "char_count": 2400,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Regular (8.9pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 332,
        "text": "2. Classifying fake versus real: A simple setup for this problem would be to build a\nparallel data corpus with instances of fake and real news excerpts and classify\nthem as real or fake. While the setup is simple, it could be very hard for a\nmachine to solve this task reasonably well due to the fact that people may use\nvarious linguistic nuances to confuse the machine in flagging fake content.\nResearchers from Harvard recently developed a system [42] to identify which\ntext is written by humans and which text is generated by machines (and therefore\ncould be fake). This system uses statistical methods to understand the facts and\nuses the fact that, when generating text, machines tend to use generic and com‐\nmon words. This is different from humans, who tend to use words that are more\nspecific and adhere to an individual’s writing style. Their methods show that\nthere could often be a clear distinction in the statistical properties of word usage\nthat can be used to distinguish fake text from real text. We encourage readers to\nlook into the work of Sebastian Gehrmann et al. [42, 43] for a complete under‐\nstanding of the method.\nA similar technique was used by the AllenNLP team to develop a tool called\nGrover [44], which uses an ML model to generate text that looks human-written.\nThey exploit the nuances present in the text generated to understand the quirks\nand attributes, which can then be used to build a system that helps in detecting\npotentially fake, machine-generated articles. We encourage you to play with the\ndemo [44] that’s been open sourced by the team to understand its mechanism.\nWe discussed two critical issues in social media—memes and fake news—and pro‐\nvided a quick survey of how to detect them. We also discussed how we can pose these\nproblems as a simple natural language understanding task (such as classification) and\nwhat a potential dataset to solve that task might look like. This section should give\nyou a good starting point to build systems that can identify malicious or fake content\npresent in social media.\nWrapping Up\nIn this chapter, we started with an overview of the various applications of NLP in\nsocial media and discussed some of the unique challenges social media text data poses\nto traditional NLP methods. We then took a detailed look at different NLP applica‐\ntions, such as building word clouds, detecting trending topics on Twitter, under‐\nstanding tweet sentiment, customer support on social media, and detecting memes\nand fake news. We also saw a range of text processing issues we might encounter\nwhile developing these tools and how to solve them. We hope this gave you a good\nunderstanding of how to apply NLP techniques on SMTD and solve an NLP problem\ndealing with social media text data you may encounter in your workplace. Let’s now\nmove on to the next chapter, where we’ll address another vertical where NLP has pro‐\nven to be very useful: e-commerce.\n302 \n| \nChapter 8: Social Media\n",
        "word_count": 507,
        "char_count": 2958,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 333,
        "text": "References\n[1] Twitter. Quarterly results: 2019 Fourth quarter. Last accessed June 15, 2020.\n[2] Internet Live Stats. “Twitter Usage Statistics”. Last accessed June 15, 2020.\n[3] Zephoria Digital Marketing. “The Top 20 Valuable Facebook Statistics–Updated\nMay 2020”.\n[4] Lewis, Lori. “This Is What Happens In An Internet Minute”. March 5, 2019.\n[5] Choudhury, Monojit. “CS60017 - Social Computing, Indian Institute of Technol‐\nogy Kharagpur, Lecture 1: NLP for Social Media: What, Why and How?”. Last\naccessed June 15, 2020.\n[6] Ritter, Alan, Sam Clark, and Oren Etzioni. “Named Entity Recognition in Tweets:\nAn Experimental Study.” Proceedings of the 2011 Conference on Empirical Methods in\nNatural Language Processing (2011): 1524–1534.\n[7] Barman, Utsab, Amitava Das, Joachim Wagner, and Jennifer Foster. “Code Mix‐\ning: A Challenge for Language Identification in the Language of Social Media.” Pro‐\nceedings of the First Workshop on Computational Approaches to Code Switching (2014):\n13–23.\n[8] Gupta, Anuj, Saurabh Arora, Satyam Saxena, and Navaneethan Santhanam. “Con‐\ntinuous Learning Systems: Building ML systems that learn from their mistakes”. Open\nData Science Conference (2019).\n[9] Mueller, Andreas. word_cloud: A little word cloud generator in Python, (GitHub\nrepo). Last accessed June 15, 2020.\n[10] Mueller, Andreas. “Gallery of Examples”. Last accessed June 15, 2020.\n[11] Ritter, Allen. “Twokenize”. Last accessed June 15, 2020.\n[12] Ritter, Allen. “OSU Twitter NLP Tools”. Last accessed June 15, 2020.\n[13] Noah’s ARK lab. “Tweet NLP”. Last accessed June 15, 2020.\n[14] Natural Language Toolkit. TweetTokenizer. Last accessed June 15, 2020.\n[15] Routar de Sousa, J. Guilherme. Twikenizer. Last accessed June 15, 2020.\n[16] Twitter’s Trending Topics. Last accessed June 15, 2020.\n[17] Tweepy, an easy-to-use Python library for accessing the Twitter API. Last\naccessed June 15, 2020.\n[18] Twitter. Enterprise Data: Unleash the Power of Twitter Data. Last accessed June\n15, 2020.\nWrapping Up \n| \n303\n",
        "word_count": 301,
        "char_count": 2016,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 334,
        "text": "[19] Wexler, Steve. “How to Visualize Sentiment and Inclination”. Tableau (blog), Jan‐\nuary 14, 2016.\n[20] Kaggle. UMICH SI650—Sentiment Classification. Last accessed June 15, 2020.\n[21] Sanders Twitter sentiment corpus, (GitHub repo). Last accessed June 15, 2020.\n[22] Loria, Steven. “TextBlob: Simple, Pythonic, text processing––Sentiment analysis,\npart-of-speech tagging, noun phrase extraction, translation, and more”. Last accessed\nJune 15, 2020.\n[23] Beautiful Soup. Last accessed June 15, 2020.\n[24] Solomon, Brad. Demoji. Last accessed June 15, 2020.\n[25] Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. “GloVe:\nGlobal Vectors for Word Representation”. Last accessed June 15, 2020.\n[26] The Stanford Natural Language Procesisng Group. “Pre-trained GloVe embed‐\ndings from Tweets”. Last accessed June 15, 2020.\n[27] Dhingra, Bhuwan, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl, and William\nW. Cohen. “Tweet2Vec: Character-Based Distributed Representations for Social\nMedia”. (2016).\n[28] Yang, Zhilin, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W. Cohen, and Rus‐\nlan Salakhutdinov. “Words or Characters? Fine-grained Gating for Reading Compre‐\nhension”. (2016).\n[29] Kuru, Onur, Ozan Arkan Can, and Deniz Yuret. “CharNER: Character-Level\nNamed Entity Recognition.” Proceedings of COLING 2016, the 26th International Con‐\nference on Computational Linguistics: Technical Papers (2016): 911–921.\n[30] Godin, Fredric. “Twitter word embeddings” and \"TwitterEmbeddings”. Last\naccessed June 15, 2020.\n[31] Lull, Travis. “Announcing new customer support features for businesses”. Twitter\n(blog), September 15, 2016; Facebook Help Center. “How does my Facebook Page get\nthe ‘Very responsive to messages’ badge?”; Facebook Help Center. “How are response\nrate and response time defined for my Page?”.\n[32] Apple’s and Bank of America’s support handles on Twitter: https://twitter.com/\nAppleSupport and https://twitter.com/BofA_Help. Last accessed June 15, 2020.\n[33] Rogers, Kara. “Meme: Cultural Concept”. Encyclopedia Britannica. Last modified\nMarch 5, 2020.\n[34] Adamic, Lada A., Thomas M. Lento, Eytan Adar, and Pauline C. Ng. “Informa‐\ntion Evolution in Social Networks.” Proceedings of the Ninth ACM International Con‐\nference on Web Search and Data Mining (2016): 473–482.\n304 \n| \nChapter 8: Social Media\n",
        "word_count": 321,
        "char_count": 2328,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 335,
        "text": "[35] Popsugar Tech. Last accessed June 15, 2020.\n[36] “Lottery Winner Arrested for Dumping $200,000 of Manure on Ex-Boss’ Lawn”.\nWorld News Daily Report. Last accessed June 15, 2020.\n[37] Silverman, Craig. “Publishers Are Switching Domain Names to Try and Stay\nAhead of Facebook’s Algorithm Changes”. BuzzFeed News, March 1, 2018.\n[38] Thorne, James, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mit‐\ntal. “FEVER: a large-scale dataset for Fact Extraction and VERification”, (2018).\n[39] Hassan, Naeemul, Bill Adair, James T. Hamilton, Chengkai Li, Mark Tremayne,\nJun Yang, and Cong Yu. “The Quest to Automate Fact Checking.” Proceedings of the\n2015 Computation+ Journalism Symposium (2015).\n[40] Graves, Lucas. “Understanding the Promise and Limits of Automated Fact-\nChecking.” Reuters Institute, February 28, 2018.\n[41] Karadzhov, Georgi, Preslav Nakov, Lluís Màrquez, Alberto Barrón-Cedeño, and\nIvan Koychev. “Fully Automated Fact Checking Using External Sources.” Proceedings\nof the International Conference Recent Advances in Natural Language Processing\n(2017).\n[42] Strobelt, Hendrik and Sebastian Gehrmann. “Catching a Unicorn with GLTR: A\nTool to Detect Automatically Generated Text”. Last accessed June 15, 2020.\n[43] Gehrmann, Sebastian, Hendrik Strobelt, and Alexander M. Rush. “GLTR: Statis‐\ntical Detection and Visualization of Generated Text”, (2019).\n[44] Allen Institute for AI. “Grover: A State-of-the-Art Defense against Neural Fake\nNews”. Last accessed June 15, 2020.\nWrapping Up \n| \n305\n",
        "word_count": 215,
        "char_count": 1519,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 336,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 337,
        "text": "CHAPTER 9\nE-Commerce and Retail\nToday’s new marketplaces must nurture\nand manage perfect competition to thrive.\n—Jeff Jordan, Andreessen Horowitz\nIn today’s world, e-commerce has become synonymous with shopping. An enriched\ncustomer experience compared to what a physical retail store offers has fueled this\ngrowth of e-commerce. Worldwide retail e-commerce sales in 2019 were $3.5 trillion\nand are projected to reach $6.5 trillion by 2022 [1]. Recent advancements in ML and\nNLP have played a major role in this rapid growth.\nVisit the home page of any e-retailer, and you’ll find a lot of information in the form\nof text and images. A significant portion of this information consists of text in the\nform of product descriptions, reviews, etc. Retailers strive to utilize this information\nintelligently to deliver customer delight and build competitive advantage. An e-\ncommerce portal faces a range of text-related problems that can be solved by NLP\ntechniques. We saw different kinds of NLP problems and solutions in the previous\nsection (Chapters 4 through 7). In this chapter, we’ll give an overview of how NLP\nproblems in the e-commerce domain can be addressed using what we’ve learned in\nthis book so far. We’ll discuss some of the key NLP tasks in this domain, including\nsearch, \nbuilding \na \nproduct \ncatalog, \ncollecting \nreviews, \nand \nproviding\nrecommendations.\nFigure 9-1 shows some of these e-commerce tasks. Let’s start with an overview of\nthem.\n307\n",
        "word_count": 233,
        "char_count": 1464,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (9.3pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 338,
        "text": "Figure 9-1. NLP applications in e-commerce\nE-Commerce Catalog\nAny large e-commerce enterprise needs an easy-to-access product catalog. A product\ncatalog is a database of the products that the enterprise deals or a user can purchase.\nThis contains product description attributes as well as images for each product. Bet‐\nter product descriptions with relevant information help the customer choose the right\nproduct through the catalog. Such information can also help in product search and\nrecommendations. Imagine a recommendation engine that automatically knows that\nyou like the color blue! That’s certainly not possible unless and until the engine noti‐\nces that most of your recent purchases or searches were on apparel of the color blue.\nThe first thing needed to achieve this is identifying that “blue” is associated with the\nproducts as a color attribute. Extracting such information automatically is called\nattribute extraction. Attribute extraction from product descriptions can guarantee\nthat all the relevant product information is properly indexed and displayed for each\nproduct, improving product discoverability.\nReview Analysis\nThe most notable part of an e-commerce platform is the user reviews section for\nevery product. Reviews provide a different perspective of the product that cannot be\nobtained from the product attributes alone, such as quality, usability, comparisons\nwith other products, and delivery feedback. All reviews may not be useful, or they\nmight not come from trusted users. Further, it’s hard to process multiple reviews for a\ngiven product manually. NLP techniques provide an overall perspective for all\nreviews by performing tasks such as sentiment analysis, review summarization, iden‐\ntifying review helpfulness, and so on. We saw one example of NLP for review analysis\nin Chapter 5 when we discussed keyphrase extraction. We’ll see other use cases later\nin this chapter.\n308 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 295,
        "char_count": 1951,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 859,
            "height": 500,
            "ext": "png",
            "size_bytes": 27188
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 339,
        "text": "Product Search\nSearch systems in e-commerce are different compared to general search engines like\nGoogle, Bing, and Yahoo. An e-commerce search engine is closely tied to the prod‐\nucts available and the different kinds of information associated with them. For\ninstance, in a regular search engine, we’re dealing largely with free-form text data\n(like news articles or blogs) as opposed to structured sales and review data for e-\ncommerce. We might search for “red checkered shirt for a wedding,” and the e-\ncommerce search engine should be able to fetch it. Similar forms of focused search\ncan also be seen on travel websites for flight and hotel bookings, such as Airbnb and\nTripAdvisor. The specific nature of the information associated with each type of e-\ncommerce business calls for a customized pipeline of information processing, extrac‐\ntion, and search.\nProduct Recommendations\nWithout a recommendation engine, any e-commerce platform would be incomplete.\nA customer likes when the platform intelligently understands their choices and sug‐\ngests products to buy next. It actually helps the customer organize their thoughts\nabout shopping and helps to achieve better utility. Recommendations of discounted\nitems, same-brand products, or products with favorite attributes can really engage the\ncustomer on the website and make them spend more time. This directly increases the\npossibility of the customers buying those products. In addition to transaction-based\nrecommendation facilities, there is a rich set of algorithms that are developed based\non product content information and reviews that are textual in nature. NLP is used to\nbuild such recommendation systems.\nWith this overview, we’re all set to explore the role of NLP in e-commerce in more\ndetail. Let’s start with how it’s used in building search for e-commerce.\nSearch in E-Commerce\nCustomers visit an e-commerce website to find and purchase their desired products\nquickly. Ideally, a search feature should enable the customer to reach the right prod‐\nuct with the least number of clicks. The search needs to be fast and precise and fetch\nresults that closely match customers’ needs. A good search mechanism positively\nimpacts the conversion rate, which directly impacts the revenue of the e-retailer.\nGlobally, on average, only 4.3% of user search attempts convert to a purchase. By\nsome estimates, 34% of results in search on the top 50 portals do not produce useful\nresults [2], and there’s often a large scope for improvement.\nIn Chapter 7, we discussed how general search engines work and where NLP is use‐\nful. However, for e-commerce, the search engine needs to be more fine-tuned to the\nbusiness needs. Search in e-commerce is closed domain—i.e., the search engine\nSearch in E-Commerce \n| \n309\n",
        "word_count": 438,
        "char_count": 2773,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 340,
        "text": "typically fetches items from within the product information, rather than from a\ngeneric set of documents or content on the open web (like Google or Bing). The\nunderlying product information is built on the product catalog, attributes, and\nreviews. Search works on different facets of this information, like color, style, or cate‐\ngory. This kind of search in e-commerce is generally called “faceted search,” which is\nthe focus of this section.\nFaceted search is a specialized variant of search that allows the customer to navigate\nin a streamlined way with filters. For example, if we’re planning to buy a TV, then we\nmight look for filters like brand, price, TV size, etc. In e-commerce websites, users are\npresented with a set of search filters depending on the product. Figures 9-2 and 9-3\nillustrate search in e-commerce through Amazon and Walmart.\nThe left-most section of both images depicts a set of filters (alternatively, “facets”) that\nallows the customer to guide their search in a way that matches their buying needs. In\nFigure 9-2, we see a search for television models, so the filters show aspects such as\nresolution and display size. Along with such custom filters, there are also some gen‐\neral features that are valid for many such product searches, such as brand, price\nrange, and mode of shipping, as shown in Figure 9-3. These filters are explicit dimen‐\nsions to perceive the product. And this guided search enables the user to arrange the\nsearch results on their own to get more control over shopping, rather than having to\nsift through a lot of results to get what they’re looking for.\nFigure 9-2. Faceted search on Amazon.com\n310 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 284,
        "char_count": 1691,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1258,
            "height": 723,
            "ext": "png",
            "size_bytes": 391534
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 341,
        "text": "Figure 9-3. Faceted search on Walmart.com\nThese filters are the key that defines the faceted search. However, they may not always\nbe readily available for all products. Some reasons for that are:\n• The seller didn’t upload all the required information while listing the product on\nthe e-commerce website. This is typically the case when a new e-commerce busi‐\nness ramps up and aggressively promotes quick onboarding of various sellers. To\nachieve this, they often allow the sellers to list without having quality checks in\nplace for the product metadata.\n• Some of the filters are difficult to obtain, or the seller may not have the complete\ninformation to provide—for example, the caloric value of a food product, which\nis typically derived from the nutrient information provided on the product case.\nE-retailers don’t expect this information to be provided by the seller, but it’s cru‐\ncial because it may capture important customer signals that are directly related to\nthe conversation of that product sale.\nFaceted search can be built with most popular search engine backends like Solr and\nElasticsearch. Besides regular text search, different facet attributes are also added to\nthe search query. Elasticsearch’s DSL also comes with a built-in faceted search\ninterface [3].\nSearch in E-Commerce \n| \n311\n",
        "word_count": 209,
        "char_count": 1308,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1442,
            "height": 839,
            "ext": "png",
            "size_bytes": 580196
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 342,
        "text": "In an e-commerce setting, we also need to account for business\nneeds other than relevance in terms of facets and text. For instance,\nproducts that are part of a promotion or sale may be bumped up in\nresults. This can be built by utilizing features like Elasticsearch\nboosting.\nApart from search algorithms, there are many nuances associated with faceted search,\nand we’ll focus on these for the rest of this chapter. The issues mentioned above relate\nto the problem we’ll discuss in the next section: building an e-commerce catalog.\nBuilding an E-Commerce Catalog\nAs we saw earlier in this chapter, building an informative catalog is one of the pri‐\nmary problems in e-commerce. It can be split into several subproblems:\n• Attribute extraction\n• Product categorization and taxonomy creation\n• Product enrichment\n• Product deduplication and matching\nLet’s take a look at each of these in this section.\nAttribute Extraction\nAttributes are properties that define a product. For example, in Figure 9-2, we saw\nbrand, resolution, TV size, etc., as relevant attributes. An accurate display of these\nattributes will provide a complete overview of the product on the e-commerce web‐\nsite so that the customer can make an informed choice. A rich set of attributes relates\ndirectly to the improvement of clicks and click-through rates, which influence the\nproduct’s sale. Figure 9-4 shows an example of a product description obtained by a set\nof filters or attributes.\nAs you can see, attributes like {clothing, color, size} are basically what defines this\nproduct to a customer. Each of these attributes can have multiple values, as shown in\nthe figure. In this example, color takes seven values. However, directly obtaining\nattributes from the sellers for all products is difficult. Moreover, the quality of the\nattributes should be consistent enough to allow a customer to have the correct and\nrelevant information about a product.\n312 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 318,
        "char_count": 1966,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (9.6pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 343,
        "text": "Figure 9-4. Product obtained by a set of filters or attributes\nTraditionally, e-commerce websites employed manual labeling or crowdsourcing\ntechniques to obtain the attributes. This is typically done by third-party companies or\ncrowdsourcing platforms (e.g., Mechanical Turk), where specific questions about\neach product are asked and the crowd workers are expected to answer them. Some‐\ntimes, the questions are framed in a multiple-choice way to restrict the answer into a\nset of values. But generally, it’s expensive and not scalable with the increase in the vol‐\nume of products. That’s where techniques from machine learning step in. This is a\nchallenging task because it requires an understanding of the context of the informa‐\ntion present in the product. For example, look at the two product descriptions shown\nin Figure 9-5.\nPink is a popular brand with younger women. Similarly, pink is a very common color\nof apparel. Hence, in the first case, Pink is a brand name attribute, whereas in the\nother case, pink is just a color. In Figure 9-5, we see that the backpack is from the\nbrand “Pink” with a color of neon red, whereas the sweatshirt is of the color pink.\nCases like these and many more are prevalent and pose a challenging task for a com‐\nputer to solve.\nBuilding an E-Commerce Catalog \n| \n313\n",
        "word_count": 220,
        "char_count": 1311,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 420,
            "height": 423,
            "ext": "png",
            "size_bytes": 104075
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 344,
        "text": "Figure 9-5. Cases where “pink” is the attribute value for two different attributes\nIf we can obtain a set of attributes in some structured data format, then the search\nmechanism can accurately utilize them to retrieve results according to customer\nneeds. The algorithms that extract the attribute information from various product\ndescriptions are generally called attribute extraction algorithms. These algorithms take\na collection of textual data as input and produce the attribute-value pairs as output.\nThere are two types of attribute extraction algorithms: direct and derived.\nDirect attribute extraction algorithms assume the presence of the attribute value in the\ninput text. For example, “Sony XBR49X900E 49-Inch 4K Ultra HD Smart LED TV\n(2017 Model)” contains the brand “Sony.” A brand is typically an attribute that’s\n314 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 132,
        "char_count": 869,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1442,
            "height": 1585,
            "ext": "png",
            "size_bytes": 1876710
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 345,
        "text": "expected to be present in the product title in most cases. On the other hand, derived\nattribute extraction algorithms do not assume that the attribute of interest is present in\nthe input text. They derive that information from the context. Gender is one such\nattribute that is usually not present in the product title, but from the input text, the\nalgorithm can identify if the product is specifically for men or women. Consider the\nproduct description: “YunJey Short Sleeve Round Neck Triple Color Block Stripe T-\nShirt Casual Blouse.” The product is for women, but the gender “female” is not explic‐\nitly mentioned in the product description or title. In this case, the gender has to be\ninferred from the text (for instance, from the product description).\nDirect attribute extraction\nTypically, the direct attribute extraction is modeled as a sequence-to-sequence label‐\ning problem. A sequence labeling model takes a sequence (e.g., of words) as input and\noutputs another sequence of the same length. In Chapter 5, we briefly touched on this\nkind of problem in the notebook on training a named entity recognizer. Following a\nsimilar approach, let’s take a look at how direct attribute extraction algorithms work.\nOur training data will be of the form shown in Figure 9-6, for an example product\ntitled, “The Green Pet Shop Self Cooling Dog Pad.”\nFigure 9-6. Training data format for direct attribute extraction\nHere, what we have to extract is “The Green Pet Shop,” which is indicated by the -\nattribute tags, whereas the rest of it is indicated by an O (Other) tag. Getting labeled\ndata in BIO is crucial for any direct attribute extraction process. We should also have\ndata that represents various categories (e.g., B-Attribute1, B-Attribute2, etc.).\nThere are two broad ways to collect this data. A simpler one would be to use regular\nexpressions on existing text descriptions with brands and attributes and use that data‐\nset. This is akin to weak supervision. We can also get a subset of the data labeled by\nhuman annotators. With such labeled data, a rich set of features needs to be extracted\nto train an ML model. Ideally, the input features should capture the attribute charac‐\nteristics and locational and contextual information. Here’s a list of some of the fea‐\ntures that can capture all three of these aspects. We can develop more complex\nfeatures along similar lines and perform analysis to understand if they’re significant\nin improving performance. Some common features for this task are:\nCharacteristic features\nThese are typically token-based features, such as the letter case of the token,\nlength of the token, and its character composition.\nBuilding an E-Commerce Catalog \n| \n315\n",
        "word_count": 445,
        "char_count": 2704,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1133,
            "height": 122,
            "ext": "png",
            "size_bytes": 12525
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 346,
        "text": "Locational features\nThese features capture the positional aspect of the token in the input sequence,\nsuch as the number of tokens before the given token or the ratio of the token\nposition and the total length of the sequence.\nContextual features\nThese features mostly encode information about the neighboring tokens, such as\nthe identity of the preceding/succeeding token, POS tag of the token, whether the\npreceding token is a conjunction, etc.\nOnce the features are generated and output tags are encoded properly, we get the\nsequence pairs for training the model. At this point, the training process is similar to\nthat of an NER system. Even though the pipeline looks simple and similar to NER\nsystems, there are challenges with these feature-generation schemes and modeling\ntechniques because of domain-specific knowledge. Further, it’s a challenge to obtain\nlarge enough datasets that cover a range of attributes.\nTo deal with such data sparsity and other feature incompleteness issues, some\napproaches suggest the use of a sequence of word embeddings in the input. The input\nsequence will be passed to the model as is, and it’s supposed to predict the output\nsequence. Recent efforts include deep recurrent structures like RNN or LSTM-CRF to\nperform the seq2seq labeling task [4]. We saw how word embeddings and RNNs are\nuseful in NLP in Chapters 3 and 4. This is another example of where such representa‐\ntions can be useful. Figure 9-7 shows an example of how one such DL model [5] per‐\nforms better than the typical ML models.\nFigure 9-7. Characteristic performance improvement in the LSTM framework for\nattribute extraction [5]\nIndirect attribute extraction\nIndirect attributes are attributes that are not directly mentioned in the description.\nThese attributes, however, can be inferred from other direct attributes or the overall\ndescription. For instance, gender- or age-specific words can be inferred from the text.\nA phrase like “Suit for your baby aged 1–5 years” implies that the product is for tod‐\ndlers. Due to the absence of explicit mentions, a sequence labeling approach won’t\nwork.\n316 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 346,
        "char_count": 2146,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 968,
            "height": 172,
            "ext": "png",
            "size_bytes": 23174
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 347,
        "text": "For indirect attribute classification, we use text classification, since instead of extract‐\ning information, we can infer high-level classes (i.e., indirect attributes) from the\noverall input. Recall the example of “YunJae Short Sleeve Round Neck Triple Color\nBlock Stripe T-Shirt Casual Blouse.” For this case, we represent the whole input string\nusing any of the sentence representation methods from Chapter 3. We can also create\nfeatures, such as the presence of class-specific words, character n-grams, and word n-\ngrams. Then, we can train a model to classify the input to an indirect attribute label.\nIn this example here, for the “gender” attribute, we should use men, women, unisex,\nand child as different class labels.\nFor the models that use deep recurrent structures, the amount of\ndata needed is typically much more than what’s needed when less-\ncomplex ML models such as CRF and HMM are used. The more\ndata there is, the better the deep models learn. This is common to\nall DL models, as we saw in earlier chapters, but for e-commerce,\ngetting a large set of well-sampled, annotated data is very expen‐\nsive. Hence, it needs to be taken care of before we to build any\nsophisticated models.\nSo far, we’ve discussed attribute extraction from textual data and the various recent\napproaches that extend this to multimodal attribute extraction, incorporating various\nmodalities such as title, description, image, reviews, etc., about the product [6].\nIn the next sections, we’ll talk about expanding techniques similar to the ones we\napplied to product attributes to other facets of e-commerce and retail.\nProduct Categorization and Taxonomy\nProduct categorization is a process of dividing products into groups. These groups\ncan be defined based on similarity—e.g., products of the same brand or products of\nthe same type can be grouped together. Generally, e-commerce has pre-defined broad\ncategories of products, such as electronics, personal care products, and foods. When a\nnew product arrives, it should be categorized into the taxonomy before it’s put in the\ncatalog. Figure 9-8 shows a taxonomy for the electronics category with a hierarchy of\ngranular subcategories.\nWe can further define successively smaller groups with stricter definitions of prod‐\nucts, such as laptops and tablets inside the computer category. For a more contextual\nexample, this book will have a level category of technical books, while it’s subcatego‐\nries will be related to AI or natural language processing. This task is a lot like the text\nclassification we covered in Chapter 4.\nBuilding an E-Commerce Catalog \n| \n317\n",
        "word_count": 416,
        "char_count": 2613,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 348,
        "text": "Figure 9-8. A typical category hierarchy—taxonomy of a product\nA good taxonomy and properly linked products can be critical because it allows an e-\ncommerce site to:\n• Show products similar to the product searched\n• Provide better recommendations\n• Select appropriate bundles of products for better deals for the customer\n• Replace old products with new ones\n• Show price comparisons of different products in the same category\nThis categorization process is typically manual to start at small scale, but as the vari‐\nety of products increases, it gets harder and harder to process them manually. At\nscale, this categorization is typically posed as a classification task where the algorithm\ntakes information from a variety of sources and applies the classification technique to\nsolve it [7, 8].\nSpecifically, there are cases where algorithms take input as the title or description and\nclassify the product into a suitable category when all the categories are known. This\nagain falls into the typical case of text classification. In this way, the categorization\nprocess can be automated. Once the category is determined, it’s extended directly to\n318 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 190,
        "char_count": 1187,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1260,
            "height": 1043,
            "ext": "png",
            "size_bytes": 82267
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 349,
        "text": "the relevant attribute extraction process that we discussed earlier. It’s logical that a\nproduct will be passed to the attribute extraction process only when its category is\ndiscovered.\nThe accuracy of the algorithm can be improved when both images and text can be\nused to solve the problem. Images can be passed to a convolutional neural network\nfor generating image embedding, and the text sequence can be encoded via LSTM,\nboth of which, in turn, can be concatenated and passed to any classifier for the final\noutput [6].\nBuilding a taxonomy tree is an extensive process. Placing the products at the right\nlevel in the taxonomy can be done via a hierarchical text classification. A hierarchical\ntext classification in context is nothing more than applying classification models in\nhierarchy according to levels in a taxonomy.\nGenerally simple rule-based classification methods are used mainly for the high-level\ncategories. They can use a dictionary-based matching as a start. Subcategories that are\ncomplex and require deeper context to determine the right taxonomic level are dealt\nwith by ML classification techniques such as SVM or decision tree [9]. Figure 9-9\nshows various taxonomy levels for a specific product example.\nFigure 9-9. Taxonomy tree with different levels [9]\nBuilding an E-Commerce Catalog \n| \n319\n",
        "word_count": 209,
        "char_count": 1322,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1080,
            "height": 956,
            "ext": "png",
            "size_bytes": 52649
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 350,
        "text": "For a new e-commerce platform, creating a product taxonomy via product categori‐\nzation can be an insurmountable task. Building rich content requires a huge amount\nof relevant data, manual interventions, and category experts’ domain knowledge. All\nthese can be expensive for a nascent e-commerce platform. However, there are some\nAPIs offered by Semantics3, eBay, and Lucidworks that can help with the process.\nThese APIs typically build on large catalog content of various big retailers and pro‐\nvide the intelligence inside to categorize a product by scanning its unique product\ncode. Small-scale e-commerce can use the power of such cloud APIs for bootstrap‐\nping taxonomy creation and categorization. Figure 9-10 shows a snapshot of one such\nAPI from Semantics3 [10]. Their API helps categorize a product from its name.\nFigure 9-10. Semantics3 terminal snapshot\n320 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 140,
        "char_count": 907,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 720,
            "height": 762,
            "ext": "png",
            "size_bytes": 65958
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 351,
        "text": "Once a significant amount of product information has been gathered, it’s advisable to\nuse custom rule-based systems. Some of these APIs also support user-defined rules,\nas well as product enrichment and deduplication, which we’ll cover in the next\nsections.\nProduct Enrichment\nFor better search and recommendations, it’s important to gather richer product infor‐\nmation. Some potential sources of this information are short and long titles, product\nimages, and product descriptions. But this information is often either incorrect or\nincomplete. For example, a misleading title can hamper the faceted search in an e-\ncommerce platform. Improving a product title will not only improve the click-\nthrough rate in search, but also the conversion rate in terms of product purchase.\nIn the example shown in Figure 9-11, the product title is too long and contains words\nlike iPad, iPhone, and Samsung, which can easily mislead the search. The full title is\n“Stylus Pen LIBERRWAY 10 Pack of Pink Purple Black Green Silver Stylus Universal\nTouch Screen Capacitive Stylus for Kindle Touch ipad iphone 6/6s 6Plus 6s Plus Sam‐\nsung S5 S6 S7 Edge S8 Plus Note.” This text is too complicated even for a human to\nparse and make sense of, let alone a machine. Such cases are ideal for product\nenrichment.\nFigure 9-11. Example of a clumsy product title and an ideal case for product enrichment\nFirst, we’ll go through the problem scenario shown in Figure 9-11. When different\ntaxonomic and enrichment levels are filled, at least to an acceptable threshold (typi‐\ncally defined by the retail platform itself), then we can attempt to make the product\ntitle more expressive and accurate.\nBuilding an E-Commerce Catalog \n| \n321\n",
        "word_count": 277,
        "char_count": 1707,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1442,
            "height": 642,
            "ext": "png",
            "size_bytes": 450246
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 352,
        "text": "The process can start with direct string matching. It’s also necessary to filter out\ntokens that are not part of the product’s attribute values. In the example, the product\nis a stylus, and iPad and iPhone are not part of its attribute values. These tokens are\nmisleading and can affect the quality of faceted search. Hence, such tokens should be\nremoved from the product title, unless they’re important to indicate domain-specific\ncontext for the product.\nIdeally, a pre-defined template for the product titles helps maintain consistency\nacross products. A good approach is to build a template composed of attributes from\nthe taxonomy tree. The product category or type could be the first token in the prod‐\nuct title—e.g., “iPad” or “Macbook.” That will follow lower-level or granular attributes\nfrom the taxonomy tree, such as brand, size, color, etc. So, the combined title would\nbe: “iPad 64GB - Space Grey.” Attributes from the leaf of the taxonomy can be omit‐\nted to keep the product title simple.\nProduct enrichment is typically seen as a larger and more continuous process than\njust improving product titles in any online retail setup. Apart from taxonomic levels,\nthere are other ways to define the enrichment levels. Most of them are based on the\nimportance of the attribute information. [9] has defined these taxonomies, shown in\nFigure 9-12. Mandatory attributes are part of every product, while nice-to-have\nattributes provide a high level of detail that can be missing.\nFigure 9-12. Table showing the categorization of various enrichment levels [9]\nNext, we’ll turn our attention to product duplication and matching.\n322 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 270,
        "char_count": 1674,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 764,
            "height": 301,
            "ext": "png",
            "size_bytes": 24687
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 353,
        "text": "Product Deduplication and Matching\nProducts are often added to the platform by third-party sellers, and different sellers\ncan refer to the same product by different names. They seldom follow the same ter‐\nminology, which can result in the same product getting listed with multiple titles and\nproduct images. For example, “Garmin nuvi 2699LMTHD GPS Device” and “nuvi\n2699LMTHD Automobile Portable GPS Navigator” refer to the same product.\nIn addition to product categorization and attribute extraction, product deduplication\nis also an important aspect of e-commerce. Identifying duplicate products is also a\nchallenging task, and we’ll discuss ways to handle this problem via attribute match,\ntitle match, and image match.\nAttribute match\nIf two products are the same, then the values of various attributes must be the same.\nHence, once the attributes are extracted, we compare values for attributes for both of\nthe products in question. Ideally, maximum overlap of the attributes will indicate\nstrong product matching. In order to match the attribute values, we can use string\nmatching [11]. Two strings can be matched via exact character match or using string\nsimilarity metrics. String similarity metrics are typically built to take care of slight\nspelling mistakes, abbreviations, etc.\nAbbreviations are a big problem in product-related data. The same word can be rep‐\nresented in multiple accepted abbreviations. They should be mapped to a consistent\nform (discussed in “Product Enrichment” on page 321) or form agnostic rules formu‐\nlated to tackle the problem. An intuitive rule to tackle abbreviations while matching\ntwo words could be matching the first and last characters and checking whether those\ncharacters belong to the shorter or longer word.\nTitle match\nOne product can often have multiple title variants. Below are some title variants for\nthe same GPS navigator, sold by different sellers:\n• Garmin nuvi 2699LMTHD GPS Device\n• nuvi 2699LMTHD Automobile Portable GPS Navigator\n• Garmin nuvi 2699LMTHD — GPS navigator — automotive 6.1 in\n• Garmin Nuvi 2699lmthd Gps Device\n• Garmin nuvi 2699LMT HD 6” GPS with Lifetime Maps and HD Traffic\n(010–01188–00)\nTo retrieve all such instances, a matching mechanism is needed to identify them as\nthe same. A simple method could be to compare bigrams and trigrams among these\nBuilding an E-Commerce Catalog \n| \n323\n",
        "word_count": 373,
        "char_count": 2371,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 354,
        "text": "titles. It’s also possible to generate title-level features (such as counts of common\nbigrams and trigrams) and then calculate the Euclidean distance between them. We\ncould use sentence-level embedding and a pair of textual phrases simultaneously to\nlearn a distance metric that improves matching accuracy [12]. This can also be done\nwith a neural network architecture called the Siamese network [13]. The Siamese\nnetwork takes two sequences simultaneously and learns to generate the embeddings\nin such a way that, if the sequences are similar, they appear closer to each other in the\nembedding space, else farther.\nImage match\nFinally, there could still be irregularities (e.g., abbreviations or domain-specific word\nusage) in attributes and titles, which are difficult to align with one another. In those\ncases, product images can serve as rich source information for product matching and\ndeduplication. For image matching, pixel-to-pixel match, feature map matching, or\neven advanced image-matching techniques like Siamese networks are popular [14],\nand when applied in this setting can reduce the amount of product duplication. Most\nof the algorithms are based on the principles of computer vision approaches and\ndepend on image quality and other size-related particulars.\nA/B testing is a good method of measuring the results and effec‐\ntiveness of different algorithms in the e-commerce world. For pro‐\ncedures like attribute extraction, product enrichment and A/B\ntesting different models will lead to an impact on business metrics.\nThese metrics can be direct or indirect sales, click-through rates,\ntime spent on one web page, etc., and an improvement in relevant\nmetrics shows that a model works better.\nIn a practical setting, all these algorithms are used in conjunction, and their results\nare combined to deduplicate the products. In the next few sections, we’ll discuss NLP\nfor analyzing product reviews, which are a fundamental part of any online shopping\nexperience.\nReview Analysis\nReviews are an integral part of any e-commerce portal. They capture direct feedback\nfrom customers about products. It’s important to leverage this abundant information\nand create important signals to send feedback to the e-commerce system so that it can\nuse them to further improve the customer experience. Moreover, reviews can be\nviewed by all customers, and they directly affect the sales of the products. In this sec‐\ntion, we’ll delve deeper into the different facets of review sentiment analysis.\n324 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 392,
        "char_count": 2543,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (9.6pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 355,
        "text": "Sentiment Analysis\nWe covered generic sentiment analysis as a classification task in Chapter 4. But there\nare various nuances when it comes to sentiment analysis for e-commerce reviews.\nFigure 9-13 shows a screenshot of customer reviews of iPhone X on Amazon. Most of\nus are familiar with seeing such aspect-level reviews on e-commerce websites—this is\nwhere you can slice and dice reviews based on aspects and attributes.\nFigure 9-13. Analysis of customer reviews: ratings, keywords, and sentiments\nAs you can see, 67% of the reviews have a rating of five stars (i.e., the highest), and\n22% of the reviews have the lowest rating of one star. It’s important for an e-\ncommerce company to know what leads customers to give bad ratings. To illustrate\nthis point, Figure 9-14 shows two examples of extreme reviews of the same product.\nReview Analysis \n| \n325\n",
        "word_count": 142,
        "char_count": 856,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1118,
            "height": 832,
            "ext": "png",
            "size_bytes": 64916
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 356,
        "text": "Figure 9-14. A positive and a negative review\nCertainly, both of these reviews contain some information about the product, which\ngives the retailer cues about what customers are thinking. Specifically, negative\nreviews are more important to understand. In Figure 9-14, look at the first review\nwhere the customer states that there are issues with phones that are being shipped. It’s\nmostly related to the defective screen, which the retailer should take care of. In\ncontrast, the positive review expresses generic positive sentiment rather than explic‐\nitly pointing out what aspects the user really liked. Hence, it’s crucial to have a full\nunderstanding of the reviews. By nature, they’re in the text and mostly in an unstruc‐\ntured format, full of unforced errors such as spelling mistakes, incorrect sentence\nconstructions, incomplete words, and abbreviations. This makes review analysis even\nmore challenging.\nTypically, a review contains more than one sentence. It’s advisable\nto break a review into sentences and pass each sentence as one data\npoint. This is also relevant for sentence-wise aspect tagging, aspect-\nwise sentiment analysis, etc.\nRatings are considered to be directly proportional to the overall sentiment of the\nreviews. There are cases where the user mistakenly rates the product poorly but gives\na positive review. Understanding emotions directly from the text will help retailers\nrectify these anomalies during analysis. But in most cases, a review doesn’t talk about\njust one aspect of the product but tries to cover most aspects of it, ultimately reflect‐\ning everything in the review rating.\nTake another look at the iPhone X review screenshot in Figure 9-13. Look at the sec‐\ntion where it reads: “Read reviews that mention.” These are nothing but the important\nkeywords Amazon has found may help customers navigate better when skimming\nthrough the reviews. This clearly indicates that there are certain aspects customers are\ntalking about. It could be user experience, manufacturing aspects, price, or something\nelse. How can we know what the customer’s emotions or feedback are? So far, we’ve\nprovided only a high-level index of emotion for the entire review, but that won’t allow\nus to dig down deeper to understand it better. This necessitates an aspect-level under‐\nstanding of the reviews. These aspects could be pre-defined or extracted from the\n326 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 382,
        "char_count": 2424,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 394,
            "height": 514,
            "ext": "png",
            "size_bytes": 7986
          },
          {
            "index": 1,
            "width": 1442,
            "height": 316,
            "ext": "png",
            "size_bytes": 107095
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 357,
        "text": "review data itself. Based on that, the approaches will be supervised or unsupervised\naccordingly.\nAspect-Level Sentiment Analysis\nBefore we start the discussion of various techniques for aspect-level sentiment analy‐\nsis, we need to understand what an aspect is. An aspect is a semantically rich,\nconcept-centric collection of words that indicates certain properties or characteristics\nof the product. For example, in Figure 9-15, we’ll see the kind of aspects a travel web‐\nsite might have: location, value, and cleanliness.\nThis isn’t constrained only to the inherent attributes of the product, but also to any‐\nthing and everything related to the supply, presentation, delivery, return, quality, etc.,\naround the product. Typically, a clear distinguishing of these aspects is difficult unless\nalready assumed.\nIf the retailer has a clear understanding of the product’s aspects, then finding aspects\nfalls under the supervised category of algorithms. There’s a common technique for\nusing seed words or seed lexicons, which essentially hints at the crucial tokens that\ncould be present under a particular aspect. For example, regarding user experience as\nan aspect for iPhone X, seed words could be screen resolution, touch, response time,\netc. Again, it’s up to the retailer at what level of granularity they’d like to operate. For\nexample, screen quality alone could be a more granular aspect. In the next sections,\nwe’ll look at supervised and unsupervised techniques of aspect-level sentiment\nanalysis.\nSupervised approach\nA supervised approach depends mainly on seed words. It tries to identify the pres‐\nence of these seed words in a sentence. If it identifies a particular seed word in a sen‐\ntence, it tags the sentence with the corresponding aspect. Once all the sentences are\ntagged to any of the aspects, the sentiment analysis has to be done at a sentence level.\nNow, since we already have an additional tag for each sentence, sentences having one\ntag can be filtered, and sentiments for them can be aggregated to understand the cus‐\ntomer’s feedback for that aspect. For example, all review sentences related to screen\nquality, touch, and response time can be grouped together.\nFor a change, let’s look at an example from a travel website in Figure 9-15, where the\naspect-level sentiment analysis is apparent. As you see, there are specific ratings for\nlocation, check-in, value, and cleanliness, which are semantic concepts rightfully\nextracted from the data to present a more detailed view of the reviews.\nReview Analysis \n| \n327\n",
        "word_count": 402,
        "char_count": 2546,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 358,
        "text": "Figure 9-15. Aspect-level ratings on reviews given on a travel website\nUnsupervised approach\nAs it’s understood, arranging a good-quality seed lexicon is difficult, so there are\nunsupervised ways of detecting aspects. Topic modeling is a useful technique in iden‐\ntifying latent topics present in a document. We can think of these topics as aspects in\nour case. Imagine if we can group sentences that are talking about the same aspect.\nThat’s exactly what a topic modeling algorithm does. One of the most popular topic\nmodeling approaches is the latent Dirichlet algorithm (LDA). We covered LDA in\nmore detail in Chapter 7.\n328 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 108,
        "char_count": 665,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1344,
            "height": 1554,
            "ext": "png",
            "size_bytes": 199842
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 359,
        "text": "In a similar fashion, we can pre-define the number of aspects we expect out of the set\nof sentences. The topic modeling algorithm also outputs the probability of each word\nto be in all the topics (here, aspects). Hence, it’s also possible to group words that have\na high chance of belonging to a certain aspect and call them characteristic words for\nthat particular aspect. This will ultimately help annotate the unannotated aspects.\nFurther, a more unsupervised approach can be performed by creating sentence repre‐\nsentation and then performing clustering as opposed to LDA. In our experience, the\nlatter sometimes gives better results when there are fewer review sentences. In the\nnext section, we’ll see how we can predict ratings for all of these aspects and provide a\nmore granular view of user preferences.\nConnecting Overall Ratings to Aspects\nWe’ve already seen how we can detect the sentiment for each aspect. Typically, users\nalso give an overall rating. The idea here is to connect that rating to individual aspect-\nlevel sentiment. For this, we use a technique called latent rating regression analysis\n(LARA) [15]. Details of LARA implementation are outside the scope of this book, but\nhere’s an example of the system generating aspect-level ratings for a hotel review. The\ntable shown in Figure 9-16 from [15] gives some details on these aspect-based ratings.\nFigure 9-16. Aspect-wise sentiment prediction using LARA\nReview Analysis \n| \n329\n",
        "word_count": 235,
        "char_count": 1455,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1359,
            "height": 1053,
            "ext": "png",
            "size_bytes": 86772
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 360,
        "text": "We can assume that the final rating is nothing but a weighted combination of individ‐\nual aspect-level sentiments. The objective will be estimating the weights and the\naspect-level sentiment together. It’s also possible to perform these two operations\nsequentially—i.e., first determining the aspect-level sentiment and then the weights.\nThese weights on top of various sentiments present for each aspect will ultimately\nindicate how much importance a reviewer places on that specific topic. It’s possible\nthat a customer is extremely unhappy with some aspect, but maybe that aspect isn’t\ntheir priority. This information is crucial for e-retailers to have before they take any\naction. More details of this implementation are covered in [15].\nUser information is also key in handling reviews. Imagine a sce‐\nnario where a popular user, as opposed to a less-popular user,\nwrites a good review. The user matters! While performing the\nreview analysis, a “user weight” can be defined for all users based\non their ratings (generally given by other peers) and can be used in\nall calculations to discount the reviewer bias.\nWe’ll now go deeper into an example algorithm to understand aspects.\nUnderstanding Aspects\nIt’s a business objective for retailers to analyze a particular aspect of a product and\nhow various sentiments and opinions have been reflected in reviews. Similarly, a user\nmight be interested in a specific aspect of a product and may want to scan through all\nthe reviews on it. Hence, once we derive all the aspects and tag each sentence with\nthem, it’s possible to group the sentences by aspects. But given the huge volume of\nreviews an e-commerce website encounters, there will still be a lot of sentences under\nan aspect. Here, a summarization algorithm may save the day. Think about a situation\nwhere we need to take an action regarding an aspect but we don’t have the capacity to\ngo through all the sentences regarding that particular aspect. We’d need an automatic\nalgorithm that can pick and choose the best representative sentences for that aspect.\nLexRank [16] is an algorithm, similar to PageRank, that assumes each sentence is a\nnode and connects via sentence similarity. Once done, it picks the most central sen‐\ntences out of it and presents an extractive summary of the sentences under an aspect.\nAn example pipeline for review analysis, covering overall and aspect-level sentiments,\nis shown in Figure 9-17.\nIn this pipeline, we start with a set of reviews. After applying review-level aspect\ndetection, we can run sentiment analysis for every aspect as well as aggregate them\nbased on aspects. After aggregation, summarization algorithms such as LexRank can\nbe used to summarize them. In the end, we can take away the overall sentiment for an\naspect of a product as well as get a summary of opinions explaining the sentiment.\n330 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 473,
        "char_count": 2893,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 429,
            "height": 573,
            "ext": "png",
            "size_bytes": 13997
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 361,
        "text": "Figure 9-17. The complete flowchart of review analysis: overall sentiments, aspect-level\nsentiments, and aspect-wise significant reviews\nA complete understanding of a product can only be achieved by\nboth user reviews and editorial reviews. Editorial reviews are gen‐\nerally provided by expert users or domain experts. These reviews\nare more reliable and can be shown at the top of the review section.\nBut on the other hand, general user reviews reveal the true picture\nof the product experience from all users’ perspectives. Hence,\nmelding editorial reviews with general user reviews is important.\nThat may be achieved by mixing both kinds of reviews in the top\nsection and ranking them accordingly.\nWe’ve seen how review analysis can be done from the perspective of aspects, senti‐\nment, and ratings. In the next sections, we’ll briefly cover the nuances of personaliza‐\ntion for e-commerce.\nReview Analysis \n| \n331\n",
        "word_count": 144,
        "char_count": 917,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 394,
            "height": 514,
            "ext": "png",
            "size_bytes": 7986
          },
          {
            "index": 1,
            "width": 978,
            "height": 1220,
            "ext": "png",
            "size_bytes": 59826
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 362,
        "text": "Recommendations for E-Commerce\nIn Chapter 7, we discussed various techniques for recommendations using textual\ndata. Along with product search and review analysis, product recommendation is\nanother main pillar in e-commerce. In Figure 9-18, we show a comprehensive study\non the different algorithms used as well as the data utilization required for recom‐\nmendations in various scenarios [17].\nFigure 9-18. Comprehensive study of techniques for various e-commerce recommenda‐\ntion scenarios\nIn e-commerce, products are recommended based on a user’s purchase profile: fash‐\nionista, book lover, enjoyer of popular products, etc. These purchase profiles can be\ninferred from the user’s behavior on the platform. Imagine a user has interacted with\na set of products in the platform via viewing or clicking or purchasing them. These\n332 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 130,
        "char_count": 870,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1029,
            "height": 1418,
            "ext": "png",
            "size_bytes": 97072
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 363,
        "text": "interactions contain information that can help decide the set of products the user will\nbe interested in next. This can be achieved by neighborhood-based methods where\nwe look for similar products (in terms of attributes, purchase history, customers who\npurchased them, etc.) and provide them in the form of recommendations.\nClicks, purchase history, etc., are mainly numerical data, whereas e-commerce also\nhas a huge amount of textual data that can be utilized in product recommendations.\nAlong with numerical sources, the recommendation algorithm can include product\ndescriptions in text to induce better understanding about those products and provide\nmore similar products that match with even more granular attributes. For example,\nthe clothing material (e.g., 52% cotton, 48% polyester) mentioned in a product\ndescription could be important textual information to consider while looking for\nsimilar apparel.\nRecommendation engines deal with information from various\nsources. Proper matching of various data tables and consistency of\nthe information across various data sources is important to main‐\ntain. For example, while collating the information about product\nattributes and product transaction history, the consistency of the\ninformation should be checked carefully. Complementary and sub‐\nstitute data can give indications about data quality. One should\ncheck for anomalous behavior while working with multifarious\ndata sources, as in the case of e-commerce recommendation.\nReviews contain a lot of nuanced information and user opinions about products,\nwhich can guide product recommendations. Imagine a user providing feedback\nregarding the screen size of a mobile device (e.g., “I would have preferred a smaller\nscreen”). The specific feedback from the user for a specific attribute of the product\ncan provide a strong signal to filter the set of related products to make the recom‐\nmendation more useful to the user. We’ll look at a detailed case study relating to this\nand see how we can potentially build a recommendation system for e-commerce lev‐\neraging product reviews. Reviews are not only useful for finding better products for\nrecommendation but can also reveal the interrelationships between various products\nvia nuanced feedback from customers.\nA Case Study: Substitutes and Complements\nRecommender systems are built on the idea of “similar” products. This similarity can\nbe defined as content based or user profile based. There’s another way of identifying\nitem interrelationships specifically in an e-commerce setting.\nComplements are products that are typically bought together. On the other hand,\nthere are pairs that are bought in lieu of the other, and they’re known as substitute\npairs. Even though the economic definition is much more rigorous, these lines of\nRecommendations for E-Commerce \n| \n333\n",
        "word_count": 424,
        "char_count": 2833,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-Regular (9.6pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 394,
            "height": 514,
            "ext": "png",
            "size_bytes": 7986
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 364,
        "text": "thought typically capture the behavioral aspect of product purchase. Sometimes, due\nto huge disparities in individual user behavior, it’s difficult to infer the interrelation‐\nships between products from them. But in aggregation, these user interactions can\nreveal interesting properties about substitution and complementarity between prod‐\nucts. There are several ways [18] we can identify substitutes and complements using\nuser interaction data, but here, we’ll focus on an approach that relies primarily on the\nreviews as a form of textual information present in the products.\nJulian McAuley has presented [19] a comprehensive way of understanding product\ninterrelationships in a framework where the query product is given and the frame‐\nwork returns the ranked products, both substitutes and complements, (see\nFigure 9-19). We’ll discuss this application as a case study in the context of\ne-commerce.\nFigure 9-19. Substitutes and complements based on product reviews [19]\n334 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 150,
        "char_count": 1017,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1376,
            "height": 1351,
            "ext": "png",
            "size_bytes": 694360
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 365,
        "text": "Latent attribute extraction from reviews\nTypically, as we’ve discussed, reviews contain specific information about product\nattributes. Explicit extraction of attributes from reviews may have limitations in rep‐\nresentation, as we need to define an explicit ontology, so instead, we learn them via a\nlatent vector representation. The details of latent factor models are outside the scope\nof this book, but an interested reader can find the relevant material at [20].\nEach product is associated with a review. One review can discuss or mention various\nopinions regarding aspects related to the product. While these topics are latent and\ncan’t be identified distinctly, we can obtain a distribution of the share of discussion on\nvarious attributes as they’re discussed in the review. This distribution can be modeled\non all the reviews related to that product using popular topic models like LDA [21].\nThis provides a vectorial representation, or “topic vector,” which tells us how a partic‐\nular product has been discussed in reviews. This representation can be thought of as a\nfeature representation (from the usual ML terminology) of the product itself.\nProduct linking\nThe next task is to understand how the two products are linked. We already obtained\ntopic vectors, which capture the intrinsic properties of the product in a latent\nattribute space. Now, given a pair of products, we want to create a combined feature\nvector out of the respective topic vectors for the products and then predict if there’s\nany relationship between them. This can be viewed as a binary classification problem\nwhere the features have to be obtained from the respective topic vectors for the prod‐\nuct pair. We call this process “link prediction,” similar to [22].\nTo ensure that the topic vector is expressive enough to predict a link or relationship\nbetween a product pair, the objectives of obtaining topic vectors and link prediction\ncan be solved jointly rather than one after the other—i.e., we learn topic vectors for\neach product as well as the function to combine them for a product pair.\nFigure 9-20 depicts the interpretation of a topic vector after it’s learned, which is cov‐\nered in detail in [19]. It shows how a topic vector becomes expressive enough to cap‐\nture the intrinsic attributes of the product. Hierarchical dependence also emerges\nfrom such a representation, which in a way depicts the taxonomy that the product\nbelongs to.\nThis case study shows that reviews contain useful information that reveals various\ninterrelationships between products. Such latent representation, which has more\nexpressivity than exact extraction of attributes from reviews, has shown to be efficient\nnot only for the link prediction task, but also for revealing meaningful notions about\nthe product taxonomy. Such representation can be useful for making better product\nrecommendations via better product linking and obtaining more similar products.\nRecommendations for E-Commerce \n| \n335\n",
        "word_count": 469,
        "char_count": 2973,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 366,
        "text": "Figure 9-20. Topic vector and topic hierarchy express how different taxonomic identities\nand relations are captured in reviews [19]\nWrapping Up\nA primary driver behind the e-commerce industry’s immense success has been mas‐\nsive data collection and adaptation of data-driven decisions. NLP techniques have\nplayed a significant role in improving user experience and driving more revenue in e-\ncommerce and retail industries.\nIn this chapter, we covered different aspects of NLP in e-commerce. We started with\nan introduction on faceted search, then delved deep into product attributes. These\nareas are closely linked to product enrichment and categorization. We then covered\nreview analysis and product recommendations for e-commerce. Most of the examples\nand the setting in this chapter are product commerce, but the same techniques can be\nused in other areas as well, such as travel and food. We hope this chapter will be a\ngood starting point for baking NLP and intelligence into your domain.\nReferences\n[1] Clement, J. “Global Retail E-commerce Sales 2014–2023”. Statista, March 19,\n2010.\n[2] Fletcher, Iain. “How to Increase E-commerce Conversion with Site Search”. Search\nand Content Analytics (blog). Last accessed June 15, 2020.\n[3] Elasticsearch DSL. Faceted Search. Last accessed June 15, 2020.\n[4] Huang, Zhiheng, Wei Xu, and Kai Yu. “Bidirectional LSTM-CRF Models for\nSequence Tagging”. 2015.\n[5] Majumder, B. P., Aditya Subramanian, Abhinandan Krishnan, Shreyansh Gandhi,\nand Ajinkya More. “Deep Recurrent Neural Networks for Product Attribute Extrac‐\ntion in eCommerce”. 2018.\n[6] Logan IV, Robert L., Samuel Humeau, and Sameer Singh. “Multimodal Attribute\nExtraction”. 2017.\n336 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 260,
        "char_count": 1730,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1420,
            "height": 271,
            "ext": "png",
            "size_bytes": 67511
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 367,
        "text": "[7] Popescu, Ana-Maria, and Oren Etzioni. “Extracting Product Features and Opin‐\nion from Reviews.” Proceedings of the Conference on Human Language Technology and\nEmpirical Methods in Natural Language Processing (2005): 339–346.\n[8] Wang, Tao, Yi Cai, Ho-fung Leung, Raymond YK Lau, Qing Li, and Huaqing Min.\n“Product Aspect Extraction Supervised with Online Domain Knowledge.” Knowledge-\nBased Systems 71 (2014): 86–100.\n[9] Trietsch, R. C. “Product Attribute Value Classification from Unstructured Text in\nE-Commerce.” (master’s thesis, Eindhoven University of Technology, 2016).\n[10] “Product Classification with AI: How Machine Learning Sped Up Logistics for\nAeropost”. Semantics3 (blog), June 25, 2018.\n[11] Cheatham, Michelle, and Pascal Hitzler. “String Similarity Metrics For Ontology\nAlignment.” International Semantic Web Conference. Berlin: Springer, 2013: 294–309\n[12] Bilenko, Mikhail and Raymond J. Mooney. “Adaptive Duplicate Detection Using\nLearnable String Similarity Measures.” Proceedings of the Ninth ACM SIGKDD Inter‐\nnational Conference on Knowledge Discovery and Data Mining (2003): 39–48.\n[13] Neculoiu, Paul, Maarten Versteegh, and Mihai Rotaru. “Learning Text Similarity\nwith Siamese Recurrent Networks.” Proceedings of the First Workshop on Representa‐\ntion Learning for NLP (2016): 148–157.\n[14] Zagoruyko, Sergey and Nikos Komodakis. “Learning to Compare Image Patches\nvia Convolutional Neural Networks.” Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (2015): 4353–4361.\n[15] Wang, Hongning, Yue Lu, and Chengxiang Zhai. “Latent Aspect Rating Analysis\non Review Text Data: A Rating Regressions Approach.” Proceedings of the 16th ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining (2010):\n783–792.\n[16] Erkan, Günes and Dragomir R. Radev. “LexRank: Graph-Based Lexical Central‐\nity as Salience in Text Summarization.” Journal of Artificial Intelligence Research 22\n(2004): 457–479.\n[17] Sarwar, Badrul, George Karypis, Joseph Konstan, and John Riedl. “Analysis of\nRecommendation Algorithms for E-Commerce.” Proceedings of the 2nd ACM Confer‐\nence on Electronic Commerce (2000): 158–167.\n[18] Misra, Subhasish, Arunita Das, Bodhisattwa Majumder, and Amlan Das. “System\nfor calculating competitive interrelationships in item-pairs.” US Patent Application\n15/834,054, filed April 25, 2019.\n[19] McAuley, Julian, Rahul Pandey, and Jure Leskovec. “Inferring Networks of Sub‐\nstitutable and Complementary Products.” Proceedings of the 21th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining (2015): 785–794.\nWrapping Up \n| \n337\n",
        "word_count": 356,
        "char_count": 2629,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 368,
        "text": "[20] McAuley, Julian and Jure Leskovec. “Hidden Factor and Hidden Topics: Under‐\nstanding Rating Dimensions with Review Text.” Proceedings of the 7th ACM Confer‐\nence on Recommender Systems (2013): 165–172.\n[21] Blei, David M., Andrew Y. Ng, and Michael I. Jordan. “Latent Dirichlet Alloca‐\ntion.” Journal of Machine Learning Research 3 (2003): 993–1022.\n[22] Menon, Aditya Krishna and Charles Elkan. “Link Prediction via Matrix Factori‐\nzation.” Joint European Conference on Machine Learning and Knowledge Discovery in\nDatabases. Berlin: Springer, 2011: 437–452\n338 \n| \nChapter 9: E-Commerce and Retail\n",
        "word_count": 88,
        "char_count": 604,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 369,
        "text": "CHAPTER 10\nHealthcare, Finance, and Law\nSoftware is eating the world,\nbut AI is going to eat software.\n—Jensen Huang, Nvidia CEO\nNLP is affecting and improving all major industries and sectors. In the last two chap‐\nters, we covered how NLP is being utilized in the e-commerce, retail, and social\nmedia sectors. In this chapter, we’ll cover three major industries where the impact of\nNLP is rapidly increasing to have a substantial influence on the global economy:\nhealthcare, finance, and law. We’ve chosen these areas to demonstrate a wide range of\nproblems, solutions, and challenges you might face in your organization.\nThe term healthcare encompasses all goods and services for maintenance and\nimprovement of health and well-being. It’s estimated to be worth over 10 trillion dol‐\nlars as a market globally and accounts for tens of millions of people in the workforce\n[1]. The financial industry is one of the bedrocks of modern civilization and is esti‐\nmated to be worth over 26.5 trillion dollars. The legal services industry is estimated to\nbe worth over 850 billion dollars annually and is projected to cross a trillion dollars\nby 2021.\nIn this first section, we’ll start with an overview of the healthcare industry. Then we’ll\ncover broad applications in the healthcare landscape, along with a detailed discussion\nof specific use cases.\nHealthcare\nHealthcare as an industry encompasses both goods (i.e., medicines and equipment)\nand services (consultation or diagnostic testing) for curative, preventive, palliative,\nand rehabilitative care.\n339\n",
        "word_count": 247,
        "char_count": 1557,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (9.3pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 370,
        "text": "Curative care is provided to cure a patient suffering from a curable\ndisease, and preventative care is meant to prevent one from falling\nsick. Rehabilitative care helps patients recuperate from illness and\nincludes activities like physical therapy. Palliative care focuses on\nimproving the quality of life for patients suffering from terminal\nconditions.\nFor most advanced economies, healthcare accounts for a substantial part of the gross\ndomestic product, often exceeding 10%. Being such a large segment, there are mas‐\nsive benefits to automating and optimizing these processes and systems, and that’s\nwhere NLP comes in. Figure 10-1 from Chilmark Research [2] shows a range of\napplications where NLP helps. Each column shows the broad area, like clinical\nresearch or revenue cycle management. The blue cells show the applications that are\nused currently, the purple cells are applications that are emerging and being tested,\nand the red cells are more next generation and will be practically applicable in a\nlonger time horizon.\nFigure 10-1. NLP in healthcare use cases by Chilmark Research [2]\nHealthcare deals with large amounts of unstructured text, and NLP can be used in\nsuch places to improve health outcomes. Broad areas where NLP can help include but\nare not limited to analyzing medical records, billing, and ensuring drug safety. In the\nnext sections we’ll briefly cover some of these applications.\n340 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 230,
        "char_count": 1462,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 394,
            "height": 514,
            "ext": "png",
            "size_bytes": 7986
          },
          {
            "index": 1,
            "width": 1403,
            "height": 805,
            "ext": "png",
            "size_bytes": 68192
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 371,
        "text": "Health and Medical Records\nA large proportion of health and medical data is often collected and stored in\nunstructured text formats. This includes medical notes, prescriptions, and audio tran‐\nscripts, as well as pathology and radiology reports. An example of such a record is\nshown in Figure 10-2.\nFigure 10-2. An example of an electronic medical record [3]\nThis makes the data hard to search, organize, study, and understand in its raw form.\nThis is exacerbated by a lack of standardization in how the data is stored. NLP can\nhelp doctors search and analyze this data better and even automate some of the work‐\nflows, such as by building automated question-answering systems to decrease time to\nlook up relevant patient information. We’ll cover some of these in detail later in the\nchapter.\nHealthcare \n| \n341\n",
        "word_count": 135,
        "char_count": 812,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1023,
            "height": 737,
            "ext": "png",
            "size_bytes": 698629
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 372,
        "text": "Patient Prioritization and Billing\nNLP techniques can be used on physician notes to understand their state and urgency\nto prioritize various health procedures and checkups. This can minimize delays and\nadministrative errors and automate processes. Similarly, parsing and extracting infor‐\nmation from unstructured notes to identify medical codes can facilitate billing.\nPharmacovigilance\nPharmacovigilance entails all activities that are needed to ensure that a drug is safe.\nThis involves collection and detection and monitoring of adverse drug or medication\nreactions. A medical procedure or drug can have unintended or noxious effects, and\nmonitoring and preventing these effects is essential to making sure the drug acts as\nintended. With increasing use of social media, more of such side effects are being\nmentioned in social media messages; monitoring and identifying these is part of the\nsolution. We covered some of these techniques in Chapter 8, which focused on\ngeneric social media analysis. We’ll also cover some social media–specific cases later\nin this chapter. Besides social media, NLP techniques applied to medical records also\nfacilitate pharmacovigilance.\nClinical Decision Support Systems\nDecision support systems assist medical workers in making healthcare-related deci‐\nsions. These include screening, diagnosis, treatments, and monitoring. Various text\ndata can be used as an input to these systems, including electronic health records,\ncolumn-tabulated laboratory results, and operative notes. NLP is utilized on all of\nthese to improve the decision support systems.\nHealth Assistants\nHealth assistants and chatbots can improve the patient and caregiver experiences by\nusing various aspects of expert systems and NLP. For instance, services like Woebot\n[4] (Figure 10-3) can keep the spirits of patients suffering from mental illness and\ndepression high. Woebot combines NLP with cognitive therapy to do this by asking\nrelevant questions reinforcing positive thoughts.\n342 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 296,
        "char_count": 2042,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 373,
        "text": "Figure 10-3. A Woebot conversation\nSimilarly, assistants can assess patients’ symptoms to diagnose potential medical\nissues. Depending on the urgency and critical nature of the diagnoses, chatbots can\nbook appointments with relevant doctors. One example of such a system is Buoy [5].\nThese systems can also be built based on the user’s specific needs by utilizing existing\ndiagnostic frameworks. One example of such a framework is Infermedica [6]\n(Figure 10-4), where a chat interface can elicit symptoms from the user as well as give\na list of possible ailments with their probability.\nHealthcare \n| \n343\n",
        "word_count": 95,
        "char_count": 606,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 350,
            "height": 623,
            "ext": "png",
            "size_bytes": 90478
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 374,
        "text": "Figure 10-4. Diagnosis chatbot made by Infermedica API\nIn the next sections, we’ll cover some of these applications in more detail.\nElectronic Health Records\nIncreased adoption of storing clinical and healthcare data electronically has led to an\nexplosion of medical data and overwhelmingly large personal records. With this\nincreasing adoption and larger document size and history, it’s getting harder for doc‐\ntors and clinical staff to access this data, leading to an information overload. This, in\nturn, leads to more errors, omissions, and delays and affects patient safety.\nIn the next few sections, we’ll broadly cover how NLP can help manage this overload\nand improve patient outcomes. In this section, we’ll deal with electronic health\nrecords (EHRs).\nHARVEST: Longitudinal report understanding\nVarious tools have been built to overcome the informational overload we mentioned\nearlier. A notable effort is called HARVEST [7] from Columbia University. The tool\nhas been used extensively across hospitals in New York City. To start with, however,\nwe need to cover how a standard clinical information system works.\nFigure 10-5 shows a screenshot of a standard clinical information review system that’s\nused at New York Presbyterian Hospital (iNYP). iNYP delivers text-heavy, dense,\ntime-consuming, and generally unwieldy reports. There’s an option for basic text\n344 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 214,
        "char_count": 1418,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1441,
            "height": 862,
            "ext": "png",
            "size_bytes": 373282
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 375,
        "text": "search, but the text-heavy information lends itself to being skimmed over, which is an\nimpediment in the context of a busy, minute-to-minute hospital environment.\nFigure 10-5. Screenshot of the standard clinical information review system at New York\nPresbyterian Hospital\nIn contrast, HARVEST parses all of the medical data to make it easy to analyze and\ncan sit on top of any medical system. Figure 10-6 demonstrates how HARVEST is\nused on the iNYP system, showing the revamped and evolved visual depiction of the\nformerly text-heavy reporting format.\nHealthcare \n| \n345\n",
        "word_count": 90,
        "char_count": 572,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 1210,
            "ext": "png",
            "size_bytes": 942759
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 376,
        "text": "Figure 10-6. HARVEST system [7] for the same patient from Figure 10-5\nWe can see a timeline of each visit to the clinic or hospital. It’s accompanied by a\nword cloud of important medical conditions for the patient in the given time range.\nThe user can drill down to detailed notes and history if needed as well. All of this is\nalso supported by summaries of each report so a user can get the gist of a patient’s\nmedical history quickly. HARVEST is much more than a reformatted novelty—it’s\nextremely useful for giving not just doctors, but also general medical staff and care‐\ngivers, a near real-time, informative snapshot of what’s going on with a patient.\n346 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 124,
        "char_count": 708,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1439,
            "height": 1300,
            "ext": "png",
            "size_bytes": 1143964
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 377,
        "text": "All historical observations (from doctors, nurses, nutritionists, etc.) related to that\npatient are run through a named entity recognizer called HealthTermFinder. This\nfinds all healthcare-related terms, which are then mapped to the Unified Medical\nLanguage System (UMLS) semantic group. These terms are visualized in the word\ncloud. Word cloud weights are determined by TF-IDF, which we covered in detail in\nChapter 7. Also, the larger to smaller font sizes indicate the degree and frequency of\nthe various issues a patient has been carrying. This visual pattern can also drive the\nidentification and exploration of issues that otherwise might not have been\nconsidered.\nHARVEST is able to depict a patient’s medical history across a period of time, how‐\never long that might be, in a much more effective and easy-to-comprehend fashion.\nWhat becomes more valuable in such instances is that it helps with the analytical\ncapability of the medical professional to home in on root issues and not get caught up\nin merely treating symptoms or biased misdiagnoses. A study was conducted where\nthe HARVEST system was tested by medical practitioners at New York Presbyterian\nHospital. In this test, more than 75% of participants said they would definitely use\nHARVEST regularly in the future, despite it being a completely new user interface,\nwhile the rest also showed some leaning toward using the system. Figure 10-7 shows\na snapshot of some of the feedback provided by these practitioners at the time.\nHARVEST delivers understandable summaries and conclusions by collating a\npatient’s history of healthcare issues across their lifetime. Its unique selling point is\nthat it can mine, extract, and visually present content at a macro level—based on\ndetailed micro-level observations—irrespective of where and by whom in the hospital\na patient might have been seen. Such systems can be built to visualize and analyze a\nlarge amount of information. When the underlying knowledge base is unstructured\ntext, as it is in the case of EHRs, NLP techniques play a key role in such analytics and\ninformation visualization tools.\nHealthcare \n| \n347\n",
        "word_count": 338,
        "char_count": 2132,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 378,
        "text": "Figure 10-7. Clinical feedback on HARVEST at New York Presbyterian Hospital [7]\n348 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 20,
        "char_count": 129,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1318,
            "height": 1648,
            "ext": "png",
            "size_bytes": 177720
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 379,
        "text": "Question answering for health\nIn the last section, we looked at how basic NLP techniques like NER can be used to\nimprove the user’s experience with handling records and information at scale. But to\ntake the user experience to the next level, we can consider building a question-\nanswering (QA) system on top of these records.\nWe’ve covered question-answering systems in Chapter 7, but our focus here is on the\nnuances of questions that arise specifically in healthcare scenarios. For example, these\nquestions can include:\n• What dosage of a particular medicine is a patient required to take?\n• For what ailment is a particular medication taken?\n• What were the results of a medical test?\n• By how much was the result of a medical test out of range for a given test date?\n• What lab test confirmed a particular disease?\nAs we’ve discussed throughout the book, building the right dataset for a particular\ntask is often the key to solving any NLP problem. For the particular problem of the\nQA system in the healthcare domain, we’ll focus on a dataset known as emrQA,\nwhich was created by a joint collaboration between IBM Research Center, MIT, and\nUIUC [8, 9]. Figure 10-8 shows an example of what such a dataset entails. For\ninstance, for the question, “Has the patient ever had an abnormal BMI?”, a correct\nanswer is extracted from past health records.\nFigure 10-8. Example of a question-answer pair in emrQA\nHealthcare \n| \n349\n",
        "word_count": 247,
        "char_count": 1427,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 968,
            "height": 622,
            "ext": "png",
            "size_bytes": 80589
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 380,
        "text": "To create such datasets of questions and answers and build a QA system on them, a\ngeneral question-answering dataset creation framework consists of:\n1. Collecting domain-specific questions and then normalizing them. For instance, a\npatient’s treatment can be asked about in multiple ways, like, “How was the prob‐\nlem managed?” or “What was done to correct the patient’s problem?” These all\nhave to be normalized in the same logical form.\n2. Question templates are mapped with expert domain knowledge and logical forms\nare assigned to them. The question template is an abstract question. For example,\nfor a certain type of question, we expect a number or a medication type as a\nresponse. More concretely, a question template is “What is the dosage of medica‐\ntion?”, which then maps to an exact question, like, “What is the dosage of Nitro‐\nglycerin?” This question is of a logical form that expects a dosage as response.\nWe’ll see this in more detail in Figure 10-9.\n3. Existing annotations and the information collected in (1) and (2) are used to cre‐\nate a range of question-and-answer pairs. Here, already available information like\nNE tags as well as answer types linked to the logical form are used to bootstrap\ndata. This step is especially relevant, as it reduces the manual effort needed in the\ncreation of the QA dataset.\nMore specifically for emrQA, this process involved polling physicians at the Veterans\nAdministration to gather prototypical questions, which led to over 2,000 noisy tem‐\nplates that were normalized to around 600. These prototypical questions were then\nlogically mapped to an i2b2 dataset [10]. i2b2 datasets are already expertly annotated\nwith a range of fine-grained information like medication concepts, relations, asser‐\ntions, coreference resolution, etc. Although they’re not made explicitly for QA pur‐\nposes, by using logical mapping and existing annotations, questions and answers are\ngenerated out of them. A high-level overview of this process is shown in Figure 10-9.\nThis process is closely supervised by a set of physicians to ensure the quality of the\ndataset.\nTo build a baseline QA system, neural seq-to-seq models and heuristic-based models\nwere used. These models are covered in more detail in the emrQA team’s work. To\nevaluate these models, they divided the dataset into two sets: emrQL-1 and emrQL-2.\nemrQL-1 had more diversity in vocabulary in test and training data. Heuristic models\nperformed better than neural models for emrQL-1, while neural models did better for\nemrQL-2.\nMore broadly, this is an interesting use case on how to build complex datasets using\nheuristics, mapping, and other simpler annotated datasets. These learnings can be\napplied to a range of other problems, beyond processing health records, that require\ngeneration of a QA-like dataset. Now, we’ll cover how health records can be used to\npredict health outcomes.\n350 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 472,
        "char_count": 2942,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 381,
        "text": "Figure 10-9. QA dataset generation using existing annotations\nOutcome prediction and best practices\nWe’ve seen how NLP can aid exploration and how doctors can ask questions from\npatient health records. Here, we’ll cover a more cutting-edge application using health\nrecords: predicting health outcomes. Health outcomes are a set of attributes that\nexplain the consequences of a disease for a patient. They include how fast and how\ncompletely a patient recovers. They’re also important in measuring efficacies of dif‐\nferent treatments. This work is a joint collaboration between Google AI, Stanford\nMedicine, and UCSF [11].\nBesides predicting health outcomes, another focus of scalable and accurate deep\nlearning with electronic health records is to ensure that we can build models and sys‐\ntems that can be both scalable as well as highly accurate. Scalability is necessary, as\nhealthcare has a diverse set of inputs—data collected from one hospital or depart‐\nment can be different from another. So it should be simple to train the system for a\ndifferent outcome or different hospital. It’s necessary to be accurate in order not to\nraise too many false alarms; the need for accuracy is obvious in the healthcare indus‐\ntry, where people’s lives are on the line.\nAs simple as EHRs might sound, they are far from it; there are a lot of nuances and\ncomplexity attached to them. Even something as simple as body temperature can\nhave a range of diagnoses depending on whether it was taken via tongue, forehead, or\nother body parts. To handle all these cases, an open Fast Healthcare Interoperability\nResources (FHIR) standard was created, which used a standardized format with\nunique locators for consistency and reliability.\nOnce the data is in a consistent format, it’s fed into a model based on RNNs. All his‐\ntorical data is fed from the start of the record to its end. The output variable is the\noutcome we’re looking to predict.\nHealthcare \n| \n351\n",
        "word_count": 321,
        "char_count": 1950,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1431,
            "height": 624,
            "ext": "png",
            "size_bytes": 80005
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 382,
        "text": "The model was evaluated on a range of health outcomes. It achieved an AUC score\n(or area under the curve) of 0.86 on whether the patients would stay longer in the\nhospital, 0.77 on unexpected readmissions, and 0.95 on predicting patient mortality.\nAn AUC score [12] is a measure used often in such cases because AUC is a summary\nmeasure of performance across all potential diagnostic thresholds for positivity,\nrather than performance at any specific threshold [13]. A score of 1.0 indicates per‐\nfect accuracy, while 0.5 is the same as a random chance.\nIt’s important in healthcare that models are interpretable. In other words, they should\npinpoint why they suggested a particular outcome. Without interpretability, it’s hard\nfor doctors to accommodate the results in their diagnosis. To achieve this, attention, a\nconcept in deep learning, is used to understand what data points and incidents are\nmost important for an outcome. An example of this attention map can be seen in\nFigure 10-10.\nFigure 10-10. An example of attention applied to a health record\nThis Google AI team also came up with some of the best practices one should keep in\nmind while building ML models for healthcare, outlining ideas in all parts of the\nmachine learning life cycle, from defining the problem and collecting data to validat‐\ning the results. These suggestions are relevant to NLP and computer vision as well as\nstructured data problems. The reader can peruse them in detail in [14].\n352 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 253,
        "char_count": 1518,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1441,
            "height": 1035,
            "ext": "png",
            "size_bytes": 718480
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 383,
        "text": "These techniques focus mostly on managing the physical well-being of humans,\nwhich is relatively easy to quantify because there is a variety of numerical measures\navailable, but there are no obvious quantifiable measures for a person’s mental well-\nbeing. Let’s look at some techniques for monitoring a person’s mental health.\nThe following section includes discussions of mental health issues\nand suicide.\nMental Healthcare Monitoring\nGiven the fast-moving pace of economic and technological change and the fast pace\nof life in today’s world, it’s no surprise that most people, particularly in generations X,\nY, and Z, tend to experience some form of mental health issue in their lifetimes. By\nsome estimates, over 790 million people are affected by mental health–related issues\nglobally, which translates to more than 1 in every 10 people [15]. A study by the\nNational Institutes of Health estimated that one in four Americans are likely to be\naffected by one or more mental health conditions in a given year. Over 47,000\nAmericans committed suicide in 2017, and this number has been increasing at a\nrapid pace [16].\nWith social media usage at an all-time high, it’s increasingly possible to use signals\nfrom social media to track the emotional state and mental balance of both particular\nindividuals and across groups of individuals. It should also be possible to gain\ninsights into these aspects across various demographic groups, including age and\ngender. In this section, we’ll briefly cover an exploratory analysis [17] on public data\nfrom Twitter users and how techniques learned in Chapter 9 can be applied to this\nproblem.\nThere are innumerable aspects to evaluating an individual’s mental well-being. The\nstudy by Glen Coppersmith et al. focuses, as an illustrative example, on utilizing\nsocial media in identifying individuals who are at risk for suicide. The goal of the\nstudy was to develop an early warning system along with identifying the root causes\nof the issues.\nIn this study, 554 users were identified and evaluated who stated that they attempted\nto take their lives. 312 of these users gave an explicit indication of their latest suicide\nattempt. Profiles that were marked as private were not included in this study. They\nonly examined public data, which does not include any direct messages or deleted\nposts.\nHealthcare \n| \n353\n",
        "word_count": 379,
        "char_count": 2352,
        "fonts": [
          "MinionPro-Regular (9.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 503,
            "height": 479,
            "ext": "png",
            "size_bytes": 10854
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 384,
        "text": "Each user’s tweets were analyzed with the following perspectives:\n• Is the user’s statement of attempting to take their life apparently genuine?\n• Is the user is speaking about their own suicide attempt?\n• Is the suicide attempt localizable in time?\nSee Figure 10-11 for a few example tweets.\nFigure 10-11. Nuances of building a social dataset\nThe first two tweets refer to genuine suicide attempts, while the bottom two are sar‐\ncastic or false statements. The middle two are examples where an explicit suicide\nattempt date is mentioned.\nIn order to analyze the data, the following steps were followed:\n1. Pre-processing: Because Twitter data is often noisy, it was normalized and cleaned\nfirst. URLs and usernames are represented with homogenous tokens. We covered\nvarious aspects of cleaning social media data in detail in Chapter 9.\n2. Character models: Character n-gram–based models followed by logistic regres‐\nsion were used to classify various tweets. Performance was measured with 10-\nfold cross validation.\n3. Emotional states: To estimate emotional content in tweets, a dataset was boot‐\nstrapped using hashtags. For instance, all tweets containing #anger but not con‐\ntaining #sarcasm and #jk were put into an emotional label. Tweets with no\nemotional content were also classified as No Emotion.\nThese models were then tested on how well they could flag potential suicide risks.\nThey were able to identify 70% of people who were very likely to attempt suicide,\nwith only 10% false alarms. Figure 10-12 shows a confusion matrix detailing misclas‐\nsification of various emotions that were modeled.\nIdentifying potential mental health issues can be used to intervene in flagged cases.\nWith accurate monitoring and alerting, NLP bots like Woebot can also be used to ele‐\nvate the moods of folks at higher risk. In the next section, we’ll dig deeper into\nextracting entities from medical data.\n354 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 313,
        "char_count": 1950,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1012,
            "height": 222,
            "ext": "png",
            "size_bytes": 48960
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 385,
        "text": "Figure 10-12. Confusion matrix for emotion classification\nMedical Information Extraction and Analysis\nWe’ve seen a range of applications built on health records and information. If we were\nto start building applications using health records, one of the first steps would be to\nextract medical entities and relations from it. Medical information extraction (IE)\nhelps to identify clinical syndromes, medical conditions, medication, dosage,\nstrength, and common biomedical concepts from health records, radiology reports,\nand discharge summaries, as well as nursing documentation and medical education\ndocuments. We can use both cloud APIs and pre-built models for it.\nFirst, we’ll start with understanding Amazon Comprehend Medical [18]. It’s a part of\na larger suite by AWS, Amazon Comprehend, that allows us to do popular NLP tasks\nlike keyphrase extraction, and sentiment and syntax analysis, as well as language and\nentity recognition in the cloud. Amazon Comprehend Medical helps process medical\ndata, including medical named entity and relationship extraction and medical ontol‐\nogy linking.\nHealthcare \n| \n355\n",
        "word_count": 163,
        "char_count": 1116,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1353,
            "height": 1121,
            "ext": "png",
            "size_bytes": 56973
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 386,
        "text": "We can use Amazon Comprehend Medical as a cloud API on our medical text. We\ncover the cloud API in detail in this chapter’s notebooks, but here, we’ll give a short\noverview of how they function. To start, we’ll take health records from FHIR as an\ninput [19]. As a reminder, FHIR is a standard that describes how healthcare informa‐\ntion is documented and shared across the United States. We’ll take a sample elec‐\ntronic health record from a hypothetical Good Health Clinic [20]. To robustly test\nComprehend Medical, we’ll also remove all formatting and line breaks from it to see\nhow well the system can do on this. As a starting input, let’s consider a small sequence\nof this medical record:\nGood Health Clinic Consultation Note Robert Dolin MD Robert Dolin MD Good Health\nClinic Henry Levin the 7th Robert Dolin MD History of Present Illness Henry \nLevin, the 7th is a 67 year old male referred for further asthma management.\nOnset of asthma in his twenties teens. He was hospitalized twice last year, and\nalready twice this year. He has not been able to be weaned off steroids for the \npast several months. Past Medical History Asthma Hypertension (see HTN.cda for \ndetails) Osteoarthritis, right knee Medications Theodur 200mg BID Proventil \ninhaler 2puffs QID PRN Prednisone 20mg qd HCTZ 25mg qd Theodur 200mg BID\nProventil inhaler 2puffs QID PRN Prednisone 20mg qd HCTZ 25mg qd\nWhen we provide this as an input to Comprehend Medical, we get the output shown\nin Figure 10-13.\nFigure 10-13. Comprehend Medical output for the FHIR record example\n356 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 272,
        "char_count": 1599,
        "fonts": [
          "UbuntuMono-Regular (8.5pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1438,
            "height": 779,
            "ext": "png",
            "size_bytes": 247310
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 387,
        "text": "As we can see, we were able to extract everything, from clinic and doctor details to\ndiagnosis and medications, as well as their frequency, dosage, and route. If we need\nto, we can also link extracted information to standard medical ontologies such as\nICD-10-CM or RxNorm. Access to all Comprehend Medical features is through an\nAWS boto library, which we cover in more detail in the notebooks for Chapter 10.\nCloud APIs and libraries can be a good starting point for building medical informa‐\ntion extraction, but if we have specific requirements and prefer to build our own sys‐\ntem, we recommend BioBERT as a starting point. We’ve covered BERT, Bidirectional\nEncoder Representations, throughout the book. However, the default BERT model is\ntrained on regular web text, which is very different from medical text and records.\nFor instance, the different word distributions vary substantially between regular\nEnglish and medical records. This affects the performance of BERT in medical tasks.\nIn order to build better models for biomedical data, BERT for Biomedical Text (Bio‐\nBERT) was created [21]. It adapts BERT to biomedical texts to get better perfor‐\nmance. In the domain adaptation phase, we initialize the model weights with a\nstandard BERT model and pre-trained biomedical texts, including texts from\nPubMed, a search engine for medical results. Figure 10-14 shows the process of pre-\ntraining and fine-tuning BioBERT.\nThis model and weights were open sourced and can be found on GitHub [22, 23].\nBioBERT can be fine-tuned on a range of specific medical problems like medical\nnamed entity recognition and relation extraction. It has also been applied to\nquestion-answering on healthcare texts. BioBERT obtains significantly higher perfor‐\nmance than BERT and other state-of-the-art techniques. It can also be adapted\ndepending on the medical task and dataset.\nWe’ve discussed a range of healthcare applications where NLP can help. We covered\ndifferent facets of applications that can be built on health records and learned how\nsocial media monitoring can be applied to mental health issues. At the end, we saw\nhow to lay the foundations of our healthcare application. Now, we’ll delve into the\nworld of finance and law and see how NLP helps.\nHealthcare \n| \n357\n",
        "word_count": 365,
        "char_count": 2271,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 388,
        "text": "Figure 10-14. BioBERT pre-training and fine-tuning\nFinance and Law\nFinance is a diverse area that encompasses a wide spectrum, from public company\nmonitoring to investment banking deal flow. Globally, the financial services industry\nis expected to grow to 26 trillion USD by 2022 [24]. As finance and law are more\ninterrelated, we’ll cover them in the same section. In the context of integrating and\nutilizing NLP in the context of finance frameworks, operations, reporting, and evalu‐\nation, we can look at finance from the following three angles:\nOrganization perspectives\nDifferent organization types have different requirements and perspectives that\nneed to be taken into account. These perspectives include:\n• Private companies\n• Public companies\n358 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 121,
        "char_count": 801,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1068,
            "height": 1270,
            "ext": "png",
            "size_bytes": 170646
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 389,
        "text": "• Non-profit enterprises\n• Governmental organizations\nActions\nThere are different actions that an organization can take, including:\n• Allocating and reallocating funds\n• Accounting and auditing, which includes identifying anomalies and outliers\nto investigate for both value and risk\n• Prioritization and resource planning\n• Compliance with legal and policy norms\nFinancial context\nThese actions can have various contexts, including:\n• Forecasting and budgeting\n• Retail banking\n• Investment banking\n• Stock market operations\n• Cryptocurrency operations\nTo make real-time, thoughtful, planned decisions around structuring, viewing, man‐\naging, and reporting financial flows, there must be a constant focus on the changing\nnature of the company, and the financial infrastructure must be built and designed\naccordingly. ML and NLP can help design such a system. Figure 10-15 [25] shows\nhow UK bankers think ML and NLP can improve their operations and in what areas.\nFigure 10-15. Estimated ML benefits survey in the UK [25]\nFinance and Law \n| \n359\n",
        "word_count": 158,
        "char_count": 1046,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1397,
            "height": 599,
            "ext": "png",
            "size_bytes": 45594
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 390,
        "text": "They estimate large improvements in operational efficiency as well as analytics\ninsights. With the application of ML and NLP, anti-fraud and anti–money laundering\nefforts are also expected to yield better benefits.\nNLP Applications in Finance\nIn this section, we’ll cover some specific applications of NLP in finance, including\nloan risk assessments, auditing and accounting problems, and financial sentiment\nanalysis.\nFinancial sentiment\nStock market trading relies on a set of information about specific companies. This\nknowledge helps create a set of actions that determine whether to buy, hold, or sell off\nstock. This analysis can be based on companies’ quarterly financial reports or on what\nanalysts are commenting about the companies in their reports. This can also come\nfrom social media.\nSocial media analysis, which we covered in detail in Chapter 8, helps in monitoring\nsocial media posts and pointing out potential opportunities for trading. For instance,\nif a CEO is resigning, that sentiment is often negative, which can negatively affect the\ncompany’s stock price. On the other hand, if the CEO is not performing well and\nmarkets welcome their resignation, that could lead to an increase in stock price.\nExamples of companies that provide this information for trading include DataMinr\nand Bloomberg. Figure 10-16 shows the DataMinr terminal, where alerts and\nmarketing-affecting news related to Dell is surfaced to the user.\nFigure 10-16. Dataminr social terminal\n360 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 234,
        "char_count": 1529,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 629,
            "height": 321,
            "ext": "png",
            "size_bytes": 113379
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 391,
        "text": "Financial sentiment analysis is different from regular sentiment analysis. It’s not just\ndifferent in domain, but also in purpose. Generally, the purpose is to guess how the\nmarkets will react to a piece of news, as opposed to whether the news is inherently\npositive or not. Just like we saw earlier in BioBERT for healthcare, there have been\nefforts to adapt BERT to the financial domain. One of these is FinBERT [26].\nFinBERT uses a subset of financial news from Reuters. For sentiment classification, it\nuses Financial PhraseBank, which has over 4,000 sentences labeled by people with\nbackgrounds in business and finance. Unlike regular sentiment analysis, where posi‐\ntive means that something is of positive emotion, in Financial PhraseBank, a positive\nsentiment indicates that the stock price of the company will increase based on the\nnews in the sentence. FinBERT led to an accuracy of 0.97 and an F1 of 0.95—a sub‐\nstantial improvement over other general state-of-the-art methods. FinBERT is a\nlibrary that’s available on GitHub, along with its data [26]. We can build on this\nlibrary for custom problems and use the pre-trained models for financial sentiment\nclassification.\nRisk assessments\nCredit risk is a way to quantify the chances of a successful loan repayment. It’s gener‐\nally calculated by an individual’s past spending and loan repayment history. However,\nthis information is limited in many scenarios, especially in underprivileged commun‐\nities. It’s estimated that more than half of the world’s population is excluded from\nfinancial services [27]. NLP can help alleviate this problem. NLP techniques can add\na lot more data points that can be used to assess credit risk. For example, in business\nloans, entrepreneurial ability and attitude can be measured using NLP. This approach\nis used by Capital Float and Microbnk. Similarly, incoherencies in data provided by\nthe borrower can also be surfaced for more scrutiny. Other more nuanced aspects,\nsuch as lenders’ and borrowers’ emotions while applying for a loan, can also be incor‐\nporated. This is covered in more detail in [27].\nOften in personal loan agreements, various information has to be captured from loan\ndocuments, which are then fed to credit risk models. The information captured helps\nin identifying credit risk, and erroneous data extraction from these documents can\nlead to flawed assessments. Named entity recognition (NER), which we covered in\ndetail in Chapter 5, can improve this. An example of such a loan agreement is shown\nin Figure 10-17, where we see a loan agreement and different relevant entities extrac‐\nted from it. This example is taken from a work [28] on domain adaptation of NER for\nthe finance domain. We’ll cover such entity extraction in more detail in “NLP and the\nLegal Landscape” on page 363.\nFinance and Law \n| \n361\n",
        "word_count": 459,
        "char_count": 2831,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 392,
        "text": "Figure 10-17. Loan agreement with annotated entities\nAccounting and auditing\nThe global firms Deloitte, Ernst & Young, and PwC now have a significant focus on\ndelivering more meaningful, actionable, and relevant audit conclusions and observa‐\ntions on a company’s annual performance. While applying aspects of NLP and ML to\nareas like contract document reviews and long-term procurement agreements,\nDeloitte, for example, has evolved its Audit Command Language into a more efficient\nNLP application. This is covered in more detail in their report on government data\n[29].\nIn addition, after decades of long, drawn-out ticking and tying of reams of endless,\ntypical day-to-day transactions and other pieces of paper like invoices, companies\nhave finally realized that NLP and ML has a significant advantage in the audit pro‐\ncess. This advantage manifests in the direct identification, focus, visualization, and\ntrend analysis of outliers in transaction types. Time and effort are spent on the inves‐\ntigation of these outliers and their causes. This results in early identification of poten‐\ntially significant risks and possible fraudulent activity like money laundering along\n362 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 185,
        "char_count": 1227,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 1227,
            "ext": "png",
            "size_bytes": 877423
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 393,
        "text": "with potentially value-generating activities that can be emulated and extrapolated\nacross a company and customized for various business processes.\nNext, we’ll turn our attention to the use of NLP in legal matters.\nNLP and the Legal Landscape\nThe integration and utilization of technological tools in the law profession has been\nin progress for decades. Given the amount of research, case referencing, brief prepa‐\nration, document review, contract design, background analysis, and opinion drafting,\nthose in the legal profession, including law offices and court systems, have long\nlooked for a multitude of ways, means, and tools to slash their hours of manual effort.\nWe won’t cover legal NLP in as much detail, as research work in the domain is pro‐\ntected by patents instead of open or partially open. So, we’ll discuss the ideas in gen‐\neral terms.\nSome core tasks where NLP helps legal services include:\nLegal research\nThis involves finding relevant information for a specific case, including searching\nboth legislatures and case law and regulations. One such service is ROSS Intelli‐\ngence [30]. It allows matching of facts and relevant cases and also analyzes legal\ndocuments. We can see it in action in Figure 10-18.\nFigure 10-18. ROSS match for relevant passages\nContract review\nThis refers to reviewing a contract and making sure it follows a set of norms and\nregulations. It involves making comments and suggesting edits for different clau‐\nses. One example is SpotDraft [31], which focuses on GPDR-based regulations.\nContract generation\nThis refers to generating contracts based on a question-and-answer setup. Simple\ncases may just require a simple form, whereas for more complex cases, an inter‐\nactive chatbot system may be more suitable. After taking in all the responses, a\nslot-filling algorithm generates the contract.\nFinance and Law \n| \n363\n",
        "word_count": 294,
        "char_count": 1862,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 991,
            "height": 278,
            "ext": "png",
            "size_bytes": 37213
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 394,
        "text": "Legal Discovery\nThis refers to finding anomalies and patterns in electronically stored information\nthat can be used for the case. In some cases, this discovery is completely unsuper‐\nvised. In other cases, it can involve more active learning (i.e., providing an initial\nset of tagged documents). One such product is siren.io [32], which aids discovery\nfor intelligence, law enforcement, cyber security, and financial crime domains.\nLegal entity extraction with LexNLP\nIn any kind of contract, there are a bunch of legal terms and entities we need to\nextract before building any kind of intelligent application. LexNLP [33] helps with\nthat because it has legal word segmentation and tokenization. This is important\nbecause of legal abbreviations like LLC or F.3d, which regular parsers can’t handle.\nSimilarly, LexNLP helps us segment documents into sections and extract facts like\nrecurring contract dates or regulations. Moreover, it plugs into the ContraxSuite,\nwhich has a range of other legal features that we’ll cover later.\nNow, let’s see how this works in action:\nimport lexnlp.extract.en.acts\nimport lexnlp.extract.en.definitions\nprint(\"List of acts in the document\")\ndata_contract = list(lexnlp.extract.en.acts.get_acts(text))\ndf = pd.DataFrame(data=data_contract,columns=data_contract[0].keys())\ndf['Act_annotations'] = list(lexnlp.extract.en.acts.get_acts_annotations(text))\ndf.head(10)\nprint(\"Different ACT definitions in the contract\")\ndata_acts = list(lexnlp.extract.en.definitions.get_definitions(text))\ndf = pd.DataFrame(data=data_acts,columns=[\"Acts\"])\ndf.head(20)\nFigure 10-19 shows the list of acts in the document extracted using LexNLP.\nAs shown in the code, we extracted information from a SAFE (simple agreement for\nfuture equity), a common document for investments. We extracted all the acts and\ntheir definitions that were present in the document. Similarly, this can be extended to\nextract companies, citations, constraints, legal durations, regulations, etc. We cover\nsome of these in the notebook for Chapter 10.\n364 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 281,
        "char_count": 2090,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "MinionPro-It (10.5pt)",
          "UbuntuMono-Regular (8.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 395,
        "text": "Figure 10-19. Output from LexNLP\nBesides legal entity extraction, LexNLP also provides legal dictionaries [34] and\nknowledge sets for multiple countries for accounting, financial information, regula‐\ntors, and legal and medical areas. It also integrates with ContraxSuite [35], which\nallows us to deduplicate documents, cluster legal entities according to how they’re\nmentioned (as seen in Figure 10-20), and so on. When building custom applications,\nwe can also inject code to build on the baseline platform.\nFinance and Law \n| \n365\n",
        "word_count": 80,
        "char_count": 534,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 1254,
            "ext": "png",
            "size_bytes": 249375
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 396,
        "text": "Figure 10-20. Clustering of legal entities from a set of documents\nWrapping Up\nIn this chapter, we learned about how NLP is utilized in healthcare, finance, and law,\ncovering everything from model building, using online APIs, and dataset creation.\nThese areas offer a diverse set of issues and solutions, so even if the domain you’re\nworking in is unrelated to these areas, the techniques learned here may be applicable\nin solving any unconventional problem. In the next chapter, we’ll see how all of this\ncomes together in building a complete NLP solution.\nReferences\n[1] Business Wire. “The $11.9 Trillion Global Healthcare Market: Key Opportunities\n& Strategies (2014–2022)”. June 25, 2019.\n[2] Chilmark Research. “NLP Use Cases for Healthcare Providers”. July 17, 2019.\n[3] Wikipedia. “Electronic health record”. Last modified April 17, 2020.\n[4] Woebot. Last accessed June 15, 2020.\n[5] Buoy: a healthcare chatbot. Last accessed June 15, 2020.\n[6] Infermedica. Last accessed June 15, 2020.\n[7] Hirsch, Jamie S., Jessica S. Tanenbaum, Sharon Lipsky Gorman, Connie Liu, Eric\nSchmitz, Dritan Hashorva, Artem Ervits, David Vawdrey, Marc Sturm, and Noémie\nElhadad. “HARVEST, a longitudinal patient record summarizer.” Journal of the Amer‐\nican Medical Informatics Association 22.2 (2015): 263–274.\n366 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 206,
        "char_count": 1347,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1190,
            "height": 619,
            "ext": "png",
            "size_bytes": 293359
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 397,
        "text": "[8] Raghavan, Preethi and Siddharth Patwardhan. “Question Answering on Elec‐\ntronic Medical Records.” Proceedings of the 2016 Summit on Clinical Research Infor‐\nmatics (2016).\n[9] Raghavan, Preethi, Siddharth Patwardhan, Jennifer J. Liang, and Murthy V.\nDevarakonda. “Annotating Electronic Medical Records for Question Answering”,\n(2018).\n[10] i2b2. “NLP Research Datasets”. Last accessed June 15, 2020.\n[11] Rajkumar, Alvin and Oren, Eyal. “Deep Learning for Electronic Health Records”.\nGoogle AI Blog, May 8, 2018.\n[12] Google Machine Learning Crash Course. “Classification: ROC Curve and AUC”.\nLast accessed June 15, 2020.\n[13] Hilden, Jørgen. “The Area Under the Roc Curve and Its Competitors.” Medical\nDecision Making 11.2 (1991): 95–101.\n[14] Liu, Yun and Po-Hsuan Cameron Chen. “Lessons Learned from Developing ML\nfor Healthcare”. Google AI Blog, December 10, 2019.\n[15] Ritchie, Hanna and Max Roser. “Mental Health”. Our World In Data, April 2018.\n[16] National Institute of Mental Health (NIMH). “Suicide”. Last accessed June 15,\n2020.\n[17] Coppersmith, Glen, Kim Ngo, Ryan Leary, and Anthony Wood. “Exploratory\nAnalysis of Social Media Prior to a Suicide Attempt.” Proceedings of the Third Work‐\nshop on Computational Linguistics and Clinical Psychology (2016): 106–117.\n[18] Amazon Comprehend Medical. Last accessed June 15, 2020.\n[19] Fast Healthcare Interoperability Resources (FHIR) specification. Last accessed\nJune 15, 2020.\n[20] FHIR sample healthcare record, (download).\n[21] Lee, Jinhyuk, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,\nChan Ho So, and Jaewoo Kang. “BioBERT: A Pre-Trained Biomedical Language Rep‐\nresentation Model for Biomedical Text Mining.” Bioinformatics 36.4 (2020): 1234–\n1240.\n[22] DMIS Laboratory - Korea University. BioBERT: a pre-trained biomedical lan‐\nguage representation model, (GitHub repo). Last accessed June 15, 2020.\n[23] NAVER. BioBERT: a pre-trained biomedical language representation model for\nbiomedical text mining, (GitHub repo). Last accessed June 15, 2020.\n[24] Ross, Sean. “What Percentage of the Global Economy Is the Financial Services\nSector?” Investopedia, February 6, 2020.\nWrapping Up \n| \n367\n",
        "word_count": 311,
        "char_count": 2170,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 398,
        "text": "[25] Bank of England. “Machine Learning in UK Financial Services”. October 2019.\n[26] Araci, Dogu. “FinBERT: Financial Sentiment Analysis with Pre-trained Lan‐\nguage Models”, (2019).\n[27] Crouspeyre, Charles, Eleonore Alesi, and Karine Lespinasse. “From Creditwor‐\nthiness to Trustworthiness with Alternative NLP/NLU Approaches.” Proceedings of\nthe First Workshop on Financial Technology and Natural Language Processing (2019):\n96–98.\n[28] Alvarado, Julio Cesar Salinas, Karin Verspoor, and Timothy Baldwin. “Domain\nAdaption of Named Entity Recognition to Support Credit Risk Assessment.” Proceed‐\nings of the Australasian Language Technology Association Workshop (2015): 84–90.\n[29] Eggers, William D., Neha Malik, and Matt Gracie. “Using AI to Unleash the\nPower of Unstructured Government Data.” Deloitte Insights (2019).\n[30] Ross Intelligence. Last accessed June 15, 2020.\n[31] SpotDraft. Last accessed June 15, 2020.\n[32] Siren: Investigative Intelligence Platform. Last accessed June 15, 2020.\n[33] LexPredict. LexNLP by LexPredict, (GitHub repo). Last accessed June 15, 2020.\n[34] LexPredict. LexPredict Legal Dictionaries, (GitHub repo). Last accessed June 15,\n2020.\n[35] ContraxSuite. Last accessed June 15, 2020.\n368 \n| \nChapter 10: Healthcare, Finance, and Law\n",
        "word_count": 174,
        "char_count": 1272,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 399,
        "text": "PART IV\nBringing It All Together\n",
        "word_count": 6,
        "char_count": 33,
        "fonts": [
          "MyriadPro-SemiboldCond (28.4pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 400,
        "text": "",
        "word_count": 0,
        "char_count": 0,
        "fonts": [],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 401,
        "text": "CHAPTER 11\nThe End-to-End NLP Process\nThe process is more important than the goal. The person you become\nis infinitely more valuable than whatever the result is.\n—Anthony Moore\nSo far in the book, we’ve addressed a range of NLP problems, starting from what an\nNLP pipeline looks like to how NLP is applied in different domains. Efficiently apply‐\ning what we’ve learned to build end-to-end software products involving NLP takes\nmore than just stitching together various steps in an NLP pipeline—there are several\ndecision points during the process. While a lot of this knowledge comes only with\nexperience, we’ve distilled some of our knowledge about the end-to-end NLP process\nin this chapter to help you hit the ground running faster and better.\nIn Chapter 2, we already saw what a typical pipeline for an NLP system looks like.\nHow is this chapter then any different from that? In Chapter 2, we focused primarily\non the technical aspects of the pipeline—for example, how do we represent text?\nWhat pre-processing steps should we do? How do we build a model, and then how do\nwe evaluate it? In the subsequent chapters in Parts I and II of the book, we delved\ndeeper into different algorithms to perform various NLP tasks. We also saw how NLP\nis used in various industry domains, such as healthcare, e-commerce, and social\nmedia. However, in all these chapters, we spent little time on the issues related to\ndeploying and maintaining such systems and on the processes to follow when manag‐\ning such projects. These are the focus of this chapter. Most of the points discussed\nhere are broadly applicable not just to NLP, but also to other concepts, such as data\nscience (DS), machine learning, artificial intelligence (AI), etc. Throughout this chap‐\nter, we use these terms interchangeably; where the focus is specifically on NLP tasks,\nwe mention that explicitly.\n371\n",
        "word_count": 315,
        "char_count": 1870,
        "fonts": [
          "MyriadPro-SemiboldCond (16.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (9.3pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MinionPro-Regular (9.3pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 402,
        "text": "We’ll start the discussion by revisiting the NLP pipeline we introduced in Chapter 2\nand take a look at the last two steps: deployment, followed by monitoring and updat‐\ning the model, which we didn’t cover in earlier chapters. We’ll also see what it takes to\nbuild and maintain a mature NLP system. This is followed by a discussion on the data\nscience processes followed in various AI teams, especially with respect to building\nNLP software in particular. We’ll conclude the chapter with a lot of recommenda‐\ntions, best practices, and do’s and don’ts to successfully deliver NLP projects. Let’s\nstart by looking at how to deploy NLP software.\nRevisiting the NLP Pipeline: Deploying NLP Software\nIn Chapter 2, we saw that a typical production pipeline for NLP projects consists of\nthe following stages: data acquisition, text cleaning, text pre-processing, text repre‐\nsentation and feature engineering, modeling, evaluation, deployment, monitoring,\nand model updating. When we encounter a new problem scenario involving NLP in\nour organization, we have to first start thinking about creating an NLP pipeline cov‐\nering these stages. Some of the questions we should ask ourselves in this process are:\n• What kind of data do we need for training the NLP system? Where do we get this\ndata from? These questions are important at the start and also later as the model\nmatures.\n• How much data is available? If it’s not enough, what data augmentation techni‐\nques can we try?\n• How will we label the data, if necessary?\n• How will we quantify the performance of our model? What metrics will we use to\ndo that?\n• How will we deploy the system? Using API calls over the cloud, or a monolith\nsystem, or an embedded module on an edge device?\n• How will the predictions be served: streaming or batch process?\n• Would we need to update the model? If yes, what will the update frequency be:\ndaily, weekly, monthly?\n• Do we need a monitoring and alerting mechanism for model performance? If\nyes, what kind of mechanism do we need and how will we put it in place?\n372 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 363,
        "char_count": 2098,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 403,
        "text": "Once we’ve thought through these key decision points, a broad design of our pipeline\nis ready! We can then start to focus on building version 1 of the model with strong\nbaselines, implementing the pipeline, deploying the model, and from there, iteratively\nimproving our solution. In Chapter 2, we saw how different stages of the NLP pipe‐\nline before deployment are implemented for various NLP tasks. Let’s now take a look\nat the final stages of the pipeline: deployment, monitoring, and model updating.\nWhat does deployment mean? Any NLP model we build is typically a part of some\nlarger software system. Once our model is working well in isolation, we plug it into a\nlarger system and ensure that everything is working well. The set of all of the tasks\nrelated to integrating the model with the rest of the software and making it\nproduction-ready is called deployment. Typical steps in deployment of a model\ninclude:\n1. Model packaging: If the model is large, it might need to be saved in persistent\ncloud storage, such as AWS S3, Azure Blob Storage, or Google Cloud Storage, for\neasy access. It might also be serialized and wrapped up in a library call for easy\naccess. There are also open formats like ONNX [1] that provide interoperability\nacross different frameworks.\n2. Model serving: The model can be made available as a web service for other serv‐\nices to consume. In cases where a more tightly coupled system and batch process\nis more applicable, the model could be part of a task flow system like Airflow [2],\nOozie [3], or Chef [4], instead of a web service. Microsoft has also released refer‐\nence pipelines for MLOps [5] and MLOps in Python [6].\n3. Model scaling: Models that are hosted as web services should be able to scale with\nrespect to request traffic. Models that are running as part of a batch service\nshould also be able to scale with respect to the input batch size. Public cloud plat‐\nforms as well as on-premise cloud systems have technologies that enable that.\nFigure 11-1 shows one such pipeline for text classification on AWS. More details\non the engineering of this pipeline can be found in the AWS post [7].\nRevisiting the NLP Pipeline: Deploying NLP Software \n| \n373\n",
        "word_count": 382,
        "char_count": 2200,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 404,
        "text": "Figure 11-1. AWS Cloud and SageMaker to serve text classification [8]\nLet’s look at an example to understand the deployment of an NLP model into a larger\nsystem.\nAn Example Scenario\nLet’s say we work for a social media platform and are asked to build a classifier to\nidentify abusive user comments. The goal of this classifier is to prevent abusive con‐\ntent from appearing on the platform by flagging any content that’s potentially abusive\nand sending it for human moderation. We worked hard on collecting the data rele‐\nvant to this task, designing a set of features, and testing a range of algorithms, and we\nbuilt a predictive model that takes a new comment as input and classifies it as abusive\nor safe. What next?\n374 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 134,
        "char_count": 767,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1409,
            "height": 1344,
            "ext": "png",
            "size_bytes": 122048
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 405,
        "text": "Our model is just a small part of the larger social media platform. There are several\ncomponents: content is being rendered dynamically, and there are various modules to\ninteract with users, components responsible for storage and retrieval of data, and so\non. It’s possible that different subsystems of the platform are written in different pro‐\ngramming languages. Our classifier is just a small component of the product, and we\nneed to integrate it into the larger setup. How do we go about doing this? A common\nway to address this scenario is to create a web service where the model sits behind the\nweb service. The rest of the product interacts with the model via this web service. It\nqueries the service with the new comment(s) and gets back the prediction(s). The call\nto this web service is integrated into the product wherever necessary. Popular web\napplication frameworks such as Flask [9], Falcon [10], and Django [11] are typically\nused to create such web services.\nDeveloping various NLP solutions involves relying on a range of pre-existing libra‐\nries. Setting up a web service and hosting what we built in the cloud or some server\nrequires us to ensure that there are no compatibility issues. To address this, there is a\nrange of options available. The most common option is to package various libraries\ninto a container like Docker [12] or Kubernetes [13]. Operationalizing a web service\nfor production requires addressing many other issues, such as tech stack, load balanc‐\ning, latency, throughput, availability, and reliability. Building and making a model\nproduction ready includes a whole lot of engineering tasks, which can often be time\nconsuming. Cloud services such as AWS SageMaker [14] and Azure Cognitive Serv‐\nices [15] try to make these engineering tasks easy. Sometimes, the whole process, to\nthe last detail, is automated to such an extent that it’s as simple as one-click-get-done\nto set up the service. The idea is to let the AI teams focus on the most important part:\nmodel building.\nAnother important issue to address is model size. Modern NLP models can be quite\nlarge. For example, Google’s Word2vec model is 4.8 GB in size and takes over 100 sec‐\nonds just to load into memory (refer back to Ch3/Pre_Trained_Word_Embed‐\ndings.ipynb). Likewise, a fastText classification model is typically over 2 GB in size.\nDL models like BERT are known to be even bulkier. Hosting such large models in the\ncloud can be both challenging and expensive. There’s a lot of work happening in the\narea of model compression to address such scenarios. Some of them are listed below:\n• “Compressing BERT for Faster Prediction,” a blog post by a team at Rasa NLP\n[16]\n• “A Survey of Model Compression and Acceleration for Deep Neural Networks,” a\nreport by a team at Microsoft Research and Tsinghua University [17]\n• “FastText.zip: Compressing text classification models,” a report by a team at Face‐\nbook AI Research [18]\nRevisiting the NLP Pipeline: Deploying NLP Software \n| \n375\n",
        "word_count": 500,
        "char_count": 2996,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 406,
        "text": "• “Awesome ML Model Compression,” a GitHub repository by Cedric Chee that\nincludes relevant papers, videos, libraries, and tools [19]\nThis is just a brief overview of various steps that go into deploying our NLP model.\nThere are books and other materials that cover this in complete detail. As a start,\ninterested readers can look at the later chapters of the book Machine Learning Engi‐\nneering [20].\nFor most industry use cases, model building is seldom a one-time activity. As the\ndeployed system gets used more, the models built need to adapt to new scenarios and\nnew data points. Hence, the models should be updated regularly. Let’s discuss the\nissues to consider while building and maintaining mature NLP software.\nBuilding and Maintaining a Mature System\nIn most real-world settings, the underlying patterns in data change over a period of\ntime. This means that the models that were trained long before can become stale—\ni.e., the data used to train the model is very different from the data in the production\nenvironment that’s being fed to the model for predictions. This is called covariate\nshift, and it results in a performance drop of the model. Model update is a common\napproach to deal with such scenarios. On a similar note, in most industrial settings,\nonce the first version of a model is consumed, improving the model becomes inevita‐\nble. Updating and improving an existing NLP model could just mean retraining with\nnewer or additional training data, or it sometimes involves adding new features.\nWhen updating such models, the goal is to ensure that the deployed system performs\nat least as well as the existing system. Most model updates and improvements lead to\nmore complex models. As the models grow in complexity, we need to ensure that the\nsystem doesn’t crumble under increasing complexity. We need to manage the com‐\nplexity of a mature NLP model while making sure it’s also maintainable. Some of the\nissues we need to consider in this process are:\n• Finding better features\n• Iterating existing models\n• Code and model reproducibility\n• Troubleshooting and testing\n• Minimizing technical debt\n• Automating the ML process\nIn this section, let’s take a look at these issues one by one, starting with a discussion\nabout how to find better features.\n376 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 389,
        "char_count": 2323,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 407,
        "text": "Finding Better Features\nThroughout this book, we’ve repeatedly stressed the importance of building a simple\nmodel first. This version 1 model is seldom an end in itself. We may keep on adding\nnew features and periodically retraining the model beyond V1. Our goal is to find the\nfeatures that are most expressive to capture the regularities in the data that are useful\nfor making predictions. How can we develop such features? We saw different ways to\ngenerate textual feature representations in Chapter 3. We can start with one of those\nthat doesn’t require prior knowledge about the problem domain (e.g., basic vectoriza‐\ntion, distributed representations, and universal representations) or use our prior\nknowledge about the problem and domain to develop specific features for our prob‐\nlem (i.e., handcrafted features) or use a combination of both.\nDesigning specific features for a given problem (or feature engineering) can be both\ndifficult and expensive. This is why problem-agnostic text representations are com‐\nmonly used as a starting point. However, domain-specific features have their own\nvalue. For example, in a task of sentiment classification, more than vector representa‐\ntions of raw text, domain-specific indicators, such as count of negative words, count\nof positive words, and other word- and phrase-level features, are useful to extract the\nsentiment in a more robust manner.\nLet’s say we implemented a bunch of features to build our NLP models. Does the best\nmodel need each one of these features? How do we choose the most informative fea‐\ntures among the several we implemented? For example, if we use two features where\none can be derived from the other, we’re not adding any extra information to the\nmodel. Feature selection is a great technique to handle such cases and make informed\ndecisions. There are plenty of statistical methods that can be used to fine-tune our\nfeature sets by removing redundant or irrelevant features. This broad area is called\nfeature selection.\nTwo popular techniques for feature selection are wrapper methods and filter meth‐\nods. Wrapper methods use an ML model to score feature subsets. Each new subset is\nused to train a model, which is tested on a hold-out set and then used to identify the\nbest features based on the error rate of the model. Wrapper methods are computa‐\ntionally expensive, but they often provide the best set of features. Filter methods use\nsome sort of proxy measure instead of the error rate to rank and score features (e.g.,\ncorrelation among the features and correlation with the output predictions). Such\nmeasures are fast to compute while still capturing the usefulness of the feature set.\nFilter methods are usually less computationally expensive than wrappers, but they\nproduce a feature set that’s not as well optimized to a specific type of predictive\nmodel. In DL-based approaches, while feature engineering and feature selection is\nautomated, we have to experiment with various model architectures.\nBuilding and Maintaining a Mature System \n| \n377\n",
        "word_count": 488,
        "char_count": 3040,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 408,
        "text": "Since feature selection methods are usually task specific (i.e., methods for classifica‐\ntion tasks are different from methods for, say, machine translation), interested readers\ncan look into resources such as sparse features, dense features, and feature interac‐\ntions from Wide and Deep Learning from Google AI [21]. The book Feature Engi‐\nneering for Machine Learning [22] would also be useful. However, we hope this\noverview convinced you of feature selection’s role in building mature, production-\nquality NLP systems. Assuming we’re going through this process of adding new fea‐\ntures and evaluating them, how should we incorporate them into our training process\nand update our NLP models? Let’s take a look at this question now.\nIterating Existing Models\nAs we mentioned earlier, any NLP model is seldom a static entity. We’re often\nrequired to update our models even in production systems. There are several reasons\nfor this. We may get more (and newer) data that differs from previous training data.\nIf we don’t update our model to reflect this change, it will soon become stale and\nchurn out poor predictions. We may get some user feedback on where the model pre‐\ndictions are going wrong. This will then require us to reflect on the model and its fea‐\ntures and make amendments accordingly. In both cases, we need to set up a process\nto periodically retrain and update the existing model and deploy the new model in\nproduction.\nWhen we develop a new model, intuitively, it’s always good to compare the results\nwith our previous best models to understand the incremental value addition. How do\nwe know this new model is better than the existing one? The analysis of model per‐\nformance can be based on comparing raw predictions from both models, or it could\nbe from a perspective of a derived performance based on the predictions. Let’s explain\nthese two cases by revisiting the abusive comments detection example from earlier in\nthis chapter.\nLet’s say we have a gold standard test set of abusive versus non-abusive comments.\nWe can always use this to compare an old model with the new one in terms of, say,\nclassification accuracy. We can also follow an external validation approach and look\nfor other aspects, such as how many model decisions were contested by users every\nday. It would be practical to set up a dashboard to monitor these metrics periodically\nand display them for each model so that we can choose the one that’s the best\nimprovement over the current model among the various models we may build. We\ncan also A/B test a new model with an old model (or any baseline system) and meas‐\nure business KPIs to see how well the new model performs. When onboarding a new\nmodel, it might also be a good practice to first roll it out to a small fraction of users,\nmonitor its performance, and then progressively expand it to the entire user base.\n378 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 497,
        "char_count": 2912,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 409,
        "text": "Code and Model Reproducibility\nMaking sure your NLP models continue working in the same fashion in different\nenvironments can be critical for the long-term success of any project. A model or\nresult that’s reproducible is generally considered more robust. There is a range of best\npractices you can use to achieve this while building systems.\nMaintaining separation between code, data, and model(s) is always a good strategy.\nSeparating code and data is generally a best practice in software engineering, and it\nbecomes even more critical for AI systems. While there are established version con‐\ntrol systems for code, such as Git, versioning of models and datasets can be different.\nAs of recently, there are tools like Data Version Control [23] that address this issue.\nIt’s always a good practice to name model and data versions appropriately so that we\ncan revert back easily, if needed. While storing the models, you should try to have all\nyour model parameters, along with other variables, in a separate file. Similarly, try to\navoid hardcoded parameter values in your model. If you have to use arbitrary num‐\nbers in your training process (e.g., a seed value somewhere), explain it in the code as\ncomments.\nAnother good practice is creating checkpoints in your code and model often. You\nshould store your learned model in a repository both periodically and at milestones.\nWhen training a model, it’s also a good idea to use the same seed wherever random\ninitialization is used. This ensures that the model creates similar results (and internal\nrepresentation) every time the same parameters and data are used.\nA keystone for improving reproducibility is to make sure to note all steps explicitly.\nThis is especially necessary in the exploratory phase of data analysis. On the same\nnote, it helps to record as many intermediate steps and data outputs as possible. This\nhelps in transforming your experimental model to an in-production model without\nany loss of information. To read further, we would suggest a report on AI reproduci‐\nbility state of the art [24] and an interview of a reproducibility researcher at Face‐\nbook, Joelle Pineau [25]. This brings us to the next topic in this section. While\nmaking all these iterations and building multiple models, how do we ensure there are\nno errors and bugs in the training process and that our data isn’t noisy? How do we\ntroubleshoot and test our code and models?\nTroubleshooting and Interpretability\nTo maintain the quality of software, testing is a key step in any software development\nprocess. However, considering the probabilistic nature of ML models, how to test ML\nmodels is not obvious. Figures 11-2 and 11-3 illustrate some of the good practices for\ntesting out AI systems. We already saw how to use Lime (Figure 11-3) in Chapter 4.\nBuilding and Maintaining a Mature System \n| \n379\n",
        "word_count": 472,
        "char_count": 2847,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 410,
        "text": "Figure 11-2. TensorFlow Model Analysis (TFMA) [26]\nFigure 11-3. Lime for NLP model analysis\n380 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 22,
        "char_count": 139,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1111,
            "height": 668,
            "ext": "png",
            "size_bytes": 56192
          },
          {
            "index": 1,
            "width": 1307,
            "height": 935,
            "ext": "png",
            "size_bytes": 47189
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 411,
        "text": "As we discussed earlier in the chapter, a model is just a small component of any AI\nsystem. When it comes to testing the entire system, barring the model, most techni‐\nques for testing of software engineering are applicable and work well. When it comes\nto testing the model, the following steps are helpful:\n• Run the model on train, validation, and test datasets used during the model-\nbuilding phase. There should not be any major deviation in the results for any of\nthe metrics. K-fold cross validation is often used to verify model performance.\n• Test the model for edge cases. For example, for sentiment classification, test with\nsentences with double or triple negation.\n• Analyze the mistakes the model is making. The findings from the analysis should\nbe similar to the findings from the analysis of the mistakes it was making during\nthe development phase. For NLP, packages and techniques like TensorFlow\nModel Analysis [26], Lime [27], Shap [28], and attention networks [5] can give a\ndeeper understanding of what the model is doing deep down. You can see this in\naction in Figures 11-2 and 11-3. The insights from these during development and\nproduction should not change much.\n• Another good practice is to build a subsystem that keeps track of key statistics of\nthe features. Since all features are numerical, we can maintain statistics like mean,\nmedian, standard deviation, distribution plots, etc. Any deviation in these statis‐\ntics is a red flag, and we’re likely to see the system churning out wrong predic‐\ntions. The reason could be as simple as a bug in the pipeline or as complex as a\ncovariate shift in the underlying data. Packages like TensorFlow Model Analysis\n[26] can track these metrics. Figure 11-4 shows distributions for metrics of vari‐\nous features for a dataset that can be tracked to find covariate shift or bugs.\nFigure 11-4. Feature statistics in TensorFlow Extended [29]\nBuilding and Maintaining a Mature System \n| \n381\n",
        "word_count": 330,
        "char_count": 1959,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1124,
            "height": 531,
            "ext": "png",
            "size_bytes": 48454
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 412,
        "text": "• Create dashboards for tracking model metrics and create an alerting mechanism\non them in case there are any deviations in the metrics. We’ll discuss this point in\ndetail in the next section.\n• It’s always good to know what a model is doing inside. This goes a long way\ntoward understanding why a model is behaving in a certain way. A key question\nin AI has been how to create intelligent systems where we can explain why the\nmodel is doing what it is doing. This is called interpretability. It’s the degree to\nwhich a human can understand the cause of a decision [30]. While many algo‐\nrithms in machine learning (such as decision trees, random forest, XGboost, etc.)\nand computer vision have been very interpretable, this is not true for NLP, espe‐\ncially DL algorithms. With recent techniques such as attention networks, Lime,\nand Shapley, we have greater interpretability in NLP models. Interested readers\ncan look at Interpretable Machine Learning by Christoph Molnar [31] for further\ndiscussion on this topic.\nMonitoring\nOnce an ML system has been deployed and is in production, we need to make sure\nthe model continues working well. As an example deployment, if the model is being\ntrained automatically every day with new data points, certain bugs can creep in, or\nthe model can malfunction. To ensure that this doesn’t happen, we need to monitor\nthe model for a range of things and trigger alerts at the right points:\n• Model performance has to be monitored regularly. For a web service–based\nmodel, it can be the mean and various percentiles—50th (median), 90th, 95th,\nand 99th (or deeper)—for response time. If the model is deployed as a batch ser‐\nvice, statistics on the batch processing and task times have to be monitored.\n• Similarly, it helps to store monitor model parameters, behavior, and KPIs. Model\nKPIs for the abusive comments example would be the percentage of comments\nthat were reported by users but not flagged by the model. For a text classification\nservice, it could be the distribution of classes that are classified each day.\n• For all the metrics we’re monitoring, we need to periodically run them through\nan anomaly detection system that can alert changes in normal behavior. This\ncould be a sudden spike in the response rate of a web service or a sudden drop in\nretraining times. In the worst case, when the performance drops substantially, we\nmay also want to hit circuit breakers (i.e., move to a more stable model or a\ndefault approach).\n• If our overall engineering pipeline is using a logging framework, there’s a good\nchance it also has support for monitoring anomalies over time for any metric.\nFor instance, ELK stack by Elastic offers built-in anomaly detection [7].\n382 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 470,
        "char_count": 2758,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 413,
        "text": "Sumo Logic also flags outliers that can be queried as needed [32]. Microsoft also\noffers anomaly detection as a service [33].\nMonitoring our ML models and their deployments can save substantial time as the\nproject scales. As the system matures and the model stabilizes, proper monitoring\nallows MLOps teams to largely manage it, so data scientists can solve other harder\nproblems. Although, as systems mature, we also start accumulating more technical\ndebt, which we’ll cover in the next section.\nMinimizing Technical Debt\nThroughout this book, and especially in this chapter, we’ve seen various aspects of\ntraining NLP models, deploying them as a part of a larger system, and iteratively\nimproving from there on. As we start iterating from the first version of the system,\nthe system and various components, including the model, can easily become com‐\nplex. This brings the challenges of maintaining the system. We may have scenarios\nwhere we don’t know if the incremental improvements justify the complexity. Such\nscenarios can create a technical debt. Let’s take a brief look at addressing technical\ndebt in building AI software.\nIt’s important to plan and build for the future when working with any software sys‐\ntem. We have to ensure that our system continues being both performant and easy to\nmaintain after all these continuous iterations and testing. Unused and poorly imple‐\nmented improvements can create technical debt. If we’re not using a feature or any of\nits combinations with other features, it’s important to drop it out of the pipeline. A\nfeature or part of the code that doesn’t work just clogs our infrastructure, hinders fast\niteration, and brings down clarity.\nA good rule of thumb is to look at the coverage a feature provides. If a feature is\npresent in only a few data points, say, 1%, then maybe it’s not worth keeping. But even\nsomething like this can’t be applied blindly. For example, if the same feature covers\njust 1% of the data but gives 95% classification accuracy just based on that feature,\nthen it’s really effective and most certainly worth continuing to use. From our experi‐\nence, an important tip (that we’ve also reiterated several times in the book) is: opt for\na simpler model that has performance comparable to a much more complex model if you\nwant to minimize technical debt. Complex models may become necessary if there’s no\nequivalent simple model though.\nBuilding and Maintaining a Mature System \n| \n383\n",
        "word_count": 406,
        "char_count": 2454,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 414,
        "text": "Besides these recommendations, we’d also like to share some landmark work on\nbuilding mature ML systems:\n• “A Few Useful Things to Know About Machine Learning” by Pedro Domingoes\nof the University of Washington [34]\n• “Machine Learning: The High-Interest Credit Card of Technical Debt” by a team\nat Google AI [35]\n• “Hidden Technical Debt in Machine Learning Systems” by a team at Google\nAI [36]\n• Feature Engineering for Machine Learning, a book written by Alice Zheng and\nAmanda Casari [22]\n• “Ad Click Prediction: A View from the Trenches,” a work by a Google Search\nteam on the issues faced by a large online ML system [37]\n• “Rules of Machine Learning,” an online guide created by Martin Zenkovich of\nGoogle [38]\n• “The Unreasonable Effectiveness of Data,” a report by renowned UC Berkeley\nresearcher Peter Norvig and a Google AI team [39]\n• “Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,” another\nmodern look at the previous report by a team from Carnegie Mellon\nUniversity [40]\nSo far, we’ve discussed various best practices used in building mature AI systems.\nFrom finding better features to version control of datasets, these practices are manual\nand effort intensive. Driven by the ultimate goal of building intelligent machines and\nreducing manual effort, an interesting recent work has been to automate some\naspects of building AI systems. Let’s look at some key efforts in this direction.\nAutomating Machine Learning\nOne of the holy grails of machine learning is to automate more and more of the fea‐\nture engineering process. This has led to the creation of a subarea called AutoML\n(automated machine learning), which aims to make machine learning more accessi‐\nble. In most cases, it generates a data analysis pipeline that can include data pre-\nprocessing, feature selection, and feature engineering methods. This pipeline\nessentially selects ML methods and parameter settings that are optimized for a spe‐\ncific problem and data. As all of these steps can be time consuming for the ML expert\nand may be intractable for a beginner, AutoML can be a much-needed bridge for a\ngap in the world of machine learning. AutoML is itself essentially “doing machine\nlearning using machine learning,” making this powerful and complex technology\nmore widely accessible for those hoping to make use of massive amounts of data.\n384 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 395,
        "char_count": 2398,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 415,
        "text": "As an example, one research group at Google has used AutoML techniques [41] for\nlanguage modeling with the Penn Treebank dataset. Penn Treebank is a benchmark\ndataset for linguistic structure [42]. The research group at Google found that their\nAutoML approach can design models that achieve accuracies on par with state-of-\nthe-art models designed by world-class machine learning experts. Figure 11-5 shows\nan example of a neural network generated by AutoML.\nFigure 11-5. AutoML-generated network [41]\nOn the left side of the figure is a neural network that Google experts created to parse\ntext. On the right side is another network that was created automatically by Google’s\nAutoML. AutoML that explores various neural network architectures automatically\nperformed as well as the handcrafted model. It’s fascinating to see that their system\ndid almost as well as humans even for designing ML models.\nAutoML is the cutting edge of machine learning. One should only build it from the\nbottom up when more traditional methods for improving performance are exhaus‐\nted. It often requires a high amount of computing and GPU resources and a higher\nlevel of technical skill when doing it from scratch.\nauto-sklearn\nAs we mentioned previously, it’s generally a good idea to work on automating\nmachine learning only after most other options have been exhausted. In cases where\nthe need for AutoML [43] is more clear, one of the best libraries for applying it is\nauto-sklearn. It uses recent advancements in Bayesian optimization and meta-\nlearning to search in a huge hyperparameter space to figure out a reasonably good\nML model on its own. As it’s integrated with sklearn, which is one of the more popu‐\nlar ML libraries, using it is quite simple:\nBuilding and Maintaining a Mature System \n| \n385\n",
        "word_count": 292,
        "char_count": 1790,
        "fonts": [
          "MyriadPro-SemiboldCond (11.6pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 822,
            "height": 383,
            "ext": "png",
            "size_bytes": 62379
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 416,
        "text": "  import autosklearn.classification\n  import sklearn.model_selection\n  import sklearn.datasets\n  import sklearn.metrics\n  X, y = sklearn.datasets.load_digits(return_X_y=True)\n  X_train, X_test, y_train, y_test = \\\n        sklearn.model_selection.train_test_split(X, y, random_state=1)\n  automl = autosklearn.classification.AutoSklearnClassifier()\n  automl.fit(X_train, y_train)\n  y_hat = automl.predict(X_test)\n  print(\"Accuracy\", sklearn.metrics.accuracy_score(y_test, y_hat))\nThis code builds an autosklearn classifier for the MNIST digits dataset [44]. It splits\nthe dataset into training and test sets. While running for about an hour, this will auto‐\nmatically yield accuracy of over 98%.\nWhen we peek through what’s happening internally, we see different stages of\nAutoML, as shown in the snippet below:\n[(0.080000, SimpleClassificationPipeline({'balancing:strategy': 'none',\n'categorical_encoding:__choice__': 'one_hot_encoding', 'classifier:__choice__': \n'lda',\n'imputation:strategy': 'mean', 'preprocessor:__choice__': 'polynomial',\n'rescaling:__choice__': 'minmax',\n'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True',\n'classifier:lda:n_components': 151,\n'classifier:lda:shrinkage': 'auto', 'classifier:lda:tol': \n0.02939556179271624,\n'preprocessor:polynomial:degree': 2, 'preprocessor:polynomial:include_bias': \n'True',\n'preprocessor:polynomial:interaction_only': 'True',\n'categorical_encoding:one_hot_encoding:minimum_fraction': 0.0729529152649298},\ndataset_properties={\n  'task': 2,\n  'sparse': False,\n  'multilabel': False,\n  'multiclass': True,\n  'target_type': 'classification',\n  'signed': False})),\n...\n...\n...\n...\n(0.020000, SimpleClassificationPipeline({'balancing:strategy': 'none', \n'categorical_encoding:__choice__':\n'one_hot_encoding', 'classifier:__choice__': 'passive_aggressive', \n'imputation:strategy': 'mean',\n'preprocessor:__choice__': 'polynomial', 'rescaling:__choice__': 'minmax',\n'categorical_encoding:one_hot_encoding:use_minimum_fraction': 'True', \n'classifier:passive_aggressive:C':\n0.03485276894122253, 'classifier:passive_aggressive:average': 'True',\n'classifier:passive_aggressive:fit_intercept': 'True', \n386 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 162,
        "char_count": 2210,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "UbuntuMono-Bold (8.5pt)",
          "UbuntuMono-Regular (8.5pt)",
          "UbuntuMono-Regular (10.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 417,
        "text": "'classifier:passive_aggressive:loss': 'hinge',\n'classifier:passive_aggressive:tol': 4.6384320611389e-05, \n'preprocessor:polynomial:degree': 3,\n'preprocessor:polynomial:include_bias': 'True', \n'preprocessor:polynomial:interaction_only': 'True',\n'categorical_encoding:one_hot_encoding:minimum_fraction': 0.11994577706637469},\ndataset_properties={\n  'task': 2,\n  'sparse': False,\n  'multilabel': False,\n  'multiclass': True,\n  'target_type': 'classification',\n  'signed': False})),\n]\nauto-sklearn results:\n  Dataset name: d74860caaa557f473ce23908ff7ba369\n  Metric: accuracy\n  Best validation score: 0.991011\n  Number of target algorithm runs: 240\n  Number of successful target algorithm runs: 226\n  Number of crashed target algorithm runs: 1\n  Number of target algorithms that exceeded the time limit: 2\n  Number of target algorithms that exceeded the memory limit: 11\nNext, let’s take a look at Google Cloud services, as well as a few other approaches to\nNLP problems.\nGoogle Cloud AutoML and other techniques\nGoogle Cloud Services has also recently released AutoML as a service. This doesn’t\nrequire any technical knowledge beyond providing training data in the expected for‐\nmat. They’ve specifically built Cloud AutoML services for different parts of AI,\nincluding computer vision and structured tabular data, as well as for NLP.\nFor NLP, their Cloud AutoML is applied automatically when training custom models\nfor:\n• Text classification\n• Entity extraction\n• Sentiment analysis\n• Machine translation\nFor all these tasks, Google Cloud has defined a specific format that the AutoML mod‐\nels expect the data to be in. More information on these can be found in their docu‐\nmentation [45, 46]. Microsoft also has tooling for AutoML in their Azure Machine\nLearning [47].\nBuilding and Maintaining a Mature System \n| \n387\n",
        "word_count": 231,
        "char_count": 1816,
        "fonts": [
          "UbuntuMono-Regular (8.5pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (11.6pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 418,
        "text": "Another interesting approach to tackling an NLP problem in a more automated way\nis to use the AutoCompete framework created by Abhishek Thakur [48], a top-\nranked Kaggle Competitions Grandmaster. Even though his initial approach was to\nfocus on any data science problem specifically targeted to competitions, it has now\nevolved to a general framework to solve such problems. He has also released a\ndetailed notebook titled “Approaching (Almost) Any NLP Problem on Kaggle” [49]\nthat creates a general modeling framework for NLP problems with a well-defined\ndataset and goals. While this may not completely solve the specific NLP task you’re\nworking at, it’s a good start to look at creating baseline models.\nSo far, we’ve addressed a range of issues that might come up when trying to build,\ndeploy, and maintain NLP software. However, an equally important component of\nsuch an endeavor is to follow standard product development processes. While the\nfield of software development processes and life cycle is well established, there are\nsome important things to consider while working on projects that involve predictive\nmodels like the ones we’ve discussed throughout the book. Let’s now take a look at\nthat aspect.\nThe Data Science Process\nData science is a broad term describing the algorithms and processes used to extract\nmeaningful information and actionable insights from all forms of data. Thus, all NLP\nwork in the industry can be categorized under the data science umbrella. While data\nscience as a term is relatively new, it’s been around in some form or another for the\npast few decades. Over the years, people have formulated and formalized the best\nprocesses and practices of working with data. Two popular processes in the industry\nare the KDD process and the Microsoft Team Data Science Process.\nThe KDD Process\nThe ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) is\none of the oldest and most reputed data mining conferences in the world. Some of\nthe founders of the conference also created the KDD process in 1996. The KDD pro‐\ncess [50], depicted in Figure 11-6, consists of a series of steps that should be applied\nto a data science or data mining problem to get better results.\n388 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 374,
        "char_count": 2262,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 419,
        "text": "Figure 11-6. The KDD process [50]\nThese steps are ordered as follows:\n1. Understanding the domain: This includes learning about the application and\nunderstanding the goals of a problem. It also involves getting deeper into the\nproblem domain and extracting relevant domain knowledge.\n2. Target dataset creation: This includes selecting a subset of data and variables the\nproblem will focus on. We may have a plethora of data sources at our disposal,\nbut we focus on the subset we need to work on.\n3. Data pre-processing: This encompasses all activities needed so that the data can be\ntreated coherently. This includes filling missing values, noise reduction, and\nremoving outliers.\n4. Data reduction: If the data has a lot of dimensions, this step can be used to make\nit easier to work with. This includes steps like dimensionality reduction and pro‐\njecting the data into another space. This step is optional depending on the data.\n5. Choosing the data mining task: Various classes of algorithms can be applied to a\nproblem. They may be regression, classification, or clustering. It’s important to\nselect the right task based on our understanding from Step 1.\n6. Choosing the data mining algorithm: Based on the selected data mining task, we\nneed to select the right algorithm. For instance, for classification, we can choose\nalgorithms such as SVM, random forests, CNNs, etc., as we saw in Chapter 4.\n7. Data mining: This is a core step of applying the selection algorithm from Step 6\nto the given dataset and creating predictive models. Tuning with respect to\nparameters and hyperparameters also happens here.\nThe Data Science Process \n| \n389\n",
        "word_count": 272,
        "char_count": 1646,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1386,
            "height": 718,
            "ext": "png",
            "size_bytes": 73188
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 420,
        "text": "8. Interpretation: Once the algorithm is applied, the user has to interpret the results.\nThis can be done partially by visualizing various components of results.\n9. Consolidation: This is the final step where we deploy the built model into an\nexisting system, document the approach, and generate reports.\nAs seen in the figure, the KDD process is highly iterative. There can be any number of\nloops between various steps. At each step, we can and may need to go back to earlier\nsteps and refine the information there before moving ahead. This process is a good\nreference when working on a specific data science problem. While not exactly the\nsame, the pipelines we’ve discussed throughout the book deal with the same idea of\nbringing structure to building NLP systems. Now, let’s take a look at the second\nprocess.\nMicrosoft Team Data Science Process\nThe KDD process was introduced in the late ’90s. As the fields of machine learning\nand data science grew, bigger teams working exclusively on such data science projects\nbegan to emerge. Further, in the fast-moving world of data-driven development,\nmore flexible and iteration-based frameworks were needed, so other data science pro‐\ncesses began to emerge. The Microsoft Team Data Science Process (TDSP) addresses\nthis. It was released by the Microsoft Azure team in 2017 and is one of the modern\nprocesses for applying machine learning and working in data science [51].\nTDSP is an agile, iterative data science process for executing and delivering advanced\nanalytics solutions. It’s designed to improve the collaboration and efficiency of data\nscience teams in enterprise organizations. The main features of TDSP are:\n• A data science life cycle definition\n• A standardized project structure, which includes project documentation and\nreporting templates\n• An infrastructure for project execution\n• Tools for data science, like version control, data exploration, and modeling\nThe TDSP documentation [52] provides detailed insight into all of these aspects, so\nwe’ll just take a brief look in this section. The TDSP data science life cycle, showing\ndifferent phases of a data project, is shown in Figure 11-7.\n390 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 355,
        "char_count": 2206,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 421,
        "text": "Figure 11-7. The Microsoft TDSP life cycle [51]\nWhile TDSP shares some similarities with the KDD process, an interesting aspect of\nTDSP is that it defines a life cycle of a data science project from a business and team\nmanagement perspective. This includes the following stages:\n• Business understanding\n• Data acquisition and understanding\n• Modeling\n• Deployment\n• Customer acceptance\nAt a high level, the data science life cycle showcases how various components of an\neffective and agile data science team should operate. The “Charter” and “Exit Report”\ndocuments in the TDSP documentation are particularly important to consider. They\nhelp define the project at the start of an engagement and provide a final report to the\ncustomer or client.\nOverall, these processes can be useful for taking the problems and solutions we’ve\ndiscussed so far in this book from prototyping to deployment in a production system.\nThese processes are of course not specific to NLP and are more generic recommenda‐\ntions for any data-driven projects involving ML approaches. While there are other\nThe Data Science Process \n| \n391\n",
        "word_count": 179,
        "char_count": 1112,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1440,
            "height": 1001,
            "ext": "png",
            "size_bytes": 122856
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 422,
        "text": "similar project management processes for data science that are emerging as the field\ngrows, we hope this gives you an overview of what to look out for in managing your\nown NLP projects in a software development setting.\nMaking AI Succeed at Your Organization\nSo far, this book has focused on successfully building and deploying solutions for var‐\nious AI problems. Success of any AI project is dependent not just on the technical\nsuperiority of the solution—there are many other factors involved, too. It’s a known\nfact that a large number of AI projects in industry fail because the model doesn’t get\ndeployed or, if deployed, fails to achieve its objectives. According to a recent study by\nGartner [53], more than 85% of AI projects fail. Here, we discuss some key points\nand rules of thumb to make AI projects succeed. Many of these points come from our\nown experience of working in various domains of AI across various organizations.\nTeam\nIt’s important to have the right team to solve the AI problems at hand. In understand‐\ning the problem statement, prioritizing, developing, deploying, and consuming, a lot\ndepends on the skills of the team. While there’s no fixed recipe, in our experience, the\nright blend comes with having (1) scientists who build models, (2) engineers who\noperationalize and maintain models, and (3) leaders who manage AI teams and strat‐\negize. It’s good to have (4) scientists who have worked in industry after graduate\nschool, (5) engineers who understand scale and data pipelines, and 6) leaders who\nhave also been individual contributor scientists in the past. While (5) is pretty self-\nexplanatory, (4) and (6) warrant some explanation.\nLet’s look at (4) first. It’s important that scientists understand the fundamentals of\nmachine learning and are able to think of novel solutions. Graduate school (especially\na PhD) prepares you well for that. But, in industry, solving an AI problem is not just\napplying novel algorithms. It’s also about collecting and cleaning the data, making the\ndata consumption-ready, and applying known techniques. This is very different from\nacademia, where most work happens on known public datasets that are both readily\navailable and clean. Most researchers in academia work on devising novel approaches\nto beat the state-of-the-art results. In many cases, scientists fresh from academia end\nup applying sophisticated approaches that prove counterproductive. One is building\nAI for products—AI is just a means, and not the end. That’s why it’s important that\nsenior scientists on the team have built and deployed models in industrial settings.\nMoving on to (6): AI leadership is very different from software engineering leader‐\nship. Even though what runs in production in any AI system is code, AI is fundamen‐\ntally different from software engineering. Many leaders and organizations are not\naware of this nuance. They believe that, because it’s code, all the principles of software\nengineering apply to it. From defining the problem statement to planning project\n392 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 497,
        "char_count": 3078,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MyriadPro-SemiboldCond (18.9pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 423,
        "text": "timelines, developing an AI system is different from developing a traditional IT sys‐\ntem. This is why it’s recommended that AI leaders in your organization have the\nexperience of having been individual contributors (ICs) in the AI field.\nRight Problem and Right Expectations\nIn many cases, either the problem at hand is ill defined or AI teams set the wrong\nexpectations. Let’s understand this better with some examples. Consider a scenario\nwhere we’re given a dump of what customers say about a particular product or brand,\nand we’re asked to bring out “interesting” insights. This is a very common scenario in\nindustry; we discussed similar scenarios in “Topic Modeling” on page 250. Now, can\nwe apply topic modeling to this particular scenario? It depends on what “interesting”\nmeans in this context. It could be what the majority of customers are saying, or it\ncould be what a small subset of customers belonging to a particular region are saying,\nor it could be what customers are saying about a specific product feature. The possi‐\nbilities are many. It’s important to work with the stakeholders first to clearly define\nthe task. A great way to do this is to take a set of diverse example inputs that include\nedge cases and ask the stakeholders to write down the desired output. An important\nthing to keep in mind is that the ready availability of a lot of data does not make\nsomething an AI problem by default; many problems can be solved using engineering\nand rule-based and human-in-the-loop approaches.\nAnother common problem is stakeholders having wrong expectations of AI technol‐\nogy. This often happens because of articles in popular media that tend to compare AI\nto the human brain. While that’s correct as a motivation behind the area of AI, it’s far\nfrom the truth. For example, consider a scenario where we built a sentiment analysis\nsystem and, for a given input sentence, our system predicts a wrong output. It gives a\nvery high accuracy, but not 100%. Most stakeholders coming from the world of soft‐\nware engineering treat this as a bug and are not willing to accept anything that’s not\n100% correct. They are not aware of the fact that any AI system (as of today) will give\nwrong output for a subset of inputs. Another expectation of AI is that it will replace\nhuman effort completely, thus saving money. This is seldom the case. It’s better to\ntreat AI as augmented intelligence to support human efforts rather than artificial\nintelligence to replace human efforts. Also, beyond a point, model performance stag‐\nnates and doesn’t continue rising with time. We see this in Figure 11-8, where reality\nbehaves more like an S curve while the expectation continues rising.\nEven a very mature and advanced AI system requires human supervision. In many\ncases, we can reduce human efforts, but that happens over a long period of time. In\nthe same vein, stakeholders coming from software engineering may not understand\nthe importance of building responsible AI. Responsible AI ensures trustworthy solu‐\ntions that are fair, transparent, and accountable. Google [54] and Microsoft [55] have\npublished best practices for building responsible AI systems.\nMaking AI Succeed at Your Organization \n| \n393\n",
        "word_count": 537,
        "char_count": 3215,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 424,
        "text": "Figure 11-8. Expectation versus reality in AI performance\nData and Timing\nData is at the heart of any AI system. We’ve discussed various aspects of data in detail\nin previous chapters. Let’s look at one more: in many cases, just because an organiza‐\ntion has gigabytes or even petabytes of data, it doesn’t mean they’re ready for AI and\ncan quickly reap its benefits. There’s a difference between having data and having the\nright data. Let’s understand this:\nQuality of data\nTo perform well, any AI system needs a high quality of data for both training and\nprediction. What does high quality mean? Data that is structured, homogenous,\ncleaned, and free of noise and outliers. Going from a dump of noisy data to high-\nquality data is often a long process. The best way to think of it is the following\nanalogy: raw data is crude oil and AI models are fighter jets. Fighter jets need avi‐\nation fuel to fly; they cannot fly on crude oil. So, to enable fighter jets, someone\nmust set up the petroleum refinery to systematically extract the aviation fuel\nfrom the crude oil. And setting up this refinery is a long and expensive process.\nAnother important point is to have the right representative data: data that allows\nus to solve the problem at hand. For example, there’s no way we can improve our\nsearch feature if we don’t already have the metadata about what we want to\nsearch. So, if we don’t have “Adidas Shoes Size 10 Tennis Shoes” but only have\n“Adidas Shoes Size 10,” there’s no way we can easily make our search help find\ntennis shoes.\nQuantity of data\nMost AI models are a compressed representation of the dataset used to train\nthem. Not having enough data that’s a true representation of the data the model\nwill see in production is a big reason for models not performing well. How much\n394 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 328,
        "char_count": 1842,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 777,
            "height": 610,
            "ext": "png",
            "size_bytes": 21002
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 425,
        "text": "data is enough? This is a hard question to answer, but there are some rules of\nthumb. For instance, for sentence classification using baseline algorithms such as\nNaive Bayes or random forest, we’ve observed that having at least two to three\nthousand data points per class is a must to be able to build an acceptable\nclassifier.\nData labeling\nAs of today, most success stories of AI in industry have come from supervised AI.\nAs we discussed in initial chapters, it’s the subarea where, for each data point, we\nhave the ground truth. For many problems, the ground truth comes from human\nannotators. This is often a time-consuming and expensive process. In many\nindustrial settings, stakeholders aren’t aware of the importance of this step.\nData labeling is often a continuous process. While we do get data labeled in bulk\nas a one-time effort to build the first versions of our model, once the model is put\nin production and stabilizes, getting the production data annotated is a continu‐\nous process from there on. Further, we need to define processes for labeling and\nenforce quality checks to improve the accuracy and consistency of human anno‐\ntators. This is done using metrics like kappa to measure inter-annotator\nagreement [56].\nCurrently, AI talent comes at a high cost. Without the right data, it will be futile to\nhire AI talent; having the right data is a prerequisite for AI teams to deliver well and\nfast. By this, we don’t mean that we must have all of the prerequisites in place before\nbringing in AI talent. What we mean is that we must be fully aware of other prerequi‐\nsites, such as the right data, and have realistic expectations in the absence of it.\nA Good Process\nAnother important factor that often leads to the failure of AI projects is not following\nthe right process. In this chapter, we’ve already discussed both the KDD and Micro‐\nsoft processes. Both of them are great starting points. Here are some other important\npoints to consider when getting started:\nSet up the right metrics\nMost AI projects in industry aim to solve a business problem. In many cases,\nteams set up AI metrics like precision, recall, etc., as success metrics. But we must\nalso set up the right business metrics along with AI metrics. For example, let’s say\nwe’re building a text classifier to automatically assign customer complaints to the\nright customer care teams. The right metric for this is the number of times a\ncomplaint is reassigned to another team. A classifier that has a 95% F1 score but\nleads to many complaints being reassigned multiple times is of no use. Another\nexample of this is a chatbot system that correctly detects intent but has high user\ndrop-off rates. User interaction and drop-off rates provide a complete picture\nthat’s missed by using only AI-specific metrics.\nMaking AI Succeed at Your Organization \n| \n395\n",
        "word_count": 487,
        "char_count": 2841,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 426,
        "text": "Start simple, establish strong baselines\nAI scientists are often influenced by the latest techniques and recent state-of-the-\nart (SOTA) models and apply those in their work straight away. Most SOTA tech‐\nniques are both compute- and data-intensive, which leads to cost and time\noverruns. The best way is to start with simple approaches and build strong base‐\nlines. Many times, a SOTA technique might only give us marginal improvement\nover a rule-based system! Try multiple simple approaches first before pondering\nover complex approaches.\nMake it work, make it better\nBuilding a model is often only 5–10% of most AI projects; the remaining 90% is\nmade up of various steps, ranging from data collection to deployment, testing,\nmaintenance, monitoring, integration, pilot testing, etc. It’s always good to build\nan acceptable model quickly and complete one full project cycle instead of\nspending a huge amount of time building an amazing model. This helps all stake‐\nholders realize the value proposition of the project.\nKeep shorter turnaround cycles\nEven when solving a standard problem with well-known approaches, we must\nstill apply them to our dataset to see if they work or not. For example, if we’re\nbuilding a sentiment analysis system, it’s a well-known fact that Naive Bayes gives\nvery strong baselines. Yet it’s very much possible that for our dataset, Naive Bayes\nmight not give good numbers. Building AI systems involves a lot of experiments\nto figure what works and what doesn’t. Hence, it’s important to build models\nquickly and present the results to stakeholders frequently. This helps raise any\nred flags early and get early feedback.\nThere are a few other important things to consider, which we’ll cover next.\nOther Aspects\nIn addition to the various points we’ve discussed so far, there are some more key\npoints to consider, including compute costs and return on investment. Let’s discuss\nthose now:\nCost of compute\nMany AI models (especially DL-based models) are compute-intensive. Over\ntime, GPUs on the cloud or physical hardware prove to be considerably expen‐\nsive. Many organizations are known to spend huge amounts on GPU and other\ncloud services—so much that they have to create parallel projects to reduce these\ncosts.\nBlindly following SOTA\nPractitioners are often keen to apply SOTA models in their work. This often\nproves to be disastrous. For example, Meena [57], a SOTA chatbot system from\n396 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 398,
        "char_count": 2471,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 427,
        "text": "Google that gave amazing results, took over 2,048 TPU for 30 days for training.\nThat compute time is worth $1.4M. While Meena has shown some very impres‐\nsive results, imagine using Meena techniques to build a chatbot for automating\ncustomer support that saves $1,000 a day. We would need to run the chatbot for\nover four years just to break even on the training cost.\nROI\nAI projects are expensive; various stages, such as data collection, labeling, hiring\nAI talent, and compute all involve costs. For this reason, it’s important to estimate\nthe gains at the start of the project itself. We must establish the process and clear\nmetrics to measure the returns early on in the project.\nFull automation is hard\nWe can never achieve complete automation, at least for any moderately complex\nAI project—it will continue to require some manual effort. Figure 11-9 repre‐\nsents this in the same S curve we discussed earlier. Levels for complete automa‐\ntion and acceptable performance might change depending on the project, but the\nbroad point will hold true.\nFigure 11-9. Complete automation can be hard\nWe’ve covered some key points in this section, but making AI succeed in business is a\nvast topic. We suggest a few articles for further reading. While some of them bring\nout the distinctions between software engineering and AI, others discuss rules of\nthumb for building AI systems:\n• “Why Is Machine Learning ‘Hard'?,” a blog post by S. Zayd Enam, a Stanford\nresearcher [58]\n• “Software 2.0,” a blog post on AI as a different way of writing software by Andrej\nKarpathy, a well-known researcher, educator, and scientist at Tesla [59]\nMaking AI Succeed at Your Organization \n| \n397\n",
        "word_count": 284,
        "char_count": 1680,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 880,
            "height": 611,
            "ext": "png",
            "size_bytes": 25699
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 428,
        "text": "• “NLP’s Clever Hans Moment Has Arrived,” an article by Benjamin Heinzerling\nthat argues the validity of SOTA results obtained on certain popular datasets [60]\n• “Closing the AI Accountability Gap,” a report by a team at Google AI and the\nnonprofit Partnership on AI [61]\n• “The Twelve Truths of Machine Learning for the Real World,” a blog post by\nDelip Rao, researcher and O’Reilly author [62]\n• “What I’ve Learned Working with 12 Machine Learning Startups,” an article by\nDaniel Shenfeld, a startup veteran and ML consultant [63]\nThese will give you a more holistic picture. Figure 11-10 demonstrates what we’ve\ncovered in this section and the chapter.\nFigure 11-10. Life cycle of an AI project\nMany of these suggestions are not hard rules set in stone; their application will\ndepend on the context of your project, problem, data, and organization. We hope the\ndiscussion in this section will help in making your AI endeavors succeed.\nPeeking over the Horizon\nWe’d like to end this chapter and the book with various perspectives of how machine\nlearning is evolving. ML will continue improving on the cutting edge, and its applica‐\ntions will be more relevant to business in the coming years. One way to look at this is\nthe influential lecture by renowned scientist C.P. Snow in 1959, titled The Two Cul‐\ntures and the Scientific Revolution [64]. In this lecture, Snow states that the intellectual\n398 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 245,
        "char_count": 1447,
        "fonts": [
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1439,
            "height": 850,
            "ext": "png",
            "size_bytes": 69491
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 429,
        "text": "world can be seen from two distinct perspectives, which seem to be getting more divi‐\nded over time. One perspective is of science and technology and the other is con‐\ncerned with arts and humanities. He argues why it’s important for these two\nperspectives to have a common core for better advancements of the entire area. This\nis true for AI as well.\nAnalogously, in the world of AI, we see a similar set of two distinct perspectives\nemerging. On one hand, we have the advances made by researchers and scientists\nworking on the forefront. On the other hand, we have businesses trying to leverage\nAI. This includes everyone from Fortune 500 companies to early stage startups. The\nworld increasingly believes that the successful adoption of AI in industry will stem\nfrom an intersection of both.\nFrom the perspective of researchers and scientists, we see two macro trends: building\ntruly intelligent machines and applying AI for social good. For instance, François Chol‐\nlet of Google has stressed the importance of building better metrics to measure intel‐\nligence in “On the Measure of Intelligence” [65]. Most evaluation of AI models at\npresent is inherently narrow in nature and measures specific skills as opposed to\nbroad abilities and general intelligence. Chollet proposes certain measures inspired\nby testing of human intelligence, including efficiencies in new skill acquisition. They\nintroduce a dataset called Abstraction and Reasoning Corpus (ARC) that’s inspired by\na classic IQ test: the Raven’s Progressive Matrices [66]. One such example is presented\nin Figure 11-11, where the task is for the computer to infer the missing area by look‐\ning at the overall input matrix pattern. Work on improving measures of AI is neces‐\nsary for developing better and more robust AI in the future.\nAI and technology in general can be a force for social good. And there are now vari‐\nous initiatives that are working on AI for social good. Wadhwani AI is working on\nimproving maternal and early childhood health with AI [67]. Google AI for Social\nGood has a range of initiatives, including applying AI to predict and better manage\nfloods [68]. Similarly, Microsoft is using AI to solve global climate issues, improve\naccessibility, and preserve cultural heritage [69]. Allen AI has been improving\ncommon-sense reasoning in NLP through the WinoGrande dataset [70]. Such work\nby foundations and research labs is helping to incorporate the forefront of ML and\nNLP to improve human well-being.\nPeeking over the Horizon \n| \n399\n",
        "word_count": 413,
        "char_count": 2523,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 430,
        "text": "Figure 11-11. Example of an ARC task for general intelligence from [66]\n400 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 20,
        "char_count": 119,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 1062,
            "height": 2096,
            "ext": "png",
            "size_bytes": 89718
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 431,
        "text": "A completely different perspective comes from the world of business. This is more\npractical and is concerned with business impact and business models. For instance,\nseveral consulting firms have conducted surveys across organizations on use cases\nand effectiveness of AI across industry verticals. McKinsey & Company’s Global AI\nsurvey is one such example [71]. They discuss how AI has helped different verticals\nsave money by reducing inefficiencies and make more money by expanding the mar‐\nket. They also assess the impact of AI on the workforce and on which parts of organi‐\nzations it’s most impactful. Another such study is a report by MIT Sloan and BCG\n[72]. This is immensely useful for business leaders to learn how to onboard and grow\nAI inside their organizations.\nVenture capital (VC) firms have been investing heavily in startups building AI-\npowered businesses. Based on their understanding of how new AI businesses are\nformed and how they can succeed, they’re compiling reports and debriefs. Andressen\nHorowitz, a major VC firm, has published a report, “The New Business of AI,” based\non their learnings in many AI investments [73]. The report addresses business issues\nthat AI startups are facing despite the hype, like lower gross margins and product-\nscaling challenges. They’ve provided practical advice on building AI businesses that\ncan scale better and be more competitive.\nThis range of perspectives will be applicable depending on where your organization is\nin their AI journey. First, when starting a new AI business, lessons from VCs will help\nyou decide what to build. Second, to formulate an AI strategy in a large organization,\nsurveys and reports from industry will align you better. Last but not least, as your\norganization matures, incorporating SOTA techniques can lead to a step change in\nyour products.\nFinal Words\nAnd here we come to the end of Practical Natural Language Processing! We hope\nyou’ve learned a few things about NLP tasks and pipelines and their applications in\nvarious domains and that these will help you in your day-to-day work. The advances\nin NLP are just starting to bear big fruits. Some of the most fundamental questions in\nNLP, like context and common sense, have probably yet to even be asked properly.\nTrue mastery of any skill requires a lifetime of learning, and we hope that our refer‐\nences, research papers, and industry reports will help you continue the journey.\nReferences\n[1] ONNX: An open format built to represent machine learning models. Last\naccessed June 15, 2020.\n[2] Apache Airflow. Last accessed June 15, 2020.\nFinal Words \n| \n401\n",
        "word_count": 426,
        "char_count": 2609,
        "fonts": [
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (18.9pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 432,
        "text": "[3] Apache Oozie. Last accessed June 15, 2020.\n[4] Chef. Last accessed June 15, 2020.\n[5] Microsoft. “MLOps examples”. Last accessed June 15, 2020.\n[6] Microsoft. MLOps using Azure ML Services and Azure DevOps, (GitHub repo).\nLast accessed June 15, 2020.\n[7] Elastic. “Anomaly Detection”.\n[8] Krzus, Matt and and Jason Berkowitz. “Text Classification with Gluon on Amazon\nSageMaker and AWS Batch”. AWS Machine Learning Blog, March 20, 2018.\n[9] The Pallets Projects. “Flask”. Last accessed June 15, 2020.\n[10] The Falcon Web Framework. Last accessed June 15, 2020.\n[11] Django: The web framework for perfectionists with deadlines. Last accessed June\n15, 2020.\n[12] Docker. Last accessed June 15, 2020.\n[13] Kubernetes: Production-Grade Container Orchestration. Last accessed June 15,\n2020.\n[14] Amazon. AWS SageMaker. Last accessed June 15, 2020.\n[15] Microsoft. Azure Cognitive Services. Last accessed June 15, 2020.\n[16] Sucik, Sam. “Compressing BERT for Faster Prediction”. Rasa (blog), August 8,\n2019.\n[17] Cheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. “A Survey of Model Compres‐\nsion and Acceleration for Deep Neural Networks.” 2017.\n[18] Joulin, Armand, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve\nJégou, and Tomas Mikolov. “FastText.zip: Compressing Text Classification Models”,\n2016.\n[19] Chee, Cedric. Awesome machine learning model compression research papers,\ntools, and learning material, (GitHub repo). Last accessed June 15, 2020.\n[20] Burkov, Andriy. Machine Learning Engineering (Draft). 2019.\n[21] Cheng, Heng-Tze. “Wide & Deep Learning: Better Together with TensorFlow.”\nGoogle AI Blog, June 29, 2016.\n[22] Zheng, Alice and Amanda Casari. Feature Engineering for Machine Learning.\nBoston: O’Reilly, 2018. ISBN: 978-9-35213-711-4\n[23] DVC: Open source version control system for machine learning projects. Last\naccessed June 15, 2020.\n402 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 281,
        "char_count": 1911,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 433,
        "text": "[24] Gundersen, Odd Erik and Sigbjørn Kjensmo. “State of the Art: Reproducibility in\nArtificial Intelligence.” The Thirty-Second AAAI Conference on Artificial Intelligence\n(2018).\n[25] Gibney, E. “This AI Researcher Is Trying to Ward Off a Reproducibility Crisis.”\nNature 577.7788 (2020): 14.\n[26] TensorFlow. “Getting Started with TensorFlow Model Analysis”. Last accessed\nJune 15, 2020.\n[27] Marco Tulio Correia Ribeiro. Lime: Explaining the predictions of any machine\nlearning classifier, (GitHub repo). Last accessed June 15, 2020.\n[28] Lundberg, Scott. Shap: A game theoretic approach to explain the output of any\nmachine learning model, (GitHub repo). Last accessed June 15, 2020.\n[29] TensorFlow. “Get started with TensorFlow Data Validation”. Last accessed June\n15, 2020.\n[30] Miller, Tim. “Explanation in Artificial Intelligence: Insights from the Social Sci‐\nences”, (2017).\n[31] Molnar, Christoph. Interpretable Machine Learning: A Guide for Making Black\nBox Models Explainable. 2019.\n[32] Sumo Logic. “Outlier”. Last accessed June 15, 2020.\n[33] Microsoft. “Anomaly Detector API Documentation”. Last accessed June 15, 2020.\n[34] Domingos, Pedro. “A Few Useful Things to Know about Machine Learning.”\nCommunications of the ACM 55.10(2012): 78–87.\n[35] Sculley, D., Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar\nEbner, Vinay Chaudhary, and Michael Young. “Machine Learning: The High Interest\nCredit Card of Technical Debt.” SE4ML: Software Engineering for Machine Learning\n(NIPS 2014 Workshop).\n[36] D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar\nEbner, VinayChaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison.\n“Hidden Technical Debt in Machine Learning Systems.” Proceedings of the 28th Inter‐\nnational Conference on Neural Information Processing Systems 2 (2015): 2503–2511.\n[37] McMahan, H. Brendan, Gary Holt, David Sculley, Michael Young, Dietmar\nEbner, Julian Grady, Lan Nie et al. “Ad Click Prediction: A View from the Trenches.”\nProceedings of the 19th ACM SIGKDD International Conference on Knowledge Discov‐\nery and Data Mining (2013): 1222–1230.\n[38] Zinkevich, Martin. “Rules of Machine Learning: Best Practices for ML Engineer‐\ning”. Google Machine Learning. Last accessed June 15, 2020.\nFinal Words \n| \n403\n",
        "word_count": 327,
        "char_count": 2296,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 434,
        "text": "[39] Halevy, Alon, Peter Norvig, and Fernando Pereira. “The Unreasonable Effective‐\nness of Data.” IEEE Intelligent Systems 24.2 (2009): 8–12.\n[40] Sun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. “Revisiting\nUnreasonable Effectiveness of Data in Deep Learning Era.” Proceedings of the IEEE\nInternational Conference on Computer Vision (2017): 843–852.\n[41] Petrov, Slav. “Announcing SyntaxNet: The World’s Most Accurate Parser Goes\nOpen Source”. Google AI Blog, May 12, 2016.\n[42] Marcus, Mitchell, Beatrice Santorini, and Mary Ann Marcinkiewicz. “Building a\nLarge Annotated Corpus of English: The Penn Treebank”. Computational Linguistics\n19, Number 2, Special Issue on Using Large Corpora: II (June 1993).\n[43] Feurer, Matthias, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Man‐\nuel Blum, and Frank Hutter. “Efficient and Robust Automated Machine Learning.”\nAdvances in Neural Information Processing Systems 28 (2015): 2962–2970.\n[44] Le Cun, Yann, Corinna Cortes and Christopher J.C. Burges. “The MNIST data‐\nbase of handwritten digits”. Last accessed June 15, 2020.\n[45] Google Cloud. “Features and capabilities of AutoML Natural Language”. Last\naccessed June 15, 2020.\n[46] Google Cloud. “AutoML Translation”. Last accessed June 15, 2020.\n[47] Microsoft Azure. “What is automated machine learning (AutoML)?”, February\n28, 2020.\n[48] Thakur, Abhishek and Artus Krohn-Grimberghe. “AutoCompete: A Framework\nfor Machine Learning Competition”, (2015).\n[49] Thakur, Abhishek. “Approaching (Almost) Any NLP Problem on Kaggle”. Last\naccessed June 15, 2020.\n[50] Fayyad, Usama, Gregory Piatetsky-Shapiro, and Padhraic Smyth. “The KDD\nProcess for Extracting Useful Knowledge from Volumes of Data.” Communications of\nthe ACM 39.11 (1996): 27–34.\n[51] Microsoft Azure. “What is the Team Data Science Process?”, January 10, 2020.\n[52] Microsoft. “Team Data Science Process Documentation”. Last accessed June 15,\n2020.\n[53] Kidd, Chrissy. “Why Does Gartner Predict up to 85% of AI Projects Will ‘Not\nDeliver’ for CIOs?”, BMC Machine Learning & Big Data Blog, December 18, 2018.\n[54] Google AI. “Responsible AI Practices”. Last accessed June 15, 2020.\n[55] Microsoft. “Microsoft AI principles”. Last accessed June 15, 2020.\n404 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 328,
        "char_count": 2289,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 435,
        "text": "[56] Artstein, Ron and Massimo Poesio. “Inter-Coder Agreement for Computational\nLinguistics.” Computational Linguistics 34.4 (2008): 555–596.\n[57] Adiwardana, Daniel and Thang Luong. “Towards a Conversational Agent that\nCan Chat About…Anything”. Google AI Blog, January 28, 2020.\n[58] Enam, S. Zayd. “Why is Machine Learning ‘Hard’?”, Zayd’s Blog, November 10,\n2016.\n[59] Karpathy, Andrej. “Software 2.0”. Medium Programming, November 11, 2017.\n[60] Heinzerling, Benjamin. “NLP’s Clever Hans Moment has Arrived”. The Gradient,\nAugust 26, 2019.\n[61] Raji, Inioluwa Deborah, Andrew Smart, Rebecca N. White, Margaret Mitchell,\nTimnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, and Parker\nBarnes. “Closing the AI Accountability Gap: Defining an End-to-End Framework for\nInternal Algorithmic Auditing”, (2020).\n[62] Rao, Delip. “The Twelve Truths of Machine Learning for the Real World”. Delip\nRao (blog), December 25, 2019.\n[63] Shenfeld, David. “What I’ve Learned Working with 12 Machine Learning Start‐\nups”. Towards Data Science (blog), May 6, 2019.\n[64] Snow, Charles Percy. The Two Cultures and the Scientific Revolution. Connecticut:\nMartino Fine Books, 2013.\n[65] Chollet, François. “On The Measure of Intelligence”, (2019).\n[66] John, Raven J. “Raven Progressive Matrices,” in Handbook of Nonverbal Assess‐\nment, Boston: Springer, 2003.\n[67] Wadhwani AI. “Maternal, Newborn, and Child Health”. Last accessed June 15,\n2020.\n[68] Matias, Yossi. “Keeping People Safe with AI-Enabled Flood Forecasting”. Google\nThe Keyword (blog), September 24, 2018.\n[69] Microsoft. “AI for Good”. Last accessed June 15, 2020.\n[70] Sakaguchi, Keisuke, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.\n“WinoGrande: An Adversarial Winograd Schema Challenge at Scale”, (2019).\n[71] Cam, Arif, Michael Chui, and Bryce Hall. “Global AI Survey: AI Proves Its\nWorth, but Few Scale Impact”. McKinsey & Company Featured Insights, November\n2019.\n[72] Ransbotham, Sam, Philipp Gerbert, Martin Reeves, David Kiron, and Michael\nSpira. “Artificial Intelligence in Business Gets Real.” MIT Sloan Management Review\n(September 2018).\nFinal Words \n| \n405\n",
        "word_count": 304,
        "char_count": 2138,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 436,
        "text": "[73] Casado, Martin and Matt Bornstein. “The New Business of AI (and How It’s Dif‐\nferent From Traditional Software)”. Andreesen Horowitz, February 16, 2020.\n406 \n| \nChapter 11: The End-to-End NLP Process\n",
        "word_count": 32,
        "char_count": 205,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MyriadPro-SemiboldCond (9.0pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 437,
        "text": "Index\nA\nA/B testing, 263, 324, 378\nAbstraction and Reasoning Corpus (ARC)\ndataset, 399, 400\nabstractive summarization, 256, 259\naccounting and auditing, 362\naccuracy, 69\nACM SIGKDD Conference on Knowledge Dis‐\ncovery and Data Mining (KDD), 388\nactive learning, 42\nNER using, 176-177\nwith Prodigy, 153\ntext classification, 150-152, 153\nAdamic, Lada, 299\nadapting to new domains, 149-152\nadult content filtering, 277\nadvanced processing, 49, 57-60\nAI (see artificial intelligence)\nAirbnb, 262, 309\nAirflow (Apache), 373\nAlgolia, 250\nAllen AI, 399\nAllenNLP, 175, 184\nDeepQA library, 269\nGrover, 302\nAmazon, 5, 122, 240\nfaceted search on, 310\n“Reviews that mention” filter, 166\nsentiment analysis APIs, 126\nAmazon Alexa, 3, 5, 7, 31, 197\nAmazon Comprehend, 65, 190\nAmazon Comprehend Medical, 356-357\nAmazon Mechanical Turk, 150\nAmazon Research, 301\nAmazon Translate, 5\nAmazon Web Services (AWS), 72\nAWS Cloud, 374\nAWS S3, 373\nSageMaker, 374, 375\nambiguity , 12-13\nanomaly detection, 382\nanswer extraction, 268\nanswering questions (see question answering)\nApache Airflow, 373\nApache Nutch, 244\nApache Oozie, 373\nApache Solr software, 245\nAPIs\nintegration of, 229\nmodeling with, 65\nMT, 264-265\nproduct categorization and taxonomy, 320,\n321\ntext classification with, 126, 153\nApple, 297\nApple Siri, 3, 5, 7, 31\nArabic, 74\nARC (see Abstraction and Reasoning Corpus)\nArria, 5\nartificial intelligence (AI), 14, 197, 371\nexpectations vs reality, 393\nfurther reading, 397-398\nkey points and rules of thumb, 392-398\nperspectives on, 399\nfor social good, 399\naspect-based sentiment analysis, 122-123,\n327-329\naspects, 327\n407\n",
        "word_count": 245,
        "char_count": 1612,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (25.2pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 438,
        "text": "connecting overall ratings to, 329-330\nunderstanding, 330-331\nASR (automatic speech recognition), 48\nassessment tools, 6\nATIS (Airline Travel Information Systems)\ndataset, 220\nattention, 352\nattention networks, 381\nattribute extraction, 308, 312-317\nalgorithms, 314\nderived, 315\ndirect, 314, 315-316\nindirect, 316-317\nlatent, 335\nattribute match, 323\nAUC, 69\nAudit Command Language (Deloitte), 362\nauditing, 362\nauthorship attribution, 123\nAutoCompete framework, 388\nautoencoders, 27-28\nautomated machine learning (AutoML),\n384-388\nautomated scoring, 6, 113\nAutomatic Content Extraction Program, 162\nautomatic response generation, 219\nautomatic speech recognition (ASR), 48\nautomation, full, 397\nautosklearn, 385-387\nautotldr bot (Reddit), 256\nAWS (see Amazon Web Services)\nB\nback translation, 41, 41, 266\nbag of n-grams (BoN), 89-90\nbag of words (BoW), 87-89, 294, 301\nbaselines, 396\nBayes’ theorem, 20\nBBC, 190\nBCG, 401\nBeautiful Soup library, 44, 291\nBender, Emily, 12\nBERT (Bidirectional Encoder Representations\nfrom Transformers), 26, 107, 221\nfact-verification with, 301\nsearch with, 246\nsize, 375\ntext classification with, 145-146\nBERT for Biomedical Text (BioBERT), 357, 358\nBERT for financial text (FinBERT), 361\nbest practices, 352\nbias, 108, 156\nbigram flipping, 41\nbilling, 342\nbinary classification, 120\nBing (search engine), 5, 266\nBing Answer Search API, 269\nBing Microsoft Translator, 5\nBing Translate API, 264\nBioBERT (BERT for Biomedical Text), 357, 358\nbiographical information extraction, 242\nBLEU (Bilingual Evaluation Understudy), 69,\n70\nBloomberg, 360\nBloomberg Terminal, 187\nBoN (bag of n-grams), 89-90\nbootstrapping, 149\nBoW (bag of words), 87-89, 294, 301\nBuoy system, 343\nBurmese, 74\nbusiness issues, 401\nC\nCapital Float and Microbnk, 361\nCaptcha tests, 150\nCarnegie Mellon University, 384\nCasari, Amanda, 384\ncatalogs, 308-309, 312-324\ncategorization, product, 317-321\nCBOW (continuous bag of words), 98-100, 102,\n104\nCDQA-Suite library, 269\nCFG (context-free grammar), 18\ncharacter embeddings, 297\ncharacter models, 354\ncharacter n-gram embeddings, 296\ncharacter n-gram–based models, 354\ncharacteristic features, 315\nchatbots, 163, 197-235, 396\napplications, 198-200\nbuild walkthrough, 206-216\ncase study, 230-234\nchitchat types, 202\nexact answer, 200\nexamples, 197\nexisting frameworks, 231-232\nFAQ bot, 199-200, 201\nflow-based, 201\n408 \n| \nIndex\n",
        "word_count": 330,
        "char_count": 2376,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 439,
        "text": "goal-oriented dialog, 202\nhealthcare, 342-344\nopen, 201\nopen-ended, 201, 233-234\npipeline for, 203-204\ntaxonomy of, 200-203\nterminology, 205\ntesting, 214-216\ntext-based, 204\nvoice-based, 204\nChee, Cedric, 376\nChef, 373\nChinese, 74\nchitchats, 202\nChollet, François, 399\nChomsky, Noam, 3, 18\nChronic, 187\nCJK languages, 74\nclassification, 119\n(see also text classification)\nbinary, 120\nmulticlass, 120\nproduct categorization and taxonomy,\n317-321\ntopic, 120\nclassification tasks, 19, 70\nclinical decision support systems, 342\nclinical information systems, 344-345\ncloud storage, 373\nclpsych.org, 123\nCMU Book Summaries dataset, 247\nCNNs (convolutional neural networks), 24,\n140, 143-144, 221\ncode mixing, 56\ncode reproducibility, 379\ncoherence, 255\ncollaborative filtering, 260\nColumbia University, 344\ncommon knowledge, 13-14\ncommon sense, 30\ncomplements, 333, 334\ncompute costs, 396\nconditional random fields (CRFs), 22, 172\nconfusion matrix, 70\ncongratsbot, 187\nCONLL-03 dataset, 175\nconnotation, 93\ncontent acquisition, 246\ncontent classification and organization, 121\ncontent discovery\nchatbots in, 198\nwith emotional content, 354\nlegal, 364\ncontext, 8, 12\ncontext window, 103\ncontext-based conversations, 228\ncontext-free grammar (CFG), 18\ncontextual features, 316\ncontextual word representations, 107\ncontinuous bag of words (CBOW), 98-100, 102,\n104\ncontract generation, 363\ncontract review, 363\nContraxSuite, 364, 365\nconversational agents, 7\ncase study, 31-33\ntypical flow, 32\nconversations\nactionable, 297\ncontext-based, 228\nconvolutional neural networks (CNNs), 140,\n143-144, 221\nCoppersmith, Glen, 353\ncoreference resolution, 32, 58\ncorporate ticketing (case study), 152-155\ncost of compute, 396\ncosts, 30\nCOTA (Customer Obsession Ticketing Assis‐\ntant), 74-76\ncovariate shift, 376\ncrawlers, 244\ncrawling/content acquisition, 246\ncreativity, 14\ncredit risk, 361-362\nCRFs (conditional random fields), 22, 172\nCRM (customer relationship management) ,\n297\ncrowdsourcing, 150\nCustomer Obsession Ticketing Assistant\n(COTA), 74-76\ncustomer relationship management (CRM),\n297\ncustomer support\nchatbots in, 199\non social channels, 121, 278, 297-299\nD\nDARPA, 198\nIndex \n| \n409\n",
        "word_count": 292,
        "char_count": 2178,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 440,
        "text": "Dartmouth College, 14\nDas, Vir, 275\ndata acquisition, 39-42\ndata annotation, 228-229\ndata augmentation, 40-42\ndata extraction (see information extraction)\ndata labeling, 395\ndata mining, 389\ndata pre-processing, 389\n(see also pre-processing)\ndata reduction, 389\ndata science (DS), 371, 388-392\nData Version Control (DVC), 379\ndatabases, in-memory, 108\nDataminr, 360\ndatasets\ncomplex, 350\ncrowdsourcing for, 150\nfor dialog systems, 220, 235\nfact-verification with, 301\ngoal-oriented, 220\ni2b2, 350\npublic, 40, 153, 289\ntarget, 389\ntext classification with, 126-127, 153\nversion control , 379\nDawkins, Richard, 299\nDBOW (distributed bag of words), 106\nDBpedia dataset, 136\nDBpedia Spotlight, 179\ndecision support systems, 342\ndeep learning (DL)\nattribute extraction with, 316\nfor dialogue generation, 225-226\nfact-verification with, 301\nfeature engineering, 61, 62\nlimitations, 28-31\nfor NLP, 23-28\noverview, 14-16\nquestion answering with, 269\nreinforcement learning, 225-226\nfor text classification, 140-147\nwith EHRs, 351\ndeep neural networks, 269\nDeepQA library, 269\nDeloitte, 362\nDemoji, 292\ndenotation, 93\ndeployment, 72, 372-374\ndevice, 30\nexample scenario, 374-376\ntypical steps, 373\nderived attribute extraction, 315\ndeterministic matches, 18\ndevice deployment, 30\ndiagnosis chatbots, 343, 344\ndialog act classification, 217\ndialog act or intent, 205\ndialog act prediction, 220-222\ndialog management, 32\ndialog managers, 204\ndialog state or context, 205\ndialog systems\ncomponents, 216-224\ndatasets for, 220, 235\ndeep reinforcement learning for, 225-226\nin detail, 204-216\nend-to-end approach, 225\nexamples with code walkthrough, 219-224\nhuman-in-the-loop, 226-227\nother pipelines, 224-227\npipeline for, 203-204\nterminology, 205\nDialogflow (Google), 206, 232\nchatbot build with, 206-216\ncreating agents with, 207\ndigits: removing, 53\ndimensionality, 142\ndirect attribute extraction, 314, 315-316\ndiscriminative classifier, 131\ndisplaCy visualizer, 171\ndistant supervision, 183\ndistributed bag of words (DBOW), 106\ndistributed memory (DM), 106\ndistributed representations, 92-106\ndistributional hypothesis, 93\ndistributional similarity, 93\ndiversity across languages, 14\nDjango Project, 375\nDL (see deep learning)\nDM (distributed memory), 106\nDoc2vec model, 106, 140\nserving recommendations with, 261\ntext classification with, 138-140\ntraining, 139\nDocker, 375\ndocument categorization, 120\n(see also text classification)\n410 \n| \nIndex\n",
        "word_count": 328,
        "char_count": 2439,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 441,
        "text": "document embeddings\ntext classification with, 138-140\nvisualizing, 110-110\nDocument Understanding Conference series,\n256\ndomain adaptation, 151-152\nDomingoes, Pedro, 384\nDS (data science), 371, 388-392\nDSTC dataset, 220\nDuckling library, 186\nDuolingo, 6\nE\ne-commerce and retail, 307-336\ncatalogs, 308-309, 312-324\nchatbots, 198\nNLP applications, 5, 308\nproduct categorization and taxonomy,\n317-321\nproduct deduplication and matching,\n323-324\nproduct enrichment, 321-322\nrecommendations for, 332-335\nreview analysis, 324-331\nsearch, 309-312\ntext classification, 122-123\nEarley parser, 18\nEasy Data Augmentation (EDA), 42\neBay, 122, 320\nEconomic News Article Tone and Relevance\ndataset (Figure Eight), 126, 148\nEDA (Easy Data Augmentation), 42\neditorial reviews, 331\nEducational Testing Service (ETS), 112, 113\nEHRs (see electronic health records)\nElastic on Azure, 249\nElasticsearch, 247-248\nDSL, 311\nELK stack, 382\nPython API, 247\nElasticsearch Learning to Rank, 249\nelectronic health records (EHRs), 344-353\nexample, 341\nEliza chatbot, 197\nELK stack (Elastic), 382\nELMo, 107\nemail platforms, 5\nemails: IE from, 186, 191\nembedding, 93\ncharacter, 297\ncharacter n-gram, 296\npre-trained, 108, 295\nsubword, 136-138\nvisualizing, 108-110\nword, 94-103\nword-based, 295\nbeyond words, 103-105\nemojis, 292\nemotion classification, 354, 355\nemr, 349\nemrQA dataset, 349, 350\nEnam, S. Zayd, 397\nEnglish, 9, 23, 74, 266\nensembling, 156\nenterprise search engines, 243, 246-247\nentity linking, 178\nEntityRuler (spaCy), 171, 176\nErnst & Young, 362\nerror correction, 47-49\nETS (Educational Testing Service), 112, 113\nevaluation, 68-72, 399\nextrinsic, 68, 71-72\nintrinsic, 68-71\nvisual methods, 70\nevent extraction, 164, 187-188\nextractive summarization, 256, 259\nF\nF1 score, 69\nFacebook, 275\ncustomer support on, 297\ndata acquisition, 40\nfastText embeddings, 95, 105, 136\nfastText library, 136-138\nmemes on, 299\nFacebook AI Research, 375\nFacebook Messenger, 186, 197\nfaceted search, 310-312\nfacets, 310\nfact-verification, 301\nfake news, 123, 277, 299, 300-302\nFalcon Web Framework, 375\nfalse propaganda, 277\nFAQ bots, 199-200, 201\nFast Healthcare Interoperability Resources\n(FHIR) standard, 351, 356\nfastText classification models, 375\nfastText embeddings (Facebook), 95, 105, 136\nIndex \n| \n411\n",
        "word_count": 319,
        "char_count": 2275,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 442,
        "text": "fastText library (Facebook), 136-138\nfat-finger problem, 42, 46\nfeature engineering, 60-62, 66, 377\nfeature extraction, 60, 81\n(see also feature engineering)\nfeature representation, 82\nhandcrafted, 112-113\nfeature selection, 377-378\nfeatures, 15\nfeedback, 245, 247\nfrom customers (see review analysis)\nexplicit, 154\nimplicit, 154\nlearning from, 154\nfew-shot learning, 29\nFHIR (Fast Healthcare Interoperability Resour‐\nces) standard, 351, 356\nfigurative language, 12\nFigure Eight, 126, 138, 148, 150\nfiltering adult content, 277\nfiltering, collaborative, 260\nfinancial industry, 339\nFinancial PhraseBank, 361\nfinancial sentiment, 360-361\nfinancial services, 5, 358-360\nestimated ML benefits, 359\nNLP applications, 360-363\nFinBERT (BERT for financial text), 361\nFinnish, 74\nfixed responses, 218\nFlask, 375\nformalism, 283\nforms: data extraction from, 163\nFreebase, 183\nFrench, 74\nG\nGATE (General Architecture for Text Engineer‐\ning), 18\ngated recurrent units (GRUs), 23\nGehrmann, Sebastian, 302\ngenerative chatbots, open-ended, 233-234\ngenerative classifier, 131\ngensim library, 96, 103, 135, 258\nKPE with, 168\ntext summarization with, 258\ntutorial on LDA, 255\nGlobal AI survey (McKinsey), 401\nGloVe , 95, 135, 142, 295\nGmail, 5, 186\nGnip, 288\ngoal-oriented dialogs, 202, 220\nGoldberg, Yoav, 81\ngood, social, 399\nGoogle\nCaptcha tests, 150\ndata acquisition, 40\ndataset search system, 126\nsearch engine, 5, 241, 244, 246, 266\nWord2vec model, 94-97, 102, 103, 105, 108,\n134, 295, 375\nGoogle AI, 351, 352, 384, 398\nGoogle AI for Social Good, 399\nGoogle Analytics, 263\nGoogle APIs, 126, 153, 264\nGoogle Assistant, 5, 197\nGoogle Cloud\nAutoML, 385, 387\nDialogflow, 206-216, 232\nNatural Language, 65, 126\nNLP, 190\ntask queues, 72\nGoogle Cloud Storage, 373\nGoogle Docs, 6\nGoogle Home, 3\nGoogle Knowledge Graph, 6, 179\nGoogle News, 163\nGoogle Search, 7, 242, 384\nGoogle Translate, 5, 7, 13, 240, 263, 264\nlanguage identification, 123\ngrammar, 280\ngrammar-correction tools, 6, 113\nGrammarly, 6, 113\nground truth, 15, 68\nGrover (AllenNLP), 302\nGRUs (gated recurrent units), 23\nH\nhandcrafted feature representations, 112-113\nHarvard University, 302\nHARVEST system, 344-347\nhealth assistants, 342-344\nhealth outcomes, 351\nhealth records, 341\nhealthcare, 5, 339-357\nchatbots in, 199\nGoogle APIs, 153\nNLP applications, 340\n412 \n| \nIndex\n",
        "word_count": 344,
        "char_count": 2318,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 443,
        "text": "patient prioritization and billing, 342\nquestion answering for, 349-350\nHealthTermFinder, 347\nHeinzerling, Benjamin, 398\nheuristics, 16-18, 104\ncombining with ML, 64\nreapplying, 67\nsimple, 63\nHidden Markov Model (HMM), 21-22\nHindi, 74\nHMM (Hidden Markov Model), 21-22\nHorowitz, Andreessen, 307\nHTML parsing and cleanup, 44-45\nHuang, Jensen, 339\nHubbard, Elbert Green, 197\nhuman-in-the-loop, 226-227\nI\ni2b2 datasets, 350\nIBM Research, 113, 178, 349\nIBM Watson, 6, 65, 179, 269\nRE with, 184-185\nICD-10-CM, 357\nIDF (inverse document frequency), 91\nIE (see information extraction)\nimage match, 324\nimage representation, 82\nimages: text extraction from, 47\nImbalanced-Learn, 130\nIMDB dataset, 289\nin-memory databases, 108\nindexers, 245\nindexing, 241, 245, 246, 247\nindirect attribute extraction, 316-317\nInfermedica API, 343, 344\ninformation, 161\ninformation extraction (IE), 7, 161-195\nadvanced tasks, 185-190\ncase study, 190-193\ngeneral pipeline, 165-166\nhistorical background, 162\nlegal, 364-365\nfrom loan agreements, 362\nmedical, 355-357\nopen, 183, 184\nreal-world applications, 162-163\ntasks, 164-165\ntemporal, 164, 186-187\ntypical pipeline, 166\ninformation retrieval, 7, 241-250\nInstagram, 275\nintelligent machines, 399\nintelligent tutoring systems, 6\ninteractive learning, 228\ninterpretability, 352, 382\ninverted indexes, 245\niNYP system, 344-345\nJ\nJapanese, 74\nJAPE (Java Annotation Patterns Engine), 18\nJeopardy!, 6, 269\nJordan, Jeff, 307\nK\nKaggle, 289\nKarpathy, Andrej, 397\nKDD (Knowledge Discovery and Data Mining)\nprocess, 388-390\nkey performance indicators (KPIs), 124, 378,\n382\nkey terms, 284\nkey words, 251\n(see also topics)\nkeyphrase extraction (KPE), 164, 166-169, 177\nkeyword extraction, 164\nknowledge bases, 6\nKnowledge Discovery and Data Mining (KDD)\nprocess, 388-390\nknowledge-based question answering, 269\nKoffka, Kurt, 37\nKorean, 74\nKPE (see keyphase extraction)\nKPIs (key performance indicators), 124, 378,\n382\nktrain, 145\nKubernetes, 375\nL\nlabeled data, 39\nlabeling, 395\nlabels, 15, 68\nlanguage\nambiguity in, 12-13\nbuilding blocks of, 8-12\ncharacteristics that make NLP challenging,\n12-14\nIndex \n| \n413\n",
        "word_count": 298,
        "char_count": 2122,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 444,
        "text": "diversity across, 14\nfigurative, 12\nnon-English languages, 56, 73-74\nlanguage detection, 56\nlanguage identification, 123\nlanguage learning apps, 6\nlanguage modeling, 6, 107\nlanguage models, large, pre-trained, 145-147\nLARA (latent rating regression analysis), 329\nlarge, pre-trained language models, 145-147\nlatent attribute extraction, 335\nlatent Dirichlet allocation (LDA), 251-253, 328,\n335\nlatent rating regression analysis (LARA), 329\nlatent semantic analysis (LSA), 251\nLatin, 74\nlaw (see legal services)\nLDA (latent Dirichlet allocation), 251-253, 328,\n335\nleadership, 392\nlearning\ninteractive, 228\nwith no or less data, 149-152\nlearning and assessment tools, 6\nlegal discovery, 364\nlegal entity extraction, 364-365\nlegal research, 363\nlegal services, 339, 358, 363-365\nchatbots, 199\nlemmatization, 54\nlength of text, 283\nlexemes, 10\nlexicon-based sentiment analysis, 125\nLexNLP, 364-365\nLexRank, 330\nLime, 147, 380, 381\nlinguistics, 8-12\nlink prediction, 335\nLinkedIn, 300\nLipton, Zachary, 31\nloan agreements: entity extraction from, 362\nlocational features, 316\nlogistic regression, 131\nlong short-term memory networks (LSTMs),\n23, 140, 144-145, 301\nLSTM-CRF, 316\nlowercasing, 53\nLSA (latent semantic analysis), 251\nLSI (Latent Semantic Indexing), 75\nLSTM-CRF, 316\nLSTMs (long short-term memory networks),\n23, 140, 144-145, 301\nLucidworks, 320\nM\nmachine intelligence, 14\n(see also artificial intelligence)\nmachine learning (ML), 371\nautomated, 384-388\nclassical, 62\nestimated benefits, 359\nfeature engineering, 62\nfuture directions, 398-401\nlandmark work, 384\nfor NLP, 19-22\noverview, 14-16\nmachine translation (MT), 7, 240, 263-266\nwith APIs, 264-265\npractical advice, 265-266\nuse cases, 264\nmachine translation services, 5\nMalayalam, 74\nMAP (Mean Average Precision), 69, 70\nMAPE (Mean Absolute Percentage Error), 69\nmarkup elements: removing, 291\nMcAuley, Julian, 334\nMcKinsey &amp; Company, 401\nMechanical Turk, 220, 313\nmedical care (see healthcare)\nmedical information extraction and analysis,\n355-357\nmedical records, 341\nMeena system, 396\nmemes, 299-300\nmental healthcare monitoring, 123, 353-354\nmental health–related issues, 353\nMessage Understanding Conferences (US\nNavy), 162\nmetadata, 249, 394\nMETEOR, 69\nmetrics, 395\nMicrosoft, 58\nAI for social good, 399\nanomaly detection, 383\ndata acquisition, 40\nreference pipelines for MLOps, 373\nsentiment analysis APIs, 126\ntranslation API, 264\n414 \n| \nIndex\n",
        "word_count": 334,
        "char_count": 2420,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 445,
        "text": "Microsoft Azure, 390\nAutoML, 387\nBlob Storage, 373\nCognitive Services, 65, 375\nMachine Learning, 387\nNEL with, 179-181\nText Analytics API, 179-181\nMicrosoft Cortana, 5\nMicrosoft Outlook, 5\nMicrosoft Research, 375\nMicrosoft REST API, 46\nMicrosoft Team Data Science Process (TDSP),\n388, 390-392\nMicrosoft Word, 6, 113\nMilne, A.A., 119\nMinimum Viable Product (MVP) approach,\nxviii\nmisinformation, 301\n(see also fake news)\nMIT, 349\nMIT Sloan, 401\nMITIE, 175\nML (see machine learning)\nMLOps, 373\nmodel compression, 375-402\nmodel ensembling, 65, 66\nmodel packaging, 373\nmodel scaling, 373\nmodel serving, 373\nmodel size, 375\nmodel stacking, 65, 66\nmodeling, 62-68, 376\nwith APIs, 65\napproaches to, 65-67\ncombining heuristics with, 64\ncustomization, 230\ndeployment, 72\nevaluation of, 399\ngoodness of fit, 68\ninterpretability, 352\niterating existing models, 378\nmonitoring, 72, 382-383\nrecommendations for, 384\nreproducibility, 379\nsequence-to-sequence (seq2seq) models,\n225\nslot identification with, 222-224\nstate-of-the-art (SOTA) models, 396\nstrategies for, 67\ntesting models, 379, 381-382\nupdating, 73, 376\nweb service–based, 382\nMolnar, Christoph, 382\nmonitoring, 72, 382-383\nMoore, Anthony, 371\nmorphemes, 10\nmorphological analysis, 10, 74\nMRR (Mean Reciprocal Rank), 69, 70\nMT (see machine translation)\nmulti-document summarization, 256\nmulticlass classification, 120\nmultilingual writing, 280\nMultiWoZ dataset, 220\nMVP approach (see Minimum Viable Product\napproach)\nN\nn-grams, 89\nNadella, Satya, 58\nNaive Bayes, 20, 395\nNaive Bayes classifier, 127-130\nnamed entity disambiguation (NED), 179\nnamed entity disambiguation and linking, 164\nnamed entity linking (NEL), 179-181\nnamed entity recognition (NER), 32, 58, 164,\n169-178\nwith active learning, 176-177\nbuilding, 171-175\nexamples, 169\nwith existing libraries, 175\nin finance, 361\nHealthTermFinder, 347\npractical advice, 177-178\nrule-based, 171\ntypical training data, 174\nNatty, 187\nnatural language generation, 204\nNatural Language Processing (NLP), xvii\napproaches to, 16-31\ncase studies, 31-33, 74-76\nclassical, 61, 62\ncore applications, 5\ndeployment, 372-374\nDL-based, 61\nend-to-end process, 371-398\nheuristics-based, 16-18\nkey decision points, 373\noverview, 3-33\nIndex \n| \n415\n",
        "word_count": 311,
        "char_count": 2232,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 446,
        "text": "pipeline (see pipeline)\nproject life cycle, 398\nfor social data, 284-299\nsoftware applications, 5-6\ntasks and applications, 4, 6-8\ntasks organized according to relative diffi‐\nculty, 8\nNatural Language Tool Kit (NLTK)\nlemmatization with, 54\nPOS tagging, 57\nsentence segmentation with, 50\nstemming with, 53\nstop words, 52\ntokenizer, 286\nword tokenization with, 51-51\nnatural language understanding (NLU), 203,\n217\ncase study, 32\nRasa, 227-230\nNavajo, 266\nNCRF++, 175\nNED (named entity disambiguation), 179\nNEL (named entity linking), 179-181\nNER (see named entity recognition)\nNetflix, 40, 240, 260\nneural embeddings, 134-140\nneural text representation, 108\nNew York Presbyterian Hospital, 347\niNYP, 344-345\nnews and content discovery, 198\nnews classification, 123, 302\nnews: tagging, 162\nNIST Text Analysis Conference, 162\nNLP (see Natural Language Processing)\nNLPAug, 42\nNLTK (see Natural Language Tool Kit)\nnltk.tokenize.TweetTokenizer, 286\nNLU (see natural language understanding)\nnoise, 298\nadding to data, 41\nexamples, 298\non social media, 283\nnon-English language detection, 56\nnon-English language processing, 73-74\nnon-text data, 291\nnormalization\ntemporal IE and normalization, 186\ntext, 56, 246\nUnicode, 45\nNorvig, Peter, 384\nnumbers: removing, 53\nO\nOccam’s razor, 29\nOCR (optical character recognition), 47, 163\nOlah, Christopher, 23\none-hot encoding, 85-87\nonline support, 123\nONNX, 373\nOntoNotes, 175\nOOV (out of vocabulary) problem, 87, 105,\n136, 281-282, 296\nOozie (Apache), 373\nopen formats, 373\nopen IE, 183, 184\nOpen Mind Common Sense, 17\nopen-ended generative chatbots, 233-234\nopinion mining, 277\noptical character recognition (OCR), 47, 163\nout of vocabulary (OOV) problem, 87, 105,\n136, 281-282, 296\noutcome prediction, 351-353\nP\npackaging, 373\nparse trees, 11\nParsedatetime, 187\nParsey McParseface Tagger, 57\npart-of-speech (POS) tagging, 21, 57, 60\nPartnership on AI, 398\nPDF documents: text extraction from, 47\nPDFMiner, 47\nPenn Treebank dataset, 385\nperplexity, 70\npersonnel, 392-393\npharmacovigilance, 342\nphonemes, 8, 9\nphonetic typing, 280\nPineau, Joelle, 379\npipeline, 37-76\ngeneric components, 38\nkey stages, 37-38\nplagiarism detection, 6\nPLSA (probabilistic latent semantic analysis),\n251\npolarity, 290\nPolyglot Python Library, 56\nPorter Stemmer, 53\n416 \n| \nIndex\n",
        "word_count": 331,
        "char_count": 2296,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 447,
        "text": "POS (part-of-speech) tagging, 21, 57, 60\npost-modeling phases, 72-73\npragmatics, 12\npre-processing, 49-60, 65, 354, 389\nadvanced processing, 60\ncommon steps, 55\nfrequent steps, 52-55\nother steps, 55-57\nSMTD, 290-294\npre-trained language models, large, 145-147\npre-training, 26, 108\nprecision, 69\npredicting health outcomes, 351-353\npregex, 18\npreliminaries, 50-52\nprobabilistic latent semantic analysis (PLSA),\n251\nProdigy, 151, 153, 176\nproduct catalogs (see catalogs)\nproduct categorization and taxonomy, 317-321\nproduct deduplication and matching, 323-324\nproduct enrichment, 321-322\nproduct intervention, 40\nproduct linking, 335\nproduct recommendations, 309, 332\n(see also recommendations)\nproduct search, 309\npropaganda, false, 277\npublic datasets, 40, 153, 289\nPubMed, 357\npunctuation: removing, 53\nPwC, 362\nPyPDF, 47\nPython, 48\nQ\nquality of data, 394\nquantity of data, 394\nquery-focused summarization, 256\nquestion answering (QA), 7, 240\nwith deep neural networks, 269\nknowledge-based, 269\nquestion-answering (QA) systems, 266-269\ndataset creation framework , 350\ndataset generation with existing annota‐\ntions, 351\ndeveloping, 268\nDL-based, 269\nfor health, 349-350\nR\nrandom forest, 395\nranking, 70, 245, 247\nRao, Delip, 398\nRasa, 175, 228-230, 233\nRasa NLP, 375\nRasa NLU, 227-230\nrecall, 69\nRecall at rank K, 70\nRecall-Oriented Understudy for Gisting Evalu‐\nation (ROUGE), 70, 259\nreceipts: data extraction from, 163\nrecommendation engines, 333\n(see also recommender systems)\nrecommendations, 240\ncase study, 333-335\nfor e-commerce, 332-335\nproduct, 309\ntechniques for, 332\nrecommender systems, 260-263, 333\ncreating, 261-262\ne-commerce, 309, 333\nexamples, 261-262\npractical advice, 262-263\nrecurrent neural networks (RNNs), 23, 107,\n140, 316\nReddit autotldr bot, 256\nRedis database, 108\nRegexNER (Stanford NLP), 171, 176\nregression techniques, 19\nregression, logistic, 131\nregular expressions (regex), 17\nreinforcement learning, 15\ndeep, 225-226\nfor dialogue generation, 225-226\nrelated queries, 242\nrelation extraction (see relationship extraction)\nrelationship extraction (RE), 58, 164, 181-185\napproaches to, 182-184\nexample, 182\nunsupervised, 183\nwith Watson API, 184-185\nreplacing entities, 41\nrepresentative data, 394\nreproducibility, 379\nresearch, legal, 363\nresponse generation, 218\nautomatic, 219\ncase study, 33\nIndex \n| \n417\n",
        "word_count": 319,
        "char_count": 2344,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 448,
        "text": "fixed responses, 218\ntemplates-based, 219\nretail (see e-commerce and retail)\nReuters, 361\nreview analysis, 308, 324-331\nconnecting overall ratings to aspects,\n329-330\nexample pipeline, 331\nlatent attribute extraction from, 335\nsubstitutes and complements based on, 334\nrisk assessments, 361-362\nRitter, Allen, 295\nRMSE (Root Mean Squared Error), 69\nRNNs (recurrent neural networks), 23, 107,\n140, 316\nROI (return on investment), 397\nRosette Text Analytics, 181\nROSS Intelligence service, 363\nROUGE (Recall-Oriented Understudy for Gist‐\ning Evaluation), 70, 259\nRuder, Sebastian, 102\nrule-based systems, 18\nrules, 18\nrumor/fake news detection, 277\nRxNorm, 357\nS\nSAFE (simple agreement for future equity), 364\nSageMaker (AWS), 374, 375\nSanders, Niek, 289\nsarcasm, 108\nscaling, 373\nscanned documents: text extraction from, 47\nscraping data, 40\nScrapy, 44, 244\nsearch, 239, 241-250\nfaceted, 310-312\nfocused, 309\nproduct, 309\nsearch engines, 5\ncase study, 249-250\ncomponents, 243-245\nin e-commerce, 309-312\nenterprise, 243\nexample build, 247-248\nfeatures that use NLP, 241\nfeedback, 247\ngeneric, 243\nmanaged services, 250\nquery processing and execution, 246\nranking, 247\nsearch results classification, 242\nsearchers, 245\nseed words or seed lexicons, 327\nself-attention, 25, 27\nsemantics, 12, 83\nSemantics3, 320\nsemi-supervised learning, 16\nsentence segmentation, 50\nsentiment analysis, 32, 122, 325-327\naspect-based, 122-123\naspect-level, 327-329\nfinancial, 360-361\nlexicon-based, 125\nwith social media data, 277, 288-290\nsupervised approach, 327\ntesting, 381\ntracking changes over time, 289\nunsupervised , 328-329\nsentiment analysis APIs, 126\nSentiment Analysis: Emotion in Text dataset\n(Figure Eight), 138\nsequence classification, 172\nsequence labeling, 172\nsequence-to-sequence models (seq2seq) , 225\nShakespeare, William, 161\nShap, 381\nShenfeld, Daniel, 398\nshopping (see e-commerce and retail)\nshort-term memory, long, 23\nshorthand, 46\nSiamese networks, 324\nsimple agreement for future equity (SAFE), 364\nsingle-document summarization, 256\nSinglish, 56\nsiren.io platform, 364\nSkipGram, 101-103\nslots or entities, 205, 217-218, 222-224\nsmall datasets, 29\nSMTD (see social media text data)\nsnippet extraction, 242\nSNIPS dataset, 220, 222\nSnips platform, 222\nSnorkel software, 42, 159\nSnow, C.P., 398\nsocial datasets, 354\nsocial good, 399\nsocial media, 5, 121, 275-302\n418 \n| \nIndex\n",
        "word_count": 334,
        "char_count": 2380,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 449,
        "text": "data generated in, 276\nexample posts, 279, 281\ninformation extraction from, 163\nsocial media text data (SMTD), 278\napplications that use, 277-278\nNLP for, 284-299\nnormalization of, 56\npre-processing, 290-294\nsentiment analysis with, 288-290\nspecial characters, 281\ntext representation for, 294-297\ntokenizers for, 286\nunique challenges, 278-284\nsoftware applications, 5-6\nsoftware deployment, 372-374\nSOTA (state-of-the-art) models, 396\nspaCy library, 175\nEntityRuler, 171, 176\nlemmatization with, 54\nNER with, 175\nPOS tagging, 57\npre-processing with, 57\nrule-based matcher, 63, 64\nsource code, 56\ntokenization with, 51\nSpanish, 74\nspecial characters, 281\nspeech recognition, 32, 203\nspeech representation, 82\nspeech synthesis, 32, 204\nspell checking (see spelling correction)\nspelling correction, 46-47, 242\nspelling, nonstandard, 74, 280, 293-294\nspelling- and grammar-correction tools, 6, 113\nspelling-correction libraries, 294\nsplit-joined words, 292\nSpoke corporate ticketing system (case study),\n152-155\nSpotDraft, 363\nSQuAD dataset, 269\nStack Overflow web pages: text extraction from,\n44\nstacking, 65, 66\nStanford Medicine, 351\nStanford Named Entity Recognizer (NER), 175\nStanford Natural Language Processing Group\nCoreNLP output, 58\nGloVe, 95, 295\nRegexNER, 171, 176\nSUTime, 186\nTokensRegex, 18, 63\nstate-of-the-art (SOTA) models, 396\nSteinhardt, Jacob, 31\nstemming, 53\nstop words, 52, 250\nstorage, 373\nsubjectivity, 290\nsubstitutes, 333, 334\nsubword embeddings, 136-138\nsuicide, 353\nsummarization (see text summarization)\nsummarization algorithms, 330\nSumo Logic, 383\nSumy library, 257\nsupervised learning, 15\nsupervised machine learning techniques, 19\nsupport vector machines (SVMs), 20, 21,\n132-133\nSUTime (Stanford NLP), 186\nSVMs (support vector machines), 20, 21,\n132-133\nSwahili, 74\nSwiftype, 250\nsynonym replacement, 40\nSynsets in Wordnet, 40\nsyntax, 11\nsynthetic data generation, 29\nT\nt-distributed Stochastic Neighboring Embed‐\nding (t-SNE), 108-110\nTaggedDocument class, 139\ntarget datasets, 389\ntask managers, 204\ntaxonomy, product, 317-321\nTeam Data Science Process (TDSP), 388,\n390-392\nteam personnel, 392-393\ntechnical debt minimization, 383-384\ntemplate filling, 165, 189-190\ntemplates-based response generation, 219\ntemporal IE and normalization, 186\ntemporal information extraction, 164, 186-187\nTensorBoard, 110, 111\nTensorFlow Extended, 381\nTensorFlow model analysis (TFMA), 380, 381\nTerm Frequency–Inverse Document Frequency\n(see TF-IDF)\nIndex \n| \n419\n",
        "word_count": 332,
        "char_count": 2480,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 450,
        "text": "term vector models, 84\n(see also vector space models)\nTesseract, 47\ntesting, 379, 381-382\nText Analysis Conference (NIST), 162\ntext categorization, 120\n(see also text classification)\ntext classification, 7, 119-156, 246\napplications, 121-123\nAWS pipeline, 374\ncase study, 152-155\nCNNs for, 143-144\ndefinition, 119\nDL for, 140-147\nwith document embeddings, 138-140\nexample scenario, 374-376\nwith existing APIs or libraries, 126, 153\ninterpreting models, 147-148\nKPIs for, 382\nwith large, pre-trained language models,\n145-147\nlogistic regression, 131\nLSTMs for, 144-145\nNaive Bayes classifier, 127-130\nwith neural embeddings, 134-140\nwith no or less data, 155\npipeline for building systems, 123-126\npractical advice, 155-156\nwith public datasets, 126-127, 153\nreasons for poor performance, 129-129\nsimple, 125-125\nwith SVMs, 132-133\ntext data, 283\ntext encoding, 45\ntext extraction and cleanup, 42-49\ntext normalization, 56, 246\ntext recommendations (see recommendations)\ntext representation, 60, 81-113\nbasic vectorization approaches, 85-92\ndistributed representations, 92-106\ndistributional, 93\nneural, 108\nfor SMTD, 294-297\nuniversal representations, 107-108\ntext summarization, 7, 240, 256-260\nabstractive, 256, 259\nexample setup, 257-258\nextractive, 256, 259\nmulti-document, 256\npractical advice, 258-260\nquery-focused, 256\nquery-independent, 256\nsingle-document, 256\nuse cases, 256-257\ntext-based chatbots, 204\ntext-generation tasks, 70-71\ntextacy, 167\nTextBlob toolkit, 290, 294\nTextEvaluator software (ETS), 112\nTextRank, 168, 258, 259\ntextual data, 260-263\nTF (term frequency), 90\nTF-IDF (Term Frequency–Inverse Document\nFrequency), 90-92, 241, 245, 246, 250, 294,\n347\nTF-IDF–based word replacement, 41\nTFMA (TensorFlow model analysis), 380, 381\nThakur, Abhishek, 388\ntitle match, 323-324\ntokenization\nlanguage-specific exceptions, 52\nsentence (see sentence segmentation)\nfor SMTD, 286, 295\nspecialized tokenizers, 286\ntweet, 51, 138\nword, 51-52\nTokensRegex (Stanford NLP), 18, 63\ntopic classification, 120\n(see also text classification)\ntopic detection, 120\ntrending topics, 277, 286-288\ntopic hierarchy, 335\ntopic modeling, 7, 15, 239, 250-256, 328\nalgorithms, 251, 328\nexample visualization, 251\ntraining models, 254-255\nuse cases, 255-256\ntopic vectors, 335\ntopics, 251\ntraining data, 15\ncrowdsourcing, 150\nno data, 149-150, 155\ntransfer learning, 26, 66, 107, 151-152\ntransformers, 25-26, 27\ntranslation (see machine translation)\ntranslation APIs, 264-265\ntranslation cache, 265\ntranslation memory, 265\n420 \n| \nIndex\n",
        "word_count": 339,
        "char_count": 2529,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 451,
        "text": "transliteration, 57, 280-281\ntrending topic detection, 277\ntrending topics, 277, 286-288\nTripAdvisor, 309\ntrolling, 299\ntroubleshooting, 379-382\ntruly intelligent machines, 399\nTsinghua University, 375\nTuring, Alan, 197\nTurkish, 74\nturnaround cycles, 396\nTurnitin, 6\nTweepy, 287-288\ntweet tokenization, 51\nTweetTokenizer, 138\n20 Newsgroups dataset, 153\nTwikenizer, 286\nTwitter, 41, 138, 275, 283\ncustomer support on, 297\nevent extraction from, 188\nexample posts, 279\nrate limits, 288\nsentiment analysis with, 288-290\nspecialized tokenizers for data from, 286\nTwitter Sentiment Corpus (Sanders), 289\ntwokenize, 286, 294, 295\nTwokenizer, 286\nU\nUber, 74-76\nUCI Machine Learning Repository, 126\nUCSF, 351\nUIUC, 349\nULMFit, 151\nUMLS (Unified Medical Language System) ,\n347\nUnicode characters, 45\nUnified Medical Language System (UMLS) ,\n347\nuniversal text representations, 107-108\nUniversity of Michigan Sentiment Analysis\ncompetition, 289\nunsupervised learning, 15\nupdating, 73, 376\nuppercasing, 53\nURLs: removal of, 293\nUS Navy Message Understanding Conferences,\n162\nuser reviews, 331\nuser weights, 330\nUzbek, 74\nV\nvector semantics, 94\nvector space models (VSMs), 84\nventure capital (VC) firms, 401\nversion control, 379\nVeterans Administration, 350\nvideo representation, 82\nvisual evaluation methods, 70\nvisualizing embeddings, 108-110\nvocabulary\never-evolving, 281-282, 296\nnew words, 281-282, 296\nsplit-joined words, 292\nvocabulary variation, 74\nvoice-based assistants, 5\ncase study, 31-33\nvoice-based chatbots, 204\n(see also chatbots)\nVSMs (vector space models), 84\nW\nWadhwani AI, 399\nWalmart, 113, 311\nweak supervision, 149, 153\nweather forecasting, 5\nweb services, 373, 375\nweb service–based models, 382\nWeizenbaum, Joseph, 197\nWhatsApp, 275\nWikipedia, 95, 179, 183, 301\nWinograd Schema Challenge, 13\nWinoGrande dataset, 399\nWittgenstein, Ludwig, 239\nWoebot, 342, 343, 354\nword clouds, 250, 284-285\nword embeddings, 94-103\npre-trained, 95-97\nfor SMTD, 296\ntext classification with, 134\ntraining, 98-103\nvisualizing, 109\nword tokenization, 51-52\nword vectors, 103\nWord2vec model (Google), 94-97, 102, 105,\n108, 295\npre-trained, 134\nIndex \n| \n421\n",
        "word_count": 301,
        "char_count": 2147,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 452,
        "text": "size, 375\ntraining, 103\nWordnet, 16\nWordNetLemmatizer, 54\nworld knowledge, 30\nY\nYouTube, 240\nZ\nZenkovich, Martin, 384\nZheng, Alice, 384\n422 \n| \nIndex\n",
        "word_count": 24,
        "char_count": 150,
        "fonts": [
          "MinionPro-Regular (9.0pt)",
          "MyriadPro-SemiboldCond (14.0pt)",
          "MyriadPro-SemiboldCond (9.0pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 453,
        "text": "About the Authors\nSowmya Vajjala has a PhD in computational linguistics from the University of\nTubingen, Germany. She currently works as a research officer at the National\nResearch Council, Canada’s largest federal research and development organization.\nHer past work experience spans both academia, as faculty at Iowa State University,\nUSA, and industry at Microsoft Research and The Globe and Mail.\nBodhisattwa Majumder is a doctoral candidate in NLP and ML at UC San Diego.\nEarlier he studied at IIT Kharagpur where he graduated summa cum laude. Previ‐\nously, he conducted ML research and built large-scale NLP systems, at Google AI\nResearch and Microsoft Research that went into products serving millions of users.\nCurrently, he is leading his university team in the Amazon Alexa Prize for 2019–\n2020.\nAnuj Gupta has built NLP and ML systems as a senior leader at Fortune 100 compa‐\nnies as well as startups. He has incubated and led multiple ML teams in his career. He\nstudied computer science at IIT Delhi and IIIT Hyderabad. He is currently the head\nof machine learning and data science at Vahan Inc. Above all, he is a father and a\nhusband.\nHarshit Surana is the CTO at DeepFlux Inc. He has built and scaled ML systems and\nengineering pipelines at several Silicon Valley startups as a founder and an advisor.\nHe studied computer science at Carnegie Mellon University where he worked with\nthe MIT Media Lab on common sense AI. His research in NLP has received over 200\ncitations.\nColophon\nThe animal on the cover of Practical Natural Language Processing is an eclectus parrot\n(Eclectus roratus). Native to the lowland rainforests of Oceania, they can be found\nanywhere from northeastern Australia to the islands that make up the Moluccas. For\ncenturies they have been domesticated in Indonesia and New Guinea, where their\nfeathers are used in elaborate headdresses used to communicate one’s standing or kin‐\nship to the birds.\nThe male’s plumage is bright green with touches of red and blue under the wings,\nwhile the female has a red crown and a purplish-blue chest. These birds are the most\nsexually dimorphic species in the parrot family, which led early biologists to classify\nthem as separate species. Another aspect that distinguishes the eclectus from other\nparrot species is their polygynandry. This allows the females to safely nest for up to\n11 months without often leaving, as they can depend on more than one male to for‐\nage for them.\n",
        "word_count": 410,
        "char_count": 2455,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)",
          "MyriadPro-SemiboldCond (15.8pt)",
          "MinionPro-Bold (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 454,
        "text": "Large populations of the eclectus remain. Many of the animals on O’Reilly covers are\nendangered; all of them are important to the world.\nThe cover illustration is by Karen Montgomery, based on a black and white engraving\nfrom Shaw’s Zoology. The cover fonts are Gilroy Semibold and Guardian Sans. The\ntext font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the\ncode font is Dalton Maag’s Ubuntu Mono.\n",
        "word_count": 72,
        "char_count": 424,
        "fonts": [
          "MinionPro-Regular (10.5pt)",
          "MinionPro-It (10.5pt)"
        ],
        "images": [],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      },
      {
        "page_number": 455,
        "text": "There’s much more  \nwhere this came from.\nExperience books, videos, live online  \ntraining courses, and more from O’Reilly  \nand our 200+ partners—all in one place.\nLearn more at oreilly.com/online-learning\n©2019 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175\n",
        "word_count": 44,
        "char_count": 299,
        "fonts": [
          "Gilroy-Light (17.0pt)",
          "Gilroy-SemiBold (28.0pt)",
          "Gilroy-Light (15.5pt)",
          "GuardianSans-Light (4.5pt)"
        ],
        "images": [
          {
            "index": 0,
            "width": 2144,
            "height": 1812,
            "ext": "jpeg",
            "size_bytes": 1212294
          }
        ],
        "bbox": [
          0.0,
          0.0,
          504.0,
          661.5
        ]
      }
    ]
  }
}